title,abstract,processed_abstract
"SVD Perspectives for Augmenting DeepONet Flexibility and
  Interpretability","  Deep operator networks (DeepONets) are powerful architectures for fast and
accurate emulation of complex dynamics. As their remarkable generalization
capabilities are primarily enabled by their projection-based attribute, we
investigate connections with low-rank techniques derived from the singular
value decomposition (SVD). We demonstrate that some of the concepts behind
proper orthogonal decomposition (POD)-neural networks can improve DeepONet's
design and training phases. These ideas lead us to a methodology extension that
we name SVD-DeepONet. Moreover, through multiple SVD analyses, we find that
DeepONet inherits from its projection-based attribute strong inefficiencies in
representing dynamics characterized by symmetries. Inspired by the work on
shifted-POD, we develop flexDeepONet, an architecture enhancement that relies
on a pre-transformation network for generating a moving reference frame and
isolating the rigid components of the dynamics. In this way, the physics can be
represented on a latent space free from rotations, translations, and stretches,
and an accurate projection can be performed to a low-dimensional basis. In
addition to flexibility and interpretability, the proposed perspectives
increase DeepONet's generalization capabilities and computational efficiencies.
For instance, we show flexDeepONet can accurately surrogate the dynamics of 19
variables in a combustion chemistry application by relying on 95% less
trainable parameters than the ones of the vanilla architecture. We argue that
DeepONet and SVD-based methods can reciprocally benefit from each other. In
particular, the flexibility of the former in leveraging multiple data sources
and multifidelity knowledge in the form of both unstructured data and
physics-informed constraints has the potential to greatly extend the
applicability of methodologies such as POD and PCA.
",deep operator network deeponet powerful architecture fast accurate emulation complex dynamic remarkable generalization capability primarily enable projection base attribute investigate connection low rank technique derive singular value decomposition svd demonstrate concept behind proper orthogonal decomposition pod -neural network improve deeponet design training phase idea lead we methodology extension name svd deeponet moreover multiple svd analysis find deeponet inherit projection base attribute strong inefficiency represent dynamic characterize symmetry inspire work shift pod develop flexdeeponet architecture enhancement rely pre transformation network generating moving reference frame isolate rigid component dynamic way physics represent latent space free rotation translation stretch accurate projection perform low dimensional basis addition flexibility interpretability propose perspective increase deeponet generalization capability computational efficiency instance show flexdeeponet accurately surrogate dynamic 19 variable combustion chemistry application rely 95 less trainable parameter one vanilla architecture argue deeponet svd base method reciprocally benefit particular flexibility former leverage multiple datum source multifidelity knowledge form unstructured data physics inform constraint potential greatly extend applicability methodology pod pca
"Towards robust audio spoofing detection: a detailed comparison of
  traditional and learned features","  Automatic speaker verification, like every other biometric system, is
vulnerable to spoofing attacks. Using only a few minutes of recorded voice of a
genuine client of a speaker verification system, attackers can develop a
variety of spoofing attacks that might trick such systems. Detecting these
attacks using the audio cues present in the recordings is an important
challenge. Most existing spoofing detection systems depend on knowing the used
spoofing technique. With this research, we aim at overcoming this limitation,
by examining robust audio features, both traditional and those learned through
an autoencoder, that are generalizable over different types of replay spoofing.
Furthermore, we provide a detailed account of all the steps necessary in
setting up state-of-the-art audio feature detection, pre-, and postprocessing,
such that the (non-audio expert) machine learning researcher can implement such
systems. Finally, we evaluate the performance of our robust replay speaker
detection system with a wide variety and different combinations of both
extracted and machine learned audio features on the `out in the wild' ASVspoof
2017 dataset. This dataset contains a variety of new spoofing configurations.
Since our focus is on examining which features will ensure robustness, we base
our system on a traditional Gaussian Mixture Model-Universal Background Model.
We then systematically investigate the relative contribution of each feature
set. The fused models, based on both the known audio features and the machine
learned features respectively, have a comparable performance with an Equal
Error Rate (EER) of 12. The final best performing model, which obtains an EER
of 10.8, is a hybrid model that contains both known and machine learned
features, thus revealing the importance of incorporating both types of features
when developing a robust spoofing prediction model.
",automatic speaker verification like every biometric system vulnerable spoofing attack use minute record voice genuine client speaker verification system attacker develop variety spoofing attack might trick system detect attack use audio cue present recording important challenge exist spoof detection system depend know use spoof technique research aim overcome limitation examine robust audio feature traditional learn autoencoder generalizable different type replay spoof furthermore provide detailed account step necessary set state of the art audio feature detection pre- postprocessing non audio expert machine learn researcher implement system finally evaluate performance robust replay speaker detection system wide variety different combination extract machine learn audio feature wild asvspoof 2017 dataset dataset contain variety new spoofing configuration since focus examine feature ensure robustness base system traditional gaussian mixture model universal background model systematically investigate relative contribution feature set fuse model base know audio feature machine learn feature respectively comparable performance equal error rate eer final good performing model obtain eer hybrid model contain know machine learn feature thus reveal importance incorporate type feature develop robust spoof prediction model
Guided Random Forest in the RRF Package,"  Random Forest (RF) is a powerful supervised learner and has been popularly
used in many applications such as bioinformatics.
  In this work we propose the guided random forest (GRF) for feature selection.
Similar to a feature selection method called guided regularized random forest
(GRRF), GRF is built using the importance scores from an ordinary RF. However,
the trees in GRRF are built sequentially, are highly correlated and do not
allow for parallel computing, while the trees in GRF are built independently
and can be implemented in parallel. Experiments on 10 high-dimensional gene
data sets show that, with a fixed parameter value (without tuning the
parameter), RF applied to features selected by GRF outperforms RF applied to
all features on 9 data sets and 7 of them have significant differences at the
0.05 level. Therefore, both accuracy and interpretability are significantly
improved. GRF selects more features than GRRF, however, leads to better
classification accuracy. Note in this work the guided random forest is guided
by the importance scores from an ordinary random forest, however, it can also
be guided by other methods such as human insights (by specifying $\lambda_i$).
GRF can be used in ""RRF"" v1.4 (and later versions), a package that also
includes the regularized random forest methods.
",random forest rf powerful supervise learner popularly use many application bioinformatics work propose guide random forest grf feature selection similar feature selection method call guide regularize random forest grrf grf build use importance score ordinary rf however tree grrf build sequentially highly correlate allow parallel computing tree grf build independently implement parallel experiment 10 high dimensional gene datum set show fix parameter value without tune parameter rf applied feature select grf outperform rf applied feature 9 datum set 7 significant difference level therefore accuracy interpretability significantly improve grf select feature grrf however lead well classification accuracy note work guide random forest guide importance score ordinary random forest however also guide method human insight specify grf use rrf late version package also includes regularize random forest method
Best Arm Identification in Generalized Linear Bandits,"  Motivated by drug design, we consider the best-arm identification problem in
generalized linear bandits. More specifically, we assume each arm has a vector
of covariates, there is an unknown vector of parameters that is common across
the arms, and a generalized linear model captures the dependence of rewards on
the covariate and parameter vectors. The problem is to minimize the number of
arm pulls required to identify an arm that is sufficiently close to optimal
with a sufficiently high probability. Building on recent progress in best-arm
identification for linear bandits (Xu et al. 2018), we propose the first
algorithm for best-arm identification for generalized linear bandits, provide
theoretical guarantees on its accuracy and sampling efficiency, and evaluate
its performance in various scenarios via simulation.
",motivated drug design consider good arm identification problem generalize linear bandit specifically assume arm vector covariate unknown vector parameter common across arm generalize linear model capture dependence reward covariate parameter vector problem minimize number arm pull require identify arm sufficiently close optimal sufficiently high probability build recent progress good arm identification linear bandit xu et al 2018 propose first algorithm good arm identification generalize linear bandit provide theoretical guarantee accuracy sampling efficiency evaluate performance various scenario via simulation
Conditional Affordance Learning for Driving in Urban Environments,"  Most existing approaches to autonomous driving fall into one of two
categories: modular pipelines, that build an extensive model of the
environment, and imitation learning approaches, that map images directly to
control outputs. A recently proposed third paradigm, direct perception, aims to
combine the advantages of both by using a neural network to learn appropriate
low-dimensional intermediate representations. However, existing direct
perception approaches are restricted to simple highway situations, lacking the
ability to navigate intersections, stop at traffic lights or respect speed
limits. In this work, we propose a direct perception approach which maps video
input to intermediate representations suitable for autonomous navigation in
complex urban environments given high-level directional inputs. Compared to
state-of-the-art reinforcement and conditional imitation learning approaches,
we achieve an improvement of up to 68 % in goal-directed navigation on the
challenging CARLA simulation benchmark. In addition, our approach is the first
to handle traffic lights and speed signs by using image-level labels only, as
well as smooth car-following, resulting in a significant reduction of traffic
accidents in simulation.
",exist approach autonomous driving fall one two category modular pipeline build extensive model environment imitation learn approach map image directly control output recently propose third paradigm direct perception aim combine advantage use neural network learn appropriate low dimensional intermediate representation however exist direct perception approach restrict simple highway situation lack ability navigate intersection stop traffic light respect speed limit work propose direct perception approach map video input intermediate representation suitable autonomous navigation complex urban environment give high level directional input compare state of the art reinforcement conditional imitation learning approach achieve improvement 68 goal direct navigation challenge carla simulation benchmark addition approach first handle traffic light speed sign use image level label well smooth car follow result significant reduction traffic accident simulation
"An Integrated Optimization and Machine Learning Models to Predict the
  Admission Status of Emergency Patients","  This work proposes a framework for optimizing machine learning algorithms.
The practicality of the framework is illustrated using an important case study
from the healthcare domain, which is predicting the admission status of
emergency department (ED) patients (e.g., admitted vs. discharged) using
patient data at the time of triage. The proposed framework can mitigate the
crowding problem by proactively planning the patient boarding process. A large
retrospective dataset of patient records is obtained from the electronic health
record database of all ED visits over three years from three major locations of
a healthcare provider in the Midwest of the US. Three machine learning
algorithms are proposed: T-XGB, T-ADAB, and T-MLP. T-XGB integrates extreme
gradient boosting (XGB) and Tabu Search (TS), T-ADAB integrates Adaboost and
TS, and T-MLP integrates multi-layer perceptron (MLP) and TS. The proposed
algorithms are compared with the traditional algorithms: XGB, ADAB, and MLP, in
which their parameters are tunned using grid search. The three proposed
algorithms and the original ones are trained and tested using nine data groups
that are obtained from different feature selection methods. In other words, 54
models are developed. Performance was evaluated using five measures: Area under
the curve (AUC), sensitivity, specificity, F1, and accuracy. The results show
that the newly proposed algorithms resulted in high AUC and outperformed the
traditional algorithms. The T-ADAB performs the best among the newly developed
algorithms. The AUC, sensitivity, specificity, F1, and accuracy of the best
model are 95.4%, 99.3%, 91.4%, 95.2%, 97.2%, respectively.
",work propose framework optimize machine learning algorithm practicality framework illustrate use important case study healthcare domain predict admission status emergency department ed patient admitted discharge use patient datum time triage propose framework mitigate crowding problem proactively plan patient boarding process large retrospective dataset patient record obtain electronic health record database ed visit three year three major location healthcare provider midw we three machine learning algorithm propose t xgb t adab t mlp t xgb integrate extreme gradient boost xgb tabu search ts t adab integrate adaboost ts t mlp integrate multi layer perceptron mlp ts propose algorithm compare traditional algorithm xgb adab mlp parameter tun use grid search three propose algorithm original one train test use nine datum group obtain different feature selection method word 54 model develop performance evaluate use five measure area curve auc sensitivity specificity f1 accuracy result show newly propose algorithm result high auc outperform traditional algorithm t adab perform good among newly develop algorithm auc sensitivity specificity f1 accuracy good model respectively
"Nash, Conley, and Computation: Impossibility and Incompleteness in Game
  Dynamics","  Under what conditions do the behaviors of players, who play a game
repeatedly, converge to a Nash equilibrium? If one assumes that the players'
behavior is a discrete-time or continuous-time rule whereby the current mixed
strategy profile is mapped to the next, this becomes a problem in the theory of
dynamical systems. We apply this theory, and in particular the concepts of
chain recurrence, attractors, and Conley index, to prove a general
impossibility result: there exist games for which any dynamics is bound to have
starting points that do not end up at a Nash equilibrium. We also prove a
stronger result for $\epsilon$-approximate Nash equilibria: there are games
such that no game dynamics can converge (in an appropriate sense) to
$\epsilon$-Nash equilibria, and in fact the set of such games has positive
measure. Further numerical results demonstrate that this holds for any
$\epsilon$ between zero and $0.09$. Our results establish that, although the
notions of Nash equilibria (and its computation-inspired approximations) are
universally applicable in all games, they are also fundamentally incomplete as
predictors of long term behavior, regardless of the choice of dynamics.
",condition behavior player play game repeatedly converge nash equilibrium one assume player behavior discrete time continuous time rule whereby current mixed strategy profile map next become problem theory dynamical system apply theory particular concept chain recurrence attractor conley index prove general impossibility result exist game dynamic bind starting point end nash equilibrium also prove strong result -approximate nash equilibria game game dynamic converge appropriate sense -nash equilibrium fact set game positive measure numerical result demonstrate hold zero result establish although notion nash equilibria computation inspire approximation universally applicable game also fundamentally incomplete predictor long term behavior regardless choice dynamic
"Multiclass Language Identification using Deep Learning on Spectral
  Images of Audio Signals","  The first step in any voice recognition software is to determine what
language a speaker is using, and ideally this process would be automated. The
technique described in this paper, language identification for audio
spectrograms (LIFAS), uses spectrograms generated from audio signals as inputs
to a convolutional neural network (CNN) to be used for language identification.
LIFAS requires minimal pre-processing on the audio signals as the spectrograms
are generated during each batch as they are input to the network during
training.
  LIFAS utilizes deep learning tools that are shown to be successful on image
processing tasks and applies it to audio signal classification. LIFAS performs
binary language classification with an accuracy of 97\%, and multi-class
classification with six languages at an accuracy of 89\% on 3.75 second audio
clips.
",first step voice recognition software determine language speaker use ideally process would automate technique describe paper language identification audio spectrogram lifas use spectrogram generate audio signal input convolutional neural network cnn use language identification lifas require minimal pre processing audio signal spectrogram generate batch input network training lifas utilize deep learning tool show successful image processing task apply audio signal classification lifas perform binary language classification accuracy multi class classification six language accuracy second audio clip
Tensorized Transformer for Dynamical Systems Modeling,"  The identification of nonlinear dynamics from observations is essential for
the alignment of the theoretical ideas and experimental data. The last, in
turn, is often corrupted by the side effects and noise of different natures, so
probabilistic approaches could give a more general picture of the process. At
the same time, high-dimensional probabilities modeling is a challenging and
data-intensive task. In this paper, we establish a parallel between the
dynamical systems modeling and language modeling tasks. We propose a
transformer-based model that incorporates geometrical properties of the data
and provide an iterative training algorithm allowing the fine-grid
approximation of the conditional probabilities of high-dimensional dynamical
systems.
",identification nonlinear dynamic observation essential alignment theoretical idea experimental datum last turn often corrupt side effect noise different nature probabilistic approach could give general picture process time high dimensional probability model challenge data intensive task paper establish parallel dynamical system model language modeling task propose transformer base model incorporate geometrical property datum provide iterative training algorithm allow fine grid approximation conditional probability high dimensional dynamical system
"Hardware Architecture Proposal for TEDA algorithm to Data Streaming
  Anomaly Detection","  The amount of data in real-time, such as time series and streaming data,
available today continues to grow. Being able to analyze this data the moment
it arrives can bring an immense added value. However, it also requires a lot of
computational effort and new acceleration techniques. As a possible solution to
this problem, this paper proposes a hardware architecture for Typicality and
Eccentricity Data Analytic (TEDA) algorithm implemented on Field Programmable
Gate Arrays (FPGA) for use in data streaming anomaly detection. TEDA is based
on a new approach to outlier detection in the data stream context. In order to
validate the proposals, results of the occupation and throughput of the
proposed hardware are presented. Besides, the bit accurate simulation results
are also presented. The project aims to Xilinx Virtex-6 xc6vlx240t-1ff1156 as
the target FPGA.
",amount datum real time time series streaming datum available today continue grow able analyze datum moment arrives bring immense add value however also require lot computational effort new acceleration technique possible solution problem paper propose hardware architecture typicality eccentricity datum analytic teda algorithm implement field programmable gate array fpga use datum streaming anomaly detection teda base new approach outlier detection datum stream context order validate proposal result occupation throughput propose hardware present besides bit accurate simulation result also present project aim xilinx virtex-6 xc6vlx240t-1ff1156 target fpga
Mode Reduction for Markov Jump Systems,"  Switched systems are capable of modeling processes with underlying dynamics
that may change abruptly over time. To achieve accurate modeling in practice,
one may need a large number of modes, but this may in turn increase the model
complexity drastically. Existing work on reducing system complexity mainly
considers state space reduction, yet reducing the number of modes is less
studied. In this work, we consider Markov jump linear systems (MJSs), a special
class of switched systems where the active mode switches according to a Markov
chain, and several issues associated with its mode complexity. Specifically,
inspired by clustering techniques from unsupervised learning, we are able to
construct a reduced MJS with fewer modes that approximates well the original
MJS under various metrics. Furthermore, both theoretically and empirically, we
show how one can use the reduced MJS to analyze stability and design
controllers with significant reduction in computational cost while achieving
guaranteed accuracy.
",switch system capable modeling process underlie dynamic may change abruptly time achieve accurate modeling practice one may need large number mode may turn increase model complexity drastically exist work reduce system complexity mainly consider state space reduction yet reduce number mode less study work consider markov jump linear system mjss special class switch system active mode switch accord markov chain several issue associate mode complexity specifically inspire cluster technique unsupervised learn able construct reduced mjs few mode approximate well original mj various metric furthermore theoretically empirically show one use reduced mjs analyze stability design controller significant reduction computational cost achieve guarantee accuracy
Rethinking Graph Regularization for Graph Neural Networks,"  The graph Laplacian regularization term is usually used in semi-supervised
representation learning to provide graph structure information for a model
$f(X)$. However, with the recent popularity of graph neural networks (GNNs),
directly encoding graph structure $A$ into a model, i.e., $f(A, X)$, has become
the more common approach. While we show that graph Laplacian regularization
brings little-to-no benefit to existing GNNs, and propose a simple but
non-trivial variant of graph Laplacian regularization, called
Propagation-regularization (P-reg), to boost the performance of existing GNN
models. We provide formal analyses to show that P-reg not only infuses extra
information (that is not captured by the traditional graph Laplacian
regularization) into GNNs, but also has the capacity equivalent to an
infinite-depth graph convolutional network. We demonstrate that P-reg can
effectively boost the performance of existing GNN models on both node-level and
graph-level tasks across many different datasets.
",graph laplacian regularization term usually use semi supervised representation learning provide graph structure information model f x however recent popularity graph neural network gnn directly encode graph structure model f x become common approach show graph laplacian regularization bring little to no benefit exist gnn propose simple non trivial variant graph laplacian regularization call propagation regularization p reg boost performance exist gnn model provide formal analysis show p reg infuse extra information capture traditional graph laplacian regularization gnn also capacity equivalent infinite depth graph convolutional network demonstrate p reg effectively boost performance exist gnn model node level graph level task across many different dataset
Chain of Thought Imitation with Procedure Cloning,"  Imitation learning aims to extract high-performance policies from logged
demonstrations of expert behavior. It is common to frame imitation learning as
a supervised learning problem in which one fits a function approximator to the
input-output mapping exhibited by the logged demonstrations (input observations
to output actions). While the framing of imitation learning as a supervised
input-output learning problem allows for applicability in a wide variety of
settings, it is also an overly simplistic view of the problem in situations
where the expert demonstrations provide much richer insight into expert
behavior. For example, applications such as path navigation, robot
manipulation, and strategy games acquire expert demonstrations via planning,
search, or some other multi-step algorithm, revealing not just the output
action to be imitated but also the procedure for how to determine this action.
While these intermediate computations may use tools not available to the agent
during inference (e.g., environment simulators), they are nevertheless
informative as a way to explain an expert's mapping of state to actions. To
properly leverage expert procedure information without relying on the
privileged tools the expert may have used to perform the procedure, we propose
procedure cloning, which applies supervised sequence prediction to imitate the
series of expert computations. This way, procedure cloning learns not only what
to do (i.e., the output action), but how and why to do it (i.e., the
procedure). Through empirical analysis on navigation, simulated robotic
manipulation, and game-playing environments, we show that imitating the
intermediate computations of an expert's behavior enables procedure cloning to
learn policies exhibiting significant generalization to unseen environment
configurations, including those configurations for which running the expert's
procedure directly is infeasible.
",imitation learning aim extract high performance policy log demonstration expert behavior common frame imitation learn supervise learning problem one fit function approximator input output mapping exhibit logged demonstration input observation output action frame imitation learn supervise input output learning problem allow applicability wide variety setting also overly simplistic view problem situation expert demonstration provide much rich insight expert behavior example application path navigation robot manipulation strategy game acquire expert demonstration via planning search multi step algorithm reveal output action imitate also procedure determine action intermediate computation may use tool available agent inference environment simulator nevertheless informative way explain expert mapping state action properly leverage expert procedure information without rely privileged tool expert may use perform procedure propose procedure cloning apply supervise sequence prediction imitate series expert computation way procedure cloning learn output action procedure empirical analysis navigation simulate robotic manipulation game play environment show imitate intermediate computation expert behavior enable procedure cloning learn policy exhibit significant generalization unseen environment configuration include configuration run expert procedure directly infeasible
VIB is Half Bayes,"  In discriminative settings such as regression and classification there are
two random variables at play, the inputs X and the targets Y. Here, we
demonstrate that the Variational Information Bottleneck can be viewed as a
compromise between fully empirical and fully Bayesian objectives, attempting to
minimize the risks due to finite sampling of Y only. We argue that this
approach provides some of the benefits of Bayes while requiring only some of
the work.
",discriminative setting regression classification two random variable play input x target demonstrate variational information bottleneck view compromise fully empirical fully bayesian objective attempt minimize risk due finite sampling argue approach provide benefit baye require work
"Deep Learning Based Multi-modal Addressee Recognition in Visual Scenes
  with Utterances","  With the widespread use of intelligent systems, such as smart speakers,
addressee recognition has become a concern in human-computer interaction, as
more and more people expect such systems to understand complicated social
scenes, including those outdoors, in cafeterias, and hospitals. Because
previous studies typically focused only on pre-specified tasks with limited
conversational situations such as controlling smart homes, we created a mock
dataset called Addressee Recognition in Visual Scenes with Utterances (ARVSU)
that contains a vast body of image variations in visual scenes with an
annotated utterance and a corresponding addressee for each scenario. We also
propose a multi-modal deep-learning-based model that takes different human
cues, specifically eye gazes and transcripts of an utterance corpus, into
account to predict the conversational addressee from a specific speaker's view
in various real-life conversational scenarios. To the best of our knowledge, we
are the first to introduce an end-to-end deep learning model that combines
vision and transcripts of utterance for addressee recognition. As a result, our
study suggests that future addressee recognition can reach the ability to
understand human intention in many social situations previously unexplored, and
our modality dataset is a first step in promoting research in this field.
",widespread use intelligent system smart speaker addressee recognition become concern human computer interaction people expect system understand complicated social scene include outdoors cafeterias hospital previous study typically focus pre specified task limit conversational situation control smart home create mock dataset call addressee recognition visual scene utterance arvsu contain vast body image variation visual scene annotate utterance corresponding addressee scenario also propose multi modal deep learn base model take different human cue specifically eye gaze transcript utterance corpus account predict conversational addressee specific speaker view various real life conversational scenario good knowledge first introduce end to end deep learning model combine vision transcript utterance addressee recognition result study suggest future addressee recognition reach ability understand human intention many social situation previously unexplored modality dataset first step promote research field
Expectation Learning for Adaptive Crossmodal Stimuli Association,"  The human brain is able to learn, generalize, and predict crossmodal stimuli.
Learning by expectation fine-tunes crossmodal processing at different levels,
thus enhancing our power of generalization and adaptation in highly dynamic
environments. In this paper, we propose a deep neural architecture trained by
using expectation learning accounting for unsupervised learning tasks. Our
learning model exhibits a self-adaptable behavior, setting the first steps
towards the development of deep learning architectures for crossmodal stimuli
association.
",human brain able learn generalize predict crossmodal stimulus learn expectation fine tune crossmodal process different level thus enhance power generalization adaptation highly dynamic environment paper propose deep neural architecture train use expectation learn accounting unsupervised learn task learn model exhibit self adaptable behavior set first step towards development deep learning architecture crossmodal stimuli association
MILP-based Imitation Learning for HVAC control,"  To optimize the operation of a HVAC system with advanced techniques such as
artificial neural network, previous studies usually need forecast information
in their method. However, the forecast information inevitably contains errors
all the time, which degrade the performance of the HVAC operation. Hence, in
this study, we propose MILP-based imitation learning method to control a HVAC
system without using the forecast information in order to reduce energy cost
and maintain thermal comfort at a given level. Our proposed controller is a
deep neural network (DNN) trained by using data labeled by a MILP solver with
historical data. After training, our controller is used to control the HVAC
system with real-time data. For comparison, we also develop a second method
named forecast-based MILP which control the HVAC system using the forecast
information. The performance of the two methods is verified by using real
outdoor temperatures and real day-ahead prices in Detroit city, Michigan,
United States. Numerical results clearly show that the performance of the
MILP-based imitation learning is better than that of the forecast-based MILP
method in terms of hourly power consumption, daily energy cost, and thermal
comfort. Moreover, the difference between results of the MILP-based imitation
learning method and optimal results is almost negligible. These optimal results
are achieved only by using the MILP solver at the end of a day when we have
full information on the weather and prices for the day.
",optimize operation hvac system advanced technique artificial neural network previous study usually need forecast information method however forecast information inevitably contain error time degrade performance hvac operation hence study propose milp base imitation learn method control hvac system without use forecast information order reduce energy cost maintain thermal comfort give level propose controller deep neural network dnn train use datum label milp solver historical datum training controller use control hvac system real time datum comparison also develop second method name forecast base milp control hvac system use forecast information performance two method verify use real outdoor temperature real day ahead price detroit city michigan united states numerical result clearly show performance milp base imitation learn well forecast base milp method term hourly power consumption daily energy cost thermal comfort moreover difference result milp base imitation learn method optimal result almost negligible optimal result achieve use milp solver end day full information weather price day
prDeep: Robust Phase Retrieval with a Flexible Deep Network,"  Phase retrieval algorithms have become an important component in many modern
computational imaging systems. For instance, in the context of ptychography and
speckle correlation imaging, they enable imaging past the diffraction limit and
through scattering media, respectively. Unfortunately, traditional phase
retrieval algorithms struggle in the presence of noise. Progress has been made
recently on more robust algorithms using signal priors, but at the expense of
limiting the range of supported measurement models (e.g., to Gaussian or coded
diffraction patterns). In this work we leverage the regularization-by-denoising
framework and a convolutional neural network denoiser to create prDeep, a new
phase retrieval algorithm that is both robust and broadly applicable. We test
and validate prDeep in simulation to demonstrate that it is robust to noise and
can handle a variety of system models.
  A MatConvNet implementation of prDeep is available at
https://github.com/ricedsp/prDeep.
",phase retrieval algorithm become important component many modern computational imaging system instance context ptychography speckle correlation imaging enable image past diffraction limit scatter medium respectively unfortunately traditional phase retrieval algorithm struggle presence noise progress make recently robust algorithm use signal prior expense limiting range support measurement model gaussian code diffraction pattern work leverage regularization by denoise framework convolutional neural network denoiser create prdeep new phase retrieval algorithm robust broadly applicable test validate prdeep simulation demonstrate robust noise handle variety system model matconvnet implementation prdeep available https
Prospection: Interpretable Plans From Language By Predicting the Future,"  High-level human instructions often correspond to behaviors with multiple
implicit steps. In order for robots to be useful in the real world, they must
be able to to reason over both motions and intermediate goals implied by human
instructions. In this work, we propose a framework for learning representations
that convert from a natural-language command to a sequence of intermediate
goals for execution on a robot. A key feature of this framework is prospection,
training an agent not just to correctly execute the prescribed command, but to
predict a horizon of consequences of an action before taking it. We demonstrate
the fidelity of plans generated by our framework when interpreting real,
crowd-sourced natural language commands for a robot in simulated scenes.
",high level human instruction often correspond behavior multiple implicit step order robot useful real world must able reason motion intermediate goal imply human instruction work propose framework learn representation convert natural language command sequence intermediate goal execution robot key feature framework prospection training agent correctly execute prescribe command predict horizon consequence action take demonstrate fidelity plan generate framework interpret real crowd source natural language command robot simulate scene
Efficient Tracking of Large Classes of Experts,"  In the framework of prediction of individual sequences, sequential prediction
methods are to be constructed that perform nearly as well as the best expert
from a given class. We consider prediction strategies that compete with the
class of switching strategies that can segment a given sequence into several
blocks, and follow the advice of a different ""base"" expert in each block. As
usual, the performance of the algorithm is measured by the regret defined as
the excess loss relative to the best switching strategy selected in hindsight
for the particular sequence to be predicted. In this paper we construct
prediction strategies of low computational cost for the case where the set of
base experts is large. In particular we provide a method that can transform any
prediction algorithm $\A$ that is designed for the base class into a tracking
algorithm. The resulting tracking algorithm can take advantage of the
prediction performance and potential computational efficiency of $\A$ in the
sense that it can be implemented with time and space complexity only
$O(n^{\gamma} \ln n)$ times larger than that of $\A$, where $n$ is the time
horizon and $\gamma \ge 0$ is a parameter of the algorithm. With $\A$ properly
chosen, our algorithm achieves a regret bound of optimal order for $\gamma>0$,
and only $O(\ln n)$ times larger than the optimal order for $\gamma=0$ for all
typical regret bound types we examined. For example, for predicting binary
sequences with switching parameters under the logarithmic loss, our method
achieves the optimal $O(\ln n)$ regret rate with time complexity
$O(n^{1+\gamma}\ln n)$ for any $\gamma\in (0,1)$.
",framework prediction individual sequence sequential prediction method construct perform nearly well good expert give class consider prediction strategy compete class switch strategy segment give sequence several block follow advice different base expert block usual performance algorithm measure regret define excess loss relative good switching strategy select hindsight particular sequence predict paper construct prediction strategy low computational cost case set base expert large particular provide method transform prediction algorithm design base class tracking algorithm result track algorithm take advantage prediction performance potential computational efficiency sense implement time space complexity n time large n time horizon 0 parameter algorithm properly choose algorithm achieve regret bind optimal order 0 n time large optimal order typical regret bind type examine example predict binary sequence switch parameter logarithmic loss method achieve optimal n regret rate time complexity n
"Risk Stratification of Lung Nodules Using 3D CNN-Based Multi-task
  Learning","  Risk stratification of lung nodules is a task of primary importance in lung
cancer diagnosis. Any improvement in robust and accurate nodule
characterization can assist in identifying cancer stage, prognosis, and
improving treatment planning. In this study, we propose a 3D Convolutional
Neural Network (CNN) based nodule characterization strategy. With a completely
3D approach, we utilize the volumetric information from a CT scan which would
be otherwise lost in the conventional 2D CNN based approaches. In order to
address the need for a large amount for training data for CNN, we resort to
transfer learning to obtain highly discriminative features. Moreover, we also
acquire the task dependent feature representation for six high-level nodule
attributes and fuse this complementary information via a Multi-task learning
(MTL) framework. Finally, we propose to incorporate potential disagreement
among radiologists while scoring different nodule attributes in a graph
regularized sparse multi-task learning. We evaluated our proposed approach on
one of the largest publicly available lung nodule datasets comprising 1018
scans and obtained state-of-the-art results in regressing the malignancy
scores.
",risk stratification lung nodules task primary importance lung cancer diagnosis improvement robust accurate nodule characterization assist identify cancer stage prognosis improve treatment planning study propose 3d convolutional neural network cnn base nodule characterization strategy completely 3d approach utilize volumetric information ct scan would otherwise lose conventional 2d cnn base approach order address need large amount training datum cnn resort transfer learning obtain highly discriminative feature moreover also acquire task dependent feature representation six high level nodule attribute fuse complementary information via multi task learn mtl framework finally propose incorporate potential disagreement among radiologist score different nodule attribute graph regularize sparse multi task learning evaluate propose approach one large publicly available lung nodule dataset comprise 1018 scan obtain state of the art result regress malignancy score
Surrogate Model For Field Optimization Using Beta-VAE Based Regression,"  Oilfield development related decisions are made using reservoir
simulation-based optimization study in which different production scenarios and
well controls are compared. Such simulations are computationally expensive and
so surrogate models are used to accelerate studies. Deep learning has been used
in past to generate surrogates, but such models often fail to quantify
prediction uncertainty and are not interpretable. In this work, beta-VAE based
regression is proposed to generate simulation surrogates for use in
optimization workflow. beta-VAE enables interpretable, factorized
representation of decision variables in latent space, which is then further
used for regression. Probabilistic dense layers are used to quantify prediction
uncertainty and enable approximate Bayesian inference. Surrogate model
developed using beta-VAE based regression finds interpretable and relevant
latent representation. A reasonable value of beta ensures a good balance
between factor disentanglement and reconstruction. Probabilistic dense layer
helps in quantifying predicted uncertainty for objective function, which is
then used to decide whether full-physics simulation is required for a case.
",oilfield development relate decision make use reservoir simulation base optimization study different production scenario well control compare simulation computationally expensive surrogate model use accelerate study deep learning use past generate surrogate model often fail quantify prediction uncertainty interpretable work beta vae base regression propose generate simulation surrogate use optimization workflow beta vae enable interpretable factorize representation decision variable latent space use regression probabilistic dense layer use quantify prediction uncertainty enable approximate bayesian inference surrogate model develop use beta vae base regression find interpretable relevant latent representation reasonable value beta ensure good balance factor disentanglement reconstruction probabilistic dense layer help quantify predict uncertainty objective function use decide whether full physics simulation require case
Learning to Initialize Gradient Descent Using Gradient Descent,"  Non-convex optimization problems are challenging to solve; the success and
computational expense of a gradient descent algorithm or variant depend heavily
on the initialization strategy. Often, either random initialization is used or
initialization rules are carefully designed by exploiting the nature of the
problem class. As a simple alternative to hand-crafted initialization rules, we
propose an approach for learning ""good"" initialization rules from previous
solutions. We provide theoretical guarantees that establish conditions that are
sufficient in all cases and also necessary in some under which our approach
performs better than random initialization. We apply our methodology to various
non-convex problems such as generating adversarial examples, generating post
hoc explanations for black-box machine learning models, and allocating
communication spectrum, and show consistent gains over other initialization
techniques.
",non convex optimization problem challenge solve success computational expense gradient descent algorithm variant depend heavily initialization strategy often either random initialization use initialization rule carefully design exploit nature problem class simple alternative hand craft initialization rule propose approach learn good initialization rule previous solution provide theoretical guarantee establish condition sufficient case also necessary approach perform well random initialization apply methodology various non convex problem generate adversarial example generate post hoc explanation black box machine learning model allocate communication spectrum show consistent gain initialization technique
Moral reinforcement learning using actual causation,"  Reinforcement learning systems will to a greater and greater extent make
decisions that significantly impact the well-being of humans, and it is
therefore essential that these systems make decisions that conform to our
expectations of morally good behavior. The morally good is often defined in
causal terms, as in whether one's actions have in fact caused a particular
outcome, and whether the outcome could have been anticipated. We propose an
online reinforcement learning method that learns a policy under the constraint
that the agent should not be the cause of harm. This is accomplished by
defining cause using the theory of actual causation and assigning blame to the
agent when its actions are the actual cause of an undesirable outcome. We
conduct experiments on a toy ethical dilemma in which a natural choice of
reward function leads to clearly undesirable behavior, but our method learns a
policy that avoids being the cause of harmful behavior, demonstrating the
soundness of our approach. Allowing an agent to learn while observing causal
moral distinctions such as blame, opens the possibility to learning policies
that better conform to our moral judgments.
",reinforcement learning system great great extent make decision significantly impact well be human therefore essential system make decision conform expectation morally good behavior morally good often define causal term whether one action fact cause particular outcome whether outcome could anticipate propose online reinforcement learning method learn policy constraint agent cause harm accomplish defining cause use theory actual causation assign blame agent action actual cause undesirable outcome conduct experiment toy ethical dilemma natural choice reward function lead clearly undesirable behavior method learn policy avoid cause harmful behavior demonstrate soundness approach allow agent learn observe causal moral distinction blame open possibility learn policy well conform moral judgment
"Deep Learning Approach for Predicting 30 Day Readmissions after Coronary
  Artery Bypass Graft Surgery","  Hospital Readmissions within 30 days after discharge following Coronary
Artery Bypass Graft (CABG) Surgery are substantial contributors to healthcare
costs. Many predictive models were developed to identify risk factors for
readmissions. However, majority of the existing models use statistical analysis
techniques with data available at discharge. We propose an ensembled model to
predict CABG readmissions using pre-discharge perioperative data and machine
learning survival analysis techniques. Firstly, we applied fifty one potential
readmission risk variables to Cox Proportional Hazard (CPH) survival regression
univariate analysis. Fourteen of them turned out to be significant (with p
value < 0.05), contributing to readmissions. Subsequently, we applied these 14
predictors to multivariate CPH model and Deep Learning Neural Network (NN)
representation of the CPH model, DeepSurv. We validated this new ensembled
model with 453 isolated adult CABG cases. Nine of the fourteen perioperative
risk variables were identified as the most significant with Hazard Ratios (HR)
of greater than 1.0. The concordance index metrics for CPH, DeepSurv, and
ensembled models were then evaluated with training and validation datasets. Our
ensembled model yielded promising results in terms of c-statistics, as we
raised the the number of iterations and data set sizes. 30 day all-cause
readmissions among isolated CABG patients can be predicted more effectively
with perioperative pre-discharge data, using machine learning survival analysis
techniques. Prediction accuracy levels could be improved further with deep
learning algorithms.
",hospital readmission within 30 day discharge follow coronary artery bypass graft cabg surgery substantial contributor healthcare cost many predictive model develop identify risk factor readmission however majority exist model use statistical analysis technique datum available discharge propose ensemble model predict cabg readmission use pre discharge perioperative datum machine learn survival analysis technique firstly apply fifty one potential readmission risk variable cox proportional hazard cph survival regression univariate analysis fourteen turn significant p value contribute readmission subsequently apply 14 predictor multivariate cph model deep learning neural network nn representation cph model deepsurv validate new ensemble model 453 isolate adult cabg case nine fourteen perioperative risk variable identify significant hazard ratio hr great concordance index metric cph deepsurv ensemble model evaluate train validation dataset ensemble model yield promising result term c statistic raise number iteration datum set size 30 day all cause readmission among isolated cabg patient predict effectively perioperative pre discharge datum use machine learn survival analysis technique prediction accuracy level could improve deep learning algorithm
"Autonomous Navigation via Deep Reinforcement Learning for Resource
  Constraint Edge Nodes using Transfer Learning","  Smart and agile drones are fast becoming ubiquitous at the edge of the cloud.
The usage of these drones are constrained by their limited power and compute
capability. In this paper, we present a Transfer Learning (TL) based approach
to reduce on-board computation required to train a deep neural network for
autonomous navigation via Deep Reinforcement Learning for a target algorithmic
performance. A library of 3D realistic meta-environments is manually designed
using Unreal Gaming Engine and the network is trained end-to-end. These trained
meta-weights are then used as initializers to the network in a test environment
and fine-tuned for the last few fully connected layers. Variation in drone
dynamics and environmental characteristics is carried out to show robustness of
the approach. Using NVIDIA GPU profiler it was shown that the energy
consumption and training latency is reduced by 3.7x and 1.8x respectively
without significant degradation in the performance in terms of average distance
traveled before crash i.e. Mean Safe Flight (MSF). The approach is also tested
on a real environment using DJI Tello drone and similar results were reported.
",smart agile drone fast become ubiquitous edge cloud usage drone constrain limited power compute capability paper present transfer learn tl base approach reduce on board computation require train deep neural network autonomous navigation via deep reinforcement learning target algorithmic performance library 3d realistic meta environment manually design use unreal gaming engine network train end to end train meta weight use initializer network test environment fine tuned last fully connect layer variation drone dynamic environmental characteristic carry show robustness approach use nvidia gpu profiler show energy consumption training latency reduce respectively without significant degradation performance term average distance travel crash mean safe flight msf approach also test real environment use dji tello drone similar result report
Tails of Lipschitz Triangular Flows,"  We investigate the ability of popular flow based methods to capture
tail-properties of a target density by studying the increasing triangular maps
used in these flow methods acting on a tractable source density. We show that
the density quantile functions of the source and target density provide a
precise characterization of the slope of transformation required to capture
tails in a target density. We further show that any Lipschitz-continuous
transport map acting on a source density will result in a density with similar
tail properties as the source, highlighting the trade-off between a complex
source density and a sufficiently expressive transformation to capture
desirable properties of a target density. Subsequently, we illustrate that flow
models like Real-NVP, MAF, and Glow as implemented originally lack the ability
to capture a distribution with non-Gaussian tails. We circumvent this problem
by proposing tail-adaptive flows consisting of a source distribution that can
be learned simultaneously with the triangular map to capture tail-properties of
a target density. We perform several synthetic and real-world experiments to
compliment our theoretical findings.
",investigate ability popular flow base method capture tail property target density study increase triangular map use flow method act tractable source density show density quantile function source target density provide precise characterization slope transformation require capture tail target density show lipschitz continuous transport map act source density result density similar tail property source highlight trade off complex source density sufficiently expressive transformation capture desirable property target density subsequently illustrate flow model like real nvp maf glow implement originally lack ability capture distribution non gaussian tail circumvent problem propose tail adaptive flow consist source distribution learn simultaneously triangular map capture tail property target density perform several synthetic real world experiment compliment theoretical finding
Embedding-based Silhouette Community Detection,"  Mining complex data in the form of networks is of increasing interest in many
scientific disciplines. Network communities correspond to densely connected
subnetworks, and often represent key functional parts of real-world systems. In
this work, we propose Silhouette Community Detection (SCD), an approach for
detecting communities, based on clustering of network node embeddings, i.e.
real valued representations of nodes derived from their neighborhoods. We
investigate the performance of the proposed SCD approach on 234 synthetic
networks, as well as on a real-life social network. Even though SCD is not
based on any form of modularity optimization, it performs comparably or better
than state-of-the-art community detection algorithms, such as the InfoMap and
Louvain algorithms. Further, we demonstrate how SCD's outputs can be used along
with domain ontologies in semantic subgroup discovery, yielding
human-understandable explanations of communities detected in a real-life
protein interaction network. Being embedding-based, SCD is widely applicable
and can be tested out-of-the-box as part of many existing network learning and
exploration pipelines.
",mining complex datum form network increase interest many scientific discipline network community correspond densely connect subnetwork often represent key functional part real world system work propose silhouette community detection scd approach detect community base cluster network node embedding real value representation node derive neighborhood investigate performance propose scd approach 234 synthetic network well real life social network even though scd base form modularity optimization perform comparably well state of the art community detection algorithm infomap louvain algorithm demonstrate scd output use along domain ontology semantic subgroup discovery yield human understandable explanation community detect real life protein interaction network embed base scd widely applicable test out of the box part many exist network learn exploration pipeline
A Latent Restoring Force Approach to Nonlinear System Identification,"  Identification of nonlinear dynamic systems remains a significant challenge
across engineering. This work suggests an approach based on Bayesian filtering
to extract and identify the contribution of an unknown nonlinear term in the
system which can be seen as an alternative viewpoint on restoring force surface
type approaches. To achieve this identification, the contribution which is the
nonlinear restoring force is modelled, initially, as a Gaussian process in
time. That Gaussian process is converted into a state-space model and combined
with the linear dynamic component of the system. Then, by inference of the
filtering and smoothing distributions, the internal states of the system and
the nonlinear restoring force can be extracted. In possession of these states a
nonlinear model can be constructed. The approach is demonstrated to be
effective in both a simulated case study and on an experimental benchmark
dataset.
",identification nonlinear dynamic system remain significant challenge across engineering work suggest approach base bayesian filtering extract identify contribution unknown nonlinear term system see alternative viewpoint restore force surface type approach achieve identification contribution nonlinear restore force model initially gaussian process time gaussian process convert state space model combine linear dynamic component system inference filtering smoothing distribution internal state system nonlinear restore force extract possession state nonlinear model construct approach demonstrate effective simulated case study experimental benchmark dataset
"Towards QD-suite: developing a set of benchmarks for Quality-Diversity
  algorithms","  While the field of Quality-Diversity (QD) has grown into a distinct branch of
stochastic optimization, a few problems, in particular locomotion and
navigation tasks, have become de facto standards. Are such benchmarks
sufficient? Are they representative of the key challenges faced by QD
algorithms? Do they provide the ability to focus on one particular challenge by
properly disentangling it from others? Do they have much predictive power in
terms of scalability and generalization? Existing benchmarks are not
standardized, and there is currently no MNIST equivalent for QD. Inspired by
recent works on Reinforcement Learning benchmarks, we argue that the
identification of challenges faced by QD methods and the development of
targeted, challenging, scalable but affordable benchmarks is an important step.
As an initial effort, we identify three problems that are challenging in sparse
reward settings, and propose associated benchmarks: (1) Behavior metric bias,
which can result from the use of metrics that do not match the structure of the
behavior space. (2) Behavioral Plateaus, with varying characteristics, such
that escaping them would require adaptive QD algorithms and (3) Evolvability
Traps, where small variations in genotype result in large behavioral changes.
The environments that we propose satisfy the properties listed above.
",field quality diversity qd grow distinct branch stochastic optimization problem particular locomotion navigation task become de facto standard benchmark sufficient representative key challenge face qd algorithm provide ability focus one particular challenge properly disentangle other much predictive power term scalability generalization exist benchmark standardize currently mnist equivalent qd inspire recent work reinforcement learning benchmark argue identification challenge face qd method development target challenge scalable affordable benchmark important step initial effort identify three problem challenge sparse reward setting propose associate benchmark 1 behavior metric bias result use metric match structure behavior space 2 behavioral plateaus vary characteristic escape would require adaptive qd algorithm 3 evolvability trap small variation genotype result large behavioral change environment propose satisfy property list
"Variation is the Norm: Brain State Dynamics Evoked By Emotional Video
  Clips","  For the last several decades, emotion research has attempted to identify a
""biomarker"" or consistent pattern of brain activity to characterize a single
category of emotion (e.g., fear) that will remain consistent across all
instances of that category, regardless of individual and context. In this
study, we investigated variation rather than consistency during emotional
experiences while people watched video clips chosen to evoke instances of
specific emotion categories. Specifically, we developed a sequential
probabilistic approach to model the temporal dynamics in a participant's brain
activity during video viewing. We characterized brain states during these clips
as distinct state occupancy periods between state transitions in blood oxygen
level dependent (BOLD) signal patterns. We found substantial variation in the
state occupancy probability distributions across individuals watching the same
video, supporting the hypothesis that when it comes to the brain correlates of
emotional experience, variation may indeed be the norm.
",last several decade emotion research attempt identify biomarker consistent pattern brain activity characterize single category emotion fear remain consistent across instance category regardless individual context study investigate variation rather consistency emotional experience people watch video clip choose evoke instance specific emotion category specifically develop sequential probabilistic approach model temporal dynamic participant brain activity video view characterized brain state clip distinct state occupancy period state transition blood oxygen level dependent bold signal pattern find substantial variation state occupancy probability distribution across individual watch video support hypothesis come brain correlate emotional experience variation may indeed norm
Microtubule Tracking in Electron Microscopy Volumes,"  We present a method for microtubule tracking in electron microscopy volumes.
Our method first identifies a sparse set of voxels that likely belong to
microtubules. Similar to prior work, we then enumerate potential edges between
these voxels, which we represent in a candidate graph. Tracks of microtubules
are found by selecting nodes and edges in the candidate graph by solving a
constrained optimization problem incorporating biological priors on microtubule
structure. For this, we present a novel integer linear programming formulation,
which results in speed-ups of three orders of magnitude and an increase of 53%
in accuracy compared to prior art (evaluated on three 1.2 x 4 x 4$\mu$m volumes
of Drosophila neural tissue). We also propose a scheme to solve the
optimization problem in a block-wise fashion, which allows distributed tracking
and is necessary to process very large electron microscopy volumes. Finally, we
release a benchmark dataset for microtubule tracking, here used for training,
testing and validation, consisting of eight 30 x 1000 x 1000 voxel blocks (1.2
x 4 x 4$\mu$m) of densely annotated microtubules in the CREMI data set
(https://github.com/nilsec/micron).
",present method microtubule tracking electron microscopy volume method first identifie sparse set voxel likely belong microtubule similar prior work enumerate potential edge voxel represent candidate graph track microtubule find select node edge candidate graph solve constrained optimization problem incorporate biological prior microtubule structure present novel integer linear programming formulation result speed up three order magnitude increase 53 accuracy compare prior art evaluate three x 4 x 4 volume drosophila neural tissue also propose scheme solve optimization problem block wise fashion allow distribute track necessary process large electron microscopy volume finally release benchmark dataset microtubule tracking use training testing validation consist eight 30 x 1000 x 1000 voxel block x 4 x 4 densely annotate microtubule cremi data set https
"Policing Chronic and Temporary Hot Spots of Violent Crime: A Controlled
  Field Experiment","  Hot-spot-based policing programs aim to deter crime through increased
proactive patrols at high-crime locations. While most hot spot programs target
easily identified chronic hot spots, we introduce models for predicting
temporary hot spots to address effectiveness and equity objectives for crime
prevention, and present findings from a crossover experiment evaluating
application of hot spot predictions to prevent serious violent crime in
Pittsburgh, PA. Over a 12-month experimental period, the Pittsburgh Bureau of
Police assigned uniformed patrol officers to weekly predicted chronic and
temporary hot spots of serious violent crimes comprising 0.5 percent of the
city's area. We find statistically and practically significant reductions in
serious violent crime counts within treatment hot spots as compared to control
hot spots, with an overall reduction of 25.3 percent in the FBI-classified Part
1 Violent (P1V) crimes of homicide, rape, robbery, and aggravated assault, and
a 39.7 percent reduction of African-American and other non-white victims of P1V
crimes. We find that temporary hot spots increase spatial dispersion of patrols
and have a greater percentage reduction in P1V crimes than chronic hot spots
but fewer total number of crimes prevented. Only foot patrols, not car patrols,
had statistically significant crime reductions in hot spots. We find no
evidence of crime displacement; instead, we find weakly statistically
significant spillover of crime prevention benefits to adjacent areas. In
addition, we find no evidence that the community-oriented hot spot patrols
produced over-policing arrests of minority or other populations.
",hot spot base policing program aim deter crime increase proactive patrol high crime location hot spot program target easily identify chronic hot spot introduce model predict temporary hot spot address effectiveness equity objective crime prevention present finding crossover experiment evaluate application hot spot prediction prevent serious violent crime pittsburgh 12 month experimental period pittsburgh bureau police assign uniformed patrol officer weekly predict chronic temporary hot spot serious violent crime comprise percent city area find statistically practically significant reduction serious violent crime count within treatment hot spot compare control hot spot overall reduction percent fbi classify part 1 violent p1v crime homicide rape robbery aggravate assault percent reduction african american non white victim p1v crime find temporary hot spot increase spatial dispersion patrol great percentage reduction p1v crime chronic hot spot few total number crime prevent foot patrol car patrol statistically significant crime reduction hot spot find evidence crime displacement instead find weakly statistically significant spillover crime prevention benefit adjacent area addition find evidence community orient hot spot patrol produce over police arrest minority population
A Hybrid Bandit Framework for Diversified Recommendation,"  The interactive recommender systems involve users in the recommendation
procedure by receiving timely user feedback to update the recommendation
policy. Therefore, they are widely used in real application scenarios. Previous
interactive recommendation methods primarily focus on learning users'
personalized preferences on the relevance properties of an item set. However,
the investigation of users' personalized preferences on the diversity
properties of an item set is usually ignored. To overcome this problem, we
propose the Linear Modular Dispersion Bandit (LMDB) framework, which is an
online learning setting for optimizing a combination of modular functions and
dispersion functions. Specifically, LMDB employs modular functions to model the
relevance properties of each item, and dispersion functions to describe the
diversity properties of an item set. Moreover, we also develop a learning
algorithm, called Linear Modular Dispersion Hybrid (LMDH) to solve the LMDB
problem and derive a gap-free bound on its n-step regret. Extensive experiments
on real datasets are performed to demonstrate the effectiveness of the proposed
LMDB framework in balancing the recommendation accuracy and diversity.
",interactive recommender system involve user recommendation procedure receive timely user feedback update recommendation policy therefore widely use real application scenario previous interactive recommendation method primarily focus learn user personalize preference relevance property item set however investigation user personalize preference diversity property item set usually ignore overcome problem propose linear modular dispersion bandit lmdb framework online learning set optimize combination modular function dispersion function specifically lmdb employ modular function model relevance property item dispersion function describe diversity property item set moreover also develop learn algorithm call linear modular dispersion hybrid lmdh solve lmdb problem derive gap free bind n step regret extensive experiment real dataset perform demonstrate effectiveness propose lmdb framework balance recommendation accuracy diversity
Observing and Recommending from a Social Web with Biases,"  The research question this report addresses is: how, and to what extent,
those directly involved with the design, development and employment of a
specific black box algorithm can be certain that it is not unlawfully
discriminating (directly and/or indirectly) against particular persons with
protected characteristics (e.g. gender, race and ethnicity)?
",research question report address extent directly involve design development employment specific black box algorithm certain unlawfully discriminate directly indirectly particular person protect characteristic gender race ethnicity
Exact-K Recommendation via Maximal Clique Optimization,"  This paper targets to a novel but practical recommendation problem named
exact-K recommendation. It is different from traditional top-K recommendation,
as it focuses more on (constrained) combinatorial optimization which will
optimize to recommend a whole set of K items called card, rather than ranking
optimization which assumes that ""better"" items should be put into top
positions. Thus we take the first step to give a formal problem definition, and
innovatively reduce it to Maximum Clique Optimization based on graph. To tackle
this specific combinatorial optimization problem which is NP-hard, we propose
Graph Attention Networks (GAttN) with a Multi-head Self-attention encoder and a
decoder with attention mechanism. It can end-to-end learn the joint
distribution of the K items and generate an optimal card rather than rank
individual items by prediction scores. Then we propose Reinforcement Learning
from Demonstrations (RLfD) which combines the advantages in behavior cloning
and reinforcement learning, making it sufficient- and-efficient to train the
model. Extensive experiments on three datasets demonstrate the effectiveness of
our proposed GAttN with RLfD method, it outperforms several strong baselines
with a relative improvement of 7.7% and 4.7% on average in Precision and Hit
Ratio respectively, and achieves state-of-the-art (SOTA) performance for the
exact-K recommendation problem.
",paper target novel practical recommendation problem name exact k recommendation different traditional top k recommendation focus constrain combinatorial optimization optimize recommend whole set k item call card rather rank optimization assume well item put top position thus take first step give formal problem definition innovatively reduce maximum clique optimization base graph tackle specific combinatorial optimization problem np hard propose graph attention network gattn multi head self attention encoder decoder attention mechanism end to end learn joint distribution k item generate optimal card rather rank individual item prediction score propose reinforcement learning demonstration rlfd combine advantage behavior clone reinforcement learning make sufficient- and efficient train model extensive experiment three dataset demonstrate effectiveness propose gattn rlfd method outperform several strong baseline relative improvement average precision hit ratio respectively achieve state of the art sota performance exact k recommendation problem
"Monitoring nonstationary processes based on recursive cointegration
  analysis and elastic weight consolidation","  This paper considers the problem of nonstationary process monitoring under
frequently varying operating conditions. Traditional approaches generally
misidentify the normal dynamic deviations as faults and thus lead to high false
alarms. Besides, they generally consider single relatively steady operating
condition and suffer from the catastrophic forgetting issue when learning
successive operating conditions. In this paper, recursive cointegration
analysis (RCA) is first proposed to distinguish the real faults from normal
systems changes, where the model is updated once a new normal sample arrives
and can adapt to slow change of cointegration relationship. Based on the
long-term equilibrium information extracted by RCA, the remaining short-term
dynamic information is monitored by recursive principal component analysis
(RPCA). Thus a comprehensive monitoring framework is built. When the system
enters a new operating condition, the RCA-RPCA model is rebuilt to deal with
the new condition. Meanwhile, elastic weight consolidation (EWC) is employed to
settle the `catastrophic forgetting' issue inherent in RPCA, where significant
information of influential parameters is enhanced to avoid the abrupt
performance degradation for similar modes. The effectiveness of the proposed
method is illustrated by a practical industrial system.
",paper consider problem nonstationary process monitor frequently vary operating condition traditional approach generally misidentify normal dynamic deviation fault thus lead high false alarm besides generally consider single relatively steady operating condition suffer catastrophic forgetting issue learn successive operating condition paper recursive cointegration analysis rca first propose distinguish real fault normal system change model update new normal sample arrives adapt slow change cointegration relationship base long term equilibrium information extract rca remain short term dynamic information monitor recursive principal component analysis rpca thus comprehensive monitoring framework build system enter new operating condition rca rpca model rebuild deal new condition meanwhile elastic weight consolidation ewc employ settle catastrophic forgetting issue inherent rpca significant information influential parameter enhance avoid abrupt performance degradation similar mode effectiveness propose method illustrate practical industrial system
"AutoKE: An automatic knowledge embedding framework for scientific
  machine learning","  Imposing physical constraints on neural networks as a method of knowledge
embedding has achieved great progress in solving physical problems described by
governing equations. However, for many engineering problems, governing
equations often have complex forms, including complex partial derivatives or
stochastic physical fields, which results in significant inconveniences from
the perspective of implementation. In this paper, a scientific machine learning
framework, called AutoKE, is proposed, and a reservoir flow problem is taken as
an instance to demonstrate that this framework can effectively automate the
process of embedding physical knowledge. In AutoKE, an emulator comprised of
deep neural networks (DNNs) is built for predicting the physical variables of
interest. An arbitrarily complex equation can be parsed and automatically
converted into a computational graph through the equation parser module, and
the fitness of the emulator to the governing equation is evaluated via
automatic differentiation. Furthermore, the fixed weights in the loss function
are substituted with adaptive weights by incorporating the Lagrangian dual
method. Neural architecture search (NAS) is also introduced into the AutoKE to
select an optimal network architecture of the emulator according to the
specific problem. Finally, we apply transfer learning to enhance the
scalability of the emulator. In experiments, the framework is verified by a
series of physical problems in which it can automatically embed physical
knowledge into an emulator without heavy hand-coding. The results demonstrate
that the emulator can not only make accurate predictions, but also be applied
to similar problems with high efficiency via transfer learning.
",impose physical constraint neural network method knowledge embed achieve great progress solve physical problem describe governing equation however many engineering problem govern equation often complex form include complex partial derivative stochastic physical field result significant inconvenience perspective implementation paper scientific machine learning framework call autoke propose reservoir flow problem take instance demonstrate framework effectively automate process embed physical knowledge autoke emulator comprise deep neural network dnn build predict physical variable interest arbitrarily complex equation parse automatically convert computational graph equation parser module fitness emulator governing equation evaluate via automatic differentiation furthermore fix weight loss function substitute adaptive weight incorporate lagrangian dual method neural architecture search nas also introduce autoke select optimal network architecture emulator accord specific problem finally apply transfer learning enhance scalability emulator experiment framework verify series physical problem automatically embe physical knowledge emulator without heavy hand code result demonstrate emulator make accurate prediction also apply similar problem high efficiency via transfer learning
"Toward smart composites: small-scale, untethered prediction and control
  for soft sensor/actuator systems","  We present a suite of algorithms and tools for model-predictive control of
sensor/actuator systems with embedded microcontroller units (MCU). These MCUs
can be colocated with sensors and actuators, thereby enabling a new class of
smart composites capable of autonomous behavior that does not require an
external computer. In this approach, kinematics are learned using a neural
network model from offline data and compiled into MCU code using nn4mc, an
open-source tool. Online Newton-Raphson optimization solves for the control
input. Shallow neural network models applied to 1D sensor signals allow for
reduced model sizes and increased control loop frequencies. We validate this
approach on a simulated mass-spring-damper system and two experimental setups
with different sensing, actuation, and computational hardware: a tendon-based
platform with embedded optical lace sensors and a HASEL-based platform with
magnetic sensors. Experimental results indicate effective high-bandwidth
tracking of reference paths (120 Hz and higher) with a small memory footprint
(less than or equal to 6.4% of available flash). The measured path following
error does not exceed 2 mm in the tendon-based platform, and the predicted path
following error does not exceed 1 mm in the HASEL-based platform. This
controller code's mean power consumption in an ARM Cortex-M4 computer is 45.4
mW. This control approach is also compatible with Tensorflow Lite models and
equivalent compilers. Embedded intelligence in composite materials enables a
new class of composites that infuse intelligence into structures and systems,
making them capable of responding to environmental stimuli using their
proprioception.
",present suite algorithm tool model predictive control system embed microcontroller unit mcu mcus colocate sensor actuator thereby enable new class smart composite capable autonomous behavior require external computer approach kinematic learn use neural network model offline datum compile mcu code use nn4mc open source tool online newton raphson optimization solve control input shallow neural network model apply 1d sensor signal allow reduced model size increase control loop frequency validate approach simulate mass spring damper system two experimental setup different sense actuation computational hardware tendon base platform embed optical lace sensor hasel base platform magnetic sensor experimental result indicate effective high bandwidth tracking reference path 120 hz high small memory footprint less equal available flash measure path follow error exceed 2 mm tendon base platform predict path follow error exceed 1 mm hasel base platform controller code mean power consumption arm cortex m4 computer mw control approach also compatible tensorflow lite model equivalent compiler embed intelligence composite material enable new class composite infuse intelligence structure system make capable respond environmental stimulus use proprioception
Deep Embedded Multi-View Clustering via Jointly Learning Latent Representations and Graphs,"With the representation learning capability of the deep learning models, deep
embedded multi-view clustering (MVC) achieves impressive performance in many
scenarios and has become increasingly popular in recent years. Although great
progress has been made in this field, most existing methods merely focus on
learning the latent representations and ignore that learning the latent graph
of nodes also provides available information for the clustering task. To
address this issue, in this paper we propose Deep Embedded Multi-view
Clustering via Jointly Learning Latent Representations and Graphs (DMVCJ),
which utilizes the latent graphs to promote the performance of deep embedded
MVC models from two aspects. Firstly, by learning the latent graphs and feature
representations jointly, the graph convolution network (GCN) technique becomes
available for our model. With the capability of GCN in exploiting the
information from both graphs and features, the clustering performance of our
model is significantly promoted. Secondly, based on the adjacency relations of
nodes shown in the latent graphs, we design a sample-weighting strategy to
alleviate the noisy issue, and further improve the effectiveness and robustness
of the model. Experimental results on different types of real-world multi-view
datasets demonstrate the effectiveness of DMVCJ.",representation learning capability deep learning model deep embed multi view cluster mvc achieve impressive performance many scenario become increasingly popular recent year although great progress make field exist method merely focus learn latent representation ignore learn latent graph node also provide available information cluster task address issue paper propose deep embed multi view cluster via jointly learn latent representation graph dmvcj utilize latent graph promote performance deep embed mvc model two aspect firstly learn latent graph feature representation jointly graph convolution network gcn technique become available model capability gcn exploit information graph feature clustering performance model significantly promote secondly base adjacency relation node show latent graph design sample weighting strategy alleviate noisy issue improve effectiveness robustness model experimental result different type real world multi view dataset demonstrate effectiveness dmvcj
"Feature Analysis for Assessing the Quality of Wikipedia Articles through
  Supervised Classification","  Nowadays, thanks to Web 2.0 technologies, people have the possibility to
generate and spread contents on different social media in a very easy way. In
this context, the evaluation of the quality of the information that is
available online is becoming more and more a crucial issue. In fact, a constant
flow of contents is generated every day by often unknown sources, which are not
certified by traditional authoritative entities. This requires the development
of appropriate methodologies that can evaluate in a systematic way these
contents, based on `objective' aspects connected with them. This would help
individuals, who nowadays tend to increasingly form their opinions based on
what they read online and on social media, to come into contact with
information that is actually useful and verified. Wikipedia is nowadays one of
the biggest online resources on which users rely as a source of information.
The amount of collaboratively generated content that is sent to the online
encyclopedia every day can let to the possible creation of low-quality articles
(and, consequently, misinformation) if not properly monitored and revised. For
this reason, in this paper, the problem of automatically assessing the quality
of Wikipedia articles is considered. In particular, the focus is on the
analysis of hand-crafted features that can be employed by supervised machine
learning techniques to perform the classification of Wikipedia articles on
qualitative bases. With respect to prior literature, a wider set of
characteristics connected to Wikipedia articles are taken into account and
illustrated in detail. Evaluations are performed by considering a labeled
dataset provided in a prior work, and different supervised machine learning
algorithms, which produced encouraging results with respect to the considered
features.
",nowadays thank web technology people possibility generate spread content different social medium easy way context evaluation quality information available online become crucial issue fact constant flow content generate every day often unknown source certify traditional authoritative entity require development appropriate methodology evaluate systematic way content base objective aspect connect would help individual nowadays tend increasingly form opinion base read online social medium come contact information actually useful verify wikipedia nowadays one big online resource user rely source information amount collaboratively generate content send online encyclopedia every day let possible creation low quality article consequently misinformation properly monitor revise reason paper problem automatically assess quality wikipedia article consider particular focus analysis hand craft feature employ supervise machine learn technique perform classification wikipedia article qualitative basis respect prior literature wider set characteristic connect wikipedia article take account illustrate detail evaluation perform consider label dataset provide prior work different supervise machine learning algorithm produce encourage result respect consider feature
Adaptive and Scalable Android Malware Detection through Online Learning,"  It is well-known that malware constantly evolves so as to evade detection and
this causes the entire malware population to be non-stationary. Contrary to
this fact, prior works on machine learning based Android malware detection have
assumed that the distribution of the observed malware characteristics (i.e.,
features) do not change over time. In this work, we address the problem of
malware population drift and propose a novel online machine learning based
framework, named DroidOL to handle it and effectively detect malware. In order
to perform accurate detection, security-sensitive behaviors are captured from
apps in the form of inter-procedural control-flow sub-graph features using a
state-of-the-art graph kernel. In order to perform scalable detection and to
adapt to the drift and evolution in malware population, an online
passive-aggressive classifier is used.
  In a large-scale comparative analysis with more than 87,000 apps, DroidOL
achieves 84.29% accuracy outperforming two state-of-the-art malware techniques
by more than 20% in their typical batch learning setting and more than 3% when
they are continuously re-trained. Our experimental findings strongly indicate
that online learning based approaches are highly suitable for real-world
malware detection.
",well know malware constantly evolve evade detection cause entire malware population non stationary contrary fact prior work machine learning base android malware detection assume distribution observe malware characteristic feature change time work address problem malware population drift propose novel online machine learn base framework name droidol handle effectively detect malware order perform accurate detection security sensitive behavior capture app form inter procedural control flow sub graph feature use state of the art graph kernel order perform scalable detection adapt drift evolution malware population online passive aggressive classifier use large scale comparative analysis app droidol achieve accuracy outperform two state of the art malware technique 20 typical batch learning set 3 continuously re train experimental finding strongly indicate online learning base approach highly suitable real world malware detection
"Predictive State Representations: A New Theory for Modeling Dynamical
  Systems","  Modeling dynamical systems, both for control purposes and to make predictions
about their behavior, is ubiquitous in science and engineering. Predictive
state representations (PSRs) are a recently introduced class of models for
discrete-time dynamical systems. The key idea behind PSRs and the closely
related OOMs (Jaeger's observable operator models) is to represent the state of
the system as a set of predictions of observable outcomes of experiments one
can do in the system. This makes PSRs rather different from history-based
models such as nth-order Markov models and hidden-state-based models such as
HMMs and POMDPs. We introduce an interesting construct, the systemdynamics
matrix, and show how PSRs can be derived simply from it. We also use this
construct to show formally that PSRs are more general than both nth-order
Markov models and HMMs/POMDPs. Finally, we discuss the main difference between
PSRs and OOMs and conclude with directions for future work.
",model dynamical system control purpose make prediction behavior ubiquitous science engineering predictive state representation psrs recently introduce class model discrete time dynamical system key idea behind psrs closely relate oom jaeger observable operator model represent state system set prediction observable outcome experiment one system make psrs rather different history base model nth order markov model hide state base model hmms pomdp introduce interesting construct systemdynamic matrix show psrs derive simply also use construct show formally psrs general nth order markov model finally discuss main difference psrs oom conclude direction future work
"Interpretable Image Classification with Differentiable Prototypes
  Assignment","  We introduce ProtoPool, an interpretable image classification model with a
pool of prototypes shared by the classes. The training is more straightforward
than in the existing methods because it does not require the pruning stage. It
is obtained by introducing a fully differentiable assignment of prototypes to
particular classes. Moreover, we introduce a novel focal similarity function to
focus the model on the rare foreground features. We show that ProtoPool obtains
state-of-the-art accuracy on the CUB-200-2011 and the Stanford Cars datasets,
substantially reducing the number of prototypes. We provide a theoretical
analysis of the method and a user study to show that our prototypes are more
distinctive than those obtained with competitive methods.
",introduce protopool interpretable image classification model pool prototype share class train straightforward exist method require pruning stage obtain introduce fully differentiable assignment prototype particular class moreover introduce novel focal similarity function focus model rare foreground feature show protopool obtain state of the art accuracy cub-200 2011 stanford car dataset substantially reduce number prototype provide theoretical analysis method user study show prototype distinctive obtain competitive method
"Timeline: A Dynamic Hierarchical Dirichlet Process Model for Recovering
  Birth/Death and Evolution of Topics in Text Stream","  Topic models have proven to be a useful tool for discovering latent
structures in document collections. However, most document collections often
come as temporal streams and thus several aspects of the latent structure such
as the number of topics, the topics' distribution and popularity are
time-evolving. Several models exist that model the evolution of some but not
all of the above aspects. In this paper we introduce infinite dynamic topic
models, iDTM, that can accommodate the evolution of all the aforementioned
aspects. Our model assumes that documents are organized into epochs, where the
documents within each epoch are exchangeable but the order between the
documents is maintained across epochs. iDTM allows for unbounded number of
topics: topics can die or be born at any epoch, and the representation of each
topic can evolve according to a Markovian dynamics. We use iDTM to analyze the
birth and evolution of topics in the NIPS community and evaluated the efficacy
of our model on both simulated and real datasets with favorable outcome.
",topic model prove useful tool discover latent structure document collection however document collection often come temporal stream thus several aspect latent structure number topic topic distribution popularity time evolve several model exist model evolution aspect paper introduce infinite dynamic topic model idtm accommodate evolution aforementioned aspect model assume document organize epoch document within epoch exchangeable order document maintain across epoch idtm allow unbounded number topic topic die bear epoch representation topic evolve accord markovian dynamic use idtm analyze birth evolution topic nip community evaluate efficacy model simulate real dataset favorable outcome
"Question Generation for Reading Comprehension Assessment by Modeling How
  and What to Ask","  Reading is integral to everyday life, and yet learning to read is a struggle
for many young learners. During lessons, teachers can use comprehension
questions to increase engagement, test reading skills, and improve retention.
Historically such questions were written by skilled teachers, but recently
language models have been used to generate comprehension questions. However,
many existing Question Generation (QG) systems focus on generating literal
questions from the text, and have no way to control the type of the generated
question. In this paper, we study QG for reading comprehension where
inferential questions are critical and extractive techniques cannot be used. We
propose a two-step model (HTA-WTA) that takes advantage of previous datasets,
and can generate questions for a specific targeted comprehension skill. We
propose a new reading comprehension dataset that contains questions annotated
with story-based reading comprehension skills (SBRCS), allowing for a more
complete reader assessment. Across several experiments, our results show that
HTA-WTA outperforms multiple strong baselines on this new dataset. We show that
the HTA-WTA model tests for strong SCRS by asking deep inferential questions.
",read integral everyday life yet learn read struggle many young learner lesson teacher use comprehension question increase engagement test reading skill improve retention historically question write skilled teacher recently language model use generate comprehension question however many exist question generation qg systems focus generate literal question text way control type generate question paper study qg reading comprehension inferential question critical extractive technique use propose two step model hta wta take advantage previous dataset generate question specific target comprehension skill propose new reading comprehension dataset contain question annotate story base reading comprehension skill sbrc allow complete reader assessment across several experiment result show hta wta outperform multiple strong baseline new dataset show hta wta model test strong scr ask deep inferential question
Learning Network Structures from Contagion,"  In 2014, Amin, Heidari, and Kearns proved that tree networks can be learned
by observing only the infected set of vertices of the contagion process under
the independent cascade model, in both the active and passive query models.
They also showed empirically that simple extensions of their algorithms work on
sparse networks. In this work, we focus on the active model. We prove that a
simple modification of Amin et al.'s algorithm works on more general classes of
networks, namely (i) networks with large girth and low path growth rate, and
(ii) networks with bounded degree. This also provides partial theoretical
explanation for Amin et al.'s experiments on sparse networks.
",2014 amin heidari kearn prove tree network learn observe infected set vertex contagion process independent cascade model active passive query model also show empirically simple extension algorithm work sparse network work focus active model prove simple modification amin et al algorithm work general class network namely network large girth low path growth rate ii network bound degree also provide partial theoretical explanation amin et al experiment sparse network
"Understanding and Improving Proximity Graph based Maximum Inner Product
  Search","  The inner-product navigable small world graph (ip-NSW) represents the
state-of-the-art method for approximate maximum inner product search (MIPS) and
it can achieve an order of magnitude speedup over the fastest baseline.
However, to date it is still unclear where its exceptional performance comes
from. In this paper, we show that there is a strong norm bias in the MIPS
problem, which means that the large norm items are very likely to become the
result of MIPS. Then we explain the good performance of ip-NSW as matching the
norm bias of the MIPS problem - large norm items have big in-degrees in the
ip-NSW proximity graph and a walk on the graph spends the majority of
computation on these items, thus effectively avoids unnecessary computation on
small norm items. Furthermore, we propose the ip-NSW+ algorithm, which improves
ip-NSW by introducing an additional angular proximity graph. Search is first
conducted on the angular graph to find the angular neighbors of a query and
then the MIPS neighbors of these angular neighbors are used to initialize the
candidate pool for search on the inner-product proximity graph. Experiment
results show that ip-NSW+ consistently and significantly outperforms ip-NSW and
provides more robust performance under different data distributions.
",inner product navigable small world graph ip nsw represent state of the art method approximate maximum inner product search mip achieve order magnitude speedup fastest baseline however date still unclear exceptional performance come paper show strong norm bias mips problem mean large norm item likely become result mips explain good performance ip nsw matching norm bias mips problem large norm item big in degree ip nsw proximity graph walk graph spend majority computation item thus effectively avoid unnecessary computation small norm item furthermore propose algorithm improve ip nsw introduce additional angular proximity graph search first conduct angular graph find angular neighbor query mips neighbors angular neighbor use initialize candidate pool search inner product proximity graph experiment result show consistently significantly outperform ip nsw provide robust performance different data distribution
"Deep Mixture Point Processes: Spatio-temporal Event Prediction with Rich
  Contextual Information","  Predicting when and where events will occur in cities, like taxi pick-ups,
crimes, and vehicle collisions, is a challenging and important problem with
many applications in fields such as urban planning, transportation optimization
and location-based marketing. Though many point processes have been proposed to
model events in a continuous spatio-temporal space, none of them allow for the
consideration of the rich contextual factors that affect event occurrence, such
as weather, social activities, geographical characteristics, and traffic. In
this paper, we propose \textsf{DMPP} (Deep Mixture Point Processes), a point
process model for predicting spatio-temporal events with the use of rich
contextual information; a key advance is its incorporation of the heterogeneous
and high-dimensional context available in image and text data. Specifically, we
design the intensity of our point process model as a mixture of kernels, where
the mixture weights are modeled by a deep neural network. This formulation
allows us to automatically learn the complex nonlinear effects of the
contextual factors on event occurrence. At the same time, this formulation
makes analytical integration over the intensity, which is required for point
process estimation, tractable. We use real-world data sets from different
domains to demonstrate that DMPP has better predictive performance than
existing methods.
",predict event occur city like taxi pick up crime vehicle collision challenge important problem many application field urban planning transportation optimization location base marketing though many point process propose model event continuous spatio temporal space none allow consideration rich contextual factor affect event occurrence weather social activity geographical characteristic traffic paper propose dmpp deep mixture point process point process model predict spatio temporal event use rich contextual information key advance incorporation heterogeneous high dimensional context available image text datum specifically design intensity point process model mixture kernel mixture weight model deep neural network formulation allow we automatically learn complex nonlinear effect contextual factor event occurrence time formulation make analytical integration intensity require point process estimation tractable use real world datum set different domain demonstrate dmpp well predictive performance exist method
"Biased Gradient Estimate with Drastic Variance Reduction for Meta
  Reinforcement Learning","  Despite the empirical success of meta reinforcement learning (meta-RL), there
are still a number poorly-understood discrepancies between theory and practice.
Critically, biased gradient estimates are almost always implemented in
practice, whereas prior theory on meta-RL only establishes convergence under
unbiased gradient estimates. In this work, we investigate such a discrepancy.
In particular, (1) We show that unbiased gradient estimates have variance
$\Theta(N)$ which linearly depends on the sample size $N$ of the inner loop
updates; (2) We propose linearized score function (LSF) gradient estimates,
which have bias $\mathcal{O}(1/\sqrt{N})$ and variance $\mathcal{O}(1/N)$; (3)
We show that most empirical prior work in fact implements variants of the LSF
gradient estimates. This implies that practical algorithms ""accidentally""
introduce bias to achieve better performance; (4) We establish theoretical
guarantees for the LSF gradient estimates in meta-RL regarding its convergence
to stationary points, showing better dependency on $N$ than prior work when $N$
is large.
",despite empirical success meta reinforcement learn meta rl still number poorly understand discrepancy theory practice critically biased gradient estimate almost always implement practice whereas prior theory meta rl establish convergence unbiased gradient estimate work investigate discrepancy particular 1 show unbiased gradient estimate variance n linearly depend sample size n inner loop update 2 propose linearize score function lsf gradient estimate bias n variance 3 show empirical prior work fact implement variants lsf gradient estimate imply practical algorithm accidentally introduce bias achieve well performance 4 establish theoretical guarantee lsf gradient estimate meta rl regard convergence stationary point show well dependency n prior work n large
"WHInter: A Working set algorithm for High-dimensional sparse second
  order Interaction models","  Learning sparse linear models with two-way interactions is desirable in many
application domains such as genomics. l1-regularised linear models are popular
to estimate sparse models, yet standard implementations fail to address
specifically the quadratic explosion of candidate two-way interactions in high
dimensions, and typically do not scale to genetic data with hundreds of
thousands of features. Here we present WHInter, a working set algorithm to
solve large l1-regularised problems with two-way interactions for binary design
matrices. The novelty of WHInter stems from a new bound to efficiently identify
working sets while avoiding to scan all features, and on fast computations
inspired from solutions to the maximum inner product search problem. We apply
WHInter to simulated and real genetic data and show that it is more scalable
and two orders of magnitude faster than the state of the art.
",learn sparse linear model two way interaction desirable many application domain genomic l1 regularise linear model popular estimate sparse model yet standard implementation fail address specifically quadratic explosion candidate two way interaction high dimension typically scale genetic datum hundred thousand feature present whinter working set algorithm solve large l1 regularise problem two way interaction binary design matrix novelty whinter stem new bind efficiently identify working set avoid scan feature fast computation inspire solution maximum inner product search problem apply whinter simulate real genetic datum show scalable two order magnitude fast state art
"The Use of Machine Learning Algorithms in Recommender Systems: A
  Systematic Review","  Recommender systems use algorithms to provide users with product or service
recommendations. Recently, these systems have been using machine learning
algorithms from the field of artificial intelligence. However, choosing a
suitable machine learning algorithm for a recommender system is difficult
because of the number of algorithms described in the literature. Researchers
and practitioners developing recommender systems are left with little
information about the current approaches in algorithm usage. Moreover, the
development of a recommender system using a machine learning algorithm often
has problems and open questions that must be evaluated, so software engineers
know where to focus research efforts. This paper presents a systematic review
of the literature that analyzes the use of machine learning algorithms in
recommender systems and identifies research opportunities for software
engineering research. The study concludes that Bayesian and decision tree
algorithms are widely used in recommender systems because of their relative
simplicity, and that requirement and design phases of recommender system
development appear to offer opportunities for further research.
",recommender system use algorithm provide user product service recommendation recently system use machine learning algorithm field artificial intelligence however choose suitable machine learn algorithm recommender system difficult number algorithm describe literature researcher practitioner develop recommender system leave little information current approach algorithm usage moreover development recommender system use machine learn algorithm often problem open question must evaluate software engineer know focus research effort paper present systematic review literature analyze use machine learning algorithm recommender system identify research opportunity software engineering research study conclude bayesian decision tree algorithm widely use recommender system relative simplicity requirement design phase recommender system development appear offer opportunity research
On Linear Convergence of Policy Gradient Methods for Finite MDPs,"  We revisit the finite time analysis of policy gradient methods in the one of
the simplest settings: finite state and action MDPs with a policy class
consisting of all stochastic policies and with exact gradient evaluations.
There has been some recent work viewing this setting as an instance of smooth
non-linear optimization problems and showing sub-linear convergence rates with
small step-sizes. Here, we take a different perspective based on connections
with policy iteration and show that many variants of policy gradient methods
succeed with large step-sizes and attain a linear rate of convergence.
",revisit finite time analysis policy gradient method one simple setting finite state action mdps policy class consist stochastic policy exact gradient evaluation recent work view set instance smooth non linear optimization problem show sub linear convergence rate small step size take different perspective base connection policy iteration show many variant policy gradient method succeed large step size attain linear rate convergence
"Continual Learning with Gated Incremental Memories for sequential data
  processing","  The ability to learn in dynamic, nonstationary environments without
forgetting previous knowledge, also known as Continual Learning (CL), is a key
enabler for scalable and trustworthy deployments of adaptive solutions. While
the importance of continual learning is largely acknowledged in machine vision
and reinforcement learning problems, this is mostly under-documented for
sequence processing tasks. This work proposes a Recurrent Neural Network (RNN)
model for CL that is able to deal with concept drift in input distribution
without forgetting previously acquired knowledge. We also implement and test a
popular CL approach, Elastic Weight Consolidation (EWC), on top of two
different types of RNNs. Finally, we compare the performances of our enhanced
architecture against EWC and RNNs on a set of standard CL benchmarks, adapted
to the sequential data processing scenario. Results show the superior
performance of our architecture and highlight the need for special solutions
designed to address CL in RNNs.
",ability learn dynamic nonstationary environment without forget previous knowledge also know continual learning cl key enabler scalable trustworthy deployment adaptive solution importance continual learning largely acknowledge machine vision reinforcement learning problem mostly under document sequence processing task work propose recurrent neural network rnn model cl able deal concept drift input distribution without forget previously acquire knowledge also implement test popular cl approach elastic weight consolidation ewc top two different type rnns finally compare performance enhance architecture ewc rnns set standard cl benchmark adapt sequential datum processing scenario result show superior performance architecture highlight need special solution design address cl rnns
"A Transferable Adaptive Domain Adversarial Neural Network for Virtual
  Reality Augmented EMG-Based Gesture Recognition","  Within the field of electromyography-based (EMG) gesture recognition,
disparities exist between the offline accuracy reported in the literature and
the real-time usability of a classifier. This gap mainly stems from two
factors: 1) The absence of a controller, making the data collected dissimilar
to actual control. 2) The difficulty of including the four main dynamic factors
(gesture intensity, limb position, electrode shift, and transient changes in
the signal), as including their permutations drastically increases the amount
of data to be recorded. Contrarily, online datasets are limited to the exact
EMG-based controller used to record them, necessitating the recording of a new
dataset for each control method or variant to be tested. Consequently, this
paper proposes a new type of dataset to serve as an intermediate between
offline and online datasets, by recording the data using a real-time
experimental protocol. The protocol, performed in virtual reality, includes the
four main dynamic factors and uses an EMG-independent controller to guide
movements. This EMG-independent feedback ensures that the user is in-the-loop
during recording, while enabling the resulting dynamic dataset to be used as an
EMG-based benchmark. The dataset is comprised of 20 able-bodied participants
completing three to four sessions over a period of 14 to 21 days. The ability
of the dynamic dataset to serve as a benchmark is leveraged to evaluate the
impact of different recalibration techniques for long-term (across-day) gesture
recognition, including a novel algorithm, named TADANN. TADANN consistently and
significantly (p<0.05) outperforms using fine-tuning as the recalibration
technique.
",within field electromyography base emg gesture recognition disparity exist offline accuracy report literature real time usability classifier gap mainly stem two factor 1 absence controller make datum collect dissimilar actual control 2 difficulty include four main dynamic factor gesture intensity limb position electrode shift transient change signal include permutation drastically increase amount datum record contrarily online dataset limit exact emg base controller use record necessitating record new dataset control method variant test consequently paper propose new type dataset serve intermediate offline online dataset record datum use real time experimental protocol protocol perform virtual reality include four main dynamic factor use emg independent controller guide movement emg independent feedback ensure user in the loop recording enable result dynamic dataset use emg base benchmark dataset comprise 20 able bodied participant complete three four session period 14 21 day ability dynamic dataset serve benchmark leverage evaluate impact different recalibration technique long term across day gesture recognition include novel algorithm name tadann tadann consistently significantly p outperform use fine tune recalibration technique
"Tackling System and Statistical Heterogeneity for Federated Learning
  with Adaptive Client Sampling","  Federated learning (FL) algorithms usually sample a fraction of clients in
each round (partial participation) when the number of participants is large and
the server's communication bandwidth is limited. Recent works on the
convergence analysis of FL have focused on unbiased client sampling, e.g.,
sampling uniformly at random, which suffers from slow wall-clock time for
convergence due to high degrees of system heterogeneity and statistical
heterogeneity. This paper aims to design an adaptive client sampling algorithm
that tackles both system and statistical heterogeneity to minimize the
wall-clock convergence time. We obtain a new tractable convergence bound for FL
algorithms with arbitrary client sampling probabilities. Based on the bound, we
analytically establish the relationship between the total learning time and
sampling probabilities, which results in a non-convex optimization problem for
training time minimization. We design an efficient algorithm for learning the
unknown parameters in the convergence bound and develop a low-complexity
algorithm to approximately solve the non-convex problem. Experimental results
from both hardware prototype and simulation demonstrate that our proposed
sampling scheme significantly reduces the convergence time compared to several
baseline sampling schemes. Notably, our scheme in hardware prototype spends 73%
less time than the uniform sampling baseline for reaching the same target loss.
",federate learning fl algorithm usually sample fraction client round partial participation number participant large server communication bandwidth limited recent work convergence analysis fl focus unbiased client sampling sampling uniformly random suffer slow wall clock time convergence due high degree system heterogeneity statistical heterogeneity paper aim design adaptive client sampling algorithm tackle system statistical heterogeneity minimize wall clock convergence time obtain new tractable convergence bind fl algorithm arbitrary client sampling probability base bind analytically establish relationship total learning time sampling probability result non convex optimization problem training time minimization design efficient algorithm learn unknown parameter convergence bind develop low complexity algorithm approximately solve non convex problem experimental result hardware prototype simulation demonstrate propose sampling scheme significantly reduce convergence time compare several baseline sample scheme notably scheme hardware prototype spend 73 less time uniform sample baseline reach target loss
Matrix embedding method in match for session-based recommendation,"  Session based model is widely used in recommend system. It use the user click
sequence as input of a Recurrent Neural Network (RNN), and get the output of
the RNN network as the vector embedding of the session, and use the inner
product of the vector embedding of session and the vector embedding of the next
item as the score that is the metric of the interest to the next item. This
method can be used for the ""match"" stage for the recommendation system whose
item number is very big by using some index method like KD-Tree or Ball-Tree
and etc.. But this method repudiate the variousness of the interest of user in
a session. We generated the model to modify the vector embedding of session to
a symmetric matrix embedding, that is equivalent to a quadratic form on the
vector space of items. The score is builded as the value of the vector
embedding of next item under the quadratic form. The eigenvectors of the
symmetric matrix embedding corresponding to the positive eigenvalues are
conjectured to represent the interests of user in the session. This method can
be used for the ""match"" stage also. The experiments show that this method is
better than the method of vector embedding.
",session base model widely use recommend system use user click sequence input recurrent neural network rnn get output rnn network vector embed session use inner product vector embed session vector embed next item score metric interest next item method use match stage recommendation system whose item number big use index method like kd tree ball tree etc method repudiate variousness interest user session generate model modify vector embed session symmetric matrix embed equivalent quadratic form vector space item score build value vector embed next item quadratic form eigenvector symmetric matrix embed correspond positive eigenvalue conjecture represent interest user session method use match stage also experiment show method well method vector embed
"Approximated Structured Prediction for Learning Large Scale Graphical
  Models","  This manuscripts contains the proofs for ""A Primal-Dual Message-Passing
Algorithm for Approximated Large Scale Structured Prediction"".
",manuscript contain proof primal dual message pass algorithm approximate large scale structured prediction
Optimal Completion Distillation for Sequence Learning,"  We present Optimal Completion Distillation (OCD), a training procedure for
optimizing sequence to sequence models based on edit distance. OCD is
efficient, has no hyper-parameters of its own, and does not require pretraining
or joint optimization with conditional log-likelihood. Given a partial sequence
generated by the model, we first identify the set of optimal suffixes that
minimize the total edit distance, using an efficient dynamic programming
algorithm. Then, for each position of the generated sequence, we use a target
distribution that puts equal probability on the first token of all the optimal
suffixes. OCD achieves the state-of-the-art performance on end-to-end speech
recognition, on both Wall Street Journal and Librispeech datasets, achieving
$9.3\%$ WER and $4.5\%$ WER respectively.
",present optimal completion distillation ocd training procedure optimize sequence sequence model base edit distance ocd efficient hyper parameter require pretraine joint optimization conditional log likelihood give partial sequence generate model first identify set optimal suffix minimize total edit distance use efficient dynamic programming algorithm position generate sequence use target distribution put equal probability first token optimal suffix ocd achieve state of the art performance end to end speech recognition wall street journal librispeech dataset achieve wer wer respectively
"Log-based Sparse Nonnegative Matrix Factorization for Data
  Representation","  Nonnegative matrix factorization (NMF) has been widely studied in recent
years due to its effectiveness in representing nonnegative data with
parts-based representations. For NMF, a sparser solution implies better
parts-based representation.However, current NMF methods do not always generate
sparse solutions.In this paper, we propose a new NMF method with log-norm
imposed on the factor matrices to enhance the sparseness.Moreover, we propose a
novel column-wisely sparse norm, named $\ell_{2,\log}$-(pseudo) norm to enhance
the robustness of the proposed method.The $\ell_{2,\log}$-(pseudo) norm is
invariant, continuous, and differentiable.For the $\ell_{2,\log}$ regularized
shrinkage problem, we derive a closed-form solution, which can be used for
other general problems.Efficient multiplicative updating rules are developed
for the optimization, which theoretically guarantees the convergence of the
objective value sequence.Extensive experimental results confirm the
effectiveness of the proposed method, as well as the enhanced sparseness and
robustness.
",nonnegative matrix factorization nmf widely study recent year due effectiveness represent nonnegative datum part base representation nmf sparser solution imply well part base current nmf method always generate sparse paper propose new nmf method log norm impose factor matrix enhance propose novel column wisely sparse norm name 2 pseudo norm enhance robustness propose 2 pseudo norm invariant continuous 2 regularized shrinkage problem derive closed form solution use general multiplicative updating rule develop optimization theoretically guarantee convergence objective value experimental result confirm effectiveness propose method well enhanced sparseness robustness
Loss Functions for Multiset Prediction,"  We study the problem of multiset prediction. The goal of multiset prediction
is to train a predictor that maps an input to a multiset consisting of multiple
items. Unlike existing problems in supervised learning, such as classification,
ranking and sequence generation, there is no known order among items in a
target multiset, and each item in the multiset may appear more than once,
making this problem extremely challenging. In this paper, we propose a novel
multiset loss function by viewing this problem from the perspective of
sequential decision making. The proposed multiset loss function is empirically
evaluated on two families of datasets, one synthetic and the other real, with
varying levels of difficulty, against various baseline loss functions including
reinforcement learning, sequence, and aggregated distribution matching loss
functions. The experiments reveal the effectiveness of the proposed loss
function over the others.
",study problem multiset prediction goal multiset prediction train predictor map input multiset consist multiple item unlike exist problem supervise learn classification rank sequence generation know order among item target multiset item multiset may appear make problem extremely challenging paper propose novel multiset loss function view problem perspective sequential decision make propose multiset loss function empirically evaluate two family dataset one synthetic real vary level difficulty various baseline loss function include reinforcement learning sequence aggregate distribution matching loss function experiment reveal effectiveness propose loss function other
Semantic Drift Compensation for Class-Incremental Learning,"  Class-incremental learning of deep networks sequentially increases the number
of classes to be classified. During training, the network has only access to
data of one task at a time, where each task contains several classes. In this
setting, networks suffer from catastrophic forgetting which refers to the
drastic drop in performance on previous tasks. The vast majority of methods
have studied this scenario for classification networks, where for each new task
the classification layer of the network must be augmented with additional
weights to make room for the newly added classes. Embedding networks have the
advantage that new classes can be naturally included into the network without
adding new weights. Therefore, we study incremental learning for embedding
networks. In addition, we propose a new method to estimate the drift, called
semantic drift, of features and compensate for it without the need of any
exemplars. We approximate the drift of previous tasks based on the drift that
is experienced by current task data. We perform experiments on fine-grained
datasets, CIFAR100 and ImageNet-Subset. We demonstrate that embedding networks
suffer significantly less from catastrophic forgetting. We outperform existing
methods which do not require exemplars and obtain competitive results compared
to methods which store exemplars. Furthermore, we show that our proposed SDC
when combined with existing methods to prevent forgetting consistently improves
results.
",class incremental learn deep network sequentially increase number class classify training network access datum one task time task contain several class set network suffer catastrophic forgetting refer drastic drop performance previous task vast majority method study scenario classification network new task classification layer network must augment additional weight make room newly add class embed network advantage new class naturally include network without add new weight therefore study incremental learning embed network addition propose new method estimate drift call semantic drift feature compensate without need exemplar approximate drift previous task base drift experience current task datum perform experiment fine grain dataset cifar100 imagenet subset demonstrate embed network suffer significantly less catastrophic forgetting outperform exist method require exemplar obtain competitive result compare method store exemplar furthermore show propose sdc combine exist method prevent forget consistently improve result
Learning the Arrow of Time,"  We humans seem to have an innate understanding of the asymmetric progression
of time, which we use to efficiently and safely perceive and manipulate our
environment. Drawing inspiration from that, we address the problem of learning
an arrow of time in a Markov (Decision) Process. We illustrate how a learned
arrow of time can capture meaningful information about the environment, which
in turn can be used to measure reachability, detect side-effects and to obtain
an intrinsic reward signal. We show empirical results on a selection of
discrete and continuous environments, and demonstrate for a class of stochastic
processes that the learned arrow of time agrees reasonably well with a known
notion of an arrow of time given by the celebrated Jordan-Kinderlehrer-Otto
result.
",human seem innate understanding asymmetric progression time use efficiently safely perceive manipulate environment draw inspiration address problem learn arrow time markov decision process illustrate learn arrow time capture meaningful information environment turn use measure reachability detect side effect obtain intrinsic reward signal show empirical result selection discrete continuous environment demonstrate class stochastic process learn arrow time agree reasonably well know notion arrow time give celebrate jordan kinderlehrer otto result
DeepWay: a Deep Learning Waypoint Estimator for Global Path Generation,"  Agriculture 3.0 and 4.0 have gradually introduced service robotics and
automation into several agricultural processes, mostly improving crops quality
and seasonal yield. Row-based crops are the perfect settings to test and deploy
smart machines capable of monitoring and manage the harvest. In this context,
global path generation is essential either for ground or aerial vehicles, and
it is the starting point for every type of mission plan. Nevertheless, little
attention has been currently given to this problem by the research community
and global path generation automation is still far to be solved. In order to
generate a viable path for an autonomous machine, the presented research
proposes a feature learning fully convolutional model capable of estimating
waypoints given an occupancy grid map. In particular, we apply the proposed
data-driven methodology to the specific case of row-based crops with the
general objective to generate a global path able to cover the extension of the
crop completely. Extensive experimentation with a custom made synthetic dataset
and real satellite-derived images of different scenarios have proved the
effectiveness of our methodology and demonstrated the feasibility of an
end-to-end and completely autonomous global path planner.
",agriculture gradually introduce service robotic automation several agricultural process mostly improve crop quality seasonal yield row base crop perfect setting test deploy smart machine capable monitoring manage harvest context global path generation essential either ground aerial vehicle start point every type mission plan nevertheless little attention currently give problem research community global path generation automation still far solve order generate viable path autonomous machine present research propose feature learn fully convolutional model capable estimating waypoint give occupancy grid map particular apply propose data drive methodology specific case row base crop general objective generate global path able cover extension crop completely extensive experimentation custom make synthetic dataset real satellite derive image different scenario prove effectiveness methodology demonstrate feasibility end to end completely autonomous global path planner
"Don't miss the Mismatch: Investigating the Objective Function Mismatch
  for Unsupervised Representation Learning","  Finding general evaluation metrics for unsupervised representation learning
techniques is a challenging open research question, which recently has become
more and more necessary due to the increasing interest in unsupervised methods.
Even though these methods promise beneficial representation characteristics,
most approaches currently suffer from the objective function mismatch. This
mismatch states that the performance on a desired target task can decrease when
the unsupervised pretext task is learned too long - especially when both tasks
are ill-posed. In this work, we build upon the widely used linear evaluation
protocol and define new general evaluation metrics to quantitatively capture
the objective function mismatch and the more generic metrics mismatch. We
discuss the usability and stability of our protocols on a variety of pretext
and target tasks and study mismatches in a wide range of experiments. Thereby
we disclose dependencies of the objective function mismatch across several
pretext and target tasks with respect to the pretext model's representation
size, target model complexity, pretext and target augmentations as well as
pretext and target task types. In our experiments, we find that the objective
function mismatch reduces performance by ~0.1-5.0% for Cifar10, Cifar100 and
PCam in many setups, and up to ~25-59% in extreme cases for the 3dshapes
dataset.
",find general evaluation metric unsupervise representation learn technique challenge open research question recently become necessary due increase interest unsupervised method even though method promise beneficial representation characteristic approach currently suffer objective function mismatch mismatch state performance desire target task decrease unsupervised pretext task learn long especially task ill pose work build upon widely use linear evaluation protocol define new general evaluation metric quantitatively capture objective function mismatch generic metric mismatch discuss usability stability protocol variety pretext target task study mismatch wide range experiment thereby disclose dependency objective function mismatch across several pretext target task respect pretext model representation size target model complexity pretext target augmentation well pretext target task type experiment find objective function mismatch reduce performance cifar10 cifar100 pcam many setup extreme case 3dshapes dataset
Multiaccurate Proxies for Downstream Fairness,"  We study the problem of training a model that must obey demographic fairness
conditions when the sensitive features are not available at training time -- in
other words, how can we train a model to be fair by race when we don't have
data about race? We adopt a fairness pipeline perspective, in which an
""upstream"" learner that does have access to the sensitive features will learn a
proxy model for these features from the other attributes. The goal of the proxy
is to allow a general ""downstream"" learner -- with minimal assumptions on their
prediction task -- to be able to use the proxy to train a model that is fair
with respect to the true sensitive features. We show that obeying multiaccuracy
constraints with respect to the downstream model class suffices for this
purpose, provide sample- and oracle efficient-algorithms and generalization
bounds for learning such proxies, and conduct an experimental evaluation. In
general, multiaccuracy is much easier to satisfy than classification accuracy,
and can be satisfied even when the sensitive features are hard to predict.
",study problem training model must obey demographic fairness condition sensitive feature available training time word train model fair race datum race adopt fairness pipeline perspective upstream learner access sensitive feature learn proxy model feature attribute goal proxy allow general downstream learner minimal assumption prediction task able use proxy train model fair respect true sensitive feature show obey multiaccuracy constraint respect downstream model class suffice purpose provide sample- oracle efficient algorithm generalization bound learn proxy conduct experimental evaluation general multiaccuracy much easy satisfy classification accuracy satisfy even sensitive feature hard predict
Planning and Execution using Inaccurate Models with Provable Guarantees,"  Models used in modern planning problems to simulate outcomes of real world
action executions are becoming increasingly complex, ranging from simulators
that do physics-based reasoning to precomputed analytical motion primitives.
However, robots operating in the real world often face situations not modeled
by these models before execution. This imperfect modeling can lead to highly
suboptimal or even incomplete behavior during execution. In this paper, we
propose CMAX an approach for interleaving planning and execution. CMAX adapts
its planning strategy online during real-world execution to account for any
discrepancies in dynamics during planning, without requiring updates to the
dynamics of the model. This is achieved by biasing the planner away from
transitions whose dynamics are discovered to be inaccurately modeled, thereby
leading to robot behavior that tries to complete the task despite having an
inaccurate model. We provide provable guarantees on the completeness and
efficiency of the proposed planning and execution framework under specific
assumptions on the model, for both small and large state spaces. Our approach
CMAX is shown to be efficient empirically in simulated robotic tasks including
4D planar pushing, and in real robotic experiments using PR2 involving a 3D
pick-and-place task where the mass of the object is incorrectly modeled, and a
7D arm planning task where one of the joints is not operational leading to
discrepancy in dynamics. The video of our physical robot experiments can be
found at https://youtu.be/eQmAeWIhjO8
",model use modern planning problem simulate outcome real world action execution become increasingly complex ranging simulator physics base reasoning precompute analytical motion primitive however robot operate real world often face situation model model execution imperfect modeling lead highly suboptimal even incomplete behavior execution paper propose cmax approach interleave planning execution cmax adapt planning strategy online real world execution account discrepancie dynamic planning without require update dynamic model achieve bias planner away transition whose dynamic discover inaccurately model thereby lead robot behavior try complete task despite inaccurate model provide provable guarantee completeness efficiency propose planning execution framework specific assumption model small large state space approach cmax show efficient empirically simulate robotic task include 4d planar push real robotic experiment use pr2 involve 3d pick and place task mass object incorrectly model 7d arm planning task one joint operational leading discrepancy dynamic video physical robot experiment find https
"Data-dependent compression of random features for large-scale kernel
  approximation","  Kernel methods offer the flexibility to learn complex relationships in
modern, large data sets while enjoying strong theoretical guarantees on
quality. Unfortunately, these methods typically require cubic running time in
the data set size, a prohibitive cost in the large-data setting. Random feature
maps (RFMs) and the Nystrom method both consider low-rank approximations to the
kernel matrix as a potential solution. But, in order to achieve desirable
theoretical guarantees, the former may require a prohibitively large number of
features J+, and the latter may be prohibitively expensive for high-dimensional
problems. We propose to combine the simplicity and generality of RFMs with a
data-dependent feature selection scheme to achieve desirable theoretical
approximation properties of Nystrom with just O(log J+) features. Our key
insight is to begin with a large set of random features, then reduce them to a
small number of weighted features in a data-dependent, computationally
efficient way, while preserving the statistical guarantees of using the
original large set of features. We demonstrate the efficacy of our method with
theory and experiments--including on a data set with over 50 million
observations. In particular, we show that our method achieves small kernel
matrix approximation error and better test set accuracy with provably fewer
random features than state-of-the-art methods.
",kernel method offer flexibility learn complex relationship modern large datum set enjoy strong theoretical guarantee quality unfortunately method typically require cubic running time datum set size prohibitive cost large data set random feature map rfms nystrom method consider low rank approximation kernel matrix potential solution order achieve desirable theoretical guarantee former may require prohibitively large number feature latter may prohibitively expensive high dimensional problem propose combine simplicity generality rfms data dependent feature selection scheme achieve desirable theoretical approximation property nystrom log feature key insight begin large set random feature reduce small number weight feature data dependent computationally efficient way preserve statistical guarantee use original large set feature demonstrate efficacy method theory experiment include datum set 50 million observation particular show method achieve small kernel matrix approximation error well test set accuracy provably few random feature state of the art method
There are No Bit Parts for Sign Bits in Black-Box Attacks,"  We present a black-box adversarial attack algorithm which sets new
state-of-the-art model evasion rates for query efficiency in the $\ell_\infty$
and $\ell_2$ metrics, where only loss-oracle access to the model is available.
On two public black-box attack challenges, the algorithm achieves the highest
evasion rate, surpassing all of the submitted attacks. Similar performance is
observed on a model that is secure against substitute-model attacks. For
standard models trained on the MNIST, CIFAR10, and IMAGENET datasets, averaged
over the datasets and metrics, the algorithm is 3.8x less failure-prone, and
spends in total 2.5x fewer queries than the current state-of-the-art attacks
combined given a budget of 10, 000 queries per attack attempt. Notably, it
requires no hyperparameter tuning or any data/time-dependent prior. The
algorithm exploits a new approach, namely sign-based rather than
magnitude-based gradient estimation. This shifts the estimation from continuous
to binary black-box optimization. With three properties of the directional
derivative, we examine three approaches to adversarial attacks. This yields a
superior algorithm breaking a standard MNIST model using just 12 queries on
average!
",present black box adversarial attack algorithm set new state of the art model evasion rate query efficiency metric loss oracle access model available two public black box attack challenge algorithm achieve high evasion rate surpass submit attack similar performance observe model secure substitute model attack standard model train mnist cifar10 imagenet dataset average dataset metric algorithm less failure prone spend total few query current state of the art attack combine give budget 10 000 query per attack attempt notably require hyperparameter tune prior algorithm exploit new approach namely sign base rather magnitude base gradient estimation shift estimation continuous binary black box optimization three property directional derivative examine three approach adversarial attack yield superior algorithm break standard mnist model use 12 query average
Bio-Inspired Hashing for Unsupervised Similarity Search,"  The fruit fly Drosophila's olfactory circuit has inspired a new locality
sensitive hashing (LSH) algorithm, FlyHash. In contrast with classical LSH
algorithms that produce low dimensional hash codes, FlyHash produces sparse
high-dimensional hash codes and has also been shown to have superior empirical
performance compared to classical LSH algorithms in similarity search. However,
FlyHash uses random projections and cannot learn from data. Building on
inspiration from FlyHash and the ubiquity of sparse expansive representations
in neurobiology, our work proposes a novel hashing algorithm BioHash that
produces sparse high dimensional hash codes in a data-driven manner. We show
that BioHash outperforms previously published benchmarks for various hashing
methods. Since our learning algorithm is based on a local and biologically
plausible synaptic plasticity rule, our work provides evidence for the proposal
that LSH might be a computational reason for the abundance of sparse expansive
motifs in a variety of biological systems. We also propose a convolutional
variant BioConvHash that further improves performance. From the perspective of
computer science, BioHash and BioConvHash are fast, scalable and yield
compressed binary representations that are useful for similarity search.
",fruit fly drosophila olfactory circuit inspire new locality sensitive hash lsh algorithm flyhash contrast classical lsh algorithm produce low dimensional hash code flyhash produce sparse high dimensional hash code also show superior empirical performance compare classical lsh algorithm similarity search however flyhash use random projection learn datum building inspiration flyhash ubiquity sparse expansive representation neurobiology work propose novel hash algorithm biohash produce sparse high dimensional hash code data drive manner show biohash outperform previously publish benchmark various hashing method since learn algorithm base local biologically plausible synaptic plasticity rule work provide evidence proposal lsh might computational reason abundance sparse expansive motifs variety biological system also propose convolutional variant bioconvhash improve performance perspective computer science biohash bioconvhash fast scalable yield compress binary representation useful similarity search
Binary Stochastic Representations for Large Multi-class Classification,"  Classification with a large number of classes is a key problem in machine
learning and corresponds to many real-world applications like tagging of images
or textual documents in social networks. If one-vs-all methods usually reach
top performance in this context, these approaches suffer from a high inference
complexity, linear w.r.t the number of categories. Different models based on
the notion of binary codes have been proposed to overcome this limitation,
achieving in a sublinear inference complexity. But they a priori need to decide
which binary code to associate to which category before learning using more or
less complex heuristics. We propose a new end-to-end model which aims at
simultaneously learning to associate binary codes with categories, but also
learning to map inputs to binary codes. This approach called Deep Stochastic
Neural Codes (DSNC) keeps the sublinear inference complexity but do not need
any a priori tuning. Experimental results on different datasets show the
effectiveness of the approach w.r.t baseline methods.
",classification large number class key problem machine learning correspond many real world application like tag image textual document social network one vs all method usually reach top performance context approach suffer high inference complexity linear number category different model base notion binary code propose overcome limitation achieve sublinear inference complexity priori need decide binary code associate category learn use less complex heuristic propose new end to end model aims simultaneously learn associate binary code category also learn map input binary code approach call deep stochastic neural code dsnc keep sublinear inference complexity need priori tune experimental result different dataset show effectiveness approach baseline method
Online and Scalable Model Selection with Multi-Armed Bandits,"  Many online applications running on live traffic are powered by machine
learning models, for which training, validation, and hyper-parameter tuning are
conducted on historical data. However, it is common for models demonstrating
strong performance in offline analysis to yield poorer performance when
deployed online. This problem is a consequence of the difficulty of training on
historical data in non-stationary environments. Moreover, the machine learning
metrics used for model selection may not sufficiently correlate with real-world
business metrics used to determine the success of the applications being
tested. These problems are particularly prominent in the Real-Time Bidding
(RTB) domain, in which ML models power bidding strategies, and a change in
models will likely affect performance of the advertising campaigns. In this
work, we present Automatic Model Selector (AMS), a system for scalable online
selection of RTB bidding strategies based on real-world performance metrics.
AMS employs Multi-Armed Bandits (MAB) to near-simultaneously run and evaluate
multiple models against live traffic, allocating the most traffic to the
best-performing models while decreasing traffic to those with poorer online
performance, thereby minimizing the impact of inferior models on overall
campaign performance. The reliance on offline data is avoided, instead making
model selections on a case-by-case basis according to actionable business
goals. AMS allows new models to be safely introduced into live campaigns as
soon as they are developed, minimizing the risk to overall performance. In
live-traffic tests on multiple ad campaigns, the AMS system proved highly
effective at improving ad campaign performance.
",many online application run live traffic power machine learning model train validation hyper parameter tuning conduct historical datum however common model demonstrate strong performance offline analysis yield poor performance deploy online problem consequence difficulty train historical datum non stationary environment moreover machine learn metric use model selection may sufficiently correlate real world business metric use determine success application test problem particularly prominent real time bidding rtb domain ml model power bidding strategy change model likely affect performance advertising campaign work present automatic model selector am system scalable online selection rtb bidding strategy base real world performance metric ams employ multi armed bandit mab near simultaneously run evaluate multiple model live traffic allocate traffic well perform model decrease traffic poor online performance thereby minimize impact inferior model overall campaign performance reliance offline datum avoid instead make model selection case by case basis accord actionable business goal ams allow new model safely introduce live campaign soon develop minimize risk overall performance live traffic test multiple ad campaign ams system prove highly effective improve ad campaign performance
"Generative Adversarial Networks for Data Generation in Structural Health
  Monitoring","  Structural Health Monitoring (SHM) has been continuously benefiting from the
advancements in the field of data science. Various types of Artificial
Intelligence (AI) methods have been utilized for the assessment and evaluation
of civil structures. In AI, Machine Learning (ML) and Deep Learning (DL)
algorithms require plenty of datasets to train; particularly, the more data DL
models are trained with, the better output it yields. Yet, in SHM applications,
collecting data from civil structures through sensors is expensive and
obtaining useful data (damage associated data) is challenging. In this paper,
1-D Wasserstein loss Deep Convolutional Generative Adversarial Networks using
Gradient Penalty (1-D WDCGAN-GP) is utilized to generate damage associated
vibration datasets that are similar to the input. For the purpose of
vibration-based damage diagnostics, a 1-D Deep Convolutional Neural Network
(1-D DCNN) is built, trained, and tested on both real and generated datasets.
The classification results from the 1-D DCNN on both datasets resulted to be
very similar to each other. The presented work in this paper shows that for the
cases of insufficient data in DL or ML-based damage diagnostics, 1-D WDCGAN-GP
can successfully generate data for the model to be trained on. Keywords: 1-D
Generative Adversarial Networks (GAN), Deep Convolutional Generative
Adversarial Networks (DCGAN), Wasserstein Generative Adversarial Networks with
Gradient Penalty (WGAN-GP), 1-D Convolutional Neural Networks (CNN), Structural
Health Monitoring (SHM), Structural Damage Diagnostics, Structural Damage
Detection
",structural health monitoring shm continuously benefit advancement field datum science various type artificial intelligence ai method utilize assessment evaluation civil structure ai machine learning ml deep learning dl algorithm require plenty dataset train particularly datum dl model train well output yield yet shm application collect data civil structure sensor expensive obtain useful datum damage associate datum challenge paper 1 d wasserstein loss deep convolutional generative adversarial network use gradient penalty 1 d wdcgan gp utilize generate damage associate vibration dataset similar input purpose vibration base damage diagnostic 1 d deep convolutional neural network 1 d dcnn build train test real generate dataset classification result 1 d dcnn dataset result similar present work paper show case insufficient datum dl ml base damage diagnostic 1 d wdcgan gp successfully generate data model train keyword 1 d generative adversarial network gin deep convolutional generative adversarial network dcgan wasserstein generative adversarial network gradient penalty wgan gp 1 d convolutional neural network cnn structural health monitoring shm structural damage diagnostic structural damage detection
"Tool- and Domain-Agnostic Parameterization of Style Transfer Effects
  Leveraging Pretrained Perceptual Metrics","  Current deep learning techniques for style transfer would not be optimal for
design support since their ""one-shot"" transfer does not fit exploratory design
processes. To overcome this gap, we propose parametric transcription, which
transcribes an end-to-end style transfer effect into parameter values of
specific transformations available in an existing content editing tool. With
this approach, users can imitate the style of a reference sample in the tool
that they are familiar with and thus can easily continue further exploration by
manipulating the parameters. To enable this, we introduce a framework that
utilizes an existing pretrained model for style transfer to calculate a
perceptual style distance to the reference sample and uses black-box
optimization to find the parameters that minimize this distance. Our
experiments with various third-party tools, such as Instagram and Blender, show
that our framework can effectively leverage deep learning techniques for
computational design support.
",current deep learning technique style transfer would optimal design support since one shot transfer fit exploratory design process overcome gap propose parametric transcription transcribe end to end style transfer effect parameter value specific transformation available exist content edit tool approach user imitate style reference sample tool familiar thus easily continue exploration manipulate parameter enable introduce framework utilize exist pretraine model style transfer calculate perceptual style distance reference sample use black box optimization find parameter minimize distance experiment various third party tool instagram blender show framework effectively leverage deep learning technique computational design support
One Scan 1-Bit Compressed Sensing,"  Based on $\alpha$-stable random projections with small $\alpha$, we develop a
simple algorithm for compressed sensing (sparse signal recovery) by utilizing
only the signs (i.e., 1-bit) of the measurements. Using only 1-bit information
of the measurements results in substantial cost reduction in collection,
storage, communication, and decoding for compressed sensing. The proposed
algorithm is efficient in that the decoding procedure requires only one scan of
the coordinates. Our analysis can precisely show that, for a $K$-sparse signal
of length $N$, $12.3K\log N/\delta$ measurements (where $\delta$ is the
confidence) would be sufficient for recovering the support and the signs of the
signal. While the method is very robust against typical measurement noises, we
also provide the analysis of the scheme under random flipping of the signs of
the measurements.
  \noindent Compared to the well-known work on 1-bit marginal regression (which
can also be viewed as a one-scan method), the proposed algorithm requires
orders of magnitude fewer measurements. Compared to 1-bit Iterative Hard
Thresholding (IHT) (which is not a one-scan algorithm), our method is still
significantly more accurate. Furthermore, the proposed method is reasonably
robust against random sign flipping while IHT is known to be very sensitive to
this type of noise.
",base -stable random projection small develop simple algorithm compressed sense sparse signal recovery utilize sign 1 bit measurement use 1 bit information measurement result substantial cost reduction collection storage communication decode compress sensing propose algorithm efficient decode procedure require one scan coordinate analysis precisely show k -sparse signal length n measurement confidence would sufficient recover support sign signal method robust typical measurement noise also provide analysis scheme random flipping sign measurement compare well know work 1 bit marginal regression also view one scan method propose algorithm require order magnitude few measurement compare 1 bit iterative hard thresholde iht one scan algorithm method still significantly accurate furthermore propose method reasonably robust random sign flip iht know sensitive type noise
"A Mixture of Views Network with Applications to the Classification of
  Breast Microcalcifications","  In this paper we examine data fusion methods for multi-view data
classification. We present a decision concept which explicitly takes into
account the input multi-view structure, where for each case there is a
different subset of relevant views. This data fusion concept, which we dub
Mixture of Views, is implemented by a special purpose neural network
architecture. It is demonstrated on the task of classifying breast
microcalcifications as benign or malignant based on CC and MLO mammography
views. The single view decisions are combined by a data-driven decision,
according to the relevance of each view in a given case, into a global
decision. The method is evaluated on a large multi-view dataset extracted from
the standardized digital database for screening mammography (DDSM). The
experimental results show that our method outperforms previously suggested
fusion methods.
",paper examine datum fusion method multi view datum classification present decision concept explicitly take account input multi view structure case different subset relevant view datum fusion concept dub mixture view implement special purpose neural network architecture demonstrate task classify breast microcalcification benign malignant base cc mlo mammography view single view decision combine data drive decision accord relevance view give case global decision method evaluate large multi view dataset extract standardized digital database screen mammography ddsm experimental result show method outperform previously suggest fusion method
Three Modern Roles for Logic in AI,"  We consider three modern roles for logic in artificial intelligence, which
are based on the theory of tractable Boolean circuits: (1) logic as a basis for
computation, (2) logic for learning from a combination of data and knowledge,
and (3) logic for reasoning about the behavior of machine learning systems.
",consider three modern role logic artificial intelligence base theory tractable boolean circuit 1 logic basis computation 2 logic learn combination datum knowledge 3 logic reasoning behavior machine learn system
"Hindsight Value Function for Variance Reduction in Stochastic Dynamic
  Environment","  Policy gradient methods are appealing in deep reinforcement learning but
suffer from high variance of gradient estimate. To reduce the variance, the
state value function is applied commonly. However, the effect of the state
value function becomes limited in stochastic dynamic environments, where the
unexpected state dynamics and rewards will increase the variance. In this
paper, we propose to replace the state value function with a novel hindsight
value function, which leverages the information from the future to reduce the
variance of the gradient estimate for stochastic dynamic environments.
  Particularly, to obtain an ideally unbiased gradient estimate, we propose an
information-theoretic approach, which optimizes the embeddings of the future to
be independent of previous actions. In our experiments, we apply the proposed
hindsight value function in stochastic dynamic environments, including
discrete-action environments and continuous-action environments. Compared with
the standard state value function, the proposed hindsight value function
consistently reduces the variance, stabilizes the training, and improves the
eventual policy.
",policy gradient method appeal deep reinforcement learning suffer high variance gradient estimate reduce variance state value function apply commonly however effect state value function become limited stochastic dynamic environment unexpected state dynamic reward increase variance paper propose replace state value function novel hindsight value function leverage information future reduce variance gradient estimate stochastic dynamic environment particularly obtain ideally unbiased gradient estimate propose information theoretic approach optimize embedding future independent previous action experiment apply propose hindsight value function stochastic dynamic environment include discrete action environment continuous action environment compare standard state value function propose hindsight value function consistently reduce variance stabilize training improve eventual policy
Offline Meta Learning of Exploration,"  Consider the following instance of the Offline Meta Reinforcement Learning
(OMRL) problem: given the complete training logs of $N$ conventional RL agents,
trained on $N$ different tasks, design a meta-agent that can quickly maximize
reward in a new, unseen task from the same task distribution. In particular,
while each conventional RL agent explored and exploited its own different task,
the meta-agent must identify regularities in the data that lead to effective
exploration/exploitation in the unseen task. Here, we take a Bayesian RL (BRL)
view, and seek to learn a Bayes-optimal policy from the offline data. Building
on the recent VariBAD BRL approach, we develop an off-policy BRL method that
learns to plan an exploration strategy based on an adaptive neural belief
estimate. However, learning to infer such a belief from offline data brings a
new identifiability issue we term MDP ambiguity. We characterize the problem,
and suggest resolutions via data collection and modification procedures.
Finally, we evaluate our framework on a diverse set of domains, including
difficult sparse reward tasks, and demonstrate learning of effective
exploration behavior that is qualitatively different from the exploration used
by any RL agent in the data.
",consider follow instance offline meta reinforcement learn omrl problem give complete training log n conventional rl agent train n different task design meta agent quickly maximize reward new unseen task task distribution particular conventional rl agent explore exploit different task meta agent must identify regularity datum lead effective unseen task take bayesian rl brl view seek learn bayes optimal policy offline datum build recent varibad brl approach develop off policy brl method learn plan exploration strategy base adaptive neural belief estimate however learn infer belief offline datum bring new identifiability issue term mdp ambiguity characterize problem suggest resolution via data collection modification procedure finally evaluate framework diverse set domain include difficult sparse reward task demonstrate learn effective exploration behavior qualitatively different exploration use rl agent datum
EvalNorm: Estimating Batch Normalization Statistics for Evaluation,"  Batch normalization (BN) has been very effective for deep learning and is
widely used. However, when training with small minibatches, models using BN
exhibit a significant degradation in performance. In this paper we study this
peculiar behavior of BN to gain a better understanding of the problem, and
identify a cause. We propose 'EvalNorm' to address the issue by estimating
corrected normalization statistics to use for BN during evaluation. EvalNorm
supports online estimation of the corrected statistics while the model is being
trained, and does not affect the training scheme of the model. As a result,
EvalNorm can also be used with existing pre-trained models allowing them to
benefit from our method. EvalNorm yields large gains for models trained with
smaller batches. Our experiments show that EvalNorm performs 6.18% (absolute)
better than vanilla BN for a batchsize of 2 on ImageNet validation set and from
1.5 to 7.0 points (absolute) gain on the COCO object detection benchmark across
a variety of setups.
",batch normalization bn effective deep learning widely use however train small minibatche model use bn exhibit significant degradation performance paper study peculiar behavior bn gain well understanding problem identify cause propose address issue estimate correct normalization statistic use bn evaluation evalnorm support online estimation correct statistic model train affect training scheme model result evalnorm also use exist pre train model allow benefit method evalnorm yield large gain model train small batch experiment show evalnorm perform absolute well vanilla bn batchsize 2 imagenet validation set point absolute gain coco object detection benchmark across variety setup
"Audio-visual speech separation based on joint feature representation
  with cross-modal attention","  Multi-modal based speech separation has exhibited a specific advantage on
isolating the target character in multi-talker noisy environments.
Unfortunately, most of current separation strategies prefer a straightforward
fusion based on feature learning of each single modality, which is far from
sufficient consideration of inter-relationships between modalites. Inspired by
learning joint feature representations from audio and visual streams with
attention mechanism, in this study, a novel cross-modal fusion strategy is
proposed to benefit the whole framework with semantic correlations between
different modalities. To further improve audio-visual speech separation, the
dense optical flow of lip motion is incorporated to strengthen the robustness
of visual representation. The evaluation of the proposed work is performed on
two public audio-visual speech separation benchmark datasets. The overall
improvement of the performance has demonstrated that the additional motion
network effectively enhances the visual representation of the combined lip
images and audio signal, as well as outperforming the baseline in terms of all
metrics with the proposed cross-modal fusion.
",multi modal base speech separation exhibit specific advantage isolate target character multi talker noisy environment unfortunately current separation strategy prefer straightforward fusion base feature learn single modality far sufficient consideration inter relationships modalite inspire learn joint feature representation audio visual stream attention mechanism study novel cross modal fusion strategy propose benefit whole framework semantic correlation different modality improve audio visual speech separation dense optical flow lip motion incorporate strengthen robustness visual representation evaluation propose work perform two public audio visual speech separation benchmark dataset overall improvement performance demonstrate additional motion network effectively enhance visual representation combine lip image audio signal well outperform baseline term metric propose cross modal fusion
Enhancing Robustness of On-line Learning Models on Highly Noisy Data,"  Classification algorithms have been widely adopted to detect anomalies for
various systems, e.g., IoT, cloud and face recognition, under the common
assumption that the data source is clean, i.e., features and labels are
correctly set. However, data collected from the wild can be unreliable due to
careless annotations or malicious data transformation for incorrect anomaly
detection. In this paper, we extend a two-layer on-line data selection
framework: Robust Anomaly Detector (RAD) with a newly designed ensemble
prediction where both layers contribute to the final anomaly detection
decision. To adapt to the on-line nature of anomaly detection, we consider
additional features of conflicting opinions of classifiers, repetitive
cleaning, and oracle knowledge. We on-line learn from incoming data streams and
continuously cleanse the data, so as to adapt to the increasing learning
capacity from the larger accumulated data set. Moreover, we explore the concept
of oracle learning that provides additional information of true labels for
difficult data points. We specifically focus on three use cases, (i) detecting
10 classes of IoT attacks, (ii) predicting 4 classes of task failures of big
data jobs, and (iii) recognising 100 celebrities faces. Our evaluation results
show that RAD can robustly improve the accuracy of anomaly detection, to reach
up to 98.95% for IoT device attacks (i.e., +7%), up to 85.03% for cloud task
failures (i.e., +14%) under 40% label noise, and for its extension, it can
reach up to 77.51% for face recognition (i.e., +39%) under 30% label noise. The
proposed RAD and its extensions are general and can be applied to different
anomaly detection algorithms.
",classification algorithm widely adopt detect anomaly various system iot cloud face recognition common assumption datum source clean feature label correctly set however datum collect wild unreliable due careless annotation malicious datum transformation incorrect anomaly detection paper extend two layer on line datum selection framework robust anomaly detector rad newly design ensemble prediction layer contribute final anomaly detection decision adapt on line nature anomaly detection consider additional feature conflict opinion classifier repetitive cleaning oracle knowledge on line learn incoming data stream continuously cleanse datum adapt increase learning capacity large accumulate datum set moreover explore concept oracle learning provide additional information true label difficult data point specifically focus three use case detect 10 class iot attack ii predict 4 class task failure big data job iii recognise 100 celebrity face evaluation result show rad robustly improve accuracy anomaly detection reach iot device attack cloud task failure 40 label noise extension reach face recognition 30 label noise propose rad extensions general apply different anomaly detection algorithm
On Faster Convergence of Scaled Sign Gradient Descent,"  Communication has been seen as a significant bottleneck in industrial
applications over large-scale networks. To alleviate the communication burden,
sign-based optimization algorithms have gained popularity recently in both
industrial and academic communities, which is shown to be closely related to
adaptive gradient methods, such as Adam. Along this line, this paper
investigates faster convergence for a variant of sign-based gradient descent,
called scaled signGD, in three cases: 1) the objective function is strongly
convex; 2) the objective function is nonconvex but satisfies the
Polyak-Lojasiewicz (PL) inequality; 3) the gradient is stochastic, called
scaled signGD in this case. For the first two cases, it can be shown that the
scaled signGD converges at a linear rate. For case 3), the algorithm is shown
to converge linearly to a neighborhood of the optimal value when a constant
learning rate is employed, and the algorithm converges at a rate of $O(1/k)$
when using a diminishing learning rate, where $k$ is the iteration number. The
results are also extended to the distributed setting by majority vote in a
parameter-server framework. Finally, numerical experiments on logistic
regression are performed to corroborate the theoretical findings.
",communication see significant bottleneck industrial application large scale network alleviate communication burden sign base optimization algorithm gain popularity recently industrial academic community show closely relate adaptive gradient method adam along line paper investigate fast convergence variant sign base gradient descent call scale signgd three case 1 objective function strongly convex 2 objective function nonconvex satisfy polyak lojasiewicz pl inequality 3 gradient stochastic call scale signgd case first two case show scale signgd converge linear rate case 3 algorithm show converge linearly neighborhood optimal value constant learning rate employ algorithm converge rate use diminish learn rate k iteration number result also extend distribute set majority vote parameter server framework finally numerical experiment logistic regression perform corroborate theoretical finding
Working Memory Graphs,"  Transformers have increasingly outperformed gated RNNs in obtaining new
state-of-the-art results on supervised tasks involving text sequences. Inspired
by this trend, we study the question of how Transformer-based models can
improve the performance of sequential decision-making agents. We present the
Working Memory Graph (WMG), an agent that employs multi-head self-attention to
reason over a dynamic set of vectors representing observed and recurrent state.
We evaluate WMG in three environments featuring factored observation spaces: a
Pathfinding environment that requires complex reasoning over past observations,
BabyAI gridworld levels that involve variable goals, and Sokoban which
emphasizes future planning. We find that the combination of WMG's
Transformer-based architecture with factored observation spaces leads to
significant gains in learning efficiency compared to baseline architectures
across all tasks. WMG demonstrates how Transformer-based models can
dramatically boost sample efficiency in RL environments for which observations
can be factored.
",transformer increasingly outperform gate rnns obtain new state of the art result supervise task involve text sequence inspire trend study question transformer base model improve performance sequential decision make agent present working memory graph wmg agent employ multi head self attention reason dynamic set vector represent observed recurrent state evaluate wmg three environment feature factored observation space pathfinde environment require complex reasoning past observation babyai gridworld level involve variable goal sokoban emphasize future planning find combination wmg transformer base architecture factor observation space lead significant gain learn efficiency compare baseline architecture across task wmg demonstrate transformer base model dramatically boost sample efficiency rl environment observation factor
Domain Adaptation: Overfitting and Small Sample Statistics,"  We study the prevalent problem when a test distribution differs from the
training distribution. We consider a setting where our training set consists of
a small number of sample domains, but where we have many samples in each
domain. Our goal is to generalize to a new domain. For example, we may want to
learn a similarity function using only certain classes of objects, but we
desire that this similarity function be applicable to object classes not
present in our training sample (e.g. we might seek to learn that ""dogs are
similar to dogs"" even though images of dogs were absent from our training set).
Our theoretical analysis shows that we can select many more features than
domains while avoiding overfitting by utilizing data-dependent variance
properties. We present a greedy feature selection algorithm based on using
T-statistics. Our experiments validate this theory showing that our T-statistic
based greedy feature selection is more robust at avoiding overfitting than the
classical greedy procedure.
",study prevalent problem test distribution differ training distribution consider set training set consist small number sample domain many sample domain goal generalize new domain example may want learn similarity function use certain class object desire similarity function applicable object class present training sample might seek learn dog similar dog even though image dog absent training set theoretical analysis show select many feature domain avoid overfitte utilize data dependent variance property present greedy feature selection algorithm base use t statistic experiment validate theory show t statistic base greedy feature selection robust avoiding overfitte classical greedy procedure
Finite-Sample Guarantees for High-Dimensional DML,"  Debiased machine learning (DML) offers an attractive way to estimate
treatment effects in observational settings, where identification of causal
parameters requires a conditional independence or unconfoundedness assumption,
since it allows to control flexibly for a potentially very large number of
covariates. This paper gives novel finite-sample guarantees for joint inference
on high-dimensional DML, bounding how far the finite-sample distribution of the
estimator is from its asymptotic Gaussian approximation. These guarantees are
useful to applied researchers, as they are informative about how far off the
coverage of joint confidence bands can be from the nominal level. There are
many settings where high-dimensional causal parameters may be of interest, such
as the ATE of many treatment profiles, or the ATE of a treatment on many
outcomes. We also cover infinite-dimensional parameters, such as impacts on the
entire marginal distribution of potential outcomes. The finite-sample
guarantees in this paper complement the existing results on consistency and
asymptotic normality of DML estimators, which are either asymptotic or treat
only the one-dimensional case.
",debiase machine learn dml offer attractive way estimate treatment effect observational setting identification causal parameter require conditional independence unconfoundedness assumption since allow control flexibly potentially large number covariate paper give novel finite sample guarantee joint inference high dimensional dml bound far finite sample distribution estimator asymptotic gaussian approximation guarantee useful apply researcher informative far coverage joint confidence band nominal level many setting high dimensional causal parameter may interest eat many treatment profile eat treatment many outcome also cover infinite dimensional parameter impact entire marginal distribution potential outcome finite sample guarantee paper complement exist result consistency asymptotic normality dml estimator either asymptotic treat one dimensional case
"Regression or Classification? Reflection on BP prediction from PPG data
  using Deep Neural Networks in the scope of practical applications","  Photoplethysmographic (PPG) signals offer diagnostic potential beyond heart
rate analysis or blood oxygen level monitoring. In the recent past, research
focused extensively on non-invasive PPG-based approaches to blood pressure (BP)
estimation. These approaches can be subdivided into regression and
classification methods. The latter assign PPG signals to predefined BP
intervals that represent clinically relevant ranges. The former predict
systolic (SBP) and diastolic (DBP) BP as continuous variables and are of
particular interest to the research community. However, the reported accuracies
of BP regression methods vary widely among publications with some authors even
questioning the feasibility of PPG-based BP regression altogether. In our work,
we compare BP regression and classification approaches. We argue that BP
classification might provide diagnostic value that is equivalent to regression
in many clinically relevant scenarios while being similar or even superior in
terms of performance. We compare several established neural architectures using
publicly available PPG data for SBP regression and classification with and
without personalization using subject-specific data. We found that
classification and regression models perform similar before personalization.
However, after personalization, the accuracy of classification based methods
outperformed regression approaches. We conclude that BP classification might be
preferable over BP regression in certain scenarios where a coarser segmentation
of the BP range is sufficient.
",photoplethysmographic ppg signal offer diagnostic potential beyond heart rate analysis blood oxygen level monitoring recent past research focus extensively non invasive ppg base approach blood pressure bp estimation approach subdivide regression classification method latter assign ppg signal predefine bp interval represent clinically relevant range former predict systolic sbp diastolic dbp bp continuous variable particular interest research community however report accuracy bp regression method vary widely among publication author even question feasibility ppg base bp regression altogether work compare bp regression classification approach argue bp classification might provide diagnostic value equivalent regression many clinically relevant scenario similar even superior term performance compare several establish neural architecture use publicly available ppg datum sbp regression classification without personalization use subject specific datum find classification regression model perform similar personalization however personalization accuracy classification base method outperform regression approach conclude bp classification might preferable bp regression certain scenario coarser segmentation bp range sufficient
Approximate Trace Reconstruction,"  In the usual trace reconstruction problem, the goal is to exactly reconstruct
an unknown string of length $n$ after it passes through a deletion channel many
times independently, producing a set of traces (i.e., random subsequences of
the string). We consider the relaxed problem of approximate reconstruction.
Here, the goal is to output a string that is close to the original one in edit
distance while using much fewer traces than is needed for exact reconstruction.
We present several algorithms that can approximately reconstruct strings that
belong to certain classes, where the estimate is within $n/\mathrm{polylog}(n)$
edit distance, and where we only use $\mathrm{polylog}(n)$ traces (or sometimes
just a single trace). These classes contain strings that require a linear
number of traces for exact reconstruction and which are quite different from a
typical random string. From a technical point of view, our algorithms
approximately reconstruct consecutive substrings of the unknown string by
aligning dense regions of traces and using a run of a suitable length to
approximate each region. To complement our algorithms, we present a general
black-box lower bound for approximate reconstruction, building on a lower bound
for distinguishing between two candidate input strings in the worst case. In
particular, this shows that approximating to within $n^{1/3 - \delta}$ edit
distance requires $n^{1 + 3\delta/2}/\mathrm{polylog}(n)$ traces for $0< \delta
< 1/3$ in the worst case.
",usual trace reconstruction problem goal exactly reconstruct unknown string length n pass deletion channel many time independently produce set trace random subsequence string consider relaxed problem approximate reconstruction goal output string close original one edit distance use much few trace need exact reconstruction present several algorithm approximately reconstruct string belong certain class estimate within polylog n edit distance use polylog n trace sometimes single trace class contain string require linear number trace exact reconstruction quite different typical random string technical point view algorithm approximately reconstruct consecutive substring unknown string align dense region trace use run suitable length approximate region complement algorithm present general black box lower bind approximate reconstruction build lower bind distinguish two candidate input string worst case particular show approximate within edit distance require 1 polylog n trace 0 bad case
"The Generalization Error of the Minimum-norm Solutions for
  Over-parameterized Neural Networks","  We study the generalization properties of minimum-norm solutions for three
over-parametrized machine learning models including the random feature model,
the two-layer neural network model and the residual network model. We proved
that for all three models, the generalization error for the minimum-norm
solution is comparable to the Monte Carlo rate, up to some logarithmic terms,
as long as the models are sufficiently over-parametrized.
",study generalization property minimum norm solution three over parametrize machine learning model include random feature model two layer neural network model residual network model prove three model generalization error minimum norm solution comparable monte carlo rate logarithmic term long model sufficiently over parametrize
Why Machine Learning Cannot Ignore Maximum Likelihood Estimation,"  The growth of machine learning as a field has been accelerating with
increasing interest and publications across fields, including statistics, but
predominantly in computer science. How can we parse this vast literature for
developments that exemplify the necessary rigor? How many of these manuscripts
incorporate foundational theory to allow for statistical inference? Which
advances have the greatest potential for impact in practice? One could posit
many answers to these queries. Here, we assert that one essential idea is for
machine learning to integrate maximum likelihood for estimation of functional
parameters, such as prediction functions and conditional densities.
",growth machine learn field accelerate increase interest publication across field include statistic predominantly computer science parse vast literature development exemplify necessary rigor many manuscript incorporate foundational theory allow statistical inference advance great potential impact practice one could posit many answer query assert one essential idea machine learn integrate maximum likelihood estimation functional parameter prediction function conditional density
"A Sim2Real Deep Learning Approach for the Transformation of Images from
  Multiple Vehicle-Mounted Cameras to a Semantically Segmented Image in Bird's
  Eye View","  Accurate environment perception is essential for automated driving. When
using monocular cameras, the distance estimation of elements in the environment
poses a major challenge. Distances can be more easily estimated when the camera
perspective is transformed to a bird's eye view (BEV). For flat surfaces,
Inverse Perspective Mapping (IPM) can accurately transform images to a BEV.
Three-dimensional objects such as vehicles and vulnerable road users are
distorted by this transformation making it difficult to estimate their position
relative to the sensor. This paper describes a methodology to obtain a
corrected 360{\deg} BEV image given images from multiple vehicle-mounted
cameras. The corrected BEV image is segmented into semantic classes and
includes a prediction of occluded areas. The neural network approach does not
rely on manually labeled data, but is trained on a synthetic dataset in such a
way that it generalizes well to real-world data. By using semantically
segmented images as input, we reduce the reality gap between simulated and
real-world data and are able to show that our method can be successfully
applied in the real world. Extensive experiments conducted on the synthetic
data demonstrate the superiority of our approach compared to IPM. Source code
and datasets are available at https://github.com/ika-rwth-aachen/Cam2BEV
",accurate environment perception essential automate driving use monocular camera distance estimation element environment pose major challenge distance easily estimate camera perspective transform bird eye view bev flat surface inverse perspective mapping ipm accurately transform image bev three dimensional object vehicle vulnerable road user distort transformation make difficult estimate position relative sensor paper describe methodology obtain correct 360 bev image give image multiple vehicle mount camera correct bev image segment semantic class include prediction occluded area neural network approach rely manually label datum train synthetic dataset way generalize well real world datum use semantically segment image input reduce reality gap simulate real world datum able show method successfully apply real world extensive experiment conduct synthetic datum demonstrate superiority approach compare ipm source code dataset available https
"vDNN: Virtualized Deep Neural Networks for Scalable, Memory-Efficient
  Neural Network Design","  The most widely used machine learning frameworks require users to carefully
tune their memory usage so that the deep neural network (DNN) fits into the
DRAM capacity of a GPU. This restriction hampers a researcher's flexibility to
study different machine learning algorithms, forcing them to either use a less
desirable network architecture or parallelize the processing across multiple
GPUs. We propose a runtime memory manager that virtualizes the memory usage of
DNNs such that both GPU and CPU memory can simultaneously be utilized for
training larger DNNs. Our virtualized DNN (vDNN) reduces the average GPU memory
usage of AlexNet by up to 89%, OverFeat by 91%, and GoogLeNet by 95%, a
significant reduction in memory requirements of DNNs. Similar experiments on
VGG-16, one of the deepest and memory hungry DNNs to date, demonstrate the
memory-efficiency of our proposal. vDNN enables VGG-16 with batch size 256
(requiring 28 GB of memory) to be trained on a single NVIDIA Titan X GPU card
containing 12 GB of memory, with 18% performance loss compared to a
hypothetical, oracular GPU with enough memory to hold the entire DNN.
",widely use machine learn framework require user carefully tune memory usage deep neural network dnn fit dram capacity gpu restriction hamper researcher flexibility study different machine learn algorithm force either use less desirable network architecture parallelize processing across multiple gpu propose runtime memory manager virtualize memory usage dnn gpu cpu memory simultaneously utilize training large dnn virtualize dnn vdnn reduce average gpu memory usage alexnet 89 overfeat 91 googlenet 95 significant reduction memory requirement dnn similar experiment vgg-16 one deep memory hungry dnn date demonstrate memory efficiency proposal vdnn enable vgg-16 batch size 256 require 28 gb memory train single nvidia titan x gpu card contain 12 gb memory 18 performance loss compare hypothetical oracular gpu enough memory hold entire dnn
"ForecastTB An R Package as a Test-Bench for Time Series Forecasting
  Application of Wind Speed and Solar Radiation Modeling","  This paper introduces an R package ForecastTB that can be used to compare the
accuracy of different forecasting methods as related to the characteristics of
a time series dataset. The ForecastTB is a plug-and-play structured module, and
several forecasting methods can be included with simple instructions. The
proposed test-bench is not limited to the default forecasting and error metric
functions, and users are able to append, remove, or choose the desired methods
as per requirements. Besides, several plotting functions and statistical
performance metrics are provided to visualize the comparative performance and
accuracy of different forecasting methods. Furthermore, this paper presents
real application examples with natural time series datasets (i.e., wind speed
and solar radiation) to exhibit the features of the ForecastTB package to
evaluate forecasting comparison analysis as affected by the characteristics of
a dataset. Modeling results indicated the applicability and robustness of the
proposed R package ForecastTB for time series forecasting.
",paper introduce r package forecasttb use compare accuracy different forecasting method relate characteristic time series dataset forecasttb plug and play structured module several forecasting method include simple instruction propose test bench limited default forecasting error metric function user able append remove choose desire method per requirement besides several plotting function statistical performance metric provide visualize comparative performance accuracy different forecasting method furthermore paper present real application example natural time series dataset wind speed solar radiation exhibit feature forecasttb package evaluate forecasting comparison analysis affect characteristic dataset modeling result indicate applicability robustness propose r package forecasttb time series forecast
"Coresets for constrained k-median and k-means clustering in low
  dimensional Euclidean space","  We study (Euclidean) $k$-median and $k$-means with constraints in the
streaming model.
  There have been recent efforts to design unified algorithms to solve
constrained $k$-means problems without using knowledge of the specific
constraint at hand aside from mild assumptions like the polynomial
computability of feasibility under the constraint (compute if a clustering
satisfies the constraint) or the presence of an efficient assignment oracle
(given a set of centers, produce an optimal assignment of points to the centers
which satisfies the constraint). These algorithms have a running time
exponential in $k$, but can be applied to a wide range of constraints.
  We demonstrate that a technique proposed in 2019 for solving a specific
constrained streaming $k$-means problem, namely fair $k$-means clustering,
actually implies streaming algorithms for all these constraints. These work for
low dimensional Euclidean space. [Note that there are more algorithms for
streaming fair $k$-means today, in particular they exist for high dimensional
spaces now as well.]
",study euclidean k -median k -mean constraint stream model recent effort design unified algorithm solve constrain k -mean problem without use knowledge specific constraint hand aside mild assumption like polynomial computability feasibility constraint compute cluster satisfie constraint presence efficient assignment oracle give set center produce optimal assignment point center satisfie constraint algorithm run time exponential k apply wide range constraint demonstrate technique propose 2019 solve specific constrain streaming k -mean problem namely fair k -mean cluster actually imply streaming algorithm constraint work low dimensional euclidean space note algorithm stream fair k -mean today particular exist high dimensional space well
Neural Network Approach to Construction of Classical Integrable Systems,"  Integrable systems have provided various insights into physical phenomena and
mathematics. The way of constructing many-body integrable systems is limited to
few ansatzes for the Lax pair, except for highly inventive findings of
conserved quantities. Machine learning techniques have recently been applied to
broad physics fields and proven powerful for building non-trivial
transformations and potential functions. We here propose a machine learning
approach to a systematic construction of classical integrable systems. Given
the Hamiltonian or samples in latent space, our neural network simultaneously
learns the corresponding natural Hamiltonian in real space and the canonical
transformation between the latent space and the real space variables. We also
propose a loss function for building integrable systems and demonstrate
successful unsupervised learning for the Toda lattice. Our approach enables
exploring new integrable systems without any prior knowledge about the
canonical transformation or any ansatz for the Lax pair.
",integrable system provide various insight physical phenomena mathematic way construct many body integrable system limit ansatze lax pair except highly inventive finding conserved quantitie machine learn technique recently apply broad physics field prove powerful build non trivial transformation potential function propose machine learning approach systematic construction classical integrable system give hamiltonian sample latent space neural network simultaneously learn correspond natural hamiltonian real space canonical transformation latent space real space variable also propose loss function build integrable system demonstrate successful unsupervised learn toda lattice approach enable explore new integrable system without prior knowledge canonical transformation ansatz lax pair
STRODE: Stochastic Boundary Ordinary Differential Equation,"  Perception of time from sequentially acquired sensory inputs is rooted in
everyday behaviors of individual organisms. Yet, most algorithms for
time-series modeling fail to learn dynamics of random event timings directly
from visual or audio inputs, requiring timing annotations during training that
are usually unavailable for real-world applications. For instance, neuroscience
perspectives on postdiction imply that there exist variable temporal ranges
within which the incoming sensory inputs can affect the earlier perception, but
such temporal ranges are mostly unannotated for real applications such as
automatic speech recognition (ASR). In this paper, we present a probabilistic
ordinary differential equation (ODE), called STochastic boundaRy ODE (STRODE),
that learns both the timings and the dynamics of time series data without
requiring any timing annotations during training. STRODE allows the usage of
differential equations to sample from the posterior point processes,
efficiently and analytically. We further provide theoretical guarantees on the
learning of STRODE. Our empirical results show that our approach successfully
infers event timings of time series data. Our method achieves competitive or
superior performances compared to existing state-of-the-art methods for both
synthetic and real-world datasets.
",perception time sequentially acquire sensory input root everyday behavior individual organism yet algorithm time series modeling fail learn dynamic random event timing directly visual audio input require timing annotation training usually unavailable real world application instance neuroscience perspective postdiction imply exist variable temporal range within incoming sensory input affect early perception temporal range mostly unannotate real application automatic speech recognition asr paper present probabilistic ordinary differential equation ode call stochastic boundary ode strode learn timing dynamic time series datum without require timing annotation training strode allow usage differential equation sample posterior point process efficiently analytically provide theoretical guarantee learn strode empirical result show approach successfully infer event timing time series datum method achieve competitive superior performance compare exist state of the art method synthetic real world dataset
"Reduced-Dimensional Reinforcement Learning Control using Singular
  Perturbation Approximations","  We present a set of model-free, reduced-dimensional reinforcement learning
(RL) based optimal control designs for linear time-invariant singularly
perturbed (SP) systems. We first present a state-feedback and output-feedback
based RL control design for a generic SP system with unknown state and input
matrices. We take advantage of the underlying time-scale separation property of
the plant to learn a linear quadratic regulator (LQR) for only its slow
dynamics, thereby saving a significant amount of learning time compared to the
conventional full-dimensional RL controller. We analyze the sub-optimality of
the design using SP approximation theorems and provide sufficient conditions
for closed-loop stability. Thereafter, we extend both designs to clustered
multi-agent consensus networks, where the SP property reflects through
clustering. We develop both centralized and cluster-wise block-decentralized RL
controllers for such networks, in reduced dimensions. We demonstrate the
details of the implementation of these controllers using simulations of
relevant numerical examples and compare them with conventional RL designs to
show the computational benefits of our approach.
",present set model free reduced dimensional reinforcement learning rl base optimal control design linear time invariant singularly perturb sp system first present state feedback output feedback base rl control design generic sp system unknown state input matrix take advantage underlie time scale separation property plant learn linear quadratic regulator lqr slow dynamic thereby save significant amount learn time compare conventional full dimensional rl controller analyze sub optimality design use sp approximation theorem provide sufficient condition close loop stability thereafter extend design cluster multi agent consensus network sp property reflect cluster develop centralized cluster wise block decentralize rl controller network reduce dimension demonstrate detail implementation controller use simulation relevant numerical example compare conventional rl design show computational benefit approach
"Semantic RL with Action Grammars: Data-Efficient Learning of
  Hierarchical Task Abstractions","  Hierarchical Reinforcement Learning algorithms have successfully been applied
to temporal credit assignment problems with sparse reward signals. However,
state-of-the-art algorithms require manual specification of sub-task
structures, a sample inefficient exploration phase or lack semantic
interpretability. Humans, on the other hand, efficiently detect hierarchical
sub-structures induced by their surroundings. It has been argued that this
inference process universally applies to language, logical reasoning as well as
motor control. Therefore, we propose a cognitive-inspired Reinforcement
Learning architecture which uses grammar induction to identify sub-goal
policies. By treating an on-policy trajectory as a sentence sampled from the
policy-conditioned language of the environment, we identify hierarchical
constituents with the help of unsupervised grammatical inference. The resulting
set of temporal abstractions is called action grammar (Pastra & Aloimonos,
2012) and unifies symbolic and connectionist approaches to Reinforcement
Learning. It can be used to facilitate efficient imitation, transfer and online
learning.
",hierarchical reinforcement learning algorithm successfully apply temporal credit assignment problem sparse reward signal however state of the art algorithm require manual specification sub task structure sample inefficient exploration phase lack semantic interpretability human hand efficiently detect hierarchical sub structure induce surrounding argue inference process universally apply language logical reasoning well motor control therefore propose cognitive inspire reinforcement learn architecture use grammar induction identify sub goal policy treat on policy trajectory sentence sample policy condition language environment identify hierarchical constituent help unsupervised grammatical inference result set temporal abstraction call action grammar pastra aloimonos 2012 unify symbolic connectionist approach reinforcement learning use facilitate efficient imitation transfer online learning
"Fluctuations, Bias, Variance & Ensemble of Learners: Exact Asymptotics
  for Convex Losses in High-Dimension","  From the sampling of data to the initialisation of parameters, randomness is
ubiquitous in modern Machine Learning practice. Understanding the statistical
fluctuations engendered by the different sources of randomness in prediction is
therefore key to understanding robust generalisation. In this manuscript we
develop a quantitative and rigorous theory for the study of fluctuations in an
ensemble of generalised linear models trained on different, but correlated,
features in high-dimensions. In particular, we provide a complete description
of the asymptotic joint distribution of the empirical risk minimiser for
generic convex loss and regularisation in the high-dimensional limit. Our
result encompasses a rich set of classification and regression tasks, such as
the lazy regime of overparametrised neural networks, or equivalently the random
features approximation of kernels. While allowing to study directly the
mitigating effect of ensembling (or bagging) on the bias-variance decomposition
of the test error, our analysis also helps disentangle the contribution of
statistical fluctuations, and the singular role played by the interpolation
threshold that are at the roots of the ""double-descent"" phenomenon.
",sample datum initialisation parameter randomness ubiquitous modern machine learning practice understand statistical fluctuation engender different source randomness prediction therefore key understanding robust generalisation manuscript develop quantitative rigorous theory study fluctuation ensemble generalised linear model train different correlated feature high dimensions particular provide complete description asymptotic joint distribution empirical risk minimiser generic convex loss regularisation high dimensional limit result encompass rich set classification regression task lazy regime overparametrise neural network equivalently random feature approximation kernel allow study directly mitigate effect ensemble bagging bias variance decomposition test error analysis also help disentangle contribution statistical fluctuation singular role play interpolation threshold root double descent phenomenon
"A Family of Exact Goodness-of-Fit Tests for High-Dimensional Discrete
  Distributions","  The objective of goodness-of-fit testing is to assess whether a dataset of
observations is likely to have been drawn from a candidate probability
distribution. This paper presents a rank-based family of goodness-of-fit tests
that is specialized to discrete distributions on high-dimensional domains. The
test is readily implemented using a simulation-based, linear-time procedure.
The testing procedure can be customized by the practitioner using knowledge of
the underlying data domain. Unlike most existing test statistics, the proposed
test statistic is distribution-free and its exact (non-asymptotic) sampling
distribution is known in closed form. We establish consistency of the test
against all alternatives by showing that the test statistic is distributed as a
discrete uniform if and only if the samples were drawn from the candidate
distribution. We illustrate its efficacy for assessing the sample quality of
approximate sampling algorithms over combinatorially large spaces with
intractable probabilities, including random partitions in Dirichlet process
mixture models and random lattices in Ising models.
",objective goodness of fit testing assess whether dataset observation likely draw candidate probability distribution paper present rank base family goodness of fit test specialize discrete distribution high dimensional domain test readily implement use simulation base linear time procedure testing procedure customize practitioner use knowledge underlie datum domain unlike exist test statistic propose test statistic distribution free exact non asymptotic sampling distribution know closed form establish consistency test alternative show test statistic distribute discrete uniform sample draw candidate distribution illustrate efficacy assess sample quality approximate sampling algorithm combinatorially large space intractable probability include random partition dirichlet process mixture model random lattice ise model
Conservative Wasserstein Training for Pose Estimation,"  This paper targets the task with discrete and periodic class labels ($e.g.,$
pose/orientation estimation) in the context of deep learning. The commonly used
cross-entropy or regression loss is not well matched to this problem as they
ignore the periodic nature of the labels and the class similarity, or assume
labels are continuous value. We propose to incorporate inter-class correlations
in a Wasserstein training framework by pre-defining ($i.e.,$ using arc length
of a circle) or adaptively learning the ground metric. We extend the ground
metric as a linear, convex or concave increasing function $w.r.t.$ arc length
from an optimization perspective. We also propose to construct the conservative
target labels which model the inlier and outlier noises using a wrapped
unimodal-uniform mixture distribution. Unlike the one-hot setting, the
conservative label makes the computation of Wasserstein distance more
challenging. We systematically conclude the practical closed-form solution of
Wasserstein distance for pose data with either one-hot or conservative target
label. We evaluate our method on head, body, vehicle and 3D object pose
benchmarks with exhaustive ablation studies. The Wasserstein loss obtaining
superior performance over the current methods, especially using convex mapping
function for ground metric, conservative label, and closed-form solution.
",paper target task discrete periodic class label estimation context deep learning commonly use cross entropy regression loss well match problem ignore periodic nature label class similarity assume label continuous value propose incorporate inter class correlation wasserstein training framework pre defining use arc length circle adaptively learn ground metric extend ground metric linear convex concave increase function arc length optimization perspective also propose construct conservative target label model inli outlier noise use wrap unimodal uniform mixture distribution unlike one hot set conservative label make computation wasserstein distance challenge systematically conclude practical closed form solution wasserstein distance pose datum either one hot conservative target label evaluate method head body vehicle 3d object pose benchmark exhaustive ablation study wasserstein loss obtain superior performance current method especially use convex mapping function grind metric conservative label closed form solution
Dueling Posterior Sampling for Preference-Based Reinforcement Learning,"  In preference-based reinforcement learning (RL), an agent interacts with the
environment while receiving preferences instead of absolute feedback. While
there is increasing research activity in preference-based RL, the design of
formal frameworks that admit tractable theoretical analysis remains an open
challenge. Building upon ideas from preference-based bandit learning and
posterior sampling in RL, we present DUELING POSTERIOR SAMPLING (DPS), which
employs preference-based posterior sampling to learn both the system dynamics
and the underlying utility function that governs the preference feedback. As
preference feedback is provided on trajectories rather than individual
state-action pairs, we develop a Bayesian approach for the credit assignment
problem, translating preferences to a posterior distribution over state-action
reward models. We prove an asymptotic Bayesian no-regret rate for DPS with a
Bayesian linear regression credit assignment model. This is the first regret
guarantee for preference-based RL to our knowledge. We also discuss possible
avenues for extending the proof methodology to other credit assignment models.
Finally, we evaluate the approach empirically, showing competitive performance
against existing baselines.
",preference base reinforcement learning rl agent interact environment receive preference instead absolute feedback increase research activity preference base rl design formal framework admit tractable theoretical analysis remain open challenge build upon idea preference base bandit learn posterior sampling rl present duel posterior sampling dps employ preference base posterior sampling learn system dynamic underlie utility function govern preference feedback preference feedback provide trajectory rather individual state action pair develop bayesian approach credit assignment problem translate preference posterior distribution state action reward model prove asymptotic bayesian no regret rate dps bayesian linear regression credit assignment model first regret guarantee preference base rl knowledge also discuss possible avenue extend proof methodology credit assignment model finally evaluate approach empirically show competitive performance exist baseline
"Generating Explanations from Deep Reinforcement Learning Using Episodic
  Memory","  Deep Reinforcement Learning (RL) involves the use of Deep Neural Networks
(DNNs) to make sequential decisions in order to maximize reward. For many tasks
the resulting sequence of actions produced by a Deep RL policy can be long and
difficult to understand for humans. A crucial component of human explanations
is selectivity, whereby only key decisions and causes are recounted. Imbuing
Deep RL agents with such an ability would make their resulting policies easier
to understand from a human perspective and generate a concise set of
instructions to aid the learning of future agents. To this end we use a Deep RL
agent with an episodic memory system to identify and recount key decisions
during policy execution. We show that these decisions form a short, human
readable explanation that can also be used to speed up the learning of naive
Deep RL agents in an algorithm-independent manner.
",deep reinforcement learning rl involve use deep neural network dnn make sequential decision order maximize reward many task result sequence action produce deep rl policy long difficult understand human crucial component human explanation selectivity whereby key decision cause recount imbue deep rl agent ability would make result policy easy understand human perspective generate concise set instruction aid learn future agent end use deep rl agent episodic memory system identify recount key decision policy execution show decision form short human readable explanation also use speed learn naive deep rl agent algorithm independent manner
A Framework for Fairer Machine Learning in Organizations,"  With the increase in adoption of machine learning tools by organizations
risks of unfairness abound, especially when human decision processes in
outcomes of socio-economic importance such as hiring, housing, lending, and
admissions are automated. We reveal sources of unfair machine learning, review
fairness criteria, and provide a framework which, if implemented, would enable
an organization to both avoid implementing an unfair machine learning model,
but also to avoid the common situation that as an algorithm learns with more
data it can become unfair over time. Issues of behavioral ethics in machine
learning implementations by organizations have not been thoroughly addressed in
the literature, because many of the necessary concepts are dispersed across
three literatures: ethics, machine learning, and management. Further, tradeoffs
between fairness criteria in machine learning have not been addressed with
regards to organizations. We advance the research by introducing an organizing
framework for selecting and implementing fair algorithms in organizations.
",increase adoption machine learning tool organization risk unfairness abound especially human decision process outcomes socio economic importance hire housing lending admission automate reveal source unfair machine learn review fairness criterion provide framework implement would enable organization avoid implement unfair machine learning model also avoid common situation algorithm learn datum become unfair time issue behavioral ethic machine learn implementation organization thoroughly address literature many necessary concept disperse across three literature ethic machine learn management tradeoff fairness criterion machine learning address regard organization advance research introduce organize framework select implement fair algorithm organization
Disentangle-based Continual Graph Representation Learning,"  Graph embedding (GE) methods embed nodes (and/or edges) in graph into a
low-dimensional semantic space, and have shown its effectiveness in modeling
multi-relational data. However, existing GE models are not practical in
real-world applications since it overlooked the streaming nature of incoming
data. To address this issue, we study the problem of continual graph
representation learning which aims to continually train a GE model on new data
to learn incessantly emerging multi-relational data while avoiding
catastrophically forgetting old learned knowledge. Moreover, we propose a
disentangle-based continual graph representation learning (DiCGRL) framework
inspired by the human's ability to learn procedural knowledge. The experimental
results show that DiCGRL could effectively alleviate the catastrophic
forgetting problem and outperform state-of-the-art continual learning models.
",graph embed ge method embed node edge graph low dimensional semantic space show effectiveness model multi relational datum however exist ge model practical real world application since overlook stream nature incoming datum address issue study problem continual graph representation learning aims continually train ge model new datum learn incessantly emerge multi relational datum avoid catastrophically forget old learn knowledge moreover propose disentangle base continual graph representation learn dicgrl framework inspire human ability learn procedural knowledge experimental result show dicgrl could effectively alleviate catastrophic forgetting problem outperform state of the art continual learning model
"Label Propagation with Augmented Anchors: A Simple Semi-Supervised
  Learning baseline for Unsupervised Domain Adaptation","  Motivated by the problem relatedness between unsupervised domain adaptation
(UDA) and semi-supervised learning (SSL), many state-of-the-art UDA methods
adopt SSL principles (e.g., the cluster assumption) as their learning
ingredients. However, they tend to overlook the very domain-shift nature of
UDA. In this work, we take a step further to study the proper extensions of SSL
techniques for UDA. Taking the algorithm of label propagation (LP) as an
example, we analyze the challenges of adopting LP to UDA and theoretically
analyze the conditions of affinity graph/matrix construction in order to
achieve better propagation of true labels to unlabeled instances. Our analysis
suggests a new algorithm of Label Propagation with Augmented Anchors (A$^2$LP),
which could potentially improve LP via generation of unlabeled virtual
instances (i.e., the augmented anchors) with high-confidence label predictions.
To make the proposed A$^2$LP useful for UDA, we propose empirical schemes to
generate such virtual instances. The proposed schemes also tackle the
domain-shift challenge of UDA by alternating between pseudo labeling via
A$^2$LP and domain-invariant feature learning. Experiments show that such a
simple SSL extension improves over representative UDA methods of
domain-invariant feature learning, and could empower two state-of-the-art
methods on benchmark UDA datasets. Our results show the value of further
investigation on SSL techniques for UDA problems.
",motivated problem relatedness unsupervised domain adaptation uda semi supervised learning ssl many state of the art uda method adopt ssl principle cluster assumption learn ingredient however tend overlook domain shift nature uda work take step study proper extension ssl technique uda take algorithm label propagation lp example analyze challenge adopt lp uda theoretically analyze condition affinity construction order achieve well propagation true label unlabeled instance analysis suggest new algorithm label propagation augment anchor lp could potentially improve lp via generation unlabeled virtual instance augment anchor high confidence label prediction make propose lp useful uda propose empirical scheme generate virtual instance propose scheme also tackle domain shift challenge uda alternate pseudo label via lp domain invariant feature learn experiment show simple ssl extension improve representative uda method domain invariant feature learning could empower two state of the art method benchmark uda dataset result show value investigation ssl technique uda problem
Deeply Supervised Active Learning for Finger Bones Segmentation,"  Segmentation is a prerequisite yet challenging task for medical image
analysis. In this paper, we introduce a novel deeply supervised active learning
approach for finger bones segmentation. The proposed architecture is fine-tuned
in an iterative and incremental learning manner. In each step, the deep
supervision mechanism guides the learning process of hidden layers and selects
samples to be labeled. Extensive experiments demonstrated that our method
achieves competitive segmentation results using less labeled samples as
compared with full annotation.
",segmentation prerequisite yet challenge task medical image analysis paper introduce novel deeply supervise active learning approach finger bone segmentation propose architecture fine tune iterative incremental learning manner step deep supervision mechanism guide learn process hide layer select sample label extensive experiment demonstrate method achieve competitive segmentation result use less label sample compare full annotation
Beyond Fine Tuning: A Modular Approach to Learning on Small Data,"  In this paper we present a technique to train neural network models on small
amounts of data. Current methods for training neural networks on small amounts
of rich data typically rely on strategies such as fine-tuning a pre-trained
neural network or the use of domain-specific hand-engineered features. Here we
take the approach of treating network layers, or entire networks, as modules
and combine pre-trained modules with untrained modules, to learn the shift in
distributions between data sets. The central impact of using a modular approach
comes from adding new representations to a network, as opposed to replacing
representations via fine-tuning. Using this technique, we are able surpass
results using standard fine-tuning transfer learning approaches, and we are
also able to significantly increase performance over such approaches when using
smaller amounts of data.
",paper present technique train neural network model small amount datum current method train neural network small amount rich datum typically rely strategy fine tune pre train neural network use domain specific hand engineer feature take approach treat network layer entire network module combine pre train module untraine module learn shift distribution datum set central impact use modular approach come add new representation network oppose replace representation via fine tuning use technique able surpass result use standard fine tune transfer learning approach also able significantly increase performance approach use small amount datum
Reinventing 2D Convolutions for 3D Images,"  There have been considerable debates over 2D and 3D representation learning
on 3D medical images. 2D approaches could benefit from large-scale 2D
pretraining, whereas they are generally weak in capturing large 3D contexts. 3D
approaches are natively strong in 3D contexts, however few publicly available
3D medical dataset is large and diverse enough for universal 3D pretraining.
Even for hybrid (2D + 3D) approaches, the intrinsic disadvantages within the 2D
/ 3D parts still exist. In this study, we bridge the gap between 2D and 3D
convolutions by reinventing the 2D convolutions. We propose ACS
(axial-coronal-sagittal) convolutions to perform natively 3D representation
learning, while utilizing the pretrained weights on 2D datasets. In ACS
convolutions, 2D convolution kernels are split by channel into three parts, and
convoluted separately on the three views (axial, coronal and sagittal) of 3D
representations. Theoretically, ANY 2D CNN (ResNet, DenseNet, or DeepLab) is
able to be converted into a 3D ACS CNN, with pretrained weight of a same
parameter size. Extensive experiments on several medical benchmarks (including
classification, segmentation and detection tasks) validate the consistent
superiority of the pretrained ACS CNNs, over the 2D / 3D CNN counterparts with
/ without pretraining. Even without pretraining, the ACS convolution can be
used as a plug-and-play replacement of standard 3D convolution, with smaller
model size and less computation.
",considerable debate 2d 3d representation learn 3d medical image 2d approach could benefit large scale 2d pretraining whereas generally weak capture large 3d context 3d approach natively strong 3d context however publicly available 3d medical dataset large diverse enough universal 3d pretraine even hybrid 2d 3d approach intrinsic disadvantage within 2d 3d part still exist study bridge gap 2d 3d convolution reinvent 2d convolution propose acs axial coronal sagittal convolution perform natively 3d representation learning utilize pretraine weight 2d dataset acs convolution 2d convolution kernels split channel three part convolute separately three view axial coronal sagittal 3d representation theoretically 2d cnn resnet densenet deeplab able convert 3d acs cnn pretraine weight parameter size extensive experiment several medical benchmark include classification segmentation detection task validate consistent superiority pretraine acs cnn 2d 3d cnn counterpart without pretraine even without pretraine acs convolution use plug and play replacement standard 3d convolution small model size less computation
Dynamic Topology Adaptation and Distributed Estimation for Smart Grids,"  This paper presents new dynamic topology adaptation strategies for
distributed estimation in smart grids systems. We propose a dynamic exhaustive
search--based topology adaptation algorithm and a dynamic sparsity--inspired
topology adaptation algorithm, which can exploit the topology of smart grids
with poor--quality links and obtain performance gains. We incorporate an
optimized combining rule, named Hastings rule into our proposed dynamic
topology adaptation algorithms. Compared with the existing works in the
literature on distributed estimation, the proposed algorithms have a better
convergence rate and significantly improve the system performance. The
performance of the proposed algorithms is compared with that of existing
algorithms in the IEEE 14--bus system.
",paper present new dynamic topology adaptation strategy distribute estimation smart grid system propose dynamic exhaustive search base topology adaptation algorithm dynamic sparsity inspire topology adaptation algorithm exploit topology smart grid poor quality link obtain performance gain incorporate optimize combine rule name hastings rule propose dynamic topology adaptation algorithm compare exist work literature distribute estimation propose algorithm well convergence rate significantly improve system performance performance propose algorithm compare exist algorithm ieee 14 bus system
"Supervised Neural Networks for Illiquid Alternative Asset Cash Flow
  Forecasting","  Institutional investors have been increasing the allocation of the illiquid
alternative assets such as private equity funds in their portfolios, yet there
exists a very limited literature on cash flow forecasting of illiquid
alternative assets. The net cash flow of private equity funds typically follow
a J-curve pattern, however the timing and the size of the contributions and
distributions depend on the investment opportunities. In this paper, we develop
a benchmark model and present two novel approaches (direct vs. indirect) to
predict the cash flows of private equity funds. We introduce a sliding window
approach to apply on our cash flow data because different vintage year funds
contain different lengths of cash flow information. We then pass the data to an
LSTM/ GRU model to predict the future cash flows either directly or indirectly
(based on the benchmark model). We further integrate macroeconomic indicators
into our data, which allows us to consider the impact of market environment on
cash flows and to apply stress testing. Our results indicate that the direct
model is easier to implement compared to the benchmark model and the indirect
model, but still the predicted cash flows align better with the actual cash
flows. We also show that macroeconomic variables improve the performance of the
direct model whereas the impact is not obvious on the indirect model.
",institutional investor increase allocation illiquid alternative asset private equity fund portfolio yet exist limited literature cash flow forecasting illiquid alternative asset net cash flow private equity fund typically follow j curve pattern however time size contribution distribution depend investment opportunity paper develop benchmark model present two novel approach direct indirect predict cash flow private equity fund introduce slide window approach apply cash flow datum different vintage year fund contain different length cash flow information pass datum gru model predict future cash flow either directly indirectly base benchmark model integrate macroeconomic indicator datum allow we consider impact market environment cash flow apply stress testing result indicate direct model easy implement compare benchmark model indirect model still predict cash flow align well actual cash flow also show macroeconomic variable improve performance direct model whereas impact obvious indirect model
Safe Exploration for Identifying Linear Systems via Robust Optimization,"  Safely exploring an unknown dynamical system is critical to the deployment of
reinforcement learning (RL) in physical systems where failures may have
catastrophic consequences. In scenarios where one knows little about the
dynamics, diverse transition data covering relevant regions of state-action
space is needed to apply either model-based or model-free RL. Motivated by the
cooling of Google's data centers, we study how one can safely identify the
parameters of a system model with a desired accuracy and confidence level. In
particular, we focus on learning an unknown linear system with Gaussian noise
assuming only that, initially, a nominal safe action is known. Define safety as
satisfying specific linear constraints on the state space (e.g., requirements
on process variable) that must hold over the span of an entire trajectory, and
given a Probably Approximately Correct (PAC) style bound on the estimation
error of model parameters, we show how to compute safe regions of action space
by gradually growing a ball around the nominal safe action. One can apply any
exploration strategy where actions are chosen from such safe regions.
Experiments on a stylized model of data center cooling dynamics show how
computing proper safe regions can increase the sample efficiency of safe
exploration.
",safely explore unknown dynamical system critical deployment reinforcement learning rl physical system failure may catastrophic consequence scenario one know little dynamic diverse transition datum cover relevant region state action space need apply either model base model free rl motivate cool google datum center study one safely identify parameter system model desire accuracy confidence level particular focus learn unknown linear system gaussian noise assume initially nominal safe action know define safety satisfy specific linear constraint state space requirement process variable must hold span entire trajectory give probably approximately correct pac style bind estimation error model parameter show compute safe region action space gradually grow ball around nominal safe action one apply exploration strategy action choose safe region experiment stylize model datum center cool dynamic show compute proper safe region increase sample efficiency safe exploration
"Model-based clustering with Hidden Markov Model regression for time
  series with regime changes","  This paper introduces a novel model-based clustering approach for clustering
time series which present changes in regime. It consists of a mixture of
polynomial regressions governed by hidden Markov chains. The underlying hidden
process for each cluster activates successively several polynomial regimes
during time. The parameter estimation is performed by the maximum likelihood
method through a dedicated Expectation-Maximization (EM) algorithm. The
proposed approach is evaluated using simulated time series and real-world time
series issued from a railway diagnosis application. Comparisons with existing
approaches for time series clustering, including the stand EM for Gaussian
mixtures, $K$-means clustering, the standard mixture of regression models and
mixture of Hidden Markov Models, demonstrate the effectiveness of the proposed
approach.
",paper introduce novel model base clustering approach cluster time series present change regime consist mixture polynomial regression govern hide markov chain underlie hidden process cluster activate successively several polynomial regime time parameter estimation perform maximum likelihood method dedicated expectation maximization em algorithm propose approach evaluate use simulated time series real world time series issue railway diagnosis application comparison exist approach time series cluster including stand em gaussian mixture k -mean cluster standard mixture regression model mixture hide markov model demonstrate effectiveness propose approach
"Evaluating Deep Vs. Wide & Deep Learners As Contextual Bandits For
  Personalized Email Promo Recommendations","  Personalization enables businesses to learn customer preferences from past
interactions and thus to target individual customers with more relevant
content. We consider the problem of predicting the optimal promotional offer
for a given customer out of several options as a contextual bandit problem.
Identifying information for the customer and/or the campaign can be used to
deduce unknown customer/campaign features that improve optimal offer
prediction. Using a generated synthetic email promo dataset, we demonstrate
similar prediction accuracies for (a) a wide and deep network that takes
identifying information (or other categorical features) as input to the wide
part and (b) a deep-only neural network that includes embeddings of categorical
features in the input. Improvements in accuracy from including categorical
features depends on the variability of the unknown numerical features for each
category. We also show that selecting options using upper confidence bound or
Thompson sampling, approximated via Monte Carlo dropout layers in the wide and
deep models, slightly improves model performance.
",personalization enable business learn customer preference past interaction thus target individual customer relevant content consider problem predict optimal promotional offer give customer several option contextual bandit problem identify information customer campaign use deduce unknown feature improve optimal offer prediction use generate synthetic email promo dataset demonstrate similar prediction accuracy wide deep network take identify information categorical feature input wide part b deep only neural network include embedding categorical feature input improvement accuracy include categorical feature depend variability unknown numerical feature category also show select option use upper confidence bind thompson sampling approximate via monte carlo dropout layer wide deep model slightly improve model performance
"On the Power and Limitations of Random Features for Understanding Neural
  Networks","  Recently, a spate of papers have provided positive theoretical results for
training over-parameterized neural networks (where the network size is larger
than what is needed to achieve low error). The key insight is that with
sufficient over-parameterization, gradient-based methods will implicitly leave
some components of the network relatively unchanged, so the optimization
dynamics will behave as if those components are essentially fixed at their
initial random values. In fact, fixing these explicitly leads to the well-known
approach of learning with random features. In other words, these techniques
imply that we can successfully learn with neural networks, whenever we can
successfully learn with random features. In this paper, we first review these
techniques, providing a simple and self-contained analysis for one-hidden-layer
networks. We then argue that despite the impressive positive results, random
feature approaches are also inherently limited in what they can explain. In
particular, we rigorously show that random features cannot be used to learn
even a single ReLU neuron with standard Gaussian inputs, unless the network
size (or magnitude of the weights) is exponentially large. Since a single
neuron is learnable with gradient-based methods, we conclude that we are still
far from a satisfying general explanation for the empirical success of neural
networks.
",recently spate paper provide positive theoretical result train over parameterize neural network network size large need achieve low error key insight sufficient over parameterization gradient base method implicitly leave component network relatively unchanged optimization dynamic behave component essentially fix initial random value fact fix explicitly lead well know approach learn random feature word technique imply successfully learn neural network whenever successfully learn random feature paper first review technique provide simple self contain analysis one hide layer network argue despite impressive positive result random feature approach also inherently limit explain particular rigorously show random feature use learn even single relu neuron standard gaussian input unless network size magnitude weight exponentially large since single neuron learnable gradient base method conclude still far satisfy general explanation empirical success neural network
"Texel-Att: Representing and Classifying Element-based Textures by
  Attributes","  Element-based textures are a kind of texture formed by nameable elements, the
texels [1], distributed according to specific statistical distributions; it is
of primary importance in many sectors, namely textile, fashion and interior
design industry. State-of-theart texture descriptors fail to properly
characterize element-based texture, so we present Texel-Att to fill this gap.
Texel-Att is the first fine-grained, attribute-based representation and
classification framework for element-based textures. It first individuates
texels, characterizing them with individual attributes; subsequently, texels
are grouped and characterized through layout attributes, which give the
Texel-Att representation. Texels are detected by a Mask-RCNN, trained on a
brand-new element-based texture dataset, ElBa, containing 30K texture images
with 3M fully-annotated texels. Examples of individual and layout attributes
are exhibited to give a glimpse on the level of achievable graininess. In the
experiments, we present detection results to show that texels can be precisely
individuated, even on textures ""in the wild""; to this sake, we individuate the
element-based classes of the Describable Texture Dataset (DTD), where almost
900K texels have been manually annotated, leading to the Element-based DTD
(E-DTD). Subsequently, classification and ranking results demonstrate the
expressivity of Texel-Att on ElBa and E-DTD, overcoming the alternative
features and relative attributes, doubling the best performance in some cases;
finally, we report interactive search results on ElBa and E-DTD: with Texel-Att
on the E-DTD dataset we are able to individuate within 10 iterations the
desired texture in the 90% of cases, against the 71% obtained with a
combination of the finest existing attributes so far. Dataset and code is
available at https://github.com/godimarcovr/Texel-Att
",element base texture kind texture form nameable element texel 1 distribute accord specific statistical distribution primary importance many sector namely textile fashion interior design industry state of theart texture descriptor fail properly characterize element base texture present texel att fill gap texel att first fine grain attribute base representation classification framework element base texture first individuate texel characterize individual attribute subsequently texel group characterize layout attribute give texel att representation texel detect mask rcnn train brand new element base texture dataset elba contain 30k texture image 3 m fully annotate texel example individual layout attribute exhibit give glimpse level achievable graininess experiment present detection result show texel precisely individuate even texture wild sake individuate element base class describable texture dataset dtd almost 900k texel manually annotate lead element base dtd e dtd subsequently classification rank result demonstrate expressivity texel att elba e dtd overcome alternative feature relative attribute double good performance case finally report interactive search result elba e dtd texel att e dtd dataset able individuate within 10 iteration desire texture 90 case 71 obtain combination fine exist attribute far dataset code available https
Concentration and Confidence for Discrete Bayesian Sequence Predictors,"  Bayesian sequence prediction is a simple technique for predicting future
symbols sampled from an unknown measure on infinite sequences over a countable
alphabet. While strong bounds on the expected cumulative error are known, there
are only limited results on the distribution of this error. We prove tight
high-probability bounds on the cumulative error, which is measured in terms of
the Kullback-Leibler (KL) divergence. We also consider the problem of
constructing upper confidence bounds on the KL and Hellinger errors similar to
those constructed from Hoeffding-like bounds in the i.i.d. case. The new
results are applied to show that Bayesian sequence prediction can be used in
the Knows What It Knows (KWIK) framework with bounds that match the
state-of-the-art.
",bayesian sequence prediction simple technique predict future symbol sample unknown measure infinite sequence countable alphabet strong bound expect cumulative error know limited result distribution error prove tight high probability bound cumulative error measure term kullback leibler kl divergence also consider problem construct upper confidence bound kl hellinger error similar construct hoeffding like bound case new result apply show bayesian sequence prediction use know know kwik framework bound match state of the art
A Generalizable Approach to Learning Optimizers,"  A core issue with learning to optimize neural networks has been the lack of
generalization to real world problems. To address this, we describe a system
designed from a generalization-first perspective, learning to update optimizer
hyperparameters instead of model parameters directly using novel features,
actions, and a reward function. This system outperforms Adam at all neural
network tasks including on modalities not seen during training. We achieve 2x
speedups on ImageNet, and a 2.5x speedup on a language modeling task using over
5 orders of magnitude more compute than the training tasks.
",core issue learn optimize neural network lack generalization real world problem address describe system design generalization first perspective learning update optimizer hyperparameter instead model parameter directly use novel feature action reward function system outperform adam neural network task include modality see training achieve 2x speedup imagenet speedup language modeling task use 5 order magnitude compute training task
"Analysis of Hydrological and Suspended Sediment Events from Mad River
  Watershed using Multivariate Time Series Clustering","  Hydrological storm events are a primary driver for transporting water quality
constituents such as turbidity, suspended sediments and nutrients. Analyzing
the concentration (C) of these water quality constituents in response to
increased streamflow discharge (Q), particularly when monitored at high
temporal resolution during a hydrological event, helps to characterize the
dynamics and flux of such constituents. A conventional approach to storm event
analysis is to reduce the C-Q time series to two-dimensional (2-D) hysteresis
loops and analyze these 2-D patterns. While effective and informative to some
extent, this hysteresis loop approach has limitations because projecting the
C-Q time series onto a 2-D plane obscures detail (e.g., temporal variation)
associated with the C-Q relationships. In this paper, we address this issue
using a multivariate time series clustering approach. Clustering is applied to
sequences of river discharge and suspended sediment data (acquired through
turbidity-based monitoring) from six watersheds located in the Lake Champlain
Basin in the northeastern United States. While clusters of the hydrological
storm events using the multivariate time series approach were found to be
correlated to 2-D hysteresis loop classifications and watershed locations, the
clusters differed from the 2-D hysteresis classifications. Additionally, using
available meteorological data associated with storm events, we examine the
characteristics of computational clusters of storm events in the study
watersheds and identify the features driving the clustering approach.
",hydrological storm event primary driver transport water quality constituent turbidity suspend sediment nutrient analyze concentration c water quality constituent response increase streamflow discharge q particularly monitor high temporal resolution hydrological event help characterize dynamic flux constituent conventional approach storm event analysis reduce c q time series two dimensional 2 d hysteresis loop analyze 2 d pattern effective informative extent hysteresis loop approach limitation project c q time series onto 2 d plane obscure detail temporal variation associate c q relationship paper address issue use multivariate time series cluster approach cluster apply sequence river discharge suspend sediment datum acquire turbidity base monitor six watershed locate lake champlain basin northeastern united states cluster hydrological storm event use multivariate time series approach found correlate 2 d hysteresis loop classification watershe location cluster differ 2 d hysteresis classification additionally use available meteorological datum associate storm event examine characteristic computational cluster storm event study watershed identify feature drive clustering approach
QUBO Formulations for Training Machine Learning Models,"  Training machine learning models on classical computers is usually a time and
compute intensive process. With Moore's law coming to an end and ever
increasing demand for large-scale data analysis using machine learning, we must
leverage non-conventional computing paradigms like quantum computing to train
machine learning models efficiently. Adiabatic quantum computers like the
D-Wave 2000Q can approximately solve NP-hard optimization problems, such as the
quadratic unconstrained binary optimization (QUBO), faster than classical
computers. Since many machine learning problems are also NP-hard, we believe
adiabatic quantum computers might be instrumental in training machine learning
models efficiently in the post Moore's law era. In order to solve a problem on
adiabatic quantum computers, it must be formulated as a QUBO problem, which is
a challenging task in itself. In this paper, we formulate the training problems
of three machine learning models---linear regression, support vector machine
(SVM) and equal-sized k-means clustering---as QUBO problems so that they can be
trained on adiabatic quantum computers efficiently. We also analyze the time
and space complexities of our formulations and compare them to the
state-of-the-art classical algorithms for training these machine learning
models. We show that the time and space complexities of our formulations are
better (in the case of SVM and equal-sized k-means clustering) or equivalent
(in case of linear regression) to their classical counterparts.
",training machine learning model classical computer usually time compute intensive process moore law come end ever increase demand large scale datum analysis use machine learning must leverage non conventional computing paradigm like quantum computing train machine learning model efficiently adiabatic quantum computer like d wave 2000q approximately solve np hard optimization problem quadratic unconstrained binary optimization qubo fast classical computer since many machine learn problem also np hard believe adiabatic quantum computer might instrumental training machine learning model efficiently post moore law era order solve problem adiabatic quantum computer must formulate qubo problem challenge task paper formulate training problem three machine learning model -linear regression support vector machine svm equal sized k mean clustering -as qubo problem train adiabatic quantum computer efficiently also analyze time space complexity formulation compare state of the art classical algorithm training machine learning model show time space complexity formulation well case svm equal sized k mean cluster equivalent case linear regression classical counterpart
"Accelerated Computation of a High Dimensional Kolmogorov-Smirnov
  Distance","  Statistical testing is widespread and critical for a variety of scientific
disciplines. The advent of machine learning and the increase of computing power
has increased the interest in the analysis and statistical testing of
multidimensional data. We extend the powerful Kolmogorov-Smirnov two sample
test to a high dimensional form in a similar manner to Fasano (Fasano, 1987).
We call our result the d-dimensional Kolmogorov-Smirnov test (ddKS) and provide
three novel contributions therewith: we develop an analytical equation for the
significance of a given ddKS score, we provide an algorithm for computation of
ddKS on modern computing hardware that is of constant time complexity for small
sample sizes and dimensions, and we provide two approximate calculations of
ddKS: one that reduces the time complexity to linear at larger sample sizes,
and another that reduces the time complexity to linear with increasing
dimension. We perform power analysis of ddKS and its approximations on a corpus
of datasets and compare to other common high dimensional two sample tests and
distances: Hotelling's T^2 test and Kullback-Leibler divergence. Our ddKS test
performs well for all datasets, dimensions, and sizes tested, whereas the other
tests and distances fail to reject the null hypothesis on at least one dataset.
We therefore conclude that ddKS is a powerful multidimensional two sample test
for general use, and can be calculated in a fast and efficient manner using our
parallel or approximate methods. Open source implementations of all methods
described in this work are located at https://github.com/pnnl/ddks.
",statistical testing widespread critical variety scientific discipline advent machine learning increase computing power increase interest analysis statistical testing multidimensional datum extend powerful kolmogorov smirnov two sample test high dimensional form similar manner fasano fasano 1987 call result d dimensional kolmogorov smirnov test ddks provide three novel contribution therewith develop analytical equation significance give ddks score provide algorithm computation ddks modern compute hardware constant time complexity small sample size dimension provide two approximate calculation ddks one reduce time complexity linear large sample size another reduce time complexity linear increase dimension perform power analysis ddks approximations corpus dataset compare common high dimensional two sample test distance hotelle test kullback leibler divergence ddks test perform well dataset dimension size test whereas test distance fail reject null hypothesis least one dataset therefore conclude ddks powerful multidimensional two sample test general use calculate fast efficient manner use parallel approximate method open source implementation method describe work locate https
"AutoFCL: Automatically Tuning Fully Connected Layers for Handling Small
  Dataset","  Deep Convolutional Neural Networks (CNN) have evolved as popular machine
learning models for image classification during the past few years, due to
their ability to learn the problem-specific features directly from the input
images. The success of deep learning models solicits architecture engineering
rather than hand-engineering the features. However, designing state-of-the-art
CNN for a given task remains a non-trivial and challenging task, especially
when training data size is less. To address this phenomena, transfer learning
has been used as a popularly adopted technique. While transferring the learned
knowledge from one task to another, fine-tuning with the target-dependent Fully
Connected (FC) layers generally produces better results over the target task.
In this paper, the proposed AutoFCL model attempts to learn the structure of FC
layers of a CNN automatically using Bayesian optimization. To evaluate the
performance of the proposed AutoFCL, we utilize five pre-trained CNN models
such as VGG-16, ResNet, DenseNet, MobileNet, and NASNetMobile. The experiments
are conducted on three benchmark datasets, namely CalTech-101, Oxford-102
Flowers, and UC Merced Land Use datasets. Fine-tuning the newly learned
(target-dependent) FC layers leads to state-of-the-art performance, according
to the experiments carried out in this research. The proposed AutoFCL method
outperforms the existing methods over CalTech-101 and Oxford-102 Flowers
datasets by achieving the accuracy of 94.38% and 98.89%, respectively. However,
our method achieves comparable performance on the UC Merced Land Use dataset
with 96.83% accuracy. The source codes of this research are available at
https://github.com/shabbeersh/AutoFCL.
",deep convolutional neural network cnn evolve popular machine learning model image classification past year due ability learn problem specific feature directly input image success deep learning model solicit architecture engineering rather hand engineering feature however design state of the art cnn give task remain non trivial challenging task especially training datum size less address phenomena transfer learning use popularly adopt technique transferring learn knowledge one task another fine tune target dependent fully connect fc layer generally produce well result target task paper propose autofcl model attempt learn structure fc layers cnn automatically use bayesian optimization evaluate performance propose autofcl utilize five pre train cnn model vgg-16 resnet densenet mobilenet nasnetmobile experiment conduct three benchmark dataset namely caltech-101 oxford-102 flower uc merce land use dataset fine tuning newly learn target dependent fc layer lead state of the art performance accord experiment carry research propose autofcl method outperform exist method caltech-101 oxford-102 flower dataset achieve accuracy respectively however method achieve comparable performance uc merce land use dataset accuracy source code research available https
"Survival Seq2Seq: A Survival Model based on Sequence to Sequence
  Architecture","  This paper introduces a novel non-parametric deep model for estimating
time-to-event (survival analysis) in presence of censored data and competing
risks. The model is designed based on the sequence-to-sequence (Seq2Seq)
architecture, therefore we name it Survival Seq2Seq. The first recurrent neural
network (RNN) layer of the encoder of our model is made up of Gated Recurrent
Unit with Decay (GRU-D) cells. These cells have the ability to effectively
impute not-missing-at-random values of longitudinal datasets with very high
missing rates, such as electronic health records (EHRs). The decoder of
Survival Seq2Seq generates a probability distribution function (PDF) for each
competing risk without assuming any prior distribution for the risks. Taking
advantage of RNN cells, the decoder is able to generate smooth and virtually
spike-free PDFs. This is beyond the capability of existing non-parametric deep
models for survival analysis. Training results on synthetic and medical
datasets prove that Survival Seq2Seq surpasses other existing deep survival
models in terms of the accuracy of predictions and the quality of generated
PDFs.
",paper introduce novel non parametric deep model estimate time to event survival analysis presence censor datum compete risk model design base sequence to sequence seq2seq architecture therefore name survival seq2seq first recurrent neural network rnn layer encoder model make gate recurrent unit decay gru d cell cell ability effectively impute not miss at random value longitudinal dataset high miss rate electronic health record ehrs decoder survival seq2seq generate probability distribution function pdf compete risk without assume prior distribution risk take advantage rnn cell decoder able generate smooth virtually spike free pdfs beyond capability exist non parametric deep model survival analysis training result synthetic medical dataset prove survival seq2seq surpasse exist deep survival model term accuracy prediction quality generate pdfs
Mondrian Forest for Data Stream Classification Under Memory Constraints,"Supervised learning algorithms generally assume the availability of enough
memory to store their data model during the training and test phases. However,
in the Internet of Things, this assumption is unrealistic when data comes in
the form of infinite data streams, or when learning algorithms are deployed on
devices with reduced amounts of memory. In this paper, we adapt the online
Mondrian forest classification algorithm to work with memory constraints on
data streams. In particular, we design five out-of-memory strategies to update
Mondrian trees with new data points when the memory limit is reached. Moreover,
we design trimming mechanisms to make Mondrian trees more robust to concept
drifts under memory constraints. We evaluate our algorithms on a variety of
real and simulated datasets, and we conclude with recommendations on their use
in different situations: the Extend Node strategy appears as the best
out-of-memory strategy in all configurations, whereas different trimming
mechanisms should be adopted depending on whether a concept drift is expected.
All our methods are implemented in the OrpailleCC open-source library and are
ready to be used on embedded systems and connected objects.",supervise learning algorithm generally assume availability enough memory store datum model training test phase however internet thing assumption unrealistic datum come form infinite data stream learn algorithm deploy device reduce amount memory paper adapt online mondrian forest classification algorithm work memory constraint data stream particular design five out of memory strategy update mondrian tree new data point memory limit reach moreover design trimming mechanism make mondrian tree robust concept drift memory constraint evaluate algorithm variety real simulate dataset conclude recommendation use different situation extend node strategy appear good out of memory strategy configuration whereas different trimming mechanism adopt depend whether concept drift expect method implement orpaillecc open source library ready use embed system connect object
"NUQSGD: Provably Communication-efficient Data-parallel SGD via
  Nonuniform Quantization","  As the size and complexity of models and datasets grow, so does the need for
communication-efficient variants of stochastic gradient descent that can be
deployed to perform parallel model training. One popular
communication-compression method for data-parallel SGD is QSGD (Alistarh et
al., 2017), which quantizes and encodes gradients to reduce communication
costs. The baseline variant of QSGD provides strong theoretical guarantees,
however, for practical purposes, the authors proposed a heuristic variant which
we call QSGDinf, which demonstrated impressive empirical gains for distributed
training of large neural networks. In this paper, we build on this work to
propose a new gradient quantization scheme, and show that it has both stronger
theoretical guarantees than QSGD, and matches and exceeds the empirical
performance of the QSGDinf heuristic and of other compression methods.
",size complexity model dataset grow need communication efficient variant stochastic gradient descent deploy perform parallel model train one popular communication compression method data parallel sgd qsgd alistarh et 2017 quantize encode gradient reduce communication cost baseline variant qsgd provide strong theoretical guarantee however practical purpose author propose heuristic variant call qsgdinf demonstrate impressive empirical gain distribute train large neural network paper build work propose new gradient quantization scheme show strong theoretical guarantee qsgd match exceed empirical performance qsgdinf heuristic compression method
"Multi-source Learning via Completion of Block-wise Overlapping Noisy
  Matrices","  Matrix completion has attracted attention in many fields, including
statistics, applied mathematics, and electrical engineering. Most of the works
focus on the independent sampling models under which the observed entries are
sampled independently. Motivated by applications in the integration of
knowledge graphs derived from multi-source biomedical data such as those from
Electronic Health Records (EHR) and biomedical text, we propose the {\bf
B}lock-wise {\bf O}verlapping {\bf N}oisy {\bf M}atrix {\bf I}ntegration
(BONMI) to treat blockwise missingness of symmetric matrices representing
relatedness between entity pairs. Our idea is to exploit the orthogonal
Procrustes problem to align the eigenspace of the two sub-matrices, then
complete the missing blocks by the inner product of the two low-rank
components. Besides, we prove the statistical rate for the eigenspace of the
underlying matrix, which is comparable to the rate under the independently
missing assumption. Simulation studies show that the method performs well under
a variety of configurations. In the real data analysis, the method is applied
to two tasks: (i) the integrating of several point-wise mutual information
matrices built by English EHR and Chinese medical text data, and (ii) the
machine translation between English and Chinese medical concepts. Our method
shows an advantage over existing methods.
",matrix completion attract attention many field include statistic apply mathematics electrical engineering work focus independent sampling model observe entry sample independently motivated application integration knowledge graph derive multi source biomedical data electronic health record ehr biomedical text propose b lock wise verlappe n oisy atrix ntegration bonmi treat blockwise missingness symmetric matrix represent relatedness entity pair idea exploit orthogonal procruste problem align eigenspace two sub matrix complete miss block inner product two low rank component besides prove statistical rate eigenspace underlie matrix comparable rate independently miss assumption simulation study show method perform well variety configuration real datum analysis method apply two task integrate several point wise mutual information matrix build english ehr chinese medical text datum ii machine translation english chinese medical concept method show advantage exist method
"Neurochaos Feature Transformation and Classification for Imbalanced
  Learning","  Learning from limited and imbalanced data is a challenging problem in the
Artificial Intelligence community. Real-time scenarios demand decision-making
from rare events wherein the data are typically imbalanced. These situations
commonly arise in medical applications, cybersecurity, catastrophic predictions
etc. This motivates the development of learning algorithms capable of learning
from imbalanced data. Human brain effortlessly learns from imbalanced data.
Inspired by the chaotic neuronal firing in the human brain, a novel learning
algorithm namely Neurochaos Learning (NL) was recently proposed. NL is
categorized in three blocks: Feature Transformation, Neurochaos Feature
Extraction (CFX), and Classification. In this work, the efficacy of neurochaos
feature transformation and extraction for classification in imbalanced learning
is studied. We propose a unique combination of neurochaos based feature
transformation and extraction with traditional ML algorithms. The explored
datasets in this study revolve around medical diagnosis, banknote fraud
detection, environmental applications and spoken-digit classification. In this
study, experiments are performed in both high and low training sample regime.
In the former, five out of nine datasets have shown a performance boost in
terms of macro F1-score after using CFX features. The highest performance boost
obtained is 25.97% for Statlog (Heart) dataset using CFX+Decision Tree. In the
low training sample regime (from just one to nine training samples per class),
the highest performance boost of 144.38% is obtained for Haberman's Survival
dataset using CFX+Random Forest. NL offers enormous flexibility of combining
CFX with any ML classifier to boost its performance, especially for learning
tasks with limited and imbalanced data.
",learn limit imbalance datum challenge problem artificial intelligence community real time scenario demand decision make rare event wherein datum typically imbalance situation commonly arise medical application cybersecurity catastrophic prediction etc motivate development learning algorithm capable learning imbalance datum human brain effortlessly learn imbalance datum inspire chaotic neuronal fire human brain novel learn algorithm namely neurochaos learn nl recently propose nl categorize three block feature transformation neurochaos feature extraction cfx classification work efficacy neurochaos feature transformation extraction classification imbalance learning study propose unique combination neurochaos base feature transformation extraction traditional ml algorithm explore dataset study revolve around medical diagnosis banknote fraud detection environmental application speak digit classification study experiment perform high low training sample regime former five nine dataset show performance boost term macro f1 score use cfx feature high performance boost obtain statlog heart dataset use tree low training sample regime one nine training sample per class high performance boost obtain haberman survival dataset use forest nl offer enormous flexibility combine cfx ml classifier boost performance especially learn task limit imbalance datum
"Extension of TSVM to Multi-Class and Hierarchical Text Classification
  Problems With General Losses","  Transductive SVM (TSVM) is a well known semi-supervised large margin learning
method for binary text classification. In this paper we extend this method to
multi-class and hierarchical classification problems. We point out that the
determination of labels of unlabeled examples with fixed classifier weights is
a linear programming problem. We devise an efficient technique for solving it.
The method is applicable to general loss functions. We demonstrate the value of
the new method using large margin loss on a number of multi-class and
hierarchical classification datasets. For maxent loss we show empirically that
our method is better than expectation regularization/constraint and posterior
regularization methods, and competitive with the version of entropy
regularization method which uses label constraints.
",transductive svm tsvm well know semi supervised large margin learning method binary text classification paper extend method multi class hierarchical classification problem point determination label unlabeled example fix classifier weights linear programming problem devise efficient technique solve method applicable general loss function demonstrate value new method use large margin loss number multi class hierarchical classification dataset maxent loss show empirically method well expectation posterior regularization method competitive version entropy regularization method use label constraint
"FedLess: Secure and Scalable Federated Learning Using Serverless
  Computing","  The traditional cloud-centric approach for Deep Learning (DL) requires
training data to be collected and processed at a central server which is often
challenging in privacy-sensitive domains like healthcare. Towards this, a new
learning paradigm called Federated Learning (FL) has been proposed that brings
the potential of DL to these domains while addressing privacy and data
ownership issues. FL enables remote clients to learn a shared ML model while
keeping the data local. However, conventional FL systems face several
challenges such as scalability, complex infrastructure management, and wasted
compute and incurred costs due to idle clients. These challenges of FL systems
closely align with the core problems that serverless computing and
Function-as-a-Service (FaaS) platforms aim to solve. These include rapid
scalability, no infrastructure management, automatic scaling to zero for idle
clients, and a pay-per-use billing model. To this end, we present a novel
system and framework for serverless FL, called FedLess. Our system supports
multiple commercial and self-hosted FaaS providers and can be deployed in the
cloud, on-premise in institutional data centers, and on edge devices. To the
best of our knowledge, we are the first to enable FL across a large fabric of
heterogeneous FaaS providers while providing important features like security
and Differential Privacy. We demonstrate with comprehensive experiments that
the successful training of DNNs for different tasks across up to 200 client
functions and more is easily possible using our system. Furthermore, we
demonstrate the practical viability of our methodology by comparing it against
a traditional FL system and show that it can be cheaper and more
resource-efficient.
",traditional cloud centric approach deep learning dl require training datum collect process central server often challenge privacy sensitive domain like healthcare towards new learning paradigm call federate learning fl propose bring potential dl domain address privacy datum ownership issue fl enable remote client learn share ml model keep datum local however conventional fl system face several challenge scalability complex infrastructure management waste compute incur cost due idle client challenge fl system closely align core problem serverless computing function as a service faas platform aim solve include rapid scalability infrastructure management automatic scaling zero idle client pay per use billing model end present novel system framework serverless fl call fedless system support multiple commercial self host faas provider deploy cloud on premise institutional datum center edge device good knowledge first enable fl across large fabric heterogeneous faas provider provide important feature like security differential privacy demonstrate comprehensive experiment successful training dnn different task across 200 client function easily possible use system furthermore demonstrate practical viability methodology compare traditional fl system show cheap resource efficient
Multiple Independent Subspace Clusterings,"  Multiple clustering aims at discovering diverse ways of organizing data into
clusters. Despite the progress made, it's still a challenge for users to
analyze and understand the distinctive structure of each output clustering. To
ease this process, we consider diverse clusterings embedded in different
subspaces, and analyze the embedding subspaces to shed light into the structure
of each clustering. To this end, we provide a two-stage approach called MISC
(Multiple Independent Subspace Clusterings). In the first stage, MISC uses
independent subspace analysis to seek multiple and statistical independent
(i.e. non-redundant) subspaces, and determines the number of subspaces via the
minimum description length principle. In the second stage, to account for the
intrinsic geometric structure of samples embedded in each subspace, MISC
performs graph regularized semi-nonnegative matrix factorization to explore
clusters. It additionally integrates the kernel trick into matrix factorization
to handle non-linearly separable clusters. Experimental results on synthetic
datasets show that MISC can find different interesting clusterings from the
sought independent subspaces, and it also outperforms other related and
competitive approaches on real-world datasets.
",multiple cluster aim discover diverse way organize data cluster despite progress make still challenge user analyze understand distinctive structure output cluster ease process consider diverse clustering embed different subspace analyze embed subspace shed light structure cluster end provide two stage approach call misc multiple independent subspace clustering first stage misc use independent subspace analysis seek multiple statistical independent non redundant subspace determine number subspace via minimum description length principle second stage account intrinsic geometric structure sample embed subspace misc perform graph regularize semi nonnegative matrix factorization explore cluster additionally integrate kernel trick matrix factorization handle non linearly separable cluster experimental result synthetic dataset show misc find different interesting clustering seek independent subspace also outperform related competitive approach real world dataset
"Distributed Processing of Biosignal-Database for Emotion Recognition
  with Mahout","  This paper investigates the use of distributed processing on the problem of
emotion recognition from physiological sensors using a popular machine learning
library on distributed mode. Specifically, we run a random forests classifier
on the biosignal-data, which have been pre-processed to form exclusive groups
in an unsupervised fashion, on a Cloudera cluster using Mahout. The use of
distributed processing significantly reduces the time required for the offline
training of the classifier, enabling processing of large physiological datasets
through many iterations.
",paper investigate use distribute processing problem emotion recognition physiological sensor use popular machine learning library distribute mode specifically run random forest classifier biosignal datum pre processed form exclusive group unsupervise fashion cloudera cluster use mahout use distribute processing significantly reduce time require offline training classifier enable process large physiological dataset many iteration
Debiased Machine Learning without Sample-Splitting for Stable Estimators,"Estimation and inference on causal parameters is typically reduced to a
generalized method of moments problem, which involves auxiliary functions that
correspond to solutions to a regression or classification problem. Recent line
of work on debiased machine learning shows how one can use generic machine
learning estimators for these auxiliary problems, while maintaining asymptotic
normality and root-$n$ consistency of the target parameter of interest, while
only requiring mean-squared-error guarantees from the auxiliary estimation
algorithms. The literature typically requires that these auxiliary problems are
fitted on a separate sample or in a cross-fitting manner. We show that when
these auxiliary estimation algorithms satisfy natural leave-one-out stability
properties, then sample splitting is not required. This allows for sample
re-use, which can be beneficial in moderately sized sample regimes. For
instance, we show that the stability properties that we propose are satisfied
for ensemble bagged estimators, built via sub-sampling without replacement, a
popular technique in machine learning practice.",estimation inference causal parameter typically reduce generalize method moment problem involve auxiliary function correspond solution regression classification problem recent line work debiase machine learning show one use generic machine learn estimator auxiliary problem maintain asymptotic normality root- n consistency target parameter interest require mean square error guarantee auxiliary estimation algorithms literature typically require auxiliary problem fit separate sample cross fitting manner show auxiliary estimation algorithm satisfy natural leave one out stability property sample splitting require allow sample re use beneficial moderately sized sample regime instance show stability property propose satisfied ensemble bag estimator build via sub sample without replacement popular technique machine learning practice
Enhancing Audio Augmentation Methods with Consistency Learning,"  Data augmentation is an inexpensive way to increase training data diversity
and is commonly achieved via transformations of existing data. For tasks such
as classification, there is a good case for learning representations of the
data that are invariant to such transformations, yet this is not explicitly
enforced by classification losses such as the cross-entropy loss. This paper
investigates the use of training objectives that explicitly impose this
consistency constraint and how it can impact downstream audio classification
tasks. In the context of deep convolutional neural networks in the supervised
setting, we show empirically that certain measures of consistency are not
implicitly captured by the cross-entropy loss and that incorporating such
measures into the loss function can improve the performance of audio
classification systems. Put another way, we demonstrate how existing
augmentation methods can further improve learning by enforcing consistency.
",datum augmentation inexpensive way increase training datum diversity commonly achieve via transformation exist datum task classification good case learn representation datum invariant transformation yet explicitly enforce classification loss cross entropy loss paper investigate use training objective explicitly impose consistency constraint impact downstream audio classification task context deep convolutional neural network supervise set show empirically certain measure consistency implicitly capture cross entropy loss incorporate measure loss function improve performance audio classification system put another way demonstrate exist augmentation method improve learn enforce consistency
"Study of Drug Assimilation in Human System using Physics Informed Neural
  Networks","  Differential equations play a pivotal role in modern world ranging from
science, engineering, ecology, economics and finance where these can be used to
model many physical systems and processes. In this paper, we study two
mathematical models of a drug assimilation in the human system using Physics
Informed Neural Networks (PINNs). In the first model, we consider the case of
single dose of drug in the human system and in the second case, we consider the
course of this drug taken at regular intervals. We have used the compartment
diagram to model these cases. The resulting differential equations are solved
using PINN, where we employ a feed forward multilayer perceptron as function
approximator and the network parameters are tuned for minimum error. Further,
the network is trained by finding the gradient of the error function with
respect to the network parameters. We have employed DeepXDE, a python library
for PINNs, to solve the simultaneous first order differential equations
describing the two models of drug assimilation. The results show high degree of
accuracy between the exact solution and the predicted solution as much as the
resulting error reaches10^(-11) for the first model and 10^(-8) for the second
model. This validates the use of PINN in solving any dynamical system.
",differential equation play pivotal role modern world range science engineering ecology economic finance use model many physical system process paper study two mathematical model drug assimilation human system use physics inform neural network pinn first model consider case single dose drug human system second case consider course drug take regular interval use compartment diagram model case result differential equation solve use pinn employ feed forward multilayer perceptron function approximator network parameter tune minimum error network train finding gradient error function respect network parameter employ deepxde python library pinn solve simultaneous first order differential equation describe two model drug assimilation result show high degree accuracy exact solution predict solution much result error -11 first model -8 second model validate use pinn solve dynamical system
Improved Meta Learning for Low Resource Speech Recognition,"We propose a new meta learning based framework for low resource speech
recognition that improves the previous model agnostic meta learning (MAML)
approach. The MAML is a simple yet powerful meta learning approach. However,
the MAML presents some core deficiencies such as training instabilities and
slower convergence speed. To address these issues, we adopt multi-step loss
(MSL). The MSL aims to calculate losses at every step of the inner loop of MAML
and then combines them with a weighted importance vector. The importance vector
ensures that the loss at the last step has more importance than the previous
steps. Our empirical evaluation shows that MSL significantly improves the
stability of the training procedure and it thus also improves the accuracy of
the overall system. Our proposed system outperforms MAML based low resource ASR
system on various languages in terms of character error rates and stable
training behavior.",propose new meta learning base framework low resource speech recognition improve previous model agnostic meta learn maml approach maml simple yet powerful meta learning approach however maml presents core deficiency training instability slow convergence speed address issue adopt multi step loss msl msl aim calculate loss every step inner loop maml combine weight importance vector importance vector ensure loss last step importance previous step empirical evaluation show msl significantly improve stability training procedure thus also improve accuracy overall system propose system outperform maml base low resource asr system various language term character error rate stable training behavior
"Voice Transformer Network: Sequence-to-Sequence Voice Conversion Using
  Transformer with Text-to-Speech Pretraining","  We introduce a novel sequence-to-sequence (seq2seq) voice conversion (VC)
model based on the Transformer architecture with text-to-speech (TTS)
pretraining. Seq2seq VC models are attractive owing to their ability to convert
prosody. While seq2seq models based on recurrent neural networks (RNNs) and
convolutional neural networks (CNNs) have been successfully applied to VC, the
use of the Transformer network, which has shown promising results in various
speech processing tasks, has not yet been investigated. Nonetheless, their
data-hungry property and the mispronunciation of converted speech make seq2seq
models far from practical. To this end, we propose a simple yet effective
pretraining technique to transfer knowledge from learned TTS models, which
benefit from large-scale, easily accessible TTS corpora. VC models initialized
with such pretrained model parameters are able to generate effective hidden
representations for high-fidelity, highly intelligible converted speech.
Experimental results show that such a pretraining scheme can facilitate
data-efficient training and outperform an RNN-based seq2seq VC model in terms
of intelligibility, naturalness, and similarity.
",introduce novel sequence to sequence seq2seq voice conversion vc model base transformer architecture text to speech tts pretraine seq2seq vc model attractive owe ability convert prosody seq2seq model base recurrent neural network rnns convolutional neural network cnn successfully apply vc use transformer network show promise result various speech processing task yet investigate nonetheless data hungry property mispronunciation convert speech make seq2seq model far practical end propose simple yet effective pretraine technique transfer knowledge learn tts model benefit large scale easily accessible tts corpora vc model initialize pretraine model parameter able generate effective hide representation high fidelity highly intelligible convert speech experimental result show pretraine scheme facilitate data efficient training outperform rnn base seq2seq vc model term intelligibility naturalness similarity
Isometric Propagation Network for Generalized Zero-shot Learning,"  Zero-shot learning (ZSL) aims to classify images of an unseen class only
based on a few attributes describing that class but no access to any training
sample. A popular strategy is to learn a mapping between the semantic space of
class attributes and the visual space of images based on the seen classes and
their data. Thus, an unseen class image can be ideally mapped to its
corresponding class attributes. The key challenge is how to align the
representations in the two spaces. For most ZSL settings, the attributes for
each seen/unseen class are only represented by a vector while the seen-class
data provide much more information. Thus, the imbalanced supervision from the
semantic and the visual space can make the learned mapping easily overfitting
to the seen classes. To resolve this problem, we propose Isometric Propagation
Network (IPN), which learns to strengthen the relation between classes within
each space and align the class dependency in the two spaces. Specifically, IPN
learns to propagate the class representations on an auto-generated graph within
each space. In contrast to only aligning the resulted static representation, we
regularize the two dynamic propagation procedures to be isometric in terms of
the two graphs' edge weights per step by minimizing a consistency loss between
them. IPN achieves state-of-the-art performance on three popular ZSL
benchmarks. To evaluate the generalization capability of IPN, we further build
two larger benchmarks with more diverse unseen classes and demonstrate the
advantages of IPN on them.
",zero shot learning zsl aim classify image unseen class base attribute describe class access training sample popular strategy learn map semantic space class attribute visual space image base see class datum thus unseen class image ideally map correspond class attribute key challenge align representation two space zsl setting attribute class represent vector see class datum provide much information thus imbalance supervision semantic visual space make learn mapping easily overfitte see class resolve problem propose isometric propagation network ipn learn strengthen relation class within space align class dependency two space specifically ipn learn propagate class representation auto generate graph within space contrast align result static representation regularize two dynamic propagation procedure isometric term two graph edge weight per step minimize consistency loss ipn achieve state of the art performance three popular zsl benchmark evaluate generalization capability ipn build two large benchmark diverse unseen class demonstrate advantage ipn
Federated Adversarial Domain Adaptation,"  Federated learning improves data privacy and efficiency in machine learning
performed over networks of distributed devices, such as mobile phones, IoT and
wearable devices, etc. Yet models trained with federated learning can still
fail to generalize to new devices due to the problem of domain shift. Domain
shift occurs when the labeled data collected by source nodes statistically
differs from the target node's unlabeled data. In this work, we present a
principled approach to the problem of federated domain adaptation, which aims
to align the representations learned among the different nodes with the data
distribution of the target node. Our approach extends adversarial adaptation
techniques to the constraints of the federated setting. In addition, we devise
a dynamic attention mechanism and leverage feature disentanglement to enhance
knowledge transfer. Empirically, we perform extensive experiments on several
image and text classification tasks and show promising results under
unsupervised federated domain adaptation setting.
",federate learning improve datum privacy efficiency machine learning perform network distribute device mobile phone iot wearable device etc yet model train federate learning still fail generalize new device due problem domain shift domain shift occur label datum collect source node statistically differ target node unlabeled data work present principle approach problem federate domain adaptation aim align representation learn among different node datum distribution target node approach extend adversarial adaptation technique constraint federate set addition devise dynamic attention mechanism leverage feature disentanglement enhance knowledge transfer empirically perform extensive experiment several image text classification task show promising result unsupervise federate domain adaptation set
"DeepDoseNet: A Deep Learning model for 3D Dose Prediction in Radiation
  Therapy","  The DeepDoseNet 3D dose prediction model based on ResNet and Dilated DenseNet
is proposed. The 340 head-and-neck datasets from the 2020 AAPM OpenKBP
challenge were utilized, with 200 for training, 40 for validation, and 100 for
testing. Structures include 56Gy, 63Gy, 70Gy PTVs, and brainstem, spinal cord,
right parotid, left parotid, larynx, esophagus, and mandible OARs. Mean squared
error (MSE) loss, mean absolute error (MAE) loss, and MAE plus dose-volume
histogram (DVH) based loss functions were investigated. Each model's
performance was compared using a 3D dose score, $\bar{S_{D}}$, (mean absolute
difference between ground truth and predicted 3D dose distributions) and a DVH
score, $\bar{S_{DVH}}$ (mean absolute difference between ground truth and
predicted dose-volume metrics).Furthermore, DVH metrics Mean[Gy] and D0.1cc
[Gy] for OARs and D99%, D95%, D1% for PTVs were computed. DeepDoseNet with the
MAE plus DVH-based loss function had the best dose score performance of the
OpenKBP entries. MAE+DVH model had the lowest prediction error (P<0.0001,
Wilcoxon test) on validation and test datasets (validation:
$\bar{S_{D}}$=2.3Gy, $\bar{S_{DVH}}$=1.9Gy; test: $\bar{S_{D}}$=2.0Gy,
$\bar{S_{DVH}}$=1.6Gy) followed by the MAE model (validation:
$\bar{S_{D}}$=3.6Gy, $\bar{S_{DVH}}$=2.4Gy; test: $\bar{S_{D}}$=3.5Gy,
$\bar{S_{DVH}}$=2.3Gy). The MSE model had the highest prediction error
(validation: $\bar{S_{D}}$=3.7Gy, $\bar{S_{DVH}}$=3.2Gy; test:
$\bar{S_{D}}$=3.6Gy, $\bar{S_{DVH}}$=3.0Gy). No significant difference was
found among models in terms of Mean [Gy], but the MAE+DVH model significantly
outperformed the MAE and MSE models in terms of D0.1cc[Gy], particularly for
mandible and parotids on both validation (P<0.01) and test (P<0.0001) datasets.
MAE+DVH outperformed (P<0.0001) in terms of D99%, D95%, D1% for targets.
MAE+DVH reduced $\bar{S_{D}}$ by ~60% and $\bar{S_{DVH}}$ by ~70%.
",deepdosenet 3d dose prediction model base resnet dilate densenet propose 340 head and neck dataset 2020 aapm openkbp challenge utilize 200 training 40 validation 100 testing structure include 56gy 63gy 70gy ptvs brainstem spinal cord right parotid leave parotid larynx esophagus mandible oars mean square error mse loss mean absolute error mae loss mae plus dose volume histogram dvh base loss function investigate model performance compare use 3d dose score s mean absolute difference ground truth predict 3d dose distribution dvh score s dvh mean absolute difference ground truth predict dose volume metric dvh metric mean gy gy oar d99 d95 d1 ptvs compute deepdosenet mae plus dvh base loss function well dose score performance openkbp entry model low prediction error p wilcoxon test validation test dataset validation s s dvh test s s dvh follow mae model validation s s dvh test s s dvh mse model high prediction error validation s s dvh test s s dvh significant difference find among model term mean gy model significantly outperform mae mse model term gy particularly mandible parotid validation p test p dataset outperform p term d99 d95 d1 target reduce s s dvh
Patch-level Representation Learning for Self-supervised Vision Transformers,"Recent self-supervised learning (SSL) methods have shown impressive results
in learning visual representations from unlabeled images. This paper aims to
improve their performance further by utilizing the architectural advantages of
the underlying neural network, as the current state-of-the-art visual pretext
tasks for SSL do not enjoy the benefit, i.e., they are architecture-agnostic.
In particular, we focus on Vision Transformers (ViTs), which have gained much
attention recently as a better architectural choice, often outperforming
convolutional networks for various visual tasks. The unique characteristic of
ViT is that it takes a sequence of disjoint patches from an image and processes
patch-level representations internally. Inspired by this, we design a simple
yet effective visual pretext task, coined SelfPatch, for learning better
patch-level representations. To be specific, we enforce invariance against each
patch and its neighbors, i.e., each patch treats similar neighboring patches as
positive samples. Consequently, training ViTs with SelfPatch learns more
semantically meaningful relations among patches (without using human-annotated
labels), which can be beneficial, in particular, to downstream tasks of a dense
prediction type. Despite its simplicity, we demonstrate that it can
significantly improve the performance of existing SSL methods for various
visual tasks, including object detection and semantic segmentation.
Specifically, SelfPatch significantly improves the recent self-supervised ViT,
DINO, by achieving +1.3 AP on COCO object detection, +1.2 AP on COCO instance
segmentation, and +2.9 mIoU on ADE20K semantic segmentation.",recent self supervise learning ssl method show impressive result learn visual representation unlabeled image paper aim improve performance utilize architectural advantage underlie neural network current state of the art visual pretext task ssl enjoy benefit architecture agnostic particular focus vision transformer vit gain much attention recently well architectural choice often outperform convolutional network various visual task unique characteristic vit take sequence disjoint patch image process patch level representation internally inspire design simple yet effective visual pretext task coin selfpatch learn well patch level representation specific enforce invariance patch neighbor patch treat similar neighboring patch positive sample consequently train vit selfpatch learn semantically meaningful relation among patch without use human annotate label beneficial particular downstream task dense prediction type despite simplicity demonstrate significantly improve performance exist ssl method various visual task include object detection semantic segmentation specifically selfpatch significantly improve recent self supervise vit dino achieve ap coco object detection ap coco instance segmentation miou ade20k semantic segmentation
MALA: Cross-Domain Dialogue Generation with Action Learning,"  Response generation for task-oriented dialogues involves two basic
components: dialogue planning and surface realization. These two components,
however, have a discrepancy in their objectives, i.e., task completion and
language quality. To deal with such discrepancy, conditioned response
generation has been introduced where the generation process is factorized into
action decision and language generation via explicit action representations. To
obtain action representations, recent studies learn latent actions in an
unsupervised manner based on the utterance lexical similarity. Such an action
learning approach is prone to diversities of language surfaces, which may
impinge task completion and language quality. To address this issue, we propose
multi-stage adaptive latent action learning (MALA) that learns semantic latent
actions by distinguishing the effects of utterances on dialogue progress. We
model the utterance effect using the transition of dialogue states caused by
the utterance and develop a semantic similarity measurement that estimates
whether utterances have similar effects. For learning semantic actions on
domains without dialogue states, MsALA extends the semantic similarity
measurement across domains progressively, i.e., from aligning shared actions to
learning domain-specific actions. Experiments using multi-domain datasets, SMD
and MultiWOZ, show that our proposed model achieves consistent improvements
over the baselines models in terms of both task completion and language
quality.
",response generation task orient dialogue involve two basic component dialogue planning surface realization two component however discrepancy objective task completion language quality deal discrepancy condition response generation introduce generation process factorize action decision language generation via explicit action representation obtain action representation recent study learn latent action unsupervise manner base utterance lexical similarity action learn approach prone diversity language surface may impinge task completion language quality address issue propose multi stage adaptive latent action learn mala learn semantic latent action distinguish effect utterance dialogue progress model utterance effect use transition dialogue state cause utterance develop semantic similarity measurement estimate whether utterance similar effect learn semantic action domain without dialogue states msala extend semantic similarity measurement across domain progressively align share action learn domain specific action experiment use multi domain dataset smd multiwoz show propose model achieve consistent improvement baseline model term task completion language quality
Beyond NDCG: behavioral testing of recommender systems with RecList,"  As with most Machine Learning systems, recommender systems are typically
evaluated through performance metrics computed over held-out data points.
However, real-world behavior is undoubtedly nuanced: ad hoc error analysis and
deployment-specific tests must be employed to ensure the desired quality in
actual deployments. In this paper, we propose RecList, a behavioral-based
testing methodology. RecList organizes recommender systems by use case and
introduces a general plug-and-play procedure to scale up behavioral testing. We
demonstrate its capabilities by analyzing known algorithms and black-box
commercial systems, and we release RecList as an open source, extensible
package for the community.
",machine learn system recommender system typically evaluate performance metric compute hold out data point however real world behavior undoubtedly nuance ad hoc error analysis deployment specific test must employ ensure desire quality actual deployment paper propose reclist behavioral base testing methodology reclist organize recommender system use case introduce general plug and play procedure scale behavioral testing demonstrate capability analyze know algorithm black box commercial system release reclist open source extensible package community
"Correct-by-synthesis reinforcement learning with temporal logic
  constraints","  We consider a problem on the synthesis of reactive controllers that optimize
some a priori unknown performance criterion while interacting with an
uncontrolled environment such that the system satisfies a given temporal logic
specification. We decouple the problem into two subproblems. First, we extract
a (maximally) permissive strategy for the system, which encodes multiple
(possibly all) ways in which the system can react to the adversarial
environment and satisfy the specifications. Then, we quantify the a priori
unknown performance criterion as a (still unknown) reward function and compute
an optimal strategy for the system within the operating envelope allowed by the
permissive strategy by using the so-called maximin-Q learning algorithm. We
establish both correctness (with respect to the temporal logic specifications)
and optimality (with respect to the a priori unknown performance criterion) of
this two-step technique for a fragment of temporal logic specifications. For
specifications beyond this fragment, correctness can still be preserved, but
the learned strategy may be sub-optimal. We present an algorithm to the overall
problem, and demonstrate its use and computational requirements on a set of
robot motion planning examples.
",consider problem synthesis reactive controller optimize priori unknown performance criterion interact uncontrolled environment system satisfie give temporal logic specification decouple problem two subproblem first extract maximally permissive strategy system encode multiple possibly way system react adversarial environment satisfy specification quantify priori unknown performance criterion still unknown reward function compute optimal strategy system within operating envelope allow permissive strategy use so call maximin q learning algorithm establish correctness respect temporal logic specification optimality respect priori unknown performance criterion two step technique fragment temporal logic specification specification beyond fragment correctness still preserve learn strategy may sub optimal present algorithm overall problem demonstrate use computational requirement set robot motion planning example
"Towards Scalable Spectral Clustering via Spectrum-Preserving
  Sparsification","  The eigendeomposition of nearest-neighbor (NN) graph Laplacian matrices is
the main computational bottleneck in spectral clustering. In this work, we
introduce a highly-scalable, spectrum-preserving graph sparsification algorithm
that enables to build ultra-sparse NN (u-NN) graphs with guaranteed
preservation of the original graph spectrums, such as the first few
eigenvectors of the original graph Laplacian. Our approach can immediately lead
to scalable spectral clustering of large data networks without sacrificing
solution quality. The proposed method starts from constructing low-stretch
spanning trees (LSSTs) from the original graphs, which is followed by
iteratively recovering small portions of ""spectrally critical"" off-tree edges
to the LSSTs by leveraging a spectral off-tree embedding scheme. To determine
the suitable amount of off-tree edges to be recovered to the LSSTs, an
eigenvalue stability checking scheme is proposed, which enables to robustly
preserve the first few Laplacian eigenvectors within the sparsified graph.
Additionally, an incremental graph densification scheme is proposed for
identifying extra edges that have been missing in the original NN graphs but
can still play important roles in spectral clustering tasks. Our experimental
results for a variety of well-known data sets show that the proposed method can
dramatically reduce the complexity of NN graphs, leading to significant
speedups in spectral clustering.
",eigendeomposition nearest neighbor nn graph laplacian matrice main computational bottleneck spectral clustering work introduce highly scalable spectrum preserve graph sparsification algorithm enable build ultra sparse nn u nn graph guarantee preservation original graph spectrum first eigenvector original graph laplacian approach immediately lead scalable spectral cluster large datum network without sacrifice solution quality propose method start construct low stretch span tree lsst original graph follow iteratively recover small portion spectrally critical off tree edge lsst leverage spectral off tree embed scheme determine suitable amount off tree edge recover lsst eigenvalue stability check scheme propose enable robustly preserve first laplacian eigenvector within sparsifie graph additionally incremental graph densification scheme propose identify extra edge miss original nn graph still play important role spectral clustering task experimental result variety well know data set show propose method dramatically reduce complexity nn graph lead significant speedup spectral clustering
"Exploring Beyond-Demonstrator via Meta Learning-Based Reward
  Extrapolation","  Extrapolating beyond-demonstrator (BD) performance through the imitation
learning (IL) algorithm aims to learn from and subsequently outperform the
demonstrator. To that end, a representative approach is to leverage inverse
reinforcement learning (IRL) to infer a reward function from demonstrations
before performing RL on the learned reward function. However, most existing
reward extrapolation methods require massive demonstrations, making it
difficult to be applied in tasks of limited training data. To address this
problem, one simple solution is to perform data augmentation to artificially
generate more training data, which may incur severe inductive bias and policy
performance loss. In this paper, we propose a novel meta learning-based reward
extrapolation (MLRE) algorithm, which can effectively approximate the
ground-truth rewards using limited demonstrations. More specifically, MLRE
first learns an initial reward function from a set of tasks that have abundant
training data. Then the learned reward function will be fine-tuned using data
of the target task. Extensive simulation results demonstrated that the proposed
MLRE can achieve impressive performance improvement as compared to other
similar BDIL algorithms.
",extrapolate beyond demonstrator bd performance imitation learn il algorithm aim learn subsequently outperform demonstrator end representative approach leverage inverse reinforcement learn irl infer reward function demonstration perform rl learn reward function however exist reward extrapolation method require massive demonstration make difficult apply task limit training datum address problem one simple solution perform datum augmentation artificially generate training datum may incur severe inductive bias policy performance loss paper propose novel meta learning base reward extrapolation mlre algorithm effectively approximate ground truth reward use limited demonstration specifically mlre first learn initial reward function set task abundant training datum learn reward function fine tune use data target task extensive simulation result demonstrate propose mlre achieve impressive performance improvement compare similar bdil algorithm
"Probabilistic partition of unity networks: clustering based deep
  approximation","  Partition of unity networks (POU-Nets) have been shown capable of realizing
algebraic convergence rates for regression and solution of PDEs, but require
empirical tuning of training parameters. We enrich POU-Nets with a Gaussian
noise model to obtain a probabilistic generalization amenable to gradient-based
minimization of a maximum likelihood loss. The resulting architecture provides
spatial representations of both noiseless and noisy data as Gaussian mixtures
with closed form expressions for variance which provides an estimator of local
error. The training process yields remarkably sharp partitions of input space
based upon correlation of function values. This classification of training
points is amenable to a hierarchical refinement strategy that significantly
improves the localization of the regression, allowing for higher-order
polynomial approximation to be utilized. The framework scales more favorably to
large data sets as compared to Gaussian process regression and allows for
spatially varying uncertainty, leveraging the expressive power of deep neural
networks while bypassing expensive training associated with other probabilistic
deep learning methods. Compared to standard deep neural networks, the framework
demonstrates hp-convergence without the use of regularizers to tune the
localization of partitions. We provide benchmarks quantifying performance in
high/low-dimensions, demonstrating that convergence rates depend only on the
latent dimension of data within high-dimensional space. Finally, we introduce a
new open-source data set of PDE-based simulations of a semiconductor device and
perform unsupervised extraction of a physically interpretable reduced-order
basis.
",partition unity network pou net show capable realize algebraic convergence rate regression solution pde require empirical tuning training parameter enrich pou net gaussian noise model obtain probabilistic generalization amenable gradient base minimization maximum likelihood loss result architecture provide spatial representation noiseless noisy data gaussian mixtures close form expression variance provide estimator local error training process yield remarkably sharp partition input space base upon correlation function value classification training point amenable hierarchical refinement strategy significantly improve localization regression allow high order polynomial approximation utilize framework scale favorably large data set compare gaussian process regression allow spatially vary uncertainty leverage expressive power deep neural network bypass expensive training associate probabilistic deep learning method compare standard deep neural network framework demonstrate hp convergence without use regularizer tune localization partition provide benchmark quantify performance demonstrate convergence rate depend latent dimension datum within high dimensional space finally introduce new open source datum set pde base simulation semiconductor device perform unsupervised extraction physically interpretable reduced order basis
"Self-supervised Knowledge Distillation Using Singular Value
  Decomposition","  To solve deep neural network (DNN)'s huge training dataset and its high
computation issue, so-called teacher-student (T-S) DNN which transfers the
knowledge of T-DNN to S-DNN has been proposed. However, the existing T-S-DNN
has limited range of use, and the knowledge of T-DNN is insufficiently
transferred to S-DNN. To improve the quality of the transferred knowledge from
T-DNN, we propose a new knowledge distillation using singular value
decomposition (SVD). In addition, we define a knowledge transfer as a
self-supervised task and suggest a way to continuously receive information from
T-DNN. Simulation results show that a S-DNN with a computational cost of 1/5 of
the T-DNN can be up to 1.1\% better than the T-DNN in terms of classification
accuracy. Also assuming the same computational cost, our S-DNN outperforms the
S-DNN driven by the state-of-the-art distillation with a performance advantage
of 1.79\%. code is available on https://github.com/sseung0703/SSKD\_SVD.
",solve deep neural network dnn huge training dataset high computation issue so call teacher student t s dnn transfer knowledge t dnn s dnn propose however exist t s dnn limited range use knowledge t dnn insufficiently transfer s dnn improve quality transfer knowledge t dnn propose new knowledge distillation use singular value decomposition svd addition define knowledge transfer self supervise task suggest way continuously receive information t dnn simulation result show s dnn computational cost t dnn well t dnn term classification accuracy also assume computational cost s dnn outperform s dnn drive state of the art distillation performance advantage code available https
"A Comparative Study of Machine Learning Methods for Predicting the
  Evolution of Brain Connectivity from a Baseline Timepoint","  Predicting the evolution of the brain network, also called connectome, by
foreseeing changes in the connectivity weights linking pairs of anatomical
regions makes it possible to spot connectivity-related neurological disorders
in earlier stages and detect the development of potential connectomic
anomalies. Remarkably, such a challenging prediction problem remains least
explored in the predictive connectomics literature. It is a known fact that
machine learning (ML) methods have proven their predictive abilities in a wide
variety of computer vision problems. However, ML techniques specifically
tailored for the prediction of brain connectivity evolution trajectory from a
single timepoint are almost absent. To fill this gap, we organized a Kaggle
competition where 20 competing teams designed advanced machine learning
pipelines for predicting the brain connectivity evolution from a single
timepoint. The competing teams developed their ML pipelines with a combination
of data pre-processing, dimensionality reduction, and learning methods.
Utilizing an inclusive evaluation approach, we ranked the methods based on two
complementary evaluation metrics (mean absolute error (MAE) and Pearson
Correlation Coefficient (PCC)) and their performances using different training
and testing data perturbation strategies (single random split and
cross-validation). The final rank was calculated using the rank product for
each competing team across all evaluation measures and validation strategies.
In support of open science, the developed 20 ML pipelines along with the
connectomic dataset are made available on GitHub. The outcomes of this
competition are anticipated to lead to the further development of predictive
models that can foresee the evolution of brain connectivity over time, as well
as other types of networks (e.g., genetic networks).
",predict evolution brain network also call connectome foresee change connectivity weight link pair anatomical region make possible spot connectivity relate neurological disorder early stage detect development potential connectomic anomaly remarkably challenging prediction problem remain least explore predictive connectomic literature know fact machine learning ml method prove predictive ability wide variety computer vision problem however ml technique specifically tailor prediction brain connectivity evolution trajectory single timepoint almost absent fill gap organize kaggle competition 20 compete team design advanced machine learn pipeline predict brain connectivity evolution single timepoint compete team develop ml pipeline combination datum pre processing dimensionality reduction learning method utilize inclusive evaluation approach rank method base two complementary evaluation metric mean absolute error mae pearson correlation coefficient pcc performance use different training testing datum perturbation strategy single random split cross validation final rank calculate use rank product compete team across evaluation measure validation strategy support open science develop 20 ml pipeline along connectomic dataset make available github outcome competition anticipate lead development predictive model foresee evolution brain connectivity time well type network genetic network
"Reconstructing a dynamical system and forecasting time series by
  self-consistent deep learning","  We introduce a self-consistent deep-learning framework which, for a noisy
deterministic time series, provides unsupervised filtering, state-space
reconstruction, identification of the underlying differential equations and
forecasting. Without a priori information on the signal, we embed the time
series in a state space, where deterministic structures, i.e. attractors, are
revealed. Under the assumption that the evolution of solution trajectories is
described by an unknown dynamical system, we filter out stochastic outliers.
The embedding function, the solution trajectories and the dynamical systems are
constructed using deep neural networks, respectively. By exploiting the
differentiability of the neural solution trajectory, the neural dynamical
system is defined locally at each time, mitigating the need for propagating
gradients through numerical solvers. On a chaotic time series masked by
additive Gaussian noise, we demonstrate the filtering ability and the
predictive power of the proposed framework.
",introduce self consistent deep learn framework noisy deterministic time series provide unsupervised filtering state space reconstruction identification underlie differential equation forecast without priori information signal embed time series state space deterministic structure attractor reveal assumption evolution solution trajectory describe unknown dynamical system filter stochastic outlier embed function solution trajectory dynamical system construct use deep neural network respectively exploit differentiability neural solution trajectory neural dynamical system define locally time mitigate need propagate gradient numerical solvers chaotic time series mask additive gaussian noise demonstrate filtering ability predictive power propose framework
Approximate inference with Wasserstein gradient flows,"  We present a novel approximate inference method for diffusion processes,
based on the Wasserstein gradient flow formulation of the diffusion. In this
formulation, the time-dependent density of the diffusion is derived as the
limit of implicit Euler steps that follow the gradients of a particular free
energy functional. Existing methods for computing Wasserstein gradient flows
rely on discretization of the domain of the diffusion, prohibiting their
application to domains in more than several dimensions. We propose instead a
discretization-free inference method that computes the Wasserstein gradient
flow directly in a space of continuous functions. We characterize approximation
properties of the proposed method and evaluate it on a nonlinear filtering
task, finding performance comparable to the state-of-the-art for filtering
diffusions.
",present novel approximate inference method diffusion process base wasserstein gradient flow formulation diffusion formulation time dependent density diffusion derive limit implicit euler step follow gradient particular free energy functional exist method compute wasserstein gradient flow rely discretization domain diffusion prohibit application domain several dimension propose instead discretization free inference method compute wasserstein gradient flow directly space continuous function characterize approximation property propose method evaluate nonlinear filtering task find performance comparable state of the art filtering diffusion
"Big-Step-Little-Step: Efficient Gradient Methods for Objectives with
  Multiple Scales","  We provide new gradient-based methods for efficiently solving a broad class
of ill-conditioned optimization problems. We consider the problem of minimizing
a function $f : \mathbb{R}^d \rightarrow \mathbb{R}$ which is implicitly
decomposable as the sum of $m$ unknown non-interacting smooth, strongly convex
functions and provide a method which solves this problem with a number of
gradient evaluations that scales (up to logarithmic factors) as the product of
the square-root of the condition numbers of the components. This complexity
bound (which we prove is nearly optimal) can improve almost exponentially on
that of accelerated gradient methods, which grow as the square root of the
condition number of $f$. Additionally, we provide efficient methods for solving
stochastic, quadratic variants of this multiscale optimization problem. Rather
than learn the decomposition of $f$ (which would be prohibitively expensive),
our methods apply a clean recursive ""Big-Step-Little-Step"" interleaving of
standard methods. The resulting algorithms use $\tilde{\mathcal{O}}(d m)$
space, are numerically stable, and open the door to a more fine-grained
understanding of the complexity of convex optimization beyond condition number.
",provide new gradient base method efficiently solve broad class ill condition optimization problem consider problem minimize function f r r implicitly decomposable sum unknown non interacting smooth strongly convex function provide method solve problem number gradient evaluation scale logarithmic factor product square root condition number component complexity bind prove nearly optimal improve almost exponentially accelerate gradient method grow square root condition number f additionally provide efficient method solve stochastic quadratic variant multiscale optimization problem rather learn decomposition f would prohibitively expensive method apply clean recursive big step little step interleave standard method result algorithm use space numerically stable open door fine grain understanding complexity convex optimization beyond condition number
"One-Shot Image-to-Image Translation via Part-Global Learning with a
  Multi-adversarial Framework","  It is well known that humans can learn and recognize objects effectively from
several limited image samples. However, learning from just a few images is
still a tremendous challenge for existing main-stream deep neural networks.
Inspired by analogical reasoning in the human mind, a feasible strategy is to
translate the abundant images of a rich source domain to enrich the relevant
yet different target domain with insufficient image data. To achieve this goal,
we propose a novel, effective multi-adversarial framework (MA) based on
part-global learning, which accomplishes one-shot cross-domain image-to-image
translation. In specific, we first devise a part-global adversarial training
scheme to provide an efficient way for feature extraction and prevent
discriminators being over-fitted. Then, a multi-adversarial mechanism is
employed to enhance the image-to-image translation ability to unearth the
high-level semantic representation. Moreover, a balanced adversarial loss
function is presented, which aims to balance the training data and stabilize
the training process. Extensive experiments demonstrate that the proposed
approach can obtain impressive results on various datasets between two
extremely imbalanced image domains and outperform state-of-the-art methods on
one-shot image-to-image translation.
",well know human learn recognize object effectively several limit image sample however learn image still tremendous challenge exist main stream deep neural network inspire analogical reasoning human mind feasible strategy translate abundant image rich source domain enrich relevant yet different target domain insufficient image datum achieve goal propose novel effective multi adversarial framework base part global learning accomplish one shoot cross domain image to image translation specific first devise part global adversarial training scheme provide efficient way feature extraction prevent discriminator over fit multi adversarial mechanism employ enhance image to image translation ability unearth high level semantic representation moreover balanced adversarial loss function present aim balance training datum stabilize training process extensive experiment demonstrate propose approach obtain impressive result various dataset two extremely imbalance image domain outperform state of the art method one shot image to image translation
"Vehicle Trajectory Prediction by Transfer Learning of Semi-Supervised
  Models","  In this work we show that semi-supervised models for vehicle trajectory
prediction significantly improve performance over supervised models on
state-of-the-art real-world benchmarks. Moving from supervised to
semi-supervised models allows scaling-up by using unlabeled data, increasing
the number of images in pre-training from Millions to a Billion. We perform
ablation studies comparing transfer learning of semi-supervised and supervised
models while keeping all other factors equal. Within semi-supervised models we
compare contrastive learning with teacher-student methods as well as networks
predicting a small number of trajectories with networks predicting
probabilities over a large trajectory set. Our results using both low-level and
mid-level representations of the driving environment demonstrate the
applicability of semi-supervised methods for real-world vehicle trajectory
prediction.
",work show semi supervised model vehicle trajectory prediction significantly improve performance supervise model state of the art real world benchmark move supervised semi supervised model allow scaling up use unlabeled datum increase number image pre training million billion perform ablation study compare transfer learn semi supervised supervised model keep factor equal within semi supervised model compare contrastive learning teacher student method well network predict small number trajectory network predict probability large trajectory set result use low level mid level representation drive environment demonstrate applicability semi supervised method real world vehicle trajectory prediction
A short note on the tail bound of Wishart distribution,"  We study the tail bound of the emperical covariance of multivariate normal
distribution. Following the work of (Gittens & Tropp, 2011), we provide a tail
bound with a small constant.
",study tail bind emperical covariance multivariate normal distribution follow work gitten tropp 2011 provide tail bind small constant
"Motion Planning for Autonomous Vehicles in the Presence of Uncertainty
  Using Reinforcement Learning","  Motion planning under uncertainty is one of the main challenges in developing
autonomous driving vehicles. In this work, we focus on the uncertainty in
sensing and perception, resulted from a limited field of view, occlusions, and
sensing range. This problem is often tackled by considering hypothetical hidden
objects in occluded areas or beyond the sensing range to guarantee passive
safety. However, this may result in conservative planning and expensive
computation, particularly when numerous hypothetical objects need to be
considered. We propose a reinforcement learning (RL) based solution to manage
uncertainty by optimizing for the worst case outcome. This approach is in
contrast to traditional RL, where the agents try to maximize the average
expected reward. The proposed approach is built on top of the Distributional RL
with its policy optimization maximizing the stochastic outcomes' lower bound.
This modification can be applied to a range of RL algorithms. As a
proof-of-concept, the approach is applied to two different RL algorithms, Soft
Actor-Critic and DQN. The approach is evaluated against two challenging
scenarios of pedestrians crossing with occlusion and curved roads with a
limited field of view. The algorithm is trained and evaluated using the SUMO
traffic simulator. The proposed approach yields much better motion planning
behavior compared to conventional RL algorithms and behaves comparably to
humans driving style.
",motion planning uncertainty one main challenge develop autonomous driving vehicle work focus uncertainty sense perception result limited field view occlusion sense range problem often tackle consider hypothetical hide object occlude area beyond sense range guarantee passive safety however may result conservative planning expensive computation particularly numerous hypothetical object need consider propose reinforcement learning rl base solution manage uncertainty optimize bad case outcome approach contrast traditional rl agent try maximize average expect reward propose approach build top distributional rl policy optimization maximize stochastic outcome lower bind modification apply range rl algorithm proof of concept approach apply two different rl algorithm soft actor critic dqn approach evaluate two challenging scenario pedestrian cross occlusion curve road limited field view algorithm train evaluate use sumo traffic simulator propose approach yield much well motion planning behavior compare conventional rl algorithms behave comparably human drive style
Topology Adaptive Graph Convolutional Networks,"  Spectral graph convolutional neural networks (CNNs) require approximation to
the convolution to alleviate the computational complexity, resulting in
performance loss. This paper proposes the topology adaptive graph convolutional
network (TAGCN), a novel graph convolutional network defined in the vertex
domain. We provide a systematic way to design a set of fixed-size learnable
filters to perform convolutions on graphs. The topologies of these filters are
adaptive to the topology of the graph when they scan the graph to perform
convolution. The TAGCN not only inherits the properties of convolutions in CNN
for grid-structured data, but it is also consistent with convolution as defined
in graph signal processing. Since no approximation to the convolution is
needed, TAGCN exhibits better performance than existing spectral CNNs on a
number of data sets and is also computationally simpler than other recent
methods.
",spectral graph convolutional neural network cnns require approximation convolution alleviate computational complexity result performance loss paper propose topology adaptive graph convolutional network tagcn novel graph convolutional network define vertex domain provide systematic way design set fix size learnable filter perform convolution graph topology filter adaptive topology graph scan graph perform convolution tagcn inherit propertie convolution cnn grid structure datum also consistent convolution define graph signal processing since approximation convolution need tagcn exhibit well performance exist spectral cnn number datum set also computationally simple recent method
"Learning Pruned Structure and Weights Simultaneously from Scratch: an
  Attention based Approach","  As a deep learning model typically contains millions of trainable weights,
there has been a growing demand for a more efficient network structure with
reduced storage space and improved run-time efficiency. Pruning is one of the
most popular network compression techniques. In this paper, we propose a novel
unstructured pruning pipeline, Attention-based Simultaneous sparse structure
and Weight Learning (ASWL). Unlike traditional channel-wise or weight-wise
attention mechanism, ASWL proposed an efficient algorithm to calculate the
pruning ratio through layer-wise attention for each layer, and both weights for
the dense network and the sparse network are tracked so that the pruned
structure is simultaneously learned from randomly initialized weights. Our
experiments on MNIST, Cifar10, and ImageNet show that ASWL achieves superior
pruning results in terms of accuracy, pruning ratio and operating efficiency
when compared with state-of-the-art network pruning methods.
",deep learning model typically contain million trainable weight grow demand efficient network structure reduce storage space improve run time efficiency prune one popular network compression technique paper propose novel unstructured pruning pipeline attention base simultaneous sparse structure weight learn aswl unlike traditional channel wise weight wise attention mechanism aswl propose efficient algorithm calculate pruning ratio layer wise attention layer weight dense network sparse network track pruned structure simultaneously learn randomly initialize weight experiment mnist cifar10 imagenet show aswl achieve superior pruning result term accuracy pruning ratio operating efficiency compare state of the art network prune method
"BERT-CNN: a Hierarchical Patent Classifier Based on a Pre-Trained
  Language Model","  The automatic classification is a process of automatically assigning text
documents to predefined categories. An accurate automatic patent classifier is
crucial to patent inventors and patent examiners in terms of intellectual
property protection, patent management, and patent information retrieval. We
present BERT-CNN, a hierarchical patent classifier based on pre-trained
language model by training the national patent application documents collected
from the State Information Center, China. The experimental results show that
BERT-CNN achieves 84.3% accuracy, which is far better than the two compared
baseline methods, Convolutional Neural Networks and Recurrent Neural Networks.
We didn't apply our model to the third and fourth hierarchical level of the
International Patent Classification - ""subclass"" and ""group"".The visualization
of the Attention Mechanism shows that BERT-CNN obtains new state-of-the-art
results in representing vocabularies and semantics. This article demonstrates
the practicality and effectiveness of BERT-CNN in the field of automatic patent
classification.
",automatic classification process automatically assign text document predefine category accurate automatic patent classifier crucial patent inventor patent examiner term intellectual property protection patent management patent information retrieval present bert cnn hierarchical patent classifier base pre train language model training national patent application document collect state information center china experimental result show bert cnn achieve accuracy far well two compare baseline method convolutional neural network recurrent neural network apply model third fourth hierarchical level international patent classification subclass group visualization attention mechanism show bert cnn obtain new state of the art result represent vocabulary semantic article demonstrate practicality effectiveness bert cnn field automatic patent classification
"Sample-efficient proper PAC learning with approximate differential
  privacy","  In this paper we prove that the sample complexity of properly learning a
class of Littlestone dimension $d$ with approximate differential privacy is
$\tilde O(d^6)$, ignoring privacy and accuracy parameters. This result answers
a question of Bun et al. (FOCS 2020) by improving upon their upper bound of
$2^{O(d)}$ on the sample complexity. Prior to our work, finiteness of the
sample complexity for privately learning a class of finite Littlestone
dimension was only known for improper private learners, and the fact that our
learner is proper answers another question of Bun et al., which was also asked
by Bousquet et al. (NeurIPS 2020). Using machinery developed by Bousquet et
al., we then show that the sample complexity of sanitizing a binary hypothesis
class is at most polynomial in its Littlestone dimension and dual Littlestone
dimension. This implies that a class is sanitizable if and only if it has
finite Littlestone dimension. An important ingredient of our proofs is a new
property of binary hypothesis classes that we call irreducibility, which may be
of independent interest.
",paper prove sample complexity properly learn class littlestone dimension approximate differential privacy ignore privacy accuracy parameter result answer question bun et al focs 2020 improve upon upper bind sample complexity prior work finiteness sample complexity privately learn class finite littlestone dimension know improper private learner fact learner proper answer another question bun et also ask bousquet et al neurip 2020 use machinery develop bousquet et show sample complexity sanitize binary hypothesis class polynomial littlestone dimension dual littlestone dimension imply class sanitizable finite littlestone dimension important ingredient proof new property binary hypothesis class call irreducibility may independent interest
"FedMVT: Semi-supervised Vertical Federated Learning with MultiView
  Training","  Federated learning allows many parties to collaboratively build a model
without exposing data. Particularly, vertical federated learning (VFL) enables
parties to build a robust shared machine learning model based upon distributed
features about the same samples. However, VFL requires all parties to share a
sufficient amount of overlapping samples. In reality, the set of overlapping
samples may be small, leaving the majority of the non-overlapping data
unutilized. In this paper, we propose Federated Multi-View Training (FedMVT), a
semi-supervised learning approach that improves the performance of VFL with
limited overlapping samples. FedMVT estimates representations for missing
features and predicts pseudo-labels for unlabeled samples to expand training
set, and trains three classifiers jointly based upon different views of the
input to improve model's representation learning. FedMVT does not require
parties to share their original data and model parameters, thus preserving data
privacy. We conduct experiments on the NUS-WIDE and the CIFAR10. The
experimental results demonstrate that FedMVT significantly outperforms vanilla
VFL that only utilizes overlapping samples, and improves the performance of the
local model in the party that owns labels.
",federate learning allow many party collaboratively build model without expose datum particularly vertical federate learning vfl enable party build robust share machine learning model base upon distribute feature sample however vfl require party share sufficient amount overlap sample reality set overlap sample may small leave majority non overlapping datum unutilized paper propose federate multi view training fedmvt semi supervised learning approach improve performance vfl limited overlap sample fedmvt estimate representation miss feature predict pseudo labels unlabeled sample expand training set train three classifier jointly base upon different view input improve model representation learn fedmvt require party share original data model parameter thus preserve datum privacy conduct experiment nus wide cifar10 experimental result demonstrate fedmvt significantly outperform vanilla vfl utilize overlap sample improve performance local model party own label
Learning to Solve PDE-constrained Inverse Problems with Graph Networks,"Learned graph neural networks (GNNs) have recently been established as fast
and accurate alternatives for principled solvers in simulating the dynamics of
physical systems. In many application domains across science and engineering,
however, we are not only interested in a forward simulation but also in solving
inverse problems with constraints defined by a partial differential equation
(PDE). Here we explore GNNs to solve such PDE-constrained inverse problems.
Given a sparse set of measurements, we are interested in recovering the initial
condition or parameters of the PDE. We demonstrate that GNNs combined with
autodecoder-style priors are well-suited for these tasks, achieving more
accurate estimates of initial conditions or physical parameters than other
learned approaches when applied to the wave equation or Navier-Stokes
equations. We also demonstrate computational speedups of up to 90x using GNNs
compared to principled solvers. Project page:
https://cyanzhao42.github.io/LearnInverseProblem",learn graph neural network gnns recently establish fast accurate alternative principle solver simulate dynamic physical system many application domain across science engineering however interested forward simulation also solve inverse problem constraint define partial differential equation pde explore gnn solve pde constrain inverse problem give sparse set measurement interested recover initial condition parameter pde demonstrate gnn combine autodecoder style prior well suit task achieve accurate estimate initial condition physical parameter learn approach apply wave equation navi stoke equation also demonstrate computational speedup 90x use gnn compare principle solver project page https
"Resampling methods for parameter-free and robust feature selection with
  mutual information","  Combining the mutual information criterion with a forward feature selection
strategy offers a good trade-off between optimality of the selected feature
subset and computation time. However, it requires to set the parameter(s) of
the mutual information estimator and to determine when to halt the forward
procedure. These two choices are difficult to make because, as the
dimensionality of the subset increases, the estimation of the mutual
information becomes less and less reliable. This paper proposes to use
resampling methods, a K-fold cross-validation and the permutation test, to
address both issues. The resampling methods bring information about the
variance of the estimator, information which can then be used to automatically
set the parameter and to calculate a threshold to stop the forward procedure.
The procedure is illustrated on a synthetic dataset as well as on real-world
examples.
",combine mutual information criterion forward feature selection strategy offer good trade off optimality select feature subset computation time however require set parameter mutual information estimator determine halt forward procedure two choice difficult make dimensionality subset increase estimation mutual information become less less reliable paper proposes use resample method k fold cross validation permutation test address issue resample method bring information variance estimator information use automatically set parameter calculate threshold stop forward procedure procedure illustrate synthetic dataset well real world example
"Efficient Reachability Analysis of Closed-Loop Systems with Neural
  Network Controllers","  Neural Networks (NNs) can provide major empirical performance improvements
for robotic systems, but they also introduce challenges in formally analyzing
those systems' safety properties. In particular, this work focuses on
estimating the forward reachable set of closed-loop systems with NN
controllers. Recent work provides bounds on these reachable sets, yet the
computationally efficient approaches provide overly conservative bounds (thus
cannot be used to verify useful properties), whereas tighter methods are too
intensive for online computation. This work bridges the gap by formulating a
convex optimization problem for reachability analysis for closed-loop systems
with NN controllers. While the solutions are less tight than prior semidefinite
program-based methods, they are substantially faster to compute, and some of
the available computation time can be used to refine the bounds through input
set partitioning, which more than overcomes the tightness gap. The proposed
framework further considers systems with measurement and process noise, thus
being applicable to realistic systems with uncertainty. Finally, numerical
comparisons show $10\times$ reduction in conservatism in $\frac{1}{2}$ of the
computation time compared to the state-of-the-art, and the ability to handle
various sources of uncertainty is highlighted on a quadrotor model.
",neural network nn provide major empirical performance improvement robotic system also introduce challenge formally analyze system safety property particular work focus estimate forward reachable set closed loop system nn controller recent work provide bound reachable set yet computationally efficient approach provide overly conservative bound thus use verify useful property whereas tight method intensive online computation work bridge gap formulate convex optimization problem reachability analysis closed loop system nn controller solution less tight prior semidefinite program base method substantially fast compute available computation time use refine bound input set partitioning overcome tightness gap propose framework consider system measurement process noise thus applicable realistic system uncertainty finally numerical comparison show reduction conservatism 1 2 computation time compare state of the art ability handle various source uncertainty highlight quadrotor model
Bring Your Own Algorithm for Optimal Differentially Private Stochastic Minimax Optimization,"We study differentially private (DP) algorithms for smooth stochastic minimax
optimization, with stochastic minimization as a byproduct. The holy grail of
these settings is to guarantee the optimal trade-off between the privacy and
the excess population loss, using an algorithm with a linear time-complexity in
the number of training samples. We provide a general framework for solving
differentially private stochastic minimax optimization (DP-SMO) problems, which
enables the practitioners to bring their own base optimization algorithm and
use it as a black-box to obtain the near-optimal privacy-loss trade-off. Our
framework is inspired from the recently proposed Phased-ERM method [20] for
nonsmooth differentially private stochastic convex optimization (DP-SCO), which
exploits the stability of the empirical risk minimization (ERM) for the privacy
guarantee. The flexibility of our approach enables us to sidestep the
requirement that the base algorithm needs to have bounded sensitivity, and
allows the use of sophisticated variance-reduced accelerated methods to achieve
near-linear time-complexity. To the best of our knowledge, these are the first
linear-time optimal algorithms, up to logarithmic factors, for smooth DP-SMO
when the objective is (strongly-)convex-(strongly-)concave. Additionally, based
on our flexible framework, we derive a new family of near-linear time
algorithms for smooth DP-SCO with optimal privacy-loss trade-offs for a wider
range of smoothness parameters compared to previous algorithms.",study differentially private dp algorithm smooth stochastic minimax optimization stochastic minimization byproduct holy grail setting guarantee optimal trade off privacy excess population loss use algorithm linear time complexity number training sample provide general framework solve differentially private stochastic minimax optimization dp smo problem enable practitioner bring base optimization algorithm use black box obtain near optimal privacy loss trade off framework inspire recently propose phase erm method 20 nonsmooth differentially private stochastic convex optimization dp sco exploit stability empirical risk minimization erm privacy guarantee flexibility approach enable we sidestep requirement base algorithm need bounded sensitivity allow use sophisticated variance reduce accelerated method achieve near linear time complexity good knowledge first linear time optimal algorithms logarithmic factor smooth dp smo objective strongly- convex- strongly- concave additionally base flexible framework derive new family near linear time algorithm smooth dp sco optimal privacy loss trade off wide range smoothness parameter compare previous algorithm
"Viewpoint Optimization for Autonomous Strawberry Harvesting with Deep
  Reinforcement Learning","  Autonomous harvesting may provide a viable solution to mounting labor
pressures in the United States's strawberry industry. However, due to
bottlenecks in machine perception and economic viability, a profitable and
commercially adopted strawberry harvesting system remains elusive. In this
research, we explore the feasibility of using deep reinforcement learning to
overcome these bottlenecks and develop a practical algorithm to address the
sub-objective of viewpoint optimization, or the development of a control policy
to direct a camera to favorable vantage points for autonomous harvesting. We
evaluate the algorithm's performance in a custom, open-source simulated
environment and observe encouraging results. Our trained agent yields 8.7 times
higher returns than random actions and 8.8 percent faster exploration than our
best baseline policy, which uses visual servoing. Visual investigation shows
the agent is able to fixate on favorable viewpoints, despite having no explicit
means to propagate information through time. Overall, we conclude that deep
reinforcement learning is a promising area of research to advance the state of
the art in autonomous strawberry harvesting.
",autonomous harvesting may provide viable solution mount labor pressure united states strawberry industry however due bottleneck machine perception economic viability profitable commercially adopt strawberry harvesting system remain elusive research explore feasibility use deep reinforcement learning overcome bottleneck develop practical algorithm address sub objective viewpoint optimization development control policy direct camera favorable vantage point autonomous harvesting evaluate algorithm performance custom open source simulated environment observe encourage result train agent yield times high return random action percent fast exploration good baseline policy use visual servoe visual investigation show agent able fixate favorable viewpoint despite explicit mean propagate information time overall conclude deep reinforcement learn promising area research advance state art autonomous strawberry harvesting
"Deep Reinforcement Learning for Real-Time Optimization of Pumps in Water
  Distribution Systems","  Real-time control of pumps can be an infeasible task in water distribution
systems (WDSs) because the calculation to find the optimal pump speeds is
resource-intensive. The computational need cannot be lowered even with the
capabilities of smart water networks when conventional optimization techniques
are used. Deep reinforcement learning (DRL) is presented here as a controller
of pumps in two WDSs. An agent based on a dueling deep q-network is trained to
maintain the pump speeds based on instantaneous nodal pressure data. General
optimization techniques (e.g., Nelder-Mead method, differential evolution)
serve as baselines. The total efficiency achieved by the DRL agent compared to
the best performing baseline is above 0.98, whereas the speedup is around 2x
compared to that. The main contribution of the presented approach is that the
agent can run the pumps in real-time because it depends only on measurement
data. If the WDS is replaced with a hydraulic simulation, the agent still
outperforms conventional techniques in search speed.
",real time control pump infeasible task water distribution system wdss calculation find optimal pump speed resource intensive computational need lower even capability smart water network conventional optimization technique use deep reinforcement learning drl present controller pump two wdss agent base duel deep q network train maintain pump speed base instantaneous nodal pressure datum general optimization technique nelder mead method differential evolution serve baseline total efficiency achieve drl agent compare well perform baseline whereas speedup around 2x compare main contribution present approach agent run pump real time depend measurement datum wds replace hydraulic simulation agent still outperform conventional technique search speed
"Semi-supervised learning of images with strong rotational disorder:
  assembling nanoparticle libraries","  The proliferation of optical, electron, and scanning probe microscopies gives
rise to large volumes of imaging data of objects as diversified as cells,
bacteria, pollen, to nanoparticles and atoms and molecules. In most cases, the
experimental data streams contain images having arbitrary rotations and
translations within the image. At the same time, for many cases, small amounts
of labeled data are available in the form of prior published results, image
collections, and catalogs, or even theoretical models. Here we develop an
approach that allows generalizing from a small subset of labeled data with a
weak orientational disorder to a large unlabeled dataset with a much stronger
orientational (and positional) disorder, i.e., it performs a classification of
image data given a small number of examples even in the presence of a
distribution shift between the labeled and unlabeled parts. This approach is
based on the semi-supervised rotationally invariant variational autoencoder
(ss-rVAE) model consisting of the encoder-decoder ""block"" that learns a
rotationally (and translationally) invariant continuous latent representation
of data and a classifier that encodes data into a finite number of discrete
classes. The classifier part of the trained ss-rVAE inherits the rotational
(and translational) invariances and can be deployed independently of the other
parts of the model. The performance of the ss-rVAE is illustrated using the
synthetic data sets with known factors of variation. We further demonstrate its
application for experimental data sets of nanoparticles, creating nanoparticle
libraries and disentangling the representations defining the physical factors
of variation in the data. The code reproducing the results is available at
https://github.com/ziatdinovmax/Semi-Supervised-VAE-nanoparticles.
",proliferation optical electron scan probe microscopy give rise large volume image datum object diversify cell bacteria pollen nanoparticle atom molecule case experimental data stream contain image arbitrary rotation translation within image time many case small amount label datum available form prior publish result image collection catalog even theoretical model develop approach allow generalize small subset label datum weak orientational disorder large unlabeled dataset much strong orientational positional disorder perform classification image datum give small number example even presence distribution shift label unlabeled part approach base semi supervised rotationally invariant variational autoencoder ss rvae model consist encoder decoder block learn rotationally translationally invariant continuous latent representation datum classifier encode datum finite number discrete class classifi part train ss rvae inherit rotational translational invariance deploy independently part model performance ss rvae illustrate use synthetic datum set know factor variation demonstrate application experimental datum set nanoparticle create nanoparticle library disentangle representation define physical factor variation data code reproducing result available https
Sequential sampling of Gaussian process latent variable models,"  We consider the problem of inferring a latent function in a probabilistic
model of data. When dependencies of the latent function are specified by a
Gaussian process and the data likelihood is complex, efficient computation
often involve Markov chain Monte Carlo sampling with limited applicability to
large data sets. We extend some of these techniques to scale efficiently when
the problem exhibits a sequential structure. We propose an approximation that
enables sequential sampling of both latent variables and associated parameters.
We demonstrate strong performance in growing-data settings that would otherwise
be unfeasible with naive, non-sequential sampling.
",consider problem infer latent function probabilistic model data dependencie latent function specify gaussian process datum likelihood complex efficient computation often involve markov chain monte carlo sampling limited applicability large datum set extend technique scale efficiently problem exhibit sequential structure propose approximation enable sequential sampling latent variable associate parameter demonstrate strong performance grow data setting would otherwise unfeasible naive non sequential sampling
Local Differential Privacy for Deep Learning,"  The internet of things (IoT) is transforming major industries including but
not limited to healthcare, agriculture, finance, energy, and transportation.
IoT platforms are continually improving with innovations such as the
amalgamation of software-defined networks (SDN) and network function
virtualization (NFV) in the edge-cloud interplay. Deep learning (DL) is
becoming popular due to its remarkable accuracy when trained with a massive
amount of data, such as generated by IoT. However, DL algorithms tend to leak
privacy when trained on highly sensitive crowd-sourced data such as medical
data. Existing privacy-preserving DL algorithms rely on the traditional
server-centric approaches requiring high processing powers. We propose a new
local differentially private (LDP) algorithm named LATENT that redesigns the
training process. LATENT enables a data owner to add a randomization layer
before data leave the data owners' devices and reach a potentially untrusted
machine learning service. This feature is achieved by splitting the
architecture of a convolutional neural network (CNN) into three layers: (1)
convolutional module, (2) randomization module, and (3) fully connected module.
Hence, the randomization module can operate as an NFV privacy preservation
service in an SDN-controlled NFV, making LATENT more practical for IoT-driven
cloud-based environments compared to existing approaches. The randomization
module employs a newly proposed LDP protocol named utility enhancing
randomization, which allows LATENT to maintain high utility compared to
existing LDP protocols. Our experimental evaluation of LATENT on convolutional
deep neural networks demonstrates excellent accuracy (e.g. 91%- 96%) with high
model quality even under low privacy budgets (e.g. $\varepsilon=0.5$).
",internet thing iot transform major industry include limited healthcare agriculture finance energy transportation iot platform continually improve innovation amalgamation software define network sdn network function virtualization nfv edge cloud interplay deep learning dl become popular due remarkable accuracy train massive amount datum generate iot however dl algorithms tend leak privacy train highly sensitive crowd source datum medical datum exist privacy preserve dl algorithm rely traditional server centric approach require high processing power propose new local differentially private ldp algorithm name latent redesign training process latent enable datum owner add randomization layer datum leave datum owner device reach potentially untrusted machine learn service feature achieve split architecture convolutional neural network cnn three layer 1 convolutional module 2 randomization module 3 fully connect module hence randomization module operate nfv privacy preservation service sdn control nfv make latent practical iot drive cloud base environment compare exist approach randomization module employs newly propose ldp protocol name utility enhance randomization allow latent maintain high utility compare exist ldp protocols experimental evaluation latent convolutional deep neural network demonstrate excellent accuracy 91 96 high model quality even low privacy budget
Re-designing cities with conditional adversarial networks,"  This paper introduces a conditional generative adversarial network to
redesign a street-level image of urban scenes by generating 1) an urban
intervention policy, 2) an attention map that localises where intervention is
needed, 3) a high-resolution street-level image (1024 X 1024 or 1536 X1536)
after implementing the intervention. We also introduce a new dataset that
comprises aligned street-level images of before and after urban interventions
from real-life scenarios that make this research possible. The introduced
method has been trained on different ranges of urban interventions applied to
realistic images. The trained model shows strong performance in re-modelling
cities, outperforming existing methods that apply image-to-image translation in
other domains that is computed in a single GPU. This research opens the door
for machine intelligence to play a role in re-thinking and re-designing the
different attributes of cities based on adversarial learning, going beyond the
mainstream of facial landmarks manipulation or image synthesis from semantic
segmentation.
",paper introduce conditional generative adversarial network redesign street level image urban scene generate 1 urban intervention policy 2 attention map localise intervention need 3 high resolution street level image 1024 x 1024 1536 x1536 implement intervention also introduce new dataset comprise align street level image urban intervention real life scenario make research possible introduce method train different range urban intervention apply realistic image train model show strong performance re model city outperform exist method apply image to image translation domain compute single gpu research open door machine intelligence play role re thinking re design different attribute city base adversarial learning go beyond mainstream facial landmark manipulation image synthesis semantic segmentation
"G-CoS: GNN-Accelerator Co-Search Towards Both Better Accuracy and
  Efficiency","  Graph Neural Networks (GNNs) have emerged as the state-of-the-art (SOTA)
method for graph-based learning tasks. However, it still remains prohibitively
challenging to inference GNNs over large graph datasets, limiting their
application to large-scale real-world tasks. While end-to-end jointly
optimizing GNNs and their accelerators is promising in boosting GNNs' inference
efficiency and expediting the design process, it is still underexplored due to
the vast and distinct design spaces of GNNs and their accelerators. In this
work, we propose G-CoS, a GNN and accelerator co-search framework that can
automatically search for matched GNN structures and accelerators to maximize
both task accuracy and acceleration efficiency. Specifically, GCoS integrates
two major enabling components: (1) a generic GNN accelerator search space which
is applicable to various GNN structures and (2) a one-shot GNN and accelerator
co-search algorithm that enables simultaneous and efficient search for optimal
GNN structures and their matched accelerators. To the best of our knowledge,
G-CoS is the first co-search framework for GNNs and their accelerators.
Extensive experiments and ablation studies show that the GNNs and accelerators
generated by G-CoS consistently outperform SOTA GNNs and GNN accelerators in
terms of both task accuracy and hardware efficiency, while only requiring a few
hours for the end-to-end generation of the best matched GNNs and their
accelerators.
",graph neural network gnns emerge state of the art sota method graph base learning task however still remain prohibitively challenge inference gnns large graph dataset limit application large scale real world task end to end jointly optimize gnns accelerator promise boost gnns inference efficiency expedite design process still underexplore due vast distinct design space gnn accelerator work propose g cos gnn accelerator co search framework automatically search match gnn structure accelerator maximize task accuracy acceleration efficiency specifically gcos integrate two major enable component 1 generic gnn accelerator search space applicable various gnn structure 2 one shot gnn accelerator co search algorithm enable simultaneous efficient search optimal gnn structure match accelerator good knowledge g cos first co search framework gnn accelerator extensive experiment ablation study show gnns accelerator generate g cos consistently outperform sota gnns gnn accelerator term task accuracy hardware efficiency require hour end to end generation well match gnn accelerator
"AMI-Net+: A Novel Multi-Instance Neural Network for Medical Diagnosis
  from Incomplete and Imbalanced Data","  In medical real-world study (RWS), how to fully utilize the fragmentary and
scarce information in model training to generate the solid diagnosis results is
a challenging task. In this work, we introduce a novel multi-instance neural
network, AMI-Net+, to train and predict from the incomplete and extremely
imbalanced data. It is more effective than the state-of-art method, AMI-Net.
First, we also implement embedding, multi-head attention and gated
attention-based multi-instance pooling to capture the relations of symptoms
themselves and with the given disease. Besides, we propose var-ious
improvements to AMI-Net, that the cross-entropy loss is replaced by focal loss
and we propose a novel self-adaptive multi-instance pooling method on
instance-level to obtain the bag representation. We validate the performance of
AMI-Net+ on two real-world datasets, from two different medical domains.
Results show that our approach outperforms other base-line models by a
considerable margin.
",medical real world study rws fully utilize fragmentary scarce information model training generate solid diagnosis result challenge task work introduce novel multi instance neural network train predict incomplete extremely imbalance datum effective state of art method ami net first also implement embed multi head attention gate attention base multi instance pool capture relation symptom give disease besides propose var ious improvement ami net cross entropy loss replace focal loss propose novel self adaptive multi instance pool method instance level obtain bag representation validate performance two real world dataset two different medical domain result show approach outperform base line model considerable margin
Adversarial Generation of Continuous Images,"  In most existing learning systems, images are typically viewed as 2D pixel
arrays. However, in another paradigm gaining popularity, a 2D image is
represented as an implicit neural representation (INR) - an MLP that predicts
an RGB pixel value given its (x,y) coordinate. In this paper, we propose two
novel architectural techniques for building INR-based image decoders:
factorized multiplicative modulation and multi-scale INRs, and use them to
build a state-of-the-art continuous image GAN. Previous attempts to adapt INRs
for image generation were limited to MNIST-like datasets and do not scale to
complex real-world data. Our proposed INR-GAN architecture improves the
performance of continuous image generators by several times, greatly reducing
the gap between continuous image GANs and pixel-based ones. Apart from that, we
explore several exciting properties of the INR-based decoders, like
out-of-the-box superresolution, meaningful image-space interpolation,
accelerated inference of low-resolution images, an ability to extrapolate
outside of image boundaries, and strong geometric prior. The project page is
located at https://universome.github.io/inr-gan.
",exist learning system image typically view 2d pixel array however another paradigm gain popularity 2d image represent implicit neural representation inr mlp predict rgb pixel value give x coordinate paper propose two novel architectural technique build inr base image decoder factorize multiplicative modulation multi scale inrs use build state of the art continuous image gin previous attempt adapt inrs image generation limit mnist like dataset scale complex real world datum propose inr gan architecture improve performance continuous image generator several time greatly reduce gap continuous image gan pixel base one apart explore several exciting property inr base decoder like out of the box superresolution meaningful image space interpolation accelerate inference low resolution image ability extrapolate outside image boundary strong geometric prior project page locate https
Skynet: A Top Deep RL Agent in the Inaugural Pommerman Team Competition,"  The Pommerman Team Environment is a recently proposed benchmark which
involves a multi-agent domain with challenges such as partial observability,
decentralized execution (without communication), and very sparse and delayed
rewards. The inaugural Pommerman Team Competition held at NeurIPS 2018 hosted
25 participants who submitted a team of 2 agents. Our submission
nn_team_skynet955_skynet955 won 2nd place of the ""learning agents'' category.
Our team is composed of 2 neural networks trained with state of the art deep
reinforcement learning algorithms and makes use of concepts like reward
shaping, curriculum learning, and an automatic reasoning module for action
pruning. Here, we describe these elements and additionally we present a
collection of open-sourced agents that can be used for training and testing in
the Pommerman environment. Code available at:
https://github.com/BorealisAI/pommerman-baseline
",pommerman team environment recently propose benchmark involve multi agent domain challenge partial observability decentralize execution without communication sparse delay reward inaugural pommerman team competition hold neurip 2018 host 25 participant submit team 2 agent submission nn_team_skynet955_skynet955 2nd place learn agent category team compose 2 neural network train state art deep reinforcement learning algorithm make use concept like reward shape curriculum learn automatic reasoning module action prune describe element additionally present collection open source agent use training testing pommerman environment code available https
"A clustering approach to time series forecasting using neural networks:
  A comparative study on distance-based vs. feature-based clustering methods","  Time series forecasting has gained lots of attention recently; this is
because many real-world phenomena can be modeled as time series. The massive
volume of data and recent advancements in the processing power of the computers
enable researchers to develop more sophisticated machine learning algorithms
such as neural networks to forecast the time series data. In this paper, we
propose various neural network architectures to forecast the time series data
using the dynamic measurements; moreover, we introduce various architectures on
how to combine static and dynamic measurements for forecasting. We also
investigate the importance of performing techniques such as anomaly detection
and clustering on forecasting accuracy. Our results indicate that clustering
can improve the overall prediction time as well as improve the forecasting
performance of the neural network. Furthermore, we show that feature-based
clustering can outperform the distance-based clustering in terms of speed and
efficiency. Finally, our results indicate that adding more predictors to
forecast the target variable will not necessarily improve the forecasting
accuracy.
",time series forecasting gain lot attention recently many real world phenomenon model time series massive volume datum recent advancement processing power computer enable researcher develop sophisticated machine learning algorithm neural network forecast time series data paper propose various neural network architecture forecast time series datum use dynamic measurement moreover introduce various architecture combine static dynamic measurement forecast also investigate importance perform technique anomaly detection cluster forecasting accuracy result indicate cluster improve overall prediction time well improve forecasting performance neural network furthermore show feature base clustering outperform distance base clustering term speed efficiency finally result indicate add predictor forecast target variable necessarily improve forecasting accuracy
BIOMRC: A Dataset for Biomedical Machine Reading Comprehension,"  We introduce BIOMRC, a large-scale cloze-style biomedical MRC dataset. Care
was taken to reduce noise, compared to the previous BIOREAD dataset of Pappas
et al. (2018). Experiments show that simple heuristics do not perform well on
the new dataset, and that two neural MRC models that had been tested on BIOREAD
perform much better on BIOMRC, indicating that the new dataset is indeed less
noisy or at least that its task is more feasible. Non-expert human performance
is also higher on the new dataset compared to BIOREAD, and biomedical experts
perform even better. We also introduce a new BERT-based MRC model, the best
version of which substantially outperforms all other methods tested, reaching
or surpassing the accuracy of biomedical experts in some experiments. We make
the new dataset available in three different sizes, also releasing our code,
and providing a leaderboard.
",introduce biomrc large scale cloze style biomedical mrc dataset care take reduce noise compare previous bioread dataset pappa et al 2018 experiment show simple heuristic perform well new dataset two neural mrc model test bioread perform much well biomrc indicate new dataset indeed less noisy least task feasible non expert human performance also high new dataset compare bioread biomedical expert perform even well also introduce new bert base mrc model good version substantially outperform method test reach surpass accuracy biomedical expert experiment make new dataset available three different size also release code provide leaderboard
"Learning in Restless Multi-Armed Bandits via Adaptive Arm Sequencing
  Rules","  We consider a class of restless multi-armed bandit (RMAB) problems with
unknown arm dynamics. At each time, a player chooses an arm out of N arms to
play, referred to as an active arm, and receives a random reward from a finite
set of reward states. The reward state of the active arm transits according to
an unknown Markovian dynamics. The reward state of passive arms (which are not
chosen to play at time t) evolves according to an arbitrary unknown random
process. The objective is an arm-selection policy that minimizes the regret,
defined as the reward loss with respect to a player that always plays the most
rewarding arm. This class of RMAB problems has been studied recently in the
context of communication networks and financial investment applications. We
develop a strategy that selects arms to be played in a consecutive manner,
dubbed Adaptive Sequencing Rules (ASR) algorithm. The sequencing rules for
selecting arms under the ASR algorithm are adaptively updated and controlled by
the current sample reward means. By designing judiciously the adaptive
sequencing rules, we show that the ASR algorithm achieves a logarithmic regret
order with time, and a finite-sample bound on the regret is established.
Although existing methods have shown a logarithmic regret order with time in
this RMAB setting, the theoretical analysis shows a significant improvement in
the regret scaling with respect to the system parameters under ASR. Extensive
simulation results support the theoretical study and demonstrate strong
performance of the algorithm as compared to existing methods.
",consider class restless multi armed bandit rmab problem unknown arm dynamic time player choose arm n arm play refer active arm receive random reward finite set reward states reward state active arm transit accord unknown markovian dynamic reward state passive arm choose play time evolve accord arbitrary unknown random process objective arm selection policy minimize regret define reward loss respect player always play rewarding arm class rmab problem study recently context communication network financial investment application develop strategy select arm play consecutive manner dub adaptive sequencing rule asr algorithm sequence rule select arm asr algorithm adaptively update control current sample reward mean design judiciously adaptive sequence rule show asr algorithm achieve logarithmic regret order time finite sample bind regret establish although existing method show logarithmic regret order time rmab set theoretical analysis show significant improvement regret scale respect system parameter asr extensive simulation result support theoretical study demonstrate strong performance algorithm compare exist method
"DeepDownscale: a Deep Learning Strategy for High-Resolution Weather
  Forecast","  Running high-resolution physical models is computationally expensive and
essential for many disciplines. Agriculture, transportation, and energy are
sectors that depend on high-resolution weather models, which typically consume
many hours of large High Performance Computing (HPC) systems to deliver timely
results. Many users cannot afford to run the desired resolution and are forced
to use low resolution output. One simple solution is to interpolate results for
visualization. It is also possible to combine an ensemble of low resolution
models to obtain a better prediction. However, these approaches fail to capture
the redundant information and patterns in the low-resolution input that could
help improve the quality of prediction. In this paper, we propose and evaluate
a strategy based on a deep neural network to learn a high-resolution
representation from low-resolution predictions using weather forecast as a
practical use case. We take a supervised learning approach, since obtaining
labeled data can be done automatically. Our results show significant
improvement when compared with standard practices and the strategy is still
lightweight enough to run on modest computer systems.
",run high resolution physical model computationally expensive essential many discipline agriculture transportation energy sector depend high resolution weather model typically consume many hour large high performance compute hpc system deliver timely result many user afford run desire resolution force use low resolution output one simple solution interpolate result visualization also possible combine ensemble low resolution model obtain well prediction however approach fail capture redundant information pattern low resolution input could help improve quality prediction paper propose evaluate strategy base deep neural network learn high resolution representation low resolution prediction use weather forecast practical use case take supervised learning approach since obtain label datum do automatically result show significant improvement compare standard practice strategy still lightweight enough run modest computer system
"Motion-from-Blur: 3D Shape and Motion Estimation of Motion-blurred
  Objects in Videos","  We propose a method for jointly estimating the 3D motion, 3D shape, and
appearance of highly motion-blurred objects from a video. To this end, we model
the blurred appearance of a fast moving object in a generative fashion by
parametrizing its 3D position, rotation, velocity, acceleration, bounces,
shape, and texture over the duration of a predefined time window spanning
multiple frames. Using differentiable rendering, we are able to estimate all
parameters by minimizing the pixel-wise reprojection error to the input video
via backpropagating through a rendering pipeline that accounts for motion blur
by averaging the graphics output over short time intervals. For that purpose,
we also estimate the camera exposure gap time within the same optimization. To
account for abrupt motion changes like bounces, we model the motion trajectory
as a piece-wise polynomial, and we are able to estimate the specific time of
the bounce at sub-frame accuracy. Experiments on established benchmark datasets
demonstrate that our method outperforms previous methods for fast moving object
deblurring and 3D reconstruction.
",propose method jointly estimate 3d motion 3d shape appearance highly motion blur object video end model blur appearance fast move object generative fashion parametrize 3d position rotation velocity acceleration bounce shape texture duration predefine time window span multiple frame use differentiable render able estimate parameter minimize pixel wise reprojection error input video via backpropagate render pipeline account motion blur average graphic output short time interval purpose also estimate camera exposure gap time within optimization account abrupt motion change like bounce model motion trajectory piece wise polynomial able estimate specific time bounce sub frame accuracy experiment establish benchmark dataset demonstrate method outperform previous method fast move object deblurre 3d reconstruction
"Learning with $\ell^{0}$-Graph: $\ell^{0}$-Induced Sparse Subspace
  Clustering","  Sparse subspace clustering methods, such as Sparse Subspace Clustering (SSC)
\cite{ElhamifarV13} and $\ell^{1}$-graph \cite{YanW09,ChengYYFH10}, are
effective in partitioning the data that lie in a union of subspaces. Most of
those methods use $\ell^{1}$-norm or $\ell^{2}$-norm with thresholding to
impose the sparsity of the constructed sparse similarity graph, and certain
assumptions, e.g. independence or disjointness, on the subspaces are required
to obtain the subspace-sparse representation, which is the key to their
success. Such assumptions are not guaranteed to hold in practice and they limit
the application of sparse subspace clustering on subspaces with general
location. In this paper, we propose a new sparse subspace clustering method
named $\ell^{0}$-graph. In contrast to the required assumptions on subspaces
for most existing sparse subspace clustering methods, it is proved that
subspace-sparse representation can be obtained by $\ell^{0}$-graph for
arbitrary distinct underlying subspaces almost surely under the mild i.i.d.
assumption on the data generation. We develop a proximal method to obtain the
sub-optimal solution to the optimization problem of $\ell^{0}$-graph with
proved guarantee of convergence. Moreover, we propose a regularized
$\ell^{0}$-graph that encourages nearby data to have similar neighbors so that
the similarity graph is more aligned within each cluster and the graph
connectivity issue is alleviated. Extensive experimental results on various
data sets demonstrate the superiority of $\ell^{0}$-graph compared to other
competing clustering methods, as well as the effectiveness of regularized
$\ell^{0}$-graph.
",sparse subspace clustering method sparse subspace cluster ssc elhamifarv13 1 -graph yanw09 chengyyfh10 effective partition datum lie union subspace method use 1 -norm 2 -norm thresholde impose sparsity construct sparse similarity graph certain assumption independence disjointness subspace require obtain subspace sparse representation key success assumption guarantee hold practice limit application sparse subspace clustering subspace general location paper propose new sparse subspace clustering method name 0 -graph contrast require assumption subspace exist sparse subspace cluster method prove subspace sparse representation obtain 0 -graph arbitrary distinct underlying subspace almost surely mild assumption datum generation develop proximal method obtain sub optimal solution optimization problem 0 -graph prove guarantee convergence moreover propose regularize 0 -graph encourage nearby datum similar neighbor similarity graph align within cluster graph connectivity issue alleviate extensive experimental result various data set demonstrate superiority 0 -graph compare compete cluster method well effectiveness regularize 0 -graph
Statistical Testing on ASR Performance via Blockwise Bootstrap,"  A common question being raised in automatic speech recognition (ASR)
evaluations is how reliable is an observed word error rate (WER) improvement
comparing two ASR systems, where statistical hypothesis testing and confidence
interval (CI) can be utilized to tell whether this improvement is real or only
due to random chance. The bootstrap resampling method has been popular for such
significance analysis which is intuitive and easy to use. However, this method
fails in dealing with dependent data, which is prevalent in speech world - for
example, ASR performance on utterances from the same speaker could be
correlated. In this paper we present blockwise bootstrap approach - by dividing
evaluation utterances into nonoverlapping blocks, this method resamples these
blocks instead of original data. We show that the resulting variance estimator
of absolute WER difference between two ASR systems is consistent under mild
conditions. We also demonstrate the validity of blockwise bootstrap method on
both synthetic and real-world speech data.
",common question raise automatic speech recognition asr evaluations reliable observe word error rate wer improvement compare two asr system statistical hypothesis testing confidence interval ci utilize tell whether improvement real due random chance bootstrap resample method popular significance analysis intuitive easy use however method fail deal dependent datum prevalent speech world example asr performance utterance speaker could correlate paper present blockwise bootstrap approach divide evaluation utterance nonoverlapping block method resample block instead original datum show result variance estimator absolute wer difference two asr system consistent mild condition also demonstrate validity blockwise bootstrap method synthetic real world speech datum
Robust mixture of experts modeling using the skew $t$ distribution,"  Mixture of Experts (MoE) is a popular framework in the fields of statistics
and machine learning for modeling heterogeneity in data for regression,
classification and clustering. MoE for continuous data are usually based on the
normal distribution. However, it is known that for data with asymmetric
behavior, heavy tails and atypical observations, the use of the normal
distribution is unsuitable. We introduce a new robust non-normal mixture of
experts modeling using the skew $t$ distribution. The proposed skew $t$ mixture
of experts, named STMoE, handles these issues of the normal mixtures experts
regarding possibly skewed, heavy-tailed and noisy data. We develop a dedicated
expectation conditional maximization (ECM) algorithm to estimate the model
parameters by monotonically maximizing the observed data log-likelihood. We
describe how the presented model can be used in prediction and in model-based
clustering of regression data. Numerical experiments carried out on simulated
data show the effectiveness and the robustness of the proposed model in fitting
non-linear regression functions as well as in model-based clustering. Then, the
proposed model is applied to the real-world data of tone perception for musical
data analysis, and the one of temperature anomalies for the analysis of climate
change data. The obtained results confirm the usefulness of the model for
practical data analysis applications.
",mixture expert moe popular framework field statistic machine learn model heterogeneity datum regression classification cluster moe continuous datum usually base normal distribution however know datum asymmetric behavior heavy tail atypical observation use normal distribution unsuitable introduce new robust non normal mixture expert model use skew distribution propose skew mixture expert name stmoe handle issue normal mixture expert regard possibly skew heavy tail noisy datum develop dedicated expectation conditional maximization ecm algorithm estimate model parameter monotonically maximize observe datum log likelihood describe present model use prediction model base clustering regression datum numerical experiment carry simulated datum show effectiveness robustness propose model fitting non linear regression function well model base clustering propose model apply real world datum tone perception musical datum analysis one temperature anomaly analysis climate change datum obtain result confirm usefulness model practical datum analysis application
"Automated Graph Machine Learning: Approaches, Libraries and Directions","  Graph machine learning has been extensively studied in both academic and
industry. However, as the literature on graph learning booms with a vast number
of emerging methods and techniques, it becomes increasingly difficult to
manually design the optimal machine learning algorithm for different
graph-related tasks. To tackle the challenge, automated graph machine learning,
which aims at discovering the best hyper-parameter and neural architecture
configuration for different graph tasks/data without manual design, is gaining
an increasing number of attentions from the research community. In this paper,
we extensively discuss automated graph machine approaches, covering
hyper-parameter optimization (HPO) and neural architecture search (NAS) for
graph machine learning. We briefly overview existing libraries designed for
either graph machine learning or automated machine learning respectively, and
further in depth introduce AutoGL, our dedicated and the world's first
open-source library for automated graph machine learning. Last but not least,
we share our insights on future research directions for automated graph machine
learning. This paper is the first systematic and comprehensive discussion of
approaches, libraries as well as directions for automated graph machine
learning.
",graph machine learn extensively study academic industry however literature graph learning boom vast number emerge method technique become increasingly difficult manually design optimal machine learn algorithm different graph relate task tackle challenge automate graph machine learning aim discover good hyper parameter neural architecture configuration different graph without manual design gain increase number attention research community paper extensively discuss automate graph machine approach cover hyper parameter optimization hpo neural architecture search nas graph machine learn briefly overview exist library design either graph machine learn automate machine learn respectively depth introduce autogl dedicate world first open source library automate graph machine learning last least share insight future research direction automate graph machine learn paper first systematic comprehensive discussion approach library well direction automate graph machine learning
"EdiBERT, a generative model for image editing","  Advances in computer vision are pushing the limits of im-age manipulation,
with generative models sampling detailed images on various tasks. However, a
specialized model is often developed and trained for each specific task, even
though many image edition tasks share similarities. In denoising, inpainting,
or image compositing, one always aims at generating a realistic image from a
low-quality one. In this paper, we aim at making a step towards a unified
approach for image editing. To do so, we propose EdiBERT, a bi-directional
transformer trained in the discrete latent space built by a vector-quantized
auto-encoder. We argue that such a bidirectional model is suited for image
manipulation since any patch can be re-sampled conditionally to the whole
image. Using this unique and straightforward training objective, we show that
the resulting model matches state-of-the-art performances on a wide variety of
tasks: image denoising, image completion, and image composition.
",advance computer vision push limit im age manipulation generative model sample detailed image various task however specialized model often develop train specific task even though many image edition task share similarity denoise inpainte image composite one always aim generate realistic image low quality one paper aim make step towards unified approach image editing propose edibert bi directional transformer train discrete latent space build vector quantize auto encoder argue bidirectional model suit image manipulation since patch re sample conditionally whole image use unique straightforward training objective show result model match state of the art performance wide variety task image denoise image completion image composition
Weakly Supervised Scene Text Detection using Deep Reinforcement Learning,"  The challenging field of scene text detection requires complex data
annotation, which is time-consuming and expensive. Techniques, such as weak
supervision, can reduce the amount of data needed. In this paper we propose a
weak supervision method for scene text detection, which makes use of
reinforcement learning (RL). The reward received by the RL agent is estimated
by a neural network, instead of being inferred from ground-truth labels. First,
we enhance an existing supervised RL approach to text detection with several
training optimizations, allowing us to close the performance gap to
regression-based algorithms. We then use our proposed system in a weakly- and
semi-supervised training on real-world data. Our results show that training in
a weakly supervised setting is feasible. However, we find that using our model
in a semi-supervised setting , e.g. when combining labeled synthetic data with
unannotated real-world data, produces the best results.
",challenge field scene text detection require complex datum annotation time consume expensive technique weak supervision reduce amount datum need paper propose weak supervision method scene text detection make use reinforcement learning rl reward receive rl agent estimate neural network instead infer ground truth label first enhance exist supervise rl approach text detection several training optimization allow we close performance gap regression base algorithm use propose system weakly- semi supervised training real world datum result show training weakly supervise setting feasible however find use model semi supervised setting combine label synthetic datum unannotate real world datum produce good result
Heterogeneous Multi-task Metric Learning across Multiple Domains,"  Distance metric learning (DML) plays a crucial role in diverse machine
learning algorithms and applications. When the labeled information in target
domain is limited, transfer metric learning (TML) helps to learn the metric by
leveraging the sufficient information from other related domains. Multi-task
metric learning (MTML), which can be regarded as a special case of TML,
performs transfer across all related domains. Current TML tools usually assume
that the same feature representation is exploited for different domains.
However, in real-world applications, data may be drawn from heterogeneous
domains. Heterogeneous transfer learning approaches can be adopted to remedy
this drawback by deriving a metric from the learned transformation across
different domains. But they are often limited in that only two domains can be
handled. To appropriately handle multiple domains, we develop a novel
heterogeneous multi-task metric learning (HMTML) framework. In HMTML, the
metrics of all different domains are learned together. The transformations
derived from the metrics are utilized to induce a common subspace, and the
high-order covariance among the predictive structures of these domains is
maximized in this subspace. There do exist a few heterogeneous transfer
learning approaches that deal with multiple domains, but the high-order
statistics (correlation information), which can only be exploited by
simultaneously examining all domains, is ignored in these approaches. Compared
with them, the proposed HMTML can effectively explore such high-order
information, thus obtaining more reliable feature transformations and metrics.
Effectiveness of our method is validated by the extensive and intensive
experiments on text categorization, scene classification, and social image
annotation.
",distance metric learning dml play crucial role diverse machine learn algorithm application label information target domain limited transfer metric learning tml help learn metric leverage sufficient information relate domain multi task metric learning mtml regard special case tml perform transfer across relate domain current tml tool usually assume feature representation exploit different domain however real world application datum may draw heterogeneous domain heterogeneous transfer learning approach adopt remedy drawback derive metric learn transformation across different domain often limit two domain handle appropriately handle multiple domain develop novel heterogeneous multi task metric learning hmtml framework hmtml metric different domain learn together transformation derive metric utilize induce common subspace high order covariance among predictive structure domain maximize subspace exist heterogeneous transfer learning approach deal multiple domain high order statistic correlation information exploit simultaneously examine domain ignore approach compare propose hmtml effectively explore high order information thus obtain reliable feature transformation metric effectiveness method validate extensive intensive experiment text categorization scene classification social image annotation
"On Constructing Confidence Region for Model Parameters in Stochastic
  Gradient Descent via Batch Means","  In this paper, we study a simple algorithm to construct asymptotically valid
confidence regions for model parameters using the batch means method. The main
idea is to cancel out the covariance matrix which is hard/costly to estimate.
In the process of developing the algorithm, we establish process-level
functional central limit theorem for Polyak-Ruppert averaging based stochastic
gradient descent estimators. We also extend the batch means method to
accommodate more general batch size specifications.
",paper study simple algorithm construct asymptotically valid confidence region model parameter use batch mean method main idea cancel covariance matrix estimate process develop algorithm establish process level functional central limit theorem polyak ruppert averaging base stochastic gradient descent estimator also extend batch mean method accommodate general batch size specification
Towards a Solution to Bongard Problems: A Causal Approach,"  To date, Bongard Problems (BP) remain one of the few fortresses of AI history
yet to be raided by the powerful models of the current era. We present a
systematic analysis using modern techniques from the intersection of causality
and AI/ML in a humble effort of reviving research around BPs. Specifically, we
first compile the BPs into a Markov decision process, then secondly pose causal
assumptions on the data generating process arguing for their applicability to
BPs, and finally apply reinforcement learning techniques for solving the BPs
subject to the causal assumptions.
",date bongard problem bp remain one fortress ai history yet raid powerful model current era present systematic analysis use modern technique intersection causality humble effort revive research around bps specifically first compile bps markov decision process secondly pose causal assumption datum generating process argue applicability bps finally apply reinforcement learning technique solve bps subject causal assumption
"MedDistant19: A Challenging Benchmark for Distantly Supervised
  Biomedical Relation Extraction","  Relation Extraction in the biomedical domain is challenging due to the lack
of labeled data and high annotation costs, needing domain experts. Distant
supervision is commonly used as a way to tackle the scarcity of annotated data
by automatically pairing knowledge graph relationships with raw texts.
Distantly Supervised Biomedical Relation Extraction (Bio-DSRE) models can
seemingly produce very accurate results in several benchmarks. However, given
the challenging nature of the task, we set out to investigate the validity of
such impressive results. We probed the datasets used by Amin et al. (2020) and
Hogan et al. (2021) and found a significant overlap between training and
evaluation relationships that, once resolved, reduced the accuracy of the
models by up to 71%. Furthermore, we noticed several inconsistencies with the
data construction process, such as creating negative samples and improper
handling of redundant relationships. We mitigate these issues and present
MedDistant19, a new benchmark dataset obtained by aligning the MEDLINE
abstracts with the widely used SNOMED Clinical Terms (SNOMED CT) knowledge
base. We experimented with several state-of-the-art models achieving an AUC of
55.4% and 49.8% at sentence- and bag-level, showing that there is still plenty
of room for improvement.
",relation extraction biomedical domain challenge due lack label datum high annotation cost need domain expert distant supervision commonly use way tackle scarcity annotate datum automatically pair knowledge graph relationship raw text distantly supervise biomedical relation extraction bio dsre model seemingly produce accurate result several benchmark however give challenge nature task set investigate validity impressive result probe dataset use amin et al 2020 hogan et al 2021 find significant overlap training evaluation relationship resolve reduced accuracy model 71 furthermore notice several inconsistency datum construction process create negative sample improper handle redundant relationship mitigate issue present meddistant19 new benchmark dataset obtain align medline abstract widely use snome clinical term snome ct knowledge base experiment several state of the art model achieve auc sentence- bag level show still plenty room improvement
Diagonal Memory Optimisation for Machine Learning on Micro-controllers,"  As machine learning spreads into more and more application areas, micro
controllers and low power CPUs are increasingly being used to perform inference
with machine learning models. The capability to deploy onto these limited
hardware targets is enabling machine learning models to be used across a
diverse range of new domains. Optimising the inference process on these targets
poses different challenges from either desktop CPU or GPU implementations,
where the small amounts of RAM available on these targets sets limits on size
of models which can be executed. Analysis of the memory use patterns of eleven
machine learning models was performed. Memory load and store patterns were
observed using a modified version of the Valgrind debugging tool, identifying
memory areas holding values necessary for the calculation as inference
progressed. These analyses identified opportunities optimise the memory use of
these models by overlapping the input and output buffers of individual tensor
operations. Three methods are presented which can calculate the safe overlap of
input and output buffers for tensor operations. Ranging from a computationally
expensive approach with the ability to operate on compiled layer operations, to
a versatile analytical solution which requires access to the original source
code of the layer. The diagonal memory optimisation technique is described and
shown to achieve memory savings of up to 34.5% when applied to eleven common
models. Micro-controller targets are identified where it is only possible to
deploy some models if diagonal memory optimisation is used.
",machine learning spread application area micro controller low power cpu increasingly use perform inference machine learning model capability deploy onto limited hardware target enable machine learning model use across diverse range new domain optimise inference process target pose different challenge either desktop cpu gpu implementation small amount ram available target set limit size model execute analysis memory use pattern eleven machine learning model perform memory load store pattern observe use modify version valgrind debug tool identify memory area hold value necessary calculation inference progress analysis identify opportunity optimise memory use model overlap input output buffer individual tensor operation three method present calculate safe overlap input output buffer tensor operation range computationally expensive approach ability operate compile layer operation versatile analytical solution require access original source code layer diagonal memory optimisation technique describe show achieve memory saving apply eleven common model micro controller target identify possible deploy model diagonal memory optimisation use
"Complexity-based partitioning of CSFI problem instances with
  Transformers","  In this paper, we propose a two-steps approach to partition instances of the
Conjunctive Normal Form (CNF) Syntactic Formula Isomorphism problem (CSFI) into
groups of different complexity. First, we build a model, based on the
Transformer architecture, that attempts to solve instances of the CSFI problem.
Then, we leverage the errors of such model and train a second Transformer-based
model to partition the problem instances into groups of different complexity,
thus detecting the ones that can be solved without using too expensive
resources. We evaluate the proposed approach on a pseudo-randomly generated
dataset and obtain promising results. Finally, we discuss the possibility of
extending this approach to other problems based on the same type of textual
representation.
",paper propose two step approach partition instance conjunctive normal form cnf syntactic formula isomorphism problem csfi group different complexity first build model base transformer architecture attempt solve instance csfi problem leverage error model train second transformer base model partition problem instance group different complexity thus detect one solve without use expensive resource evaluate proposed approach pseudo randomly generate dataset obtain promising result finally discuss possibility extend approach problem base type textual representation
Speech Denoising by Accumulating Per-Frequency Modeling Fluctuations,"  We present a method for audio denoising that combines processing done in both
the time domain and the time-frequency domain. Given a noisy audio clip, the
method trains a deep neural network to fit this signal. Since the fitting is
only partly successful and is able to better capture the underlying clean
signal than the noise, the output of the network helps to disentangle the clean
audio from the rest of the signal. This is done by accumulating a fitting score
per time-frequency bin and applying the time-frequency domain filtering based
on the obtained scores. The method is completely unsupervised and only trains
on the specific audio clip that is being denoised. Our experiments demonstrate
favorable performance in comparison to the literature methods. Our code and
samples are available at github.com/mosheman5/DNP and as supplementary. Index
Terms: Audio denoising; Unsupervised learning
",present method audio denoising combine processing do time domain time frequency domain give noisy audio clip method train deep neural network fit signal since fit partly successful able well capture underlie clean signal noise output network help disentangle clean audio rest signal do accumulate fitting score per time frequency bin apply time frequency domain filtering base obtain score method completely unsupervise train specific audio clip denoise experiment demonstrate favorable performance comparison literature method code sample available supplementary index term audio denoise unsupervised learning
Disturbance Grassmann Kernels for Subspace-Based Learning,"  In this paper, we focus on subspace-based learning problems, where data
elements are linear subspaces instead of vectors. To handle this kind of data,
Grassmann kernels were proposed to measure the space structure and used with
classifiers, e.g., Support Vector Machines (SVMs). However, the existing
discriminative algorithms mostly ignore the instability of subspaces, which
would cause the classifiers misled by disturbed instances. Thus we propose
considering all potential disturbance of subspaces in learning processes to
obtain more robust classifiers. Firstly, we derive the dual optimization of
linear classifiers with disturbance subject to a known distribution, resulting
in a new kernel, Disturbance Grassmann (DG) kernel. Secondly, we research into
two kinds of disturbance, relevant to the subspace matrix and singular values
of bases, with which we extend the Projection kernel on Grassmann manifolds to
two new kernels. Experiments on action data indicate that the proposed kernels
perform better compared to state-of-the-art subspace-based methods, even in a
worse environment.
",paper focus subspace base learning problem datum element linear subspace instead vector handle kind data grassmann kernels propose measure space structure use classifier support vector machine svms however exist discriminative algorithm mostly ignore instability subspace would cause classifier mislead disturb instance thus propose consider potential disturbance subspace learn process obtain robust classifier firstly derive dual optimization linear classifier disturbance subject know distribution result new kernel disturbance grassmann dg kernel secondly research two kind disturbance relevant subspace matrix singular value basis extend projection kernel grassmann manifold two new kernels experiment action datum indicate propose kernel perform well compare state of the art subspace base method even bad environment
Coded Distributed Computing with Partial Recovery,"  Coded computation techniques provide robustness against straggling workers in
distributed computing. However, most of the existing schemes require exact
provisioning of the straggling behaviour and ignore the computations carried
out by straggling workers. Moreover, these schemes are typically designed to
recover the desired computation results accurately, while in many machine
learning and iterative optimization algorithms, faster approximate solutions
are known to result in an improvement in the overall convergence time. In this
paper, we first introduce a novel coded matrix-vector multiplication scheme,
called coded computation with partial recovery (CCPR), which benefits from the
advantages of both coded and uncoded computation schemes, and reduces both the
computation time and the decoding complexity by allowing a trade-off between
the accuracy and the speed of computation. We then extend this approach to
distributed implementation of more general computation tasks by proposing a
coded communication scheme with partial recovery, where the results of subtasks
computed by the workers are coded before being communicated. Numerical
simulations on a large linear regression task confirm the benefits of the
proposed distributed computation scheme with partial recovery in terms of the
trade-off between the computation accuracy and latency.
",code computation technique provide robustness straggling worker distribute computing however exist scheme require exact provisioning straggle behaviour ignore computation carry straggle worker moreover scheme typically design recover desire computation result accurately many machine learn iterative optimization algorithm fast approximate solution know result improvement overall convergence time paper first introduce novel code matrix vector multiplication scheme call code computation partial recovery ccpr benefit advantage code uncoded computation scheme reduce computation time decode complexity allow trade off accuracy speed computation extend approach distribute implementation general computation task propose code communication scheme partial recovery result subtask computed worker code communicate numerical simulation large linear regression task confirm benefit propose distribute computation scheme partial recovery term trade off computation accuracy latency
Deep Learning with Logical Constraints,"  In recent years, there has been an increasing interest in exploiting
logically specified background knowledge in order to obtain neural models (i)
with a better performance, (ii) able to learn from less data, and/or (iii)
guaranteed to be compliant with the background knowledge itself, e.g., for
safety-critical applications. In this survey, we retrace such works and
categorize them based on (i) the logical language that they use to express the
background knowledge and (ii) the goals that they achieve.
",recent year increase interest exploit logically specify background knowledge order obtain neural model well performance ii able learn less datum iii guarantee compliant background knowledge safety critical application survey retrace work categorize base logical language use express background knowledge ii goal achieve
Document Neural Autoregressive Distribution Estimation,"  We present an approach based on feed-forward neural networks for learning the
distribution of textual documents. This approach is inspired by the Neural
Autoregressive Distribution Estimator(NADE) model, which has been shown to be a
good estimator of the distribution of discrete-valued igh-dimensional vectors.
In this paper, we present how NADE can successfully be adapted to the case of
textual data, retaining from NADE the property that sampling or computing the
probability of observations can be done exactly and efficiently. The approach
can also be used to learn deep representations of documents that are
competitive to those learned by the alternative topic modeling approaches.
Finally, we describe how the approach can be combined with a regular neural
network N-gram model and substantially improve its performance, by making its
learned representation sensitive to the larger, document-specific context.
",present approach base feed forward neural network learn distribution textual document approach inspire neural autoregressive distribution estimator nade model show good estimator distribution discrete value igh dimensional vector paper present nade successfully adapt case textual datum retain nade property sampling computing probability observation do exactly efficiently approach also use learn deep representation document competitive learn alternative topic modeling approach finally describe approach combine regular neural network n gram model substantially improve performance making learn representation sensitive large document specific context
"Beyond $\mathcal{O}(\sqrt{T})$ Regret for Constrained Online
  Optimization: Gradual Variations and Mirror Prox","  We study constrained online convex optimization, where the constraints
consist of a relatively simple constraint set (e.g. a Euclidean ball) and
multiple functional constraints. Projections onto such decision sets are
usually computationally challenging. So instead of enforcing all constraints
over each slot, we allow decisions to violate these functional constraints but
aim at achieving a low regret and a low cumulative constraint violation over a
horizon of $T$ time slot. The best known bound for solving this problem is
$\mathcal{O}(\sqrt{T})$ regret and $\mathcal{O}(1)$ constraint violation, whose
algorithms and analysis are restricted to Euclidean spaces. In this paper, we
propose a new online primal-dual mirror prox algorithm whose regret is measured
via a total gradient variation $V_*(T)$ over a sequence of $T$ loss functions.
Specifically, we show that the proposed algorithm can achieve an
$\mathcal{O}(\sqrt{V_*(T)})$ regret and $\mathcal{O}(1)$ constraint violation
simultaneously. Such a bound holds in general non-Euclidean spaces, is never
worse than the previously known $\big( \mathcal{O}(\sqrt{T}), \mathcal{O}(1)
\big)$ result, and can be much better on regret when the variation is small.
Furthermore, our algorithm is computationally efficient in that only two mirror
descent steps are required during each slot instead of solving a general
Lagrangian minimization problem. Along the way, our bounds also improve upon
those of previous attempts using mirror-prox-type algorithms solving this
problem, which yield a relatively worse $\mathcal{O}(T^{2/3})$ regret and
$\mathcal{O}(T^{2/3})$ constraint violation.
",study constrain online convex optimization constraint consist relatively simple constraint set euclidean ball multiple functional constraint projection onto decision set usually computationally challenge instead enforce constraint slot allow decision violate functional constraint aim achieve low regret low cumulative constraint violation horizon time slot well know bind solve problem regret 1 constraint violation whose algorithm analysis restrict euclidean space paper propose new online primal dual mirror prox algorithm whose regret measure via total gradient variation v sequence loss function specifically show propose algorithm achieve v regret 1 constraint violation simultaneously bind hold general non euclidean space never bad previously know 1 result much well regret variation small furthermore algorithm computationally efficient two mirror descent step require slot instead solve general lagrangian minimization problem along way bound also improve upon previous attempt use mirror prox type algorithm solve problem yield relatively bad regret constraint violation
On Learning and Testing Decision Tree,"  In this paper, we study learning and testing decision tree of size and depth
that are significantly smaller than the number of attributes $n$.
  Our main result addresses the problem of poly$(n,1/\epsilon)$ time algorithms
with poly$(s,1/\epsilon)$ query complexity (independent of $n$) that
distinguish between functions that are decision trees of size $s$ from
functions that are $\epsilon$-far from any decision tree of size
$\phi(s,1/\epsilon)$, for some function $\phi > s$. The best known result is
the recent one that follows from Blank, Lange and Tan,~\cite{BlancLT20}, that
gives $\phi(s,1/\epsilon)=2^{O((\log^3s)/\epsilon^3)}$. In this paper, we give
a new algorithm that achieves $\phi(s,1/\epsilon)=2^{O(\log^2 (s/\epsilon))}$.
  Moreover, we study the testability of depth-$d$ decision tree and give a {\it
distribution free} tester that distinguishes between depth-$d$ decision tree
and functions that are $\epsilon$-far from depth-$d^2$ decision tree. In
particular, for decision trees of size $s$, the above result holds in the
distribution-free model when the tree depth is $O(\log(s/\epsilon))$.
  We also give other new results in learning and testing of size-$s$ decision
trees and depth-$d$ decision trees that follow from results in the literature
and some results we prove in this paper.
",paper study learn testing decision tree size depth significantly small number attribute n main result address problem poly time algorithm poly query complexity independent n distinguish function decision tree size function -far decision tree size function well know result recent one follow blank lange tan blanclt20 give paper give new algorithm achieves moreover study testability depth- decision tree give distribution free tester distinguishe depth- decision tree function -far depth- decision tree particular decision tree size result hold distribution free model tree depth also give new result learn test size- decision tree depth- decision tree follow result literature result prove paper
"Classification of Buildings' Potential for Seismic Damage by Means of
  Artificial Intelligence Techniques","  Developing a rapid, but also reliable and efficient, method for classifying
the seismic damage potential of buildings constructed in countries with regions
of high seismicity is always at the forefront of modern scientific research.
Such a technique would be essential for estimating the pre-seismic
vulnerability of the buildings, so that the authorities will be able to develop
earthquake safety plans for seismic rehabilitation of the highly
earthquake-susceptible structures. In the last decades, several researchers
have proposed such procedures, some of which were adopted by seismic code
guidelines. These procedures usually utilize methods based either on simple
calculations or on the application of statistics theory. Recently, the increase
of the computers' power has led to the development of modern statistical
methods based on the adoption of Machine Learning algorithms. These methods
have been shown to be useful for predicting seismic performance and classifying
structural damage level by means of extracting patterns from data collected via
various sources. A large training dataset is used for the implementation of the
classification algorithms. To this end, 90 3D R/C buildings with three
different masonry infills' distributions are analysed utilizing Nonlinear Time
History Analysis method for 65 real seismic records. The level of the seismic
damage is expressed in terms of the Maximum Interstory Drift Ratio. A large
number of Machine Learning algorithms is utilized in order to estimate the
buildings' damage response. The most significant conclusion which is extracted
is that the Machine Learning methods that are mathematically well-established
and their operations that are clearly interpretable step by step can be used to
solve some of the most sophisticated real-world problems in consideration with
high accuracy.
",develop rapid also reliable efficient method classify seismic damage potential building construct country region high seismicity always forefront modern scientific research technique would essential estimate pre seismic vulnerability building authority able develop earthquake safety plan seismic rehabilitation highly earthquake susceptible structure last decade several researcher propose procedure adopt seismic code guideline procedure usually utilize method base either simple calculation application statistic theory recently increase computer power lead development modern statistical method base adoption machine learn algorithm method show useful predict seismic performance classify structural damage level mean extract pattern datum collect via various source large training dataset use implementation classification algorithm end 90 3d building three different masonry infill distribution analyse utilize nonlinear time history analysis method 65 real seismic record level seismic damage express term maximum interstory drift ratio large number machine learning algorithm utilize order estimate building damage response significant conclusion extract machine learning method mathematically well establish operation clearly interpretable step step use solve sophisticated real world problem consideration high accuracy
"Sample-based and Feature-based Federated Learning for Unconstrained and
  Constrained Nonconvex Optimization via Mini-batch SSCA","  Federated learning (FL) has become a hot research area in enabling the
collaborative training of machine learning models among multiple clients that
hold sensitive local data. Nevertheless, unconstrained federated optimization
has been studied mainly using stochastic gradient descent (SGD), which may
converge slowly, and constrained federated optimization, which is more
challenging, has not been investigated so far. This paper investigates
sample-based and feature-based federated optimization, respectively, and
considers both unconstrained and constrained nonconvex problems for each of
them. First, we propose FL algorithms using stochastic successive convex
approximation (SSCA) and mini-batch techniques. These algorithms can adequately
exploit the structures of the objective and constraint functions and
incrementally utilize samples. We show that the proposed FL algorithms converge
to stationary points and Karush-Kuhn-Tucker (KKT) points of the respective
unconstrained and constrained nonconvex problems, respectively. Next, we
provide algorithm examples with appealing computational complexity and
communication load per communication round. We show that the proposed algorithm
examples for unconstrained federated optimization are identical to FL
algorithms via momentum SGD and provide an analytical connection between SSCA
and momentum SGD. Finally, numerical experiments demonstrate the inherent
advantages of the proposed algorithms in convergence speeds, communication and
computation costs, and model specifications.
",federate learning fl become hot research area enable collaborative training machine learn model among multiple client hold sensitive local datum nevertheless unconstraine federate optimization study mainly use stochastic gradient descent sgd may converge slowly constrain federated optimization challenge investigate far paper investigate sample base feature base federated optimization respectively consider unconstrained constrained nonconvex problem first propose fl algorithm use stochastic successive convex approximation ssca mini batch technique algorithm adequately exploit structure objective constraint function incrementally utilize sample show propose fl algorithms converge stationary point karush kuhn tucker kkt point respective unconstrained constrained nonconvex problem respectively next provide algorithm example appeal computational complexity communication load per communication round show propose algorithm example unconstrained federated optimization identical fl algorithm via momentum sgd provide analytical connection ssca momentum sgd finally numerical experiment demonstrate inherent advantage propose algorithm convergence speed communication computation cost model specification
Loss Landscape Engineering via Data Regulation on PINNs,"Physics-Informed Neural Networks have shown unique utility in parameterising
the solution of a well-defined partial differential equation using automatic
differentiation and residual losses. Though they provide theoretical guarantees
of convergence, in practice the required training regimes tend to be exacting
and demanding. Through the course of this paper, we take a deep dive into
understanding the loss landscapes associated with a PINN and how that offers
some insight as to why PINNs are fundamentally hard to optimise for. We
demonstrate how PINNs can be forced to converge better towards the solution, by
way of feeding in sparse or coarse data as a regulator. The data regulates and
morphs the topology of the loss landscape associated with the PINN to make it
easily traversable for the minimiser. Data regulation of PINNs helps ease the
optimisation required for convergence by invoking a hybrid
unsupervised-supervised training approach, where the labelled data pushes the
network towards the vicinity of the solution, and the unlabelled regime
fine-tunes it to the solution.",physics inform neural network show unique utility parameterising solution well define partial differential equation use automatic differentiation residual loss though provide theoretical guarantee convergence practice require training regime tend exact demand course paper take deep dive understanding loss landscape associate pinn offer insight pinn fundamentally hard optimise demonstrate pinn force converge well towards solution way feed sparse coarse data regulator datum regulate morph topology loss landscape associate pinn make easily traversable minimiser datum regulation pinn help ease optimisation require convergence invoke hybrid unsupervised supervised training approach label datum push network towards vicinity solution unlabelle regime fine tune solution
AdaSmooth: An Adaptive Learning Rate Method based on Effective Ratio,"  It is well known that we need to choose the hyper-parameters in Momentum,
AdaGrad, AdaDelta, and other alternative stochastic optimizers. While in many
cases, the hyper-parameters are tuned tediously based on experience becoming
more of an art than science. We present a novel per-dimension learning rate
method for gradient descent called AdaSmooth. The method is insensitive to
hyper-parameters thus it requires no manual tuning of the hyper-parameters like
Momentum, AdaGrad, and AdaDelta methods. We show promising results compared to
other methods on different convolutional neural networks, multi-layer
perceptron, and alternative machine learning tasks. Empirical results
demonstrate that AdaSmooth works well in practice and compares favorably to
other stochastic optimization methods in neural networks.
",well know need choose hyper parameter momentum adagrad adadelta alternative stochastic optimizer many case hyper parameter tune tediously base experience become art science present novel per dimension learning rate method gradient descent call adasmooth method insensitive hyper parameter thus require manual tuning hyper parameter like momentum adagrad adadelta method show promising result compare method different convolutional neural network multi layer perceptron alternative machine learn task empirical result demonstrate adasmooth work well practice compare favorably stochastic optimization method neural network
"Visual Exploration of Machine Learning Model Behavior with Hierarchical
  Surrogate Rule Sets","  One of the potential solutions for model interpretation is to train a
surrogate model: a more transparent model that approximates the behavior of the
model to be explained. Typically, classification rules or decision trees are
used due to the intelligibility of their logic-based expressions. However,
decision trees can grow too deep and rule sets can become too large to
approximate a complex model. Unlike paths on a decision tree that must share
ancestor nodes (conditions), rules are more flexible. However, the unstructured
visual representation of rules makes it hard to make inferences across rules.
To address these issues, we present a workflow that includes novel algorithmic
and interactive solutions. First, we present Hierarchical Surrogate Rules
(HSR), an algorithm that generates hierarchical rules based on user-defined
parameters. We also contribute SuRE, a visual analytics (VA) system that
integrates HSR and interactive surrogate rule visualizations. Particularly, we
present a novel feature-aligned tree to overcome the shortcomings of existing
rule visualizations. We evaluate the algorithm in terms of parameter
sensitivity, time performance, and comparison with surrogate decision trees and
find that it scales reasonably well and outperforms decision trees in many
respects. We also evaluate the visualization and the VA system by a usability
study with 24 volunteers and an observational study with 7 domain experts. Our
investigation shows that the participants can use feature-aligned trees to
perform non-trivial tasks with very high accuracy. We also discuss many
interesting observations that can be useful for future research on designing
effective rule-based VA systems.
",one potential solution model interpretation train surrogate model transparent model approximate behavior model explain typically classification rule decision tree use due intelligibility logic base expression however decision tree grow deep rule set become large approximate complex model unlike path decision tree must share ancestor node condition rule flexible however unstructured visual representation rule make hard make inference across rule address issue present workflow include novel algorithmic interactive solution first present hierarchical surrogate rule hsr algorithm generate hierarchical rule base user define parameter also contribute sure visual analytic va system integrate hsr interactive surrogate rule visualization particularly present novel feature align tree overcome shortcoming exist rule visualization evaluate algorithm term parameter sensitivity time performance comparison surrogate decision tree find scale reasonably well outperform decision tree many respect also evaluate visualization va system usability study 24 volunteer observational study 7 domain expert investigation show participant use feature align tree perform non trivial task high accuracy also discuss many interesting observation useful future research design effective rule base va system
Video Action Understanding,"  Many believe that the successes of deep learning on image understanding
problems can be replicated in the realm of video understanding. However, due to
the scale and temporal nature of video, the span of video understanding
problems and the set of proposed deep learning solutions is arguably wider and
more diverse than those of their 2D image siblings. Finding, identifying, and
predicting actions are a few of the most salient tasks in this emerging and
rapidly evolving field. With a pedagogical emphasis, this tutorial introduces
and systematizes fundamental topics, basic concepts, and notable examples in
supervised video action understanding. Specifically, we clarify a taxonomy of
action problems, catalog and highlight video datasets, describe common video
data preparation methods, present the building blocks of state-of-the art deep
learning model architectures, and formalize domain-specific metrics to baseline
proposed solutions. This tutorial is intended to be accessible to a general
computer science audience and assumes a conceptual understanding of supervised
learning.
",many believe success deep learning image understand problem replicate realm video understanding however due scale temporal nature video span video understanding problem set propose deep learning solution arguably wider diverse 2d image sibling find identify predict action salient task emerge rapidly evolve field pedagogical emphasis tutorial introduces systematize fundamental topic basic concept notable example supervise video action understand specifically clarify taxonomy action problem catalog highlight video dataset describe common video datum preparation method present building block state of the art deep learning model architecture formalize domain specific metric baseline propose solution tutorial intend accessible general computer science audience assume conceptual understanding supervise learning
"KnitCity: a machine learning-based, game-theoretical framework for
  prediction assessment and seismic risk policy design","  Knitted fabric exhibits avalanche-like events when deformed: by analogy with
eathquakes, we are interested in predicting these ""knitquakes"". However, as in
most analogous seismic models, the peculiar statistics of the corresponding
time-series severely jeopardize this endeavour, due to the time intermittence
and scale-invariance of these events. But more importantly, such predictions
are hard to {\it assess}: depending on the choice of what to predict, the
results can be very different and not easily compared. Furthermore, forecasting
models may be trained with various generic metrics which ignore some important
specificities of the problem at hand, in our case seismic risk. Finally, these
models often do not provide a clear strategy regarding the best way to use
these predictions in practice. Here we introduce a framework that allows to
design, evaluate and compare not only predictors but also decision-making
policies: a model seismically active {\it city} subjected to the crackling
dynamics observed in the mechanical response of knitted fabric. We thus proceed
to study the population of KnitCity, introducing a policy through which the
mayor of the town can decide to either keep people in, which in case of large
events cause human loss, or evacuate the city, which costs a daily fee. The
policy only relies on past seismic observations. We construct efficient
policies using a reinforcement learning environment and various time-series
predictors based on artificial neural networks. By inducing a physically
motivated metric on the predictors, this mechanism allows quantitative
assessment and comparison of their relevance in the decision-making process.
",knitted fabric exhibits avalanche like event deform analogy eathquake interested predict knitquake however analogous seismic model peculiar statistic correspond time series severely jeopardize endeavour due time intermittence scale invariance event importantly prediction hard assess depend choice predict result different easily compare furthermore forecasting model may train various generic metric ignore important specificity problem hand case seismic risk finally model often provide clear strategy regard good way use prediction practice introduce framework allow design evaluate compare predictor also decision make policy model seismically active city subject crackling dynamic observe mechanical response knit fabric thus proceed study population knitcity introduce policy mayor town decide either keep people case large event cause human loss evacuate city cost daily fee policy relie past seismic observation construct efficient policy use reinforcement learning environment various time series predictor base artificial neural network induce physically motivate metric predictor mechanism allow quantitative assessment comparison relevance decision make process
"Every Corporation Owns Its Image: Corporate Credit Ratings via
  Convolutional Neural Networks","  Credit rating is an analysis of the credit risks associated with a
corporation, which reflect the level of the riskiness and reliability in
investing. There have emerged many studies that implement machine learning
techniques to deal with corporate credit rating. However, the ability of these
models is limited by enormous amounts of data from financial statement reports.
In this work, we analyze the performance of traditional machine learning models
in predicting corporate credit rating. For utilizing the powerful convolutional
neural networks and enormous financial data, we propose a novel end-to-end
method, Corporate Credit Ratings via Convolutional Neural Networks, CCR-CNN for
brevity. In the proposed model, each corporation is transformed into an image.
Based on this image, CNN can capture complex feature interactions of data,
which are difficult to be revealed by previous machine learning models.
Extensive experiments conducted on the Chinese public-listed corporate rating
dataset which we build, prove that CCR-CNN outperforms the state-of-the-art
methods consistently.
",credit rating analysis credit risk associate corporation reflect level riskiness reliability investing emerge many study implement machine learn technique deal corporate credit rating however ability model limit enormous amount datum financial statement report work analyze performance traditional machine learning model predict corporate credit rating utilize powerful convolutional neural network enormous financial datum propose novel end to end method corporate credit rating via convolutional neural network ccr cnn brevity propose model corporation transform image base image cnn capture complex feature interaction datum difficult reveal previous machine learning model extensive experiment conduct chinese public list corporate rating dataset build prove ccr cnn outperform state of the art method consistently
"You Do Not Need a Bigger Boat: Recommendations at Reasonable Scale in a
  (Mostly) Serverless and Open Stack","  We argue that immature data pipelines are preventing a large portion of
industry practitioners from leveraging the latest research on recommender
systems. We propose our template data stack for machine learning at ""reasonable
scale"", and show how many challenges are solved by embracing a serverless
paradigm. Leveraging our experience, we detail how modern open source can
provide a pipeline processing terabytes of data with limited infrastructure
work.
",argue immature datum pipeline prevent large portion industry practitioner leverage late research recommender system propose template datum stack machine learn reasonable scale show many challenge solve embrace serverless paradigm leverage experience detail modern open source provide pipeline processing terabyte datum limited infrastructure work
Time Series Analysis via Network Science: Concepts and Algorithms,"  There is nowadays a constant flux of data being generated and collected in
all types of real world systems. These data sets are often indexed by time,
space or both requiring appropriate approaches to analyze the data. In
univariate settings, time series analysis is a mature and solid field. However,
in multivariate contexts, time series analysis still presents many limitations.
In order to address these issues, the last decade has brought approaches based
on network science. These methods involve transforming an initial time series
data set into one or more networks, which can be analyzed in depth to provide
insight into the original time series. This review provides a comprehensive
overview of existing mapping methods for transforming time series into networks
for a wide audience of researchers and practitioners in machine learning, data
mining and time series. Our main contribution is a structured review of
existing methodologies, identifying their main characteristics and their
differences. We describe the main conceptual approaches, provide authoritative
references and give insight into their advantages and limitations in a unified
notation and language. We first describe the case of univariate time series,
which can be mapped to single layer networks, and we divide the current
mappings based on the underlying concept: visibility, transition and proximity.
We then proceed with multivariate time series discussing both single layer and
multiple layer approaches. Although still very recent, this research area has
much potential and with this survey we intend to pave the way for future
research on the topic.
",nowadays constant flux datum generate collect type real world system datum set often index time space require appropriate approach analyze datum univariate setting time series analysis mature solid field however multivariate context time series analysis still present many limitation order address issue last decade bring approach base network science method involve transform initial time series datum set one network analyze depth provide insight original time series review provide comprehensive overview exist mapping method transform time series network wide audience researcher practitioner machine learn datum mining time series main contribution structure review exist methodology identify main characteristic difference describe main conceptual approach provide authoritative reference give insight advantage limitation unified notation language first describe case univariate time series map single layer network divide current mapping base underlying concept visibility transition proximity proceed multivariate time series discuss single layer multiple layer approach although still recent research area much potential survey intend pave way future research topic
Cracking the Black Box: Distilling Deep Sports Analytics,"  This paper addresses the trade-off between Accuracy and Transparency for deep
learning applied to sports analytics. Neural nets achieve great predictive
accuracy through deep learning, and are popular in sports analytics. But it is
hard to interpret a neural net model and harder still to extract actionable
insights from the knowledge implicit in it. Therefore, we built a simple and
transparent model that mimics the output of the original deep learning model
and represents the learned knowledge in an explicit interpretable way. Our
mimic model is a linear model tree, which combines a collection of linear
models with a regression-tree structure. The tree version of a neural network
achieves high fidelity, explains itself, and produces insights for expert
stakeholders such as athletes and coaches. We propose and compare several
scalable model tree learning heuristics to address the computational challenge
from datasets with millions of data points.
",paper address trade off accuracy transparency deep learning apply sport analytic neural net achieve great predictive accuracy deep learn popular sport analytic hard interpret neural net model hard still extract actionable insight knowledge implicit therefore build simple transparent model mimic output original deep learning model represents learn knowledge explicit interpretable way mimic model linear model tree combine collection linear model regression tree structure tree version neural network achieve high fidelity explain produce insight expert stakeholder athlete coach propose compare several scalable model tree learn heuristic address computational challenge dataset million datum point
"APRF-Net: Attentive Pseudo-Relevance Feedback Network for Query
  Categorization","  Query categorization is an essential part of query intent understanding in
e-commerce search. A common query categorization task is to select the relevant
fine-grained product categories in a product taxonomy. For frequent queries,
rich customer behavior (e.g., click-through data) can be used to infer the
relevant product categories. However, for more rare queries, which cover a
large volume of search traffic, relying solely on customer behavior may not
suffice due to the lack of this signal. To improve categorization of rare
queries, we adapt the Pseudo-Relevance Feedback (PRF) approach to utilize the
latent knowledge embedded in semantically or lexically similar product
documents to enrich the representation of the more rare queries. To this end,
we propose a novel deep neural model named Attentive Pseudo Relevance Feedback
Network (APRF-Net) to enhance the representation of rare queries for query
categorization. To demonstrate the effectiveness of our approach, we collect
search queries from a large commercial search engine, and compare APRF-Net to
state-of-the-art deep learning models for text classification. Our results show
that the APRF-Net significantly improves query categorization by 5.9% on F1@1
score over the baselines, which increases to 8.2% improvement for the rare
(tail) queries. The findings of this paper can be leveraged for further
improvements in search query representation and understanding.
",query categorization essential part query intent understand e commerce search common query categorization task select relevant fine grain product category product taxonomy frequent query rich customer behavior click through datum use infer relevant product category however rare query cover large volume search traffic rely solely customer behavior may suffice due lack signal improve categorization rare query adapt pseudo relevance feedback prf approach utilize latent knowledge embed semantically lexically similar product document enrich representation rare query end propose novel deep neural model name attentive pseudo relevance feedback network aprf net enhance representation rare query query categorization demonstrate effectiveness approach collect search query large commercial search engine compare aprf net state of the art deep learning model text classification result show aprf net significantly improve query categorization f1 1 score baseline increase improvement rare tail query finding paper leverage improvement search query representation understanding
"Visualization of features of a series of measurements with
  one-dimensional cellular structure","  This paper describes the method of visualization of periodic constituents and
instability areas in series of measurements, being based on the algorithm of
smoothing out and concept of one-dimensional cellular automata. A method can be
used at the analysis of temporal series, related to the volumes of thematic
publications in web-space.
",paper describe method visualization periodic constituent instability area series measurement base algorithm smooth concept one dimensional cellular automata method use analysis temporal series relate volume thematic publication web space
"Patient Flow Prediction via Discriminative Learning of
  Mutually-Correcting Processes","  Over the past decade the rate of care unit (CU) use in the United States has
been increasing. With an aging population and ever-growing demand for medical
care, effective management of patients' transitions among different care
facilities will prove indispensible for shortening the length of hospital
stays, improving patient outcomes, allocating critical care resources, and
reducing preventable re-admissions. In this paper, we focus on an important
problem of predicting the so-called ""patient flow"" from longitudinal electronic
health records (EHRs), which has not been explored via existing machine
learning techniques. By treating a sequence of transition events as a point
process, we develop a novel framework for modeling patient flow through various
CUs and jointly predicting patients' destination CUs and duration days. Instead
of learning a generative point process model via maximum likelihood estimation,
we propose a novel discriminative learning algorithm aiming at improving the
prediction of transition events in the case of sparse data. By parameterizing
the proposed model as a mutually-correcting process, we formulate the
estimation problem via generalized linear models, which lends itself to
efficient learning based on alternating direction method of multipliers (ADMM).
Furthermore, we achieve simultaneous feature selection and learning by adding a
group-lasso regularizer to the ADMM algorithm. Additionally, for suppressing
the negative influence of data imbalance on the learning of model, we
synthesize auxiliary training data for the classes with extremely few samples,
and improve the robustness of our learning method accordingly. Testing on
real-world data, we show that our method obtains superior performance in terms
of accuracy of predicting the destination CU transition and duration of each CU
occupancy.
",past decade rate care unit cu use united states increase age population ever grow demand medical care effective management patient transition among different care facility prove indispensible shortening length hospital stay improve patient outcome allocate critical care resource reduce preventable re admission paper focus important problem predict so call patient flow longitudinal electronic health record ehrs explore via exist machine learn technique treat sequence transition event point process develop novel framework modeling patient flow various cus jointly predict patient destination cus duration day instead learn generative point process model via maximum likelihood estimation propose novel discriminative learn algorithm aim improve prediction transition event case sparse datum parameterizing propose model mutually correct process formulate estimation problem via generalized linear model lend efficient learning base alternate direction method multiplier admm furthermore achieve simultaneous feature selection learning add group lasso regularizer admm algorithm additionally suppress negative influence datum imbalance learning model synthesize auxiliary training datum class extremely sample improve robustness learning method accordingly test real world datum show method obtain superior performance term accuracy predict destination cu transition duration cu occupancy
ETC: Encoding Long and Structured Inputs in Transformers,"  Transformer models have advanced the state of the art in many Natural
Language Processing (NLP) tasks. In this paper, we present a new Transformer
architecture, Extended Transformer Construction (ETC), that addresses two key
challenges of standard Transformer architectures, namely scaling input length
and encoding structured inputs. To scale attention to longer inputs, we
introduce a novel global-local attention mechanism between global tokens and
regular input tokens. We also show that combining global-local attention with
relative position encodings and a Contrastive Predictive Coding (CPC)
pre-training objective allows ETC to encode structured inputs. We achieve
state-of-the-art results on four natural language datasets requiring long
and/or structured inputs.
",transformer model advance state art many natural language processing nlp task paper present new transformer architecture extend transformer construction etc address two key challenge standard transformer architecture namely scale input length encode structured input scale attention long input introduce novel global local attention mechanism global token regular input token also show combine global local attention relative position encoding contrastive predictive code cpc pre training objective allow etc encode structure input achieve state of the art result four natural language dataset require long structure input
"Multi-Agent Deep Reinforcement Learning for Large-scale Traffic Signal
  Control","  Reinforcement learning (RL) is a promising data-driven approach for adaptive
traffic signal control (ATSC) in complex urban traffic networks, and deep
neural networks further enhance its learning power. However, centralized RL is
infeasible for large-scale ATSC due to the extremely high dimension of the
joint action space. Multi-agent RL (MARL) overcomes the scalability issue by
distributing the global control to each local RL agent, but it introduces new
challenges: now the environment becomes partially observable from the viewpoint
of each local agent due to limited communication among agents. Most existing
studies in MARL focus on designing efficient communication and coordination
among traditional Q-learning agents. This paper presents, for the first time, a
fully scalable and decentralized MARL algorithm for the state-of-the-art deep
RL agent: advantage actor critic (A2C), within the context of ATSC. In
particular, two methods are proposed to stabilize the learning procedure, by
improving the observability and reducing the learning difficulty of each local
agent. The proposed multi-agent A2C is compared against independent A2C and
independent Q-learning algorithms, in both a large synthetic traffic grid and a
large real-world traffic network of Monaco city, under simulated peak-hour
traffic dynamics. Results demonstrate its optimality, robustness, and sample
efficiency over other state-of-the-art decentralized MARL algorithms.
",reinforcement learning rl promise data drive approach adaptive traffic signal control atsc complex urban traffic network deep neural network enhance learn power however centralized rl infeasible large scale atsc due extremely high dimension joint action space multi agent rl marl overcome scalability issue distribute global control local rl agent introduce new challenge environment become partially observable viewpoint local agent due limited communication among agent exist study marl focus design efficient communication coordination among traditional q learn agent paper present first time fully scalable decentralized marl algorithm state of the art deep rl agent advantage actor critic a2c within context atsc particular two method propose stabilize learn procedure improve observability reduce learn difficulty local agent propose multi agent a2c compare independent a2c independent q learning algorithm large synthetic traffic grid large real world traffic network monaco city simulate peak hour traffic dynamic result demonstrate optimality robustness sample efficiency state of the art decentralize marl algorithm
"Restricted Boltzmann Machine, recent advances and mean-field theory","  This review deals with Restricted Boltzmann Machine (RBM) under the light of
statistical physics. The RBM is a classical family of Machine learning (ML)
models which played a central role in the development of deep learning. Viewing
it as a Spin Glass model and exhibiting various links with other models of
statistical physics, we gather recent results dealing with mean-field theory in
this context. First the functioning of the RBM can be analyzed via the phase
diagrams obtained for various statistical ensembles of RBM leading in
particular to identify a {\it compositional phase} where a small number of
features or modes are combined to form complex patterns. Then we discuss recent
works either able to devise mean-field based learning algorithms; either able
to reproduce generic aspects of the learning process from some {\it ensemble
dynamics equations} or/and from linear stability arguments.
",review deal restrict boltzmann machine rbm light statistical physics rbm classical family machine learning ml model play central role development deep learning view spin glass model exhibit various link model statistical physics gather recent result deal mean field theory context first function rbm analyze via phase diagram obtain various statistical ensemble rbm lead particular identify compositional phase small number feature mode combined form complex pattern discuss recent work either able devise mean field base learn algorithm either able reproduce generic aspect learning process ensemble dynamic equation linear stability argument
"Towards Low-Latency Energy-Efficient Deep SNNs via Attention-Guided
  Compression","  Deep spiking neural networks (SNNs) have emerged as a potential alternative
to traditional deep learning frameworks, due to their promise to provide
increased compute efficiency on event-driven neuromorphic hardware. However, to
perform well on complex vision applications, most SNN training frameworks yield
large inference latency which translates to increased spike activity and
reduced energy efficiency. Hence,minimizing average spike activity while
preserving accuracy indeep SNNs remains a significant challenge and
opportunity.This paper presents a non-iterative SNN training technique
thatachieves ultra-high compression with reduced spiking activitywhile
maintaining high inference accuracy. In particular, our framework first uses
the attention-maps of an un compressed meta-model to yield compressed ANNs.
This step can be tuned to support both irregular and structured channel pruning
to leverage computational benefits over a broad range of platforms. The
framework then performs sparse-learning-based supervised SNN training using
direct inputs. During the training, it jointly optimizes the SNN weight,
threshold, and leak parameters to drastically minimize the number of time steps
required while retaining compression. To evaluate the merits of our approach,
we performed experiments with variants of VGG and ResNet, on both CIFAR-10 and
CIFAR-100, and VGG16 on Tiny-ImageNet.The SNN models generated through the
proposed technique yield SOTA compression ratios of up to 33.4x with no
significant drops in accuracy compared to baseline unpruned counterparts.
Compared to existing SNN pruning methods, we achieve up to 8.3x higher
compression with improved accuracy.
",deep spike neural network snn emerge potential alternative traditional deep learning framework due promise provide increase compute efficiency event drive neuromorphic hardware however perform well complex vision application snn training framework yield large inference latency translates increase spike activity reduce energy efficiency hence minimize average spike activity preserve accuracy indeep snn remain significant challenge paper present non iterative snn training technique thatachieve ultra high compression reduce spike activitywhile maintain high inference accuracy particular framework first use attention map un compress meta model yield compress anns step tune support irregular structured channel prune leverage computational benefit broad range platform framework perform sparse learn base supervised snn training use direct input training jointly optimize snn weight threshold leak parameter drastically minimize number time step require retain compression evaluate merit approach perform experiment variant vgg resnet cifar-10 cifar-100 vgg16 snn model generate propose technique yield sota compression ratio significant drop accuracy compare baseline unprune counterpart compare exist snn pruning method achieve high compression improve accuracy
TraceCaps: A Capsule-based Neural Network for Semantic Segmentation,"  In this paper, we propose a capsule-based neural network model to solve the
semantic segmentation problem. By taking advantage of the extractable
part-whole dependencies available in capsule layers, we derive the
probabilities of the class labels for individual capsules through a recursive,
layer-by-layer procedure. We model this procedure as a traceback pipeline and
take it as a central piece to build an end-to-end segmentation network. Under
the proposed framework, image-level class labels and object boundaries are
jointly sought in an explicit manner, which poses a significant advantage over
the state-of-the-art fully convolutional network (FCN) solutions. With the
capability to extracted part-whole information, our traceback pipeline can
potentially be utilized as the building blocks to design interpretable neural
networks. Experiments conducted on modified MNIST and neuroimages demonstrate
that our model considerably enhance the segmentation performance compared to
the leading FCN variants.
",paper propose capsule base neural network model solve semantic segmentation problem take advantage extractable part whole dependency available capsule layer derive probability class label individual capsule recursive layer by layer procedure model procedure traceback pipeline take central piece build end to end segmentation network propose framework image level class label object boundary jointly seek explicit manner pose significant advantage state of the art fully convolutional network fcn solution capability extract part whole information traceback pipeline potentially utilize building block design interpretable neural network experiment conduct modify mnist neuroimage demonstrate model considerably enhance segmentation performance compare lead fcn variant
Predicting distributions with Linearizing Belief Networks,"  Conditional belief networks introduce stochastic binary variables in neural
networks. Contrary to a classical neural network, a belief network can predict
more than the expected value of the output $Y$ given the input $X$. It can
predict a distribution of outputs $Y$ which is useful when an input can admit
multiple outputs whose average is not necessarily a valid answer. Such networks
are particularly relevant to inverse problems such as image prediction for
denoising, or text to speech. However, traditional sigmoid belief networks are
hard to train and are not suited to continuous problems. This work introduces a
new family of networks called linearizing belief nets or LBNs. A LBN decomposes
into a deep linear network where each linear unit can be turned on or off by
non-deterministic binary latent units. It is a universal approximator of
real-valued conditional distributions and can be trained using gradient
descent. Moreover, the linear pathways efficiently propagate continuous
information and they act as multiplicative skip-connections that help
optimization by removing gradient diffusion. This yields a model which trains
efficiently and improves the state-of-the-art on image denoising and facial
expression generation with the Toronto faces dataset.
",conditional belief network introduce stochastic binary variable neural network contrary classical neural network belief network predict expect value output give input x predict distribution output useful input admit multiple output whose average necessarily valid answer network particularly relevant inverse problem image prediction denoise text speech however traditional sigmoid belief network hard train suit continuous problem work introduce new family network call linearize belief net lbns lbn decompose deep linear network linear unit turn non deterministic binary latent unit universal approximator real value conditional distribution train use gradient descent moreover linear pathway efficiently propagate continuous information act multiplicative skip connection help optimization remove gradient diffusion yield model train efficiently improve state of the art image denoise facial expression generation toronto faces dataset
Gradient Boosts the Approximate Vanishing Ideal,"  In the last decade, the approximate vanishing ideal and its basis
construction algorithms have been extensively studied in computer algebra and
machine learning as a general model to reconstruct the algebraic variety on
which noisy data approximately lie. In particular, the basis construction
algorithms developed in machine learning are widely used in applications across
many fields because of their monomial-order-free property; however, they lose
many of the theoretical properties of computer-algebraic algorithms. In this
paper, we propose general methods that equip monomial-order-free algorithms
with several advantageous theoretical properties. Specifically, we exploit the
gradient to (i) sidestep the spurious vanishing problem in polynomial time to
remove symbolically trivial redundant bases, (ii) achieve consistent output
with respect to the translation and scaling of input, and (iii) remove
nontrivially redundant bases. The proposed methods work in a fully numerical
manner, whereas existing algorithms require the awkward monomial order or
exponentially costly (and mostly symbolic) computation to realize properties
(i) and (iii). To our knowledge, property (ii) has not been achieved by any
existing basis construction algorithm of the approximate vanishing ideal.
",last decade approximate vanish ideal basis construction algorithm extensively study computer algebra machine learn general model reconstruct algebraic variety noisy datum approximately lie particular basis construction algorithm develop machine learn widely use application across many field monomial order free property however lose many theoretical property computer algebraic algorithms paper propose general method equip monomial order free algorithm several advantageous theoretical property specifically exploit gradient sidestep spurious vanish problem polynomial time remove symbolically trivial redundant basis ii achieve consistent output respect translation scale input iii remove nontrivially redundant basis propose method work fully numerical manner whereas existing algorithm require awkward monomial order exponentially costly mostly symbolic computation realize property iii knowledge property ii achieve exist basis construction algorithm approximate vanish ideal
"Detecting Interlocutor Confusion in Situated Human-Avatar Dialogue: A
  Pilot Study","  In order to enhance levels of engagement with conversational systems, our
long term research goal seeks to monitor the confusion state of a user and
adapt dialogue policies in response to such user confusion states. To this end,
in this paper, we present our initial research centred on a user-avatar
dialogue scenario that we have developed to study the manifestation of
confusion and in the long term its mitigation. We present a new definition of
confusion that is particularly tailored to the requirements of intelligent
conversational system development for task-oriented dialogue. We also present
the details of our Wizard-of-Oz based data collection scenario wherein users
interacted with a conversational avatar and were presented with stimuli that
were in some cases designed to invoke a confused state in the user. Post study
analysis of this data is also presented. Here, three pre-trained deep learning
models were deployed to estimate base emotion, head pose and eye gaze. Despite
a small pilot study group, our analysis demonstrates a significant relationship
between these indicators and confusion states. We understand this as a useful
step forward in the automated analysis of the pragmatics of dialogue.
",order enhance level engagement conversational system long term research goal seek monitor confusion state user adapt dialogue policy response user confusion state end paper present initial research centre user avatar dialogue scenario develop study manifestation confusion long term mitigation present new definition confusion particularly tailor requirement intelligent conversational system development task orient dialogue also present detail wizard of oz base data collection scenario wherein user interact conversational avatar present stimulus case design invoke confuse state user post study analysis datum also present three pre trained deep learning model deploy estimate base emotion head pose eye gaze despite small pilot study group analysis demonstrate significant relationship indicator confusion state understand useful step forward automate analysis pragmatic dialogue
On Fast Leverage Score Sampling and Optimal Learning,"  Leverage score sampling provides an appealing way to perform approximate
computations for large matrices. Indeed, it allows to derive faithful
approximations with a complexity adapted to the problem at hand. Yet,
performing leverage scores sampling is a challenge in its own right requiring
further approximations. In this paper, we study the problem of leverage score
sampling for positive definite matrices defined by a kernel. Our contribution
is twofold. First we provide a novel algorithm for leverage score sampling and
second, we exploit the proposed method in statistical learning by deriving a
novel solver for kernel ridge regression. Our main technical contribution is
showing that the proposed algorithms are currently the most efficient and
accurate for these problems.
",leverage score sampling provide appealing way perform approximate computation large matrix indeed allow derive faithful approximation complexity adapt problem hand yet perform leverage score sampling challenge right require approximation paper study problem leverage score sample positive definite matrix define kernel contribution twofold first provide novel algorithm leverage score sampling second exploit propose method statistical learning derive novel solver kernel ridge regression main technical contribution show propose algorithm currently efficient accurate problem
"Understanding Neural Networks and Individual Neuron Importance via
  Information-Ordered Cumulative Ablation","  In this work, we investigate the use of three information-theoretic
quantities -- entropy, mutual information with the class variable, and a class
selectivity measure based on Kullback-Leibler divergence -- to understand and
study the behavior of already trained fully-connected feed-forward neural
networks. We analyze the connection between these information-theoretic
quantities and classification performance on the test set by cumulatively
ablating neurons in networks trained on MNIST, FashionMNIST, and CIFAR-10. Our
results parallel those recently published by Morcos et al., indicating that
class selectivity is not a good indicator for classification performance.
However, looking at individual layers separately, both mutual information and
class selectivity are positively correlated with classification performance, at
least for networks with ReLU activation functions. We provide explanations for
this phenomenon and conclude that it is ill-advised to compare the proposed
information-theoretic quantities across layers. Furthermore, we show that
cumulative ablation of neurons with ascending or descending
information-theoretic quantities can be used to formulate hypotheses regarding
the joint behavior of multiple neurons, such as redundancy and synergy, with
comparably low computational cost. We also draw connections to the information
bottleneck theory for neural networks.
",work investigate use three information theoretic quantity entropy mutual information class variable class selectivity measure base kullback leibler divergence understand study behavior already train fully connect feed forward neural network analyze connection information theoretic quantity classification performance test set cumulatively ablate neuron network train mnist fashionmnist cifar-10 result parallel recently publish morco et indicate class selectivity good indicator classification performance however look individual layer separately mutual information class selectivity positively correlate classification performance least network relu activation function provide explanation phenomenon conclude ill advise compare propose information theoretic quantity across layer furthermore show cumulative ablation neuron ascend descend information theoretic quantity use formulate hypothesis regard joint behavior multiple neuron redundancy synergy comparably low computational cost also draw connection information bottleneck theory neural network
A Unifying View of Explicit and Implicit Feature Maps of Graph Kernels,"  Non-linear kernel methods can be approximated by fast linear ones using
suitable explicit feature maps allowing their application to large scale
problems. We investigate how convolution kernels for structured data are
composed from base kernels and construct corresponding feature maps. On this
basis we propose exact and approximative feature maps for widely used graph
kernels based on the kernel trick. We analyze for which kernels and graph
properties computation by explicit feature maps is feasible and actually more
efficient. In particular, we derive approximative, explicit feature maps for
state-of-the-art kernels supporting real-valued attributes including the
GraphHopper and graph invariant kernels. In extensive experiments we show that
our approaches often achieve a classification accuracy close to the exact
methods based on the kernel trick, but require only a fraction of their running
time. Moreover, we propose and analyze algorithms for computing random walk,
shortest-path and subgraph matching kernels by explicit and implicit feature
maps. Our theoretical results are confirmed experimentally by observing a phase
transition when comparing running time with respect to label diversity, walk
lengths and subgraph size, respectively.
",non linear kernel method approximate fast linear one use suitable explicit feature map allow application large scale problem investigate convolution kernel structure datum compose base kernel construct correspond feature map basis propose exact approximative feature map widely use graph kernel base kernel trick analyze kernels graph property computation explicit feature map feasible actually efficient particular derive approximative explicit feature map state of the art kernel support real value attribute include graphhopper graph invariant kernel extensive experiment show approach often achieve classification accuracy close exact method base kernel trick require fraction running time moreover propose analyze algorithms compute random walk short path subgraph matching kernel explicit implicit feature map theoretical result confirm experimentally observe phase transition compare run time respect label diversity walk length subgraph size respectively
Graph-Based Machine Learning Improves Just-in-Time Defect Prediction,"  The increasing complexity of today's software requires the contribution of
thousands of developers. This complex collaboration structure makes developers
more likely to introduce defect-prone changes that lead to software faults.
Determining when these defect-prone changes are introduced has proven
challenging, and using traditional machine learning (ML) methods to make these
determinations seems to have reached a plateau. In this work, we build
contribution graphs consisting of developers and source files to capture the
nuanced complexity of changes required to build software. By leveraging these
contribution graphs, our research shows the potential of using graph-based ML
to improve Just-In-Time (JIT) defect prediction. We hypothesize that features
extracted from the contribution graphs may be better predictors of defect-prone
changes than intrinsic features derived from software characteristics. We
corroborate our hypothesis using graph-based ML for classifying edges that
represent defect-prone changes. This new framing of the JIT defect prediction
problem leads to remarkably better results. We test our approach on 14
open-source projects and show that our best model can predict whether or not a
code change will lead to a defect with an F1 score as high as 86.25$\%$. This
represents an increase of as much as 55.4$\%$ over the state-of-the-art in JIT
defect prediction. We describe limitations, open challenges, and how this
method can be used for operational JIT defect prediction.
",increase complexity today software require contribution thousand developer complex collaboration structure make developer likely introduce defect prone change lead software fault determine defect prone change introduce prove challenge use traditional machine learning ml method make determination seems reach plateau work build contribution graph consist developer source file capture nuance complexity change require build software leverage contribution graph research show potential use graph base ml improve just in time jit defect prediction hypothesize feature extract contribution graph may well predictor defect prone change intrinsic feature derive software characteristic corroborate hypothesis use graph base ml classify edge represent defect prone change new frame jit defect prediction problem lead remarkably well result test approach 14 open source project show good model predict whether code change lead defect f1 score high represents increase much state of the art jit defect prediction describe limitation open challenge method use operational jit defect prediction
Few-shot tweet detection in emerging disaster events,"  Social media sources can provide crucial information in crisis situations,
but discovering relevant messages is not trivial. Methods have so far focused
on universal detection models for all kinds of crises or for certain crisis
types (e.g. floods). Event-specific models could implement a more focused
search area, but collecting data and training new models for a crisis that is
already in progress is costly and may take too much time for a prompt response.
As a compromise, manually collecting a small amount of example messages is
feasible. Few-shot models can generalize to unseen classes with such a small
handful of examples, and do not need be trained anew for each event. We compare
how few-shot approaches (matching networks and prototypical networks) perform
for this task. Since this is essentially a one-class problem, we also
demonstrate how a modified one-class version of prototypical models can be used
for this application.
",social medium source provide crucial information crisis situation discover relevant message trivial method far focus universal detection model kind crise certain crisis type flood event specific model could implement focus search area collect datum train new model crisis already progress costly may take much time prompt response compromise manually collect small amount example message feasible few shot model generalize unseen class small handful example need train anew event compare few shot approach matching network prototypical network perform task since essentially one class problem also demonstrate modify one class version prototypical model use application
"Revisiting the Boundary between ASR and NLU in the Age of Conversational
  Dialog Systems","  As more users across the world are interacting with dialog agents in their
daily life, there is a need for better speech understanding that calls for
renewed attention to the dynamics between research in automatic speech
recognition (ASR) and natural language understanding (NLU). We briefly review
these research areas and lay out the current relationship between them. In
light of the observations we make in this paper, we argue that (1) NLU should
be cognizant of the presence of ASR models being used upstream in a dialog
system's pipeline, (2) ASR should be able to learn from errors found in NLU,
(3) there is a need for end-to-end datasets that provide semantic annotations
on spoken input, (4) there should be stronger collaboration between ASR and NLU
research communities.
",user across world interact dialog agent daily life need well speech understand call renew attention dynamic research automatic speech recognition asr natural language understand nlu briefly review research area lie current relationship light observation make paper argue 1 nlu cognizant presence asr model use upstream dialog system pipeline 2 asr able learn error find nlu 3 need end to end dataset provide semantic annotation speak input 4 strong collaboration asr nlu research community
SNIP: Single-shot Network Pruning based on Connection Sensitivity,"  Pruning large neural networks while maintaining their performance is often
desirable due to the reduced space and time complexity. In existing methods,
pruning is done within an iterative optimization procedure with either
heuristically designed pruning schedules or additional hyperparameters,
undermining their utility. In this work, we present a new approach that prunes
a given network once at initialization prior to training. To achieve this, we
introduce a saliency criterion based on connection sensitivity that identifies
structurally important connections in the network for the given task. This
eliminates the need for both pretraining and the complex pruning schedule while
making it robust to architecture variations. After pruning, the sparse network
is trained in the standard way. Our method obtains extremely sparse networks
with virtually the same accuracy as the reference network on the MNIST,
CIFAR-10, and Tiny-ImageNet classification tasks and is broadly applicable to
various architectures including convolutional, residual and recurrent networks.
Unlike existing methods, our approach enables us to demonstrate that the
retained connections are indeed relevant to the given task.
",prune large neural network maintain performance often desirable due reduce space time complexity exist method prune do within iterative optimization procedure either heuristically design pruning schedule additional hyperparameter undermine utility work present new approach prune give network initialization prior training achieve introduce saliency criterion base connection sensitivity identifie structurally important connection network give task eliminate need pretraine complex pruning schedule make robust architecture variation prune sparse network train standard way method obtain extremely sparse network virtually accuracy reference network mnist cifar-10 tiny imagenet classification task broadly applicable various architecture include convolutional residual recurrent network unlike exist method approach enable we demonstrate retain connection indeed relevant give task
Gradient Boost with Convolution Neural Network for Stock Forecast,"  Market economy closely connects aspects to all walks of life. The stock
forecast is one of task among studies on the market economy. However,
information on markets economy contains a lot of noise and uncertainties, which
lead economy forecasting to become a challenging task. Ensemble learning and
deep learning are the most methods to solve the stock forecast task. In this
paper, we present a model combining the advantages of two methods to forecast
the change of stock price. The proposed method combines CNN and GBoost. The
experimental results on six market indexes show that the proposed method has
better performance against current popular methods.
",market economy closely connect aspect walk life stock forecast one task among study market economy however information market economy contain lot noise uncertainty lead economy forecasting become challenge task ensemble learn deep learning method solve stock forecast task paper present model combine advantage two method forecast change stock price propose method combine cnn gboost experimental result six market index show propose method well performance current popular method
RLOps: Development Life-cycle of Reinforcement Learning Aided Open RAN,"  Radio access network (RAN) technologies continue to witness massive growth,
with Open RAN gaining the most recent momentum. In the O-RAN specifications,
the RAN intelligent controller (RIC) serves as an automation host. This article
introduces principles for machine learning (ML), in particular, reinforcement
learning (RL) relevant for the O-RAN stack. Furthermore, we review
state-of-the-art research in wireless networks and cast it onto the RAN
framework and the hierarchy of the O-RAN architecture. We provide a taxonomy of
the challenges faced by ML/RL models throughout the development life-cycle:
from the system specification to production deployment (data acquisition, model
design, testing and management, etc.). To address the challenges, we integrate
a set of existing MLOps principles with unique characteristics when RL agents
are considered. This paper discusses a systematic life-cycle model development,
testing and validation pipeline, termed: RLOps. We discuss all fundamental
parts of RLOps, which include: model specification, development and
distillation, production environment serving, operations monitoring,
safety/security and data engineering platform. Based on these principles, we
propose the best practices for RLOps to achieve an automated and reproducible
model development process.
",radio access network run technology continue witness massive growth open run gain recent momentum o run specification run intelligent controller ric serve automation host article introduce principle machine learning ml particular reinforcement learning rl relevant o run stack furthermore review state of the art research wireless network cast onto ran framework hierarchy o ran architecture provide taxonomy challenge face model throughout development life cycle system specification production deployment datum acquisition model design testing management address challenge integrate set exist mlop principle unique characteristic rl agent consider paper discuss systematic life cycle model development testing validation pipeline term rlop discuss fundamental part rlop include model specification development distillation production environment serve operation monitor datum engineering platform base principle propose good practice rlop achieve automate reproducible model development process
"""FIJO"": a French Insurance Soft Skill Detection Dataset","  Understanding the evolution of job requirements is becoming more important
for workers, companies and public organizations to follow the fast
transformation of the employment market. Fortunately, recent natural language
processing (NLP) approaches allow for the development of methods to
automatically extract information from job ads and recognize skills more
precisely. However, these efficient approaches need a large amount of annotated
data from the studied domain which is difficult to access, mainly due to
intellectual property. This article proposes a new public dataset, FIJO,
containing insurance job offers, including many soft skill annotations. To
understand the potential of this dataset, we detail some characteristics and
some limitations. Then, we present the results of skill detection algorithms
using a named entity recognition approach and show that transformers-based
models have good token-wise performances on this dataset. Lastly, we analyze
some errors made by our best model to emphasize the difficulties that may arise
when applying NLP approaches.
",understand evolution job requirement become important worker company public organization follow fast transformation employment market fortunately recent natural language processing nlp approach allow development method automatically extract information job ad recognize skill precisely however efficient approach need large amount annotate datum study domain difficult access mainly due intellectual property article propose new public dataset fijo contain insurance job offer include many soft skill annotation understand potential dataset detail characteristic limitation present result skill detection algorithm use name entity recognition approach show transformer base model good token wise performance dataset lastly analyze error make good model emphasize difficulty may arise apply nlp approach
"Online Stochastic Gradient Descent Learns Linear Dynamical Systems from
  A Single Trajectory","  This work investigates the problem of estimating the weight matrices of a
stable time-invariant linear dynamical system from a single sequence of noisy
measurements. We show that if the unknown weight matrices describing the system
are in Brunovsky canonical form, we can efficiently estimate the ground truth
unknown matrices of the system from a linear system of equations formulated
based on the transfer function of the system, using both online and offline
stochastic gradient descent (SGD) methods. Specifically, by deriving concrete
complexity bounds, we show that SGD converges linearly in expectation to any
arbitrary small Frobenius norm distance from the ground truth weights. To the
best of our knowledge, ours is the first work to establish linear convergence
characteristics for online and offline gradient-based iterative methods for
weight matrix estimation in linear dynamical systems from a single trajectory.
Extensive numerical tests verify that the performance of the proposed methods
is consistent with our theory, and show their superior performance relative to
existing state of the art methods.
",work investigate problem estimate weight matrix stable time invariant linear dynamical system single sequence noisy measurement show unknown weight matrix describe system brunovsky canonical form efficiently estimate ground truth unknown matrix system linear system equation formulate base transfer function system use online offline stochastic gradient descent sgd method specifically derive concrete complexity bound show sgd converge linearly expectation arbitrary small frobenius norm distance ground truth weight good knowledge first work establish linear convergence characteristic online offline gradient base iterative method weight matrix estimation linear dynamical system single trajectory extensive numerical test verify performance propose method consistent theory show superior performance relative exist state art method
Variational Autoencoders Pursue PCA Directions (by Accident),"  The Variational Autoencoder (VAE) is a powerful architecture capable of
representation learning and generative modeling. When it comes to learning
interpretable (disentangled) representations, VAE and its variants show
unparalleled performance. However, the reasons for this are unclear, since a
very particular alignment of the latent embedding is needed but the design of
the VAE does not encourage it in any explicit way. We address this matter and
offer the following explanation: the diagonal approximation in the encoder
together with the inherent stochasticity force local orthogonality of the
decoder. The local behavior of promoting both reconstruction and orthogonality
matches closely how the PCA embedding is chosen. Alongside providing an
intuitive understanding, we justify the statement with full theoretical
analysis as well as with experiments.
",variational autoencoder vae powerful architecture capable representation learning generative modeling come learn interpretable disentangle representation vae variant show unparalleled performance however reason unclear since particular alignment latent embed need design vae encourage explicit way address matter offer follow explanation diagonal approximation encoder together inherent stochasticity force local orthogonality decoder local behavior promote reconstruction orthogonality match closely pca embed choose alongside provide intuitive understanding justify statement full theoretical analysis well experiment
Breast Cancer Diagnosis by Higher-Order Probabilistic Perceptrons,"  A two-layer neural network model that systematically includes correlations
among input variables to arbitrary order and is designed to implement Bayes
inference has been adapted to classify breast cancer tumors as malignant or
benign, assigning a probability for either outcome. The inputs to the network
represent measured characteristics of cell nuclei imaged in Fine Needle
Aspiration biopsies. The present machine-learning approach to diagnosis (known
as HOPP, for higher-order probabilistic perceptron) is tested on the
much-studied, open-access Breast Cancer Wisconsin (Diagnosis) Data Set of
Wolberg et al. This set lists, for each tumor, measured physical parameters of
the cell nuclei of each sample. The HOPP model can identify the key factors --
input features and their combinations -- most relevant for reliable diagnosis.
HOPP networks were trained on 90\% of the examples in the Wisconsin database,
and tested on the remaining 10\%. Referred to ensembles of 300 networks,
selected randomly for cross-validation, accuracy of classification for the test
sets of up to 97\% was readily achieved, with standard deviation around 2\%,
together with average Matthews correlation coefficients reaching 0.94
indicating excellent predictive performance. Demonstrably, the HOPP is capable
of matching the predictive power attained by other advanced machine-learning
algorithms applied to this much-studied database, over several decades.
Analysis shows that in this special problem, which is almost linearly
separable, the effects of irreducible correlations among the measured features
of the Wisconsin database are of relatively minor importance, as the Naive
Bayes approximation can itself yield predictive accuracy approaching 95\%. The
advantages of the HOPP algorithm will be more clearly revealed in application
to more challenging machine-learning problems.
",two layer neural network model systematically include correlation among input variable arbitrary order design implement bayes inference adapt classify breast cancer tumor malignant benign assign probability either outcome input network represent measured characteristic cell nucleus image fine needle aspiration biopsy present machine learn approach diagnosis know hopp high order probabilistic perceptron test much study open access breast cancer wisconsin diagnosis datum set wolberg et al set list tumor measure physical parameter cell nuclei sample hopp model identify key factor input feature combination relevant reliable diagnosis hopp network train example wisconsin database test remain refer ensemble 300 network select randomly cross validation accuracy classification test set readily achieve standard deviation around together average matthews correlation coefficient reach indicate excellent predictive performance demonstrably hopp capable matching predictive power attain advanced machine learn algorithm apply much study database several decade analysis show special problem almost linearly separable effect irreducible correlation among measure feature wisconsin database relatively minor importance naive baye approximation yield predictive accuracy approach advantage hopp algorithm clearly reveal application challenge machine learn problem
"Active Inference in Robotics and Artificial Agents: Survey and
  Challenges","  Active inference is a mathematical framework which originated in
computational neuroscience as a theory of how the brain implements action,
perception and learning. Recently, it has been shown to be a promising approach
to the problems of state-estimation and control under uncertainty, as well as a
foundation for the construction of goal-driven behaviours in robotics and
artificial agents in general. Here, we review the state-of-the-art theory and
implementations of active inference for state-estimation, control, planning and
learning; describing current achievements with a particular focus on robotics.
We showcase relevant experiments that illustrate its potential in terms of
adaptation, generalization and robustness. Furthermore, we connect this
approach with other frameworks and discuss its expected benefits and
challenges: a unified framework with functional biological plausibility using
variational Bayesian inference.
",active inference mathematical framework originate computational neuroscience theory brain implement action perception learning recently show promising approach problem state estimation control uncertainty well foundation construction goal drive behaviour robotic artificial agent general review state of the art theory implementation active inference state estimation control planning learning describe current achievement particular focus robotic showcase relevant experiment illustrate potential term adaptation generalization robustness furthermore connect approach framework discuss expect benefit challenge unify framework functional biological plausibility use variational bayesian inference
Universal Regular Conditional Distributions,"  We introduce a general framework for approximating regular conditional
distributions (RCDs). Our approximations of these RCDs are implemented by a new
class of geometric deep learning models with inputs in $\mathbb{R}^d$ and
outputs in the Wasserstein-$1$ space $\mathcal{P}_1(\mathbb{R}^D)$. We find
that the models built using our framework can approximate any continuous
functions from $\mathbb{R}^d$ to $\mathcal{P}_1(\mathbb{R}^D)$ uniformly on
compacts, and quantitative rates are obtained. We identify two methods for
avoiding the ""curse of dimensionality""; i.e.: the number of parameters
determining the approximating neural network depends only polynomially on the
involved dimension and the approximation error. The first solution describes
functions in $C(\mathbb{R}^d,\mathcal{P}_1(\mathbb{R}^D))$ which can be
efficiently approximated on any compact subset of $\mathbb{R}^d$. Conversely,
the second approach describes sets in $\mathbb{R}^d$, on which any function in
$C(\mathbb{R}^d,\mathcal{P}_1(\mathbb{R}^D))$ can be efficiently approximated.
Our framework is used to obtain an affirmative answer to the open conjecture of
Bishop (1994); namely: mixture density networks are universal regular
conditional distributions. The predictive performance of the proposed models is
evaluated against comparable learning models on various probabilistic
predictions tasks in the context of ELMs, model uncertainty, and
heteroscedastic regression. All the results are obtained for more general input
and output spaces and thus apply to geometric deep learning contexts.
",introduce general framework approximate regular conditional distribution rcds approximation rcds implement new class geometric deep learning model input r output wasserstein- 1 space p 1 r find model build use framework approximate continuous function r p 1 r uniformly compact quantitative rate obtain identify two method avoid curse dimensionality number parameter determine approximating neural network depend polynomially involved dimension approximation error first solution describe function c r p 1 r efficiently approximate compact subset r conversely second approach describe set r function c r p 1 r efficiently approximate framework use obtain affirmative answer open conjecture bishop 1994 namely mixture density network universal regular conditional distribution predictive performance propose model evaluate comparable learning model various probabilistic prediction task context elm model uncertainty heteroscedastic regression result obtain general input output space thus apply geometric deep learning context
"Privacy Analysis of Online Learning Algorithms via Contraction
  Coefficients","  We propose an information-theoretic technique for analyzing privacy
guarantees of online algorithms. Specifically, we demonstrate that differential
privacy guarantees of iterative algorithms can be determined by a direct
application of contraction coefficients derived from strong data processing
inequalities for $f$-divergences. Our technique relies on generalizing the
Dobrushin's contraction coefficient for total variation distance to an
$f$-divergence known as $E_\gamma$-divergence. $E_\gamma$-divergence, in turn,
is equivalent to approximate differential privacy. As an example, we apply our
technique to derive the differential privacy parameters of gradient descent.
Moreover, we also show that this framework can be tailored to batch learning
algorithms that can be implemented with one pass over the training dataset.
",propose information theoretic technique analyze privacy guarantee online algorithm specifically demonstrate differential privacy guarantee iterative algorithm determine direct application contraction coefficient derive strong datum processing inequality f -divergences technique relie generalize dobrushin contraction coefficient total variation distance f -divergence know -divergence -divergence turn equivalent approximate differential privacy example apply technique derive differential privacy parameter gradient descent moreover also show framework tailor batch learning algorithm implement one pass training dataset
Factorised Neural Relational Inference for Multi-Interaction Systems,"  Many complex natural and cultural phenomena are well modelled by systems of
simple interactions between particles. A number of architectures have been
developed to articulate this kind of structure, both implicitly and explicitly.
We consider an unsupervised explicit model, the NRI model, and make a series of
representational adaptations and physically motivated changes. Most notably we
factorise the inferred latent interaction graph into a multiplex graph,
allowing each layer to encode for a different interaction-type. This fNRI model
is smaller in size and significantly outperforms the original in both edge and
trajectory prediction, establishing a new state-of-the-art. We also present a
simplified variant of our model, which demonstrates the NRI's formulation as a
variational auto-encoder is not necessary for good performance, and make an
adaptation to the NRI's training routine, significantly improving its ability
to model complex physical dynamical systems.
",many complex natural cultural phenomenon well model system simple interaction particle number architecture develop articulate kind structure implicitly explicitly consider unsupervised explicit model nri model make series representational adaptation physically motivate change notably factorise infer latent interaction graph multiplex graph allow layer encode different interaction type fnri model small size significantly outperform original edge trajectory prediction establish new state of the art also present simplified variant model demonstrate nri formulation variational auto encoder necessary good performance make adaptation nri training routine significantly improve ability model complex physical dynamical system
"Minimax Rate Optimal Adaptive Nearest Neighbor Classification and
  Regression","  k Nearest Neighbor (kNN) method is a simple and popular statistical method
for classification and regression. For both classification and regression
problems, existing works have shown that, if the distribution of the feature
vector has bounded support and the probability density function is bounded away
from zero in its support, the convergence rate of the standard kNN method, in
which k is the same for all test samples, is minimax optimal. On the contrary,
if the distribution has unbounded support, we show that there is a gap between
the convergence rate achieved by the standard kNN method and the minimax bound.
To close this gap, we propose an adaptive kNN method, in which different k is
selected for different samples. Our selection rule does not require precise
knowledge of the underlying distribution of features. The new proposed method
significantly outperforms the standard one. We characterize the convergence
rate of the proposed adaptive method, and show that it matches the minimax
lower bound.
",k nearest neighbor knn method simple popular statistical method classification regression classification regression problem exist work show distribution feature vector bound support probability density function bound away zero support convergence rate standard knn method k test sample minimax optimal contrary distribution unbounded support show gap convergence rate achieve standard knn method minimax bind close gap propose adaptive knn method different k select different sample selection rule require precise knowledge underlie distribution feature new propose method significantly outperform standard one characterize convergence rate propose adaptive method show match minimax lower bind
"Reduction of detection limit and quantification uncertainty due to
  interferent by neural classification with abstention","  Many measurements in the physical sciences can be cast as counting
experiments, where the number of occurrences of a physical phenomenon informs
the prevalence of the phenomenon's source. Often, detection of the physical
phenomenon (termed signal) is difficult to distinguish from naturally occurring
phenomena (termed background). In this case, the discrimination of signal
events from background can be performed using classifiers, and they may range
from simple, threshold-based classifiers to sophisticated neural networks.
These classifiers are often trained and validated to obtain optimal accuracy,
however we show that the optimal accuracy classifier does not generally
coincide with a classifier that provides the lowest detection limit, nor the
lowest quantification uncertainty. We present a derivation of the detection
limit and quantification uncertainty in the classifier-based counting
experiment case. We also present a novel abstention mechanism to minimize the
detection limit or quantification uncertainty \emph{a posteriori}. We
illustrate the method on two data sets from the physical sciences,
discriminating Ar-37 and Ar-39 radioactive decay from non-radioactive events in
a gas proportional counter, and discriminating neutrons from photons in an
inorganic scintillator and report results therefrom.
",many measurement physical sciences cast count experiment number occurrence physical phenomenon inform prevalence phenomenon source often detection physical phenomenon term signal difficult distinguish naturally occur phenomena term background case discrimination signal event background perform use classifier may range simple threshold base classifier sophisticated neural network classifier often train validate obtain optimal accuracy however show optimal accuracy classifier generally coincide classifier provide low detection limit low quantification uncertainty present derivation detection limit quantification uncertainty classifier base count experiment case also present novel abstention mechanism minimize detection limit quantification uncertainty posteriori illustrate method two datum set physical science discriminate ar-37 ar-39 radioactive decay non radioactive event gas proportional counter discriminating neutron photon inorganic scintillator report result therefrom
"The Gaussian equivalence of generative models for learning with shallow
  neural networks","  Understanding the impact of data structure on the computational tractability
of learning is a key challenge for the theory of neural networks. Many
theoretical works do not explicitly model training data, or assume that inputs
are drawn component-wise independently from some simple probability
distribution. Here, we go beyond this simple paradigm by studying the
performance of neural networks trained on data drawn from pre-trained
generative models. This is possible due to a Gaussian equivalence stating that
the key metrics of interest, such as the training and test errors, can be fully
captured by an appropriately chosen Gaussian model. We provide three strands of
rigorous, analytical and numerical evidence corroborating this equivalence.
First, we establish rigorous conditions for the Gaussian equivalence to hold in
the case of single-layer generative models, as well as deterministic rates for
convergence in distribution. Second, we leverage this equivalence to derive a
closed set of equations describing the generalisation performance of two widely
studied machine learning problems: two-layer neural networks trained using
one-pass stochastic gradient descent, and full-batch pre-learned features or
kernel methods. Finally, we perform experiments demonstrating how our theory
applies to deep, pre-trained generative models. These results open a viable
path to the theoretical study of machine learning models with realistic data.
",understand impact datum structure computational tractability learn key challenge theory neural network many theoretical work explicitly model training datum assume input draw component wise independently simple probability distribution go beyond simple paradigm study performance neural network train datum draw pre train generative model possible due gaussian equivalence state key metric interest training test error fully capture appropriately choose gaussian model provide three strand rigorous analytical numerical evidence corroborate equivalence first establish rigorous condition gaussian equivalence hold case single layer generative model well deterministic rate convergence distribution second leverage equivalence derive close set equation describe generalisation performance two widely study machine learn problem two layer neural network train use one pass stochastic gradient descent full batch pre learned feature kernel method finally perform experiment demonstrate theory apply deep pre train generative model result open viable path theoretical study machine learning model realistic datum
"Snippet Policy Network for Multi-class Varied-length ECG Early
  Classification","  Arrhythmia detection from ECG is an important research subject in the
prevention and diagnosis of cardiovascular diseases. The prevailing studies
formulate arrhythmia detection from ECG as a time series classification
problem. Meanwhile, early detection of arrhythmia presents a real-world demand
for early prevention and diagnosis. In this paper, we address a problem of
cardiovascular disease early classification, which is a varied-length and
long-length time series early classification problem as well. For solving this
problem, we propose a deep reinforcement learning-based framework, namely
Snippet Policy Network (SPN), consisting of four modules, snippet generator,
backbone network, controlling agent, and discriminator. Comparing to the
existing approaches, the proposed framework features flexible input length,
solves the dual-optimization solution of the earliness and accuracy goals.
Experimental results demonstrate that SPN achieves an excellent performance of
over 80\% in terms of accuracy. Compared to the state-of-the-art methods, at
least 7% improvement on different metrics, including the precision, recall,
F1-score, and harmonic mean, is delivered by the proposed SPN. To the best of
our knowledge, this is the first work focusing on solving the cardiovascular
early classification problem based on varied-length ECG data. Based on these
excellent features from SPN, it offers a good exemplification for addressing
all kinds of varied-length time series early classification problems.
",arrhythmia detection ecg important research subject prevention diagnosis cardiovascular disease prevail study formulate arrhythmia detection ecg time series classification problem meanwhile early detection arrhythmia present real world demand early prevention diagnosis paper address problem cardiovascular disease early classification varied length long length time series early classification problem well solve problem propose deep reinforcement learning base framework namely snippet policy network spn consist four module snippet generator backbone network control agent discriminator compare exist approach propose framework feature flexible input length solve dual optimization solution earliness accuracy goal experimental result demonstrate spn achieve excellent performance term accuracy compare state of the art method least 7 improvement different metric include precision recall f1 score harmonic mean deliver propose spn good knowledge first work focus solve cardiovascular early classification problem base varied length ecg datum base excellent feature spn offer good exemplification address kind varied length time series early classification problem
"Generalized Learning Vector Quantization for Classification in
  Randomized Neural Networks and Hyperdimensional Computing","  Machine learning algorithms deployed on edge devices must meet certain
resource constraints and efficiency requirements. Random Vector Functional Link
(RVFL) networks are favored for such applications due to their simple design
and training efficiency. We propose a modified RVFL network that avoids
computationally expensive matrix operations during training, thus expanding the
network's range of potential applications. Our modification replaces the
least-squares classifier with the Generalized Learning Vector Quantization
(GLVQ) classifier, which only employs simple vector and distance calculations.
The GLVQ classifier can also be considered an improvement upon certain
classification algorithms popularly used in the area of Hyperdimensional
Computing. The proposed approach achieved state-of-the-art accuracy on a
collection of datasets from the UCI Machine Learning Repository - higher than
previously proposed RVFL networks. We further demonstrate that our approach
still achieves high accuracy while severely limited in training iterations
(using on average only 21% of the least-squares classifier computational
costs).
",machine learning algorithm deploy edge device must meet certain resource constraint efficiency requirement random vector functional link rvfl network favor application due simple design training efficiency propose modify rvfl network avoid computationally expensive matrix operation training thus expand network range potential application modification replace least square classifier generalize learn vector quantization glvq classifier employ simple vector distance calculation glvq classifier also consider improvement upon certain classification algorithm popularly use area hyperdimensional computing propose approach achieve state of the art accuracy collection dataset uci machine learn repository high previously propose rvfl network demonstrate approach still achieve high accuracy severely limited training iteration use average 21 least square classifier computational cost
Teaching with IMPACT,"  Like many problems in AI in their general form, supervised learning is
computationally intractable. We hypothesize that an important reason humans can
learn highly complex and varied concepts, in spite of the computational
difficulty, is that they benefit tremendously from experienced and insightful
teachers. This paper proposes a new learning framework that provides a role for
a knowledgeable, benevolent teacher to guide the process of learning a target
concept in a series of ""curricular"" phases or rounds. In each round, the
teacher's role is to act as a moderator, exposing the learner to a subset of
the available training data to move it closer to mastering the target concept.
Via both theoretical and empirical evidence, we argue that this framework
enables simple, efficient learners to acquire very complex concepts from
examples. In particular, we provide multiple examples of concept classes that
are known to be unlearnable in the standard PAC setting along with provably
efficient algorithms for learning them in our extended setting. A key focus of
our work is the ability to learn complex concepts on top of simpler, previously
learned, concepts---a direction with the potential of creating more competent
artificial agents.
",like many problem ai general form supervise learn computationally intractable hypothesize important reason human learn highly complex varied concept spite computational difficulty benefit tremendously experience insightful teacher paper propose new learning framework provide role knowledgeable benevolent teacher guide process learn target concept series curricular phase round round teacher role act moderator expose learner subset available training datum move close mastering target concept via theoretical empirical evidence argue framework enable simple efficient learner acquire complex concept example particular provide multiple example concept class know unlearnable standard pac setting along provably efficient algorithm learn extend set key focus work ability learn complex concept top simple previously learn concept -a direction potential create competent artificial agent
Heavy-Tail Phenomenon in Decentralized SGD,"Recent theoretical studies have shown that heavy-tails can emerge in
stochastic optimization due to `multiplicative noise', even under surprisingly
simple settings, such as linear regression with Gaussian data. While these
studies have uncovered several interesting phenomena, they consider
conventional stochastic optimization problems, which exclude decentralized
settings that naturally arise in modern machine learning applications. In this
paper, we study the emergence of heavy-tails in decentralized stochastic
gradient descent (DE-SGD), and investigate the effect of decentralization on
the tail behavior. We first show that, when the loss function at each
computational node is twice continuously differentiable and strongly convex
outside a compact region, the law of the DE-SGD iterates converges to a
distribution with polynomially decaying (heavy) tails. To have a more explicit
control on the tail exponent, we then consider the case where the loss at each
node is a quadratic, and show that the tail-index can be estimated as a
function of the step-size, batch-size, and the topological properties of the
network of the computational nodes. Then, we provide theoretical and empirical
results showing that DE-SGD has heavier tails than centralized SGD. We also
compare DE-SGD to disconnected SGD where nodes distribute the data but do not
communicate. Our theory uncovers an interesting interplay between the tails and
the network structure: we identify two regimes of parameters (stepsize and
network size), where DE-SGD can have lighter or heavier tails than disconnected
SGD depending on the regime. Finally, to support our theoretical results, we
provide numerical experiments conducted on both synthetic data and neural
networks.",recent theoretical study show heavy tail emerge stochastic optimization due multiplicative noise even surprisingly simple setting linear regression gaussian datum study uncover several interesting phenomenon consider conventional stochastic optimization problem exclude decentralized setting naturally arise modern machine learning application paper study emergence heavy tail decentralize stochastic gradient descent de sgd investigate effect decentralization tail behavior first show loss function computational node twice continuously differentiable strongly convex outside compact region law de sgd iterate converge distribution polynomially decay heavy tail explicit control tail exponent consider case loss node quadratic show tail index estimate function step size batch size topological property network computational node provide theoretical empirical result show de sgd heavy tail centralize sgd also compare de sgd disconnected sgd node distribute datum communicate theory uncovers interesting interplay tail network structure identify two regime parameter stepsize network size de sgd lighter heavy tail disconnect sgd depend regime finally support theoretical result provide numerical experiment conduct synthetic datum neural network
Generative Adversarial Networks,"  Generative Adversarial Networks (GANs) are very popular frameworks for
generating high-quality data, and are immensely used in both the academia and
industry in many domains. Arguably, their most substantial impact has been in
the area of computer vision, where they achieve state-of-the-art image
generation. This chapter gives an introduction to GANs, by discussing their
principle mechanism and presenting some of their inherent problems during
training and evaluation. We focus on these three issues: (1) mode collapse, (2)
vanishing gradients, and (3) generation of low-quality images. We then list
some architecture-variant and loss-variant GANs that remedy the above
challenges. Lastly, we present two utilization examples of GANs for real-world
applications: Data augmentation and face images generation.
",generative adversarial network gan popular framework generate high quality datum immensely use academia industry many domain arguably substantial impact area computer vision achieve state of the art image generation chapter give introduction gan discuss principle mechanism present inherent problem train evaluation focus three issue 1 mode collapse 2 vanish gradient 3 generation low quality image list architecture variant loss variant gan remedy challenge lastly present two utilization example gan real world application datum augmentation face image generation
Robust testing of low-dimensional functions,"  A natural problem in high-dimensional inference is to decide if a classifier
$f:\mathbb{R}^n \rightarrow \{-1,1\}$ depends on a small number of linear
directions of its input data. Call a function $g: \mathbb{R}^n \rightarrow
\{-1,1\}$, a linear $k$-junta if it is completely determined by some
$k$-dimensional subspace of the input space. A recent work of the authors
showed that linear $k$-juntas are testable. Thus there exists an algorithm to
distinguish between: 1. $f: \mathbb{R}^n \rightarrow \{-1,1\}$ which is a
linear $k$-junta with surface area $s$, 2. $f$ is $\epsilon$-far from any
linear $k$-junta with surface area $(1+\epsilon)s$, where the query complexity
of the algorithm is independent of the ambient dimension $n$.
  Following the surge of interest in noise-tolerant property testing, in this
paper we prove a noise-tolerant (or robust) version of this result. Namely, we
give an algorithm which given any $c>0$, $\epsilon>0$, distinguishes between 1.
$f: \mathbb{R}^n \rightarrow \{-1,1\}$ has correlation at least $c$ with some
linear $k$-junta with surface area $s$. 2. $f$ has correlation at most
$c-\epsilon$ with any linear $k$-junta with surface area at most $s$. The query
complexity of our tester is $k^{\mathsf{poly}(s/\epsilon)}$.
  Using our techniques, we also obtain a fully noise tolerant tester with the
same query complexity for any class $\mathcal{C}$ of linear $k$-juntas with
surface area bounded by $s$. As a consequence, we obtain a fully noise tolerant
tester with query complexity $k^{O(\mathsf{poly}(\log k/\epsilon))}$ for the
class of intersection of $k$-halfspaces (for constant $k$) over the Gaussian
space. Our query complexity is independent of the ambient dimension $n$.
Previously, no non-trivial noise tolerant testers were known even for a single
halfspace.
",natural problem high dimensional inference decide classifier f r depend small number linear direction input datum call function g r linear k -junta completely determined k -dimensional subspace input space recent work author show linear k -junta testable thus exist algorithm distinguish 1 f r linear k -junta surface area 2 f -far linear k -junta surface area query complexity algorithm independent ambient dimension n follow surge interest noise tolerant property testing paper prove noise tolerant robust version result namely give algorithm give c 0 0 distinguish 1 f r correlation least c linear k -junta surface area 2 f correlation linear k -junta surface area query complexity tester poly use technique also obtain fully noise tolerant tester query complexity class c linear k -juntas surface area bound consequence obtain fully noise tolerant tester query complexity poly class intersection k -halfspace constant k gaussian space query complexity independent ambient dimension n previously non trivial noise tolerant tester know even single halfspace
Privacy-Preserving Synthetic Smart Meters Data,"  Power consumption data is very useful as it allows to optimize power grids,
detect anomalies and prevent failures, on top of being useful for diverse
research purposes. However, the use of power consumption data raises
significant privacy concerns, as this data usually belongs to clients of a
power company. As a solution, we propose a method to generate synthetic power
consumption samples that faithfully imitate the originals, but are detached
from the clients and their identities. Our method is based on Generative
Adversarial Networks (GANs). Our contribution is twofold. First, we focus on
the quality of the generated data, which is not a trivial task as no standard
evaluation methods are available. Then, we study the privacy guarantees
provided to members of the training set of our neural network. As a minimum
requirement for privacy, we demand our neural network to be robust to
membership inference attacks, as these provide a gateway for further attacks in
addition to presenting a privacy threat on their own. We find that there is a
compromise to be made between the privacy and the performance provided by the
algorithm.
",power consumption datum useful allow optimize power grid detect anomaly prevent failure top useful diverse research purpose however use power consumption datum raise significant privacy concern datum usually belong client power company solution propose method generate synthetic power consumption sample faithfully imitate original detach client identity method base generative adversarial network gan contribution twofold first focus quality generate datum trivial task standard evaluation method available study privacy guarantee provide member training set neural network minimum requirement privacy demand neural network robust membership inference attack provide gateway attack addition present privacy threat find compromise make privacy performance provide algorithm
Harnessing Deep Neural Networks with Logic Rules,"  Combining deep neural networks with structured logic rules is desirable to
harness flexibility and reduce uninterpretability of the neural models. We
propose a general framework capable of enhancing various types of neural
networks (e.g., CNNs and RNNs) with declarative first-order logic rules.
Specifically, we develop an iterative distillation method that transfers the
structured information of logic rules into the weights of neural networks. We
deploy the framework on a CNN for sentiment analysis, and an RNN for named
entity recognition. With a few highly intuitive rules, we obtain substantial
improvements and achieve state-of-the-art or comparable results to previous
best-performing systems.
",combine deep neural network structure logic rule desirable harness flexibility reduce uninterpretability neural model propose general framework capable enhance various type neural network cnns rnns declarative first order logic rule specifically develop iterative distillation method transfer structure information logic rule weight neural network deploy framework cnn sentiment analysis rnn name entity recognition highly intuitive rule obtain substantial improvement achieve state of the art comparable result previous well perform system
"Adaptivity and Optimality: A Universal Algorithm for Online Convex
  Optimization","  In this paper, we study adaptive online convex optimization, and aim to
design a universal algorithm that achieves optimal regret bounds for multiple
common types of loss functions. Existing universal methods are limited in the
sense that they are optimal for only a subclass of loss functions. To address
this limitation, we propose a novel online method, namely Maler, which enjoys
the optimal $O(\sqrt{T})$, $O(d\log T)$ and $O(\log T)$ regret bounds for
general convex, exponentially concave, and strongly convex functions
respectively. The essential idea is to run multiple types of learning
algorithms with different learning rates in parallel, and utilize a meta
algorithm to track the best one on the fly. Empirical results demonstrate the
effectiveness of our method.
",paper study adaptive online convex optimization aim design universal algorithm achieve optimal regret bound multiple common type loss function exist universal method limit sense optimal subclass loss function address limitation propose novel online method namely maler enjoy optimal regret bound general convex exponentially concave strongly convex function respectively essential idea run multiple type learn algorithm different learning rate parallel utilize meta algorithm track good one fly empirical result demonstrate effectiveness method
"Pool-Based Unsupervised Active Learning for Regression Using Iterative
  Representativeness-Diversity Maximization (iRDM)","  Active learning (AL) selects the most beneficial unlabeled samples to label,
and hence a better machine learning model can be trained from the same number
of labeled samples. Most existing active learning for regression (ALR)
approaches are supervised, which means the sampling process must use some label
information, or an existing regression model. This paper considers completely
unsupervised ALR, i.e., how to select the samples to label without knowing any
true label information. We propose a novel unsupervised ALR approach, iterative
representativeness-diversity maximization (iRDM), to optimally balance the
representativeness and the diversity of the selected samples. Experiments on 12
datasets from various domains demonstrated its effectiveness. Our iRDM can be
applied to both linear regression and kernel regression, and it even
significantly outperforms supervised ALR when the number of labeled samples is
small.
",active learning al select beneficial unlabeled sample label hence well machine learning model train number label sample exist active learning regression alr approach supervise mean sampling process must use label information exist regression model paper consider completely unsupervised alr select sample label without know true label information propose novel unsupervised alr approach iterative representativeness diversity maximization irdm optimally balance representativeness diversity select sample experiment 12 dataset various domain demonstrate effectiveness irdm apply linear regression kernel regression even significantly outperform supervise alr number label sample small
A Robust Image Watermarking System Based on Deep Neural Networks,"  Digital image watermarking is the process of embedding and extracting
watermark covertly on a carrier image. Incorporating deep learning networks
with image watermarking has attracted increasing attention during recent years.
However, existing deep learning-based watermarking systems cannot achieve
robustness, blindness, and automated embedding and extraction simultaneously.
In this paper, a fully automated image watermarking system based on deep neural
networks is proposed to generalize the image watermarking processes. An
unsupervised deep learning structure and a novel loss computation are proposed
to achieve high capacity and high robustness without any prior knowledge of
possible attacks. Furthermore, a challenging application of watermark
extraction from camera-captured images is provided to validate the practicality
as well as the robustness of the proposed system. Experimental results show the
superiority performance of the proposed system as comparing against several
currently available techniques.
",digital image watermarking process embed extract watermark covertly carrier image incorporate deep learning network image watermarking attract increase attention recent year however exist deep learning base watermarking system achieve robustness blindness automate embed extraction simultaneously paper fully automate image watermarking system base deep neural network propose generalize image watermarking process unsupervise deep learning structure novel loss computation propose achieve high capacity high robustness without prior knowledge possible attack furthermore challenging application watermark extraction camera capture image provide validate practicality well robustness propose system experimental result show superiority performance propose system compare several currently available technique
"Phenotyping OSA: a time series analysis using fuzzy clustering and
  persistent homology","  Sleep apnea is a disorder that has serious consequences for the pediatric
population. There has been recent concern that traditional diagnosis of the
disorder using the apnea-hypopnea index may be ineffective in capturing its
multi-faceted outcomes. In this work, we take a first step in addressing this
issue by phenotyping patients using a clustering analysis of airflow time
series. This is approached in three ways: using feature-based fuzzy clustering
in the time and frequency domains, and using persistent homology to study the
signal from a topological perspective. The fuzzy clusters are analyzed in a
novel manner using a Dirichlet regression analysis, while the topological
approach leverages Takens embedding theorem to study the periodicity properties
of the signals.
",sleep apnea disorder serious consequence pediatric population recent concern traditional diagnosis disorder use apnea hypopnea index may ineffective capture multi faceted outcome work take first step addressing issue phenotype patient use cluster analysis airflow time series approach three way use feature base fuzzy clustering time frequency domain use persistent homology study signal topological perspective fuzzy cluster analyze novel manner use dirichlet regression analysis topological approach leverage takens embed theorem study periodicity property signal
Pilot Interval Reduction by Deep Learning Based Detectors in Uplink NOMA,"  Non-Orthogonal Multiple Access (NOMA) has higher spectral efficiency than
orthogonal multiple access (OMA) techniques. In uplink communication systems
that the channel is not known at the receiver, pilot signals sent from each
user in different time intervals have reduced the spectral efficiency of NOMA.
In this study, in the uplink communication system, DL-deep learning based
detectors which are known to respond to the pilot signals sent from the users
at the base station have been researched. It is aimed to maintain the spectral
efficiency of NOMA by sending a single pilot from users, thus reducing the time
interval in the DL detectors.
",non orthogonal multiple access noma high spectral efficiency orthogonal multiple access oma technique uplink communication system channel know receiver pilot signal send user different time interval reduce spectral efficiency noma study uplink communication system dl deep learning base detector know respond pilot signal send user base station research aim maintain spectral efficiency noma send single pilot user thus reduce time interval dl detector
"Comment on ""AndrODet: An adaptive Android obfuscation detector""","  We have identified a methodological problem in the empirical evaluation of
the string encryption detection capabilities of the AndrODet system described
by Mirzaei et al. in the recent paper ""AndrODet: An adaptive Android
obfuscation detector"". The accuracy of string encryption detection is evaluated
using samples from the AMD and PraGuard malware datasets. However, the authors
failed to account for the fact that many of the AMD samples are highly similar
due to the fact that they come from the same malware family. This introduces a
risk that a machine learning system trained on these samples could fail to
learn a generalizable model for string encryption detection, and might instead
learn to classify samples based on characteristics of each malware family. Our
own evaluation strongly indicates that the reported high accuracy of AndrODet's
string encryption detection is indeed due to this phenomenon. When we evaluated
AndrODet, we found that when we ensured that samples from the same family never
appeared in both training and testing data, the accuracy dropped to around 50%.
Moreover, the PraGuard dataset is not suitable for evaluating a static string
encryption detector such as AndrODet, since the particular obfuscation tool
used to produce the dataset effectively makes it impossible to extract
meaningful features of static strings in Android apps.
",identify methodological problem empirical evaluation string encryption detection capability androdet system describe mirzaei et al recent paper androdet adaptive android obfuscation detector accuracy string encryption detection evaluate use sample amd praguard malware dataset however author fail account fact many amd sample highly similar due fact come malware family introduce risk machine learn system train sample could fail learn generalizable model string encryption detection might instead learn classify sample base characteristic malware family evaluation strongly indicates report high accuracy androdet string encryption detection indeed due phenomenon evaluate androdet find ensure sample family never appear training testing datum accuracy drop around 50 moreover praguard dataset suitable evaluate static string encryption detector androdet since particular obfuscation tool use produce dataset effectively make impossible extract meaningful feature static string android app
"Statistical Consistency of Finite-dimensional Unregularized Linear
  Classification","  This manuscript studies statistical properties of linear classifiers obtained
through minimization of an unregularized convex risk over a finite sample.
Although the results are explicitly finite-dimensional, inputs may be passed
through feature maps; in this way, in addition to treating the consistency of
logistic regression, this analysis also handles boosting over a finite weak
learning class with, for instance, the exponential, logistic, and hinge losses.
In this finite-dimensional setting, it is still possible to fit arbitrary
decision boundaries: scaling the complexity of the weak learning class with the
sample size leads to the optimal classification risk almost surely.
",manuscript study statistical property linear classifier obtain minimization unregularized convex risk finite sample although result explicitly finite dimensional input may pass feature map way addition treat consistency logistic regression analysis also handle boost finite weak learning class instance exponential logistic hinge loss finite dimensional setting still possible fit arbitrary decision boundary scale complexity weak learning class sample size lead optimal classification risk almost surely
Probable convexity and its application to Correlated Topic Models,"  Non-convex optimization problems often arise from probabilistic modeling,
such as estimation of posterior distributions. Non-convexity makes the problems
intractable, and poses various obstacles for us to design efficient algorithms.
In this work, we attack non-convexity by first introducing the concept of
\emph{probable convexity} for analyzing convexity of real functions in
practice. We then use the new concept to analyze an inference problem in the
\emph{Correlated Topic Model} (CTM) and related nonconjugate models. Contrary
to the existing belief of intractability, we show that this inference problem
is concave under certain conditions. One consequence of our analyses is a novel
algorithm for learning CTM which is significantly more scalable and qualitative
than existing methods. Finally, we highlight that stochastic gradient
algorithms might be a practical choice to resolve efficiently non-convex
problems. This finding might find beneficial in many contexts which are beyond
probabilistic modeling.
",non convex optimization problem often arise probabilistic modeling estimation posterior distribution non convexity make problem intractable pose various obstacle we design efficient algorithm work attack non convexity first introduce concept probable convexity analyze convexity real function practice use new concept analyze inference problem correlate topic model ctm relate nonconjugate model contrary exist belief intractability show inference problem concave certain condition one consequence analyse novel algorithm learn ctm significantly scalable qualitative exist method finally highlight stochastic gradient algorithm might practical choice resolve efficiently non convex problem find might find beneficial many context beyond probabilistic modeling
"Generating High Fidelity Images with Subscale Pixel Networks and
  Multidimensional Upscaling","  The unconditional generation of high fidelity images is a longstanding
benchmark for testing the performance of image decoders. Autoregressive image
models have been able to generate small images unconditionally, but the
extension of these methods to large images where fidelity can be more readily
assessed has remained an open problem. Among the major challenges are the
capacity to encode the vast previous context and the sheer difficulty of
learning a distribution that preserves both global semantic coherence and
exactness of detail. To address the former challenge, we propose the Subscale
Pixel Network (SPN), a conditional decoder architecture that generates an image
as a sequence of sub-images of equal size. The SPN compactly captures
image-wide spatial dependencies and requires a fraction of the memory and the
computation required by other fully autoregressive models. To address the
latter challenge, we propose to use Multidimensional Upscaling to grow an image
in both size and depth via intermediate stages utilising distinct SPNs. We
evaluate SPNs on the unconditional generation of CelebAHQ of size 256 and of
ImageNet from size 32 to 256. We achieve state-of-the-art likelihood results in
multiple settings, set up new benchmark results in previously unexplored
settings and are able to generate very high fidelity large scale samples on the
basis of both datasets.
",unconditional generation high fidelity image longstande benchmark testing performance image decoder autoregressive image model able generate small image unconditionally extension method large image fidelity readily assess remain open problem among major challenge capacity encode vast previous context sheer difficulty learn distribution preserve global semantic coherence exactness detail address former challenge propose subscale pixel network spn conditional decoder architecture generate image sequence sub image equal size spn compactly capture image wide spatial dependency require fraction memory computation require fully autoregressive model address latter challenge propose use multidimensional upscaling grow image size depth via intermediate stage utilise distinct spns evaluate spns unconditional generation celebahq size 256 imagenet size 32 achieve state of the art likelihood result multiple setting set new benchmark result previously unexplored setting able generate high fidelity large scale sample basis dataset
Hyperbolic Distance Matrices,"  Hyperbolic space is a natural setting for mining and visualizing data with
hierarchical structure. In order to compute a hyperbolic embedding from
comparison or similarity information, one has to solve a hyperbolic distance
geometry problem. In this paper, we propose a unified framework to compute
hyperbolic embeddings from an arbitrary mix of noisy metric and non-metric
data. Our algorithms are based on semidefinite programming and the notion of a
hyperbolic distance matrix, in many ways parallel to its famous Euclidean
counterpart. A central ingredient we put forward is a semidefinite
characterization of the hyperbolic Gramian -- a matrix of Lorentzian inner
products. This characterization allows us to formulate a semidefinite
relaxation to efficiently compute hyperbolic embeddings in two stages: first,
we complete and denoise the observed hyperbolic distance matrix; second, we
propose a spectral factorization method to estimate the embedded points from
the hyperbolic distance matrix. We show through numerical experiments how the
flexibility to mix metric and non-metric constraints allows us to efficiently
compute embeddings from arbitrary data.
",hyperbolic space natural set mining visualize datum hierarchical structure order compute hyperbolic embed comparison similarity information one solve hyperbolic distance geometry problem paper propose unified framework compute hyperbolic embedding arbitrary mix noisy metric non metric data algorithm base semidefinite programming notion hyperbolic distance matrix many way parallel famous euclidean counterpart central ingredient put forward semidefinite characterization hyperbolic gramian matrix lorentzian inner product characterization allow we formulate semidefinite relaxation efficiently compute hyperbolic embedding two stage first complete denoise observe hyperbolic distance matrix second propose spectral factorization method estimate embed point hyperbolic distance matrix show numerical experiment flexibility mix metric non metric constraint allow we efficiently compute embedding arbitrary datum
An Interpretability Illusion for BERT,"  We describe an ""interpretability illusion"" that arises when analyzing the
BERT model. Activations of individual neurons in the network may spuriously
appear to encode a single, simple concept, when in fact they are encoding
something far more complex. The same effect holds for linear combinations of
activations. We trace the source of this illusion to geometric properties of
BERT's embedding space as well as the fact that common text corpora represent
only narrow slices of possible English sentences. We provide a taxonomy of
model-learned concepts and discuss methodological implications for
interpretability research, especially the importance of testing hypotheses on
multiple data sets.
",describe interpretability illusion arise analyze bert model activation individual neuron network may spuriously appear encode single simple concept fact encode something far complex effect hold linear combination activation trace source illusion geometric property bert embed space well fact common text corpora represent narrow slice possible english sentence provide taxonomy model learn concept discuss methodological implication interpretability research especially importance testing hypothese multiple datum set
A chain rule for the expected suprema of Gaussian processes,"  The expected supremum of a Gaussian process indexed by the image of an index
set under a function class is bounded in terms of separate properties of the
index set and the function class. The bound is relevant to the estimation of
nonlinear transformations or the analysis of learning algorithms whenever
hypotheses are chosen from composite classes, as is the case for multi-layer
models.
",expect supremum gaussian process index image index set function class bound term separate property index set function class bind relevant estimation nonlinear transformation analysis learn algorithm whenever hypothesis choose composite class case multi layer model
"Tractable loss function and color image generation of multinary
  restricted Boltzmann machine","  The restricted Boltzmann machine (RBM) is a representative generative model
based on the concept of statistical mechanics. In spite of the strong merit of
interpretability, unavailability of backpropagation makes it less competitive
than other generative models. Here we derive differentiable loss functions for
both binary and multinary RBMs. Then we demonstrate their learnability and
performance by generating colored face images.
",restrict boltzmann machine rbm representative generative model base concept statistical mechanic spite strong merit interpretability unavailability backpropagation make less competitive generative model derive differentiable loss function binary multinary rbms demonstrate learnability performance generate colored face image
i-RevNet: Deep Invertible Networks,"  It is widely believed that the success of deep convolutional networks is
based on progressively discarding uninformative variability about the input
with respect to the problem at hand. This is supported empirically by the
difficulty of recovering images from their hidden representations, in most
commonly used network architectures. In this paper we show via a one-to-one
mapping that this loss of information is not a necessary condition to learn
representations that generalize well on complicated problems, such as ImageNet.
Via a cascade of homeomorphic layers, we build the i-RevNet, a network that can
be fully inverted up to the final projection onto the classes, i.e. no
information is discarded. Building an invertible architecture is difficult, for
one, because the local inversion is ill-conditioned, we overcome this by
providing an explicit inverse. An analysis of i-RevNets learned representations
suggests an alternative explanation for the success of deep networks by a
progressive contraction and linear separation with depth. To shed light on the
nature of the model learned by the i-RevNet we reconstruct linear
interpolations between natural image representations.
",widely believe success deep convolutional network base progressively discard uninformative variability input respect problem hand support empirically difficulty recover image hide representation commonly use network architecture paper show via one to one mapping loss information necessary condition learn representation generalize well complicated problem imagenet via cascade homeomorphic layer build i revnet network fully invert final projection onto class information discard build invertible architecture difficult one local inversion ill condition overcome provide explicit inverse analysis i revnet learn representation suggest alternative explanation success deep network progressive contraction linear separation depth shed light nature model learn i revnet reconstruct linear interpolation natural image representation
"Synthesizing Machine Learning Programs with PAC Guarantees via
  Statistical Sketching","  We study the problem of synthesizing programs that include machine learning
components such as deep neural networks (DNNs). We focus on statistical
properties, which are properties expected to hold with high probability --
e.g., that an image classification model correctly identifies people in images
with high probability. We propose novel algorithms for sketching and
synthesizing such programs by leveraging ideas from statistical learning theory
to provide statistical soundness guarantees. We evaluate our approach on
synthesizing list processing programs that include DNN components used to
process image inputs, as well as case studies on image classification and on
precision medicine. Our results demonstrate that our approach can be used to
synthesize programs with probabilistic guarantees.
",study problem synthesizing program include machine learn component deep neural network dnn focus statistical property property expect hold high probability image classification model correctly identify people image high probability propose novel algorithm sketch synthesize program leverage idea statistical learning theory provide statistical soundness guarantee evaluate approach synthesizing list processing program include dnn component use process image input well case study image classification precision medicine result demonstrate approach use synthesize program probabilistic guarantee
Data Augmentation using Pre-trained Transformer Models,"  Language model based pre-trained models such as BERT have provided
significant gains across different NLP tasks. In this paper, we study different
types of transformer based pre-trained models such as auto-regressive models
(GPT-2), auto-encoder models (BERT), and seq2seq models (BART) for conditional
data augmentation. We show that prepending the class labels to text sequences
provides a simple yet effective way to condition the pre-trained models for
data augmentation. Additionally, on three classification benchmarks,
pre-trained Seq2Seq model outperforms other data augmentation methods in a
low-resource setting. Further, we explore how different pre-trained model based
data augmentation differs in-terms of data diversity, and how well such methods
preserve the class-label information.
",language model base pre train model bert provide significant gain across different nlp task paper study different type transformer base pre train model auto regressive model gpt-2 auto encoder model bert seq2seq model bart conditional datum augmentation show prepende class label text sequence provide simple yet effective way condition pre train model data augmentation additionally three classification benchmark pre trained seq2seq model outperform data augmentation method low resource setting explore different pre train model base datum augmentation differ in term datum diversity well method preserve class label information
FedRecAttack: Model Poisoning Attack to Federated Recommendation,"  Federated Recommendation (FR) has received considerable popularity and
attention in the past few years. In FR, for each user, its feature vector and
interaction data are kept locally on its own client thus are private to others.
Without the access to above information, most existing poisoning attacks
against recommender systems or federated learning lose validity. Benifiting
from this characteristic, FR is commonly considered fairly secured. However, we
argue that there is still possible and necessary security improvement could be
made in FR. To prove our opinion, in this paper we present FedRecAttack, a
model poisoning attack to FR aiming to raise the exposure ratio of target
items. In most recommendation scenarios, apart from private user-item
interactions (e.g., clicks, watches and purchases), some interactions are
public (e.g., likes, follows and comments). Motivated by this point, in
FedRecAttack we make use of the public interactions to approximate users'
feature vectors, thereby attacker can generate poisoned gradients accordingly
and control malicious users to upload the poisoned gradients in a well-designed
way. To evaluate the effectiveness and side effects of FedRecAttack, we conduct
extensive experiments on three real-world datasets of different sizes from two
completely different scenarios. Experimental results demonstrate that our
proposed FedRecAttack achieves the state-of-the-art effectiveness while its
side effects are negligible. Moreover, even with small proportion (3%) of
malicious users and small proportion (1%) of public interactions, FedRecAttack
remains highly effective, which reveals that FR is more vulnerable to attack
than people commonly considered.
",federate recommendation fr receive considerable popularity attention past year fr user feature vector interaction datum keep locally client thus private other without access information exist poisoning attack recommender system federate learn lose validity benifite characteristic fr commonly consider fairly secured however argue still possible necessary security improvement could make fr prove opinion paper present fedrecattack model poison attack fr aiming raise exposure ratio target item recommendation scenario apart private user item interaction click watch purchase interaction public like follow comment motivate point fedrecattack make use public interaction approximate user feature vector thereby attacker generate poison gradient accordingly control malicious user upload poison gradient well design way evaluate effectiveness side effect fedrecattack conduct extensive experiment three real world dataset different size two completely different scenario experimental result demonstrate propose fedrecattack achieve state of the art effectiveness side effect negligible moreover even small proportion 3 malicious user small proportion 1 public interaction fedrecattack remain highly effective reveal fr vulnerable attack people commonly consider
Adaptive Personalized Federated Learning,"  Investigation of the degree of personalization in federated learning
algorithms has shown that only maximizing the performance of the global model
will confine the capacity of the local models to personalize. In this paper, we
advocate an adaptive personalized federated learning (APFL) algorithm, where
each client will train their local models while contributing to the global
model. We derive the generalization bound of mixture of local and global
models, and find the optimal mixing parameter. We also propose a
communication-efficient optimization method to collaboratively learn the
personalized models and analyze its convergence in both smooth strongly convex
and nonconvex settings. The extensive experiments demonstrate the effectiveness
of our personalization schema, as well as the correctness of established
generalization theories.
",investigation degree personalization federate learning algorithm show maximize performance global model confine capacity local model personalize paper advocate adaptive personalize federate learning apfl algorithm client train local model contribute global model derive generalization bind mixture local global model find optimal mixing parameter also propose communication efficient optimization method collaboratively learn personalized model analyze convergence smooth strongly convex nonconvex setting extensive experiment demonstrate effectiveness personalization schema well correctness establish generalization theory
"Qualitative neural network approximation over R and C: Elementary proofs
  for analytic and polynomial activation","  In this article, we prove approximation theorems in classes of deep and
shallow neural networks with analytic activation functions by elementary
arguments. We prove for both real and complex networks with non-polynomial
activation that the closure of the class of neural networks coincides with the
closure of the space of polynomials. The closure can further be characterized
by the Stone-Weierstrass theorem (in the real case) and Mergelyan's theorem (in
the complex case). In the real case, we further prove approximation results for
networks with higher-dimensional harmonic activation and orthogonally projected
linear maps. We further show that fully connected and residual networks of
large depth with polynomial activation functions can approximate any polynomial
under certain width requirements. All proofs are entirely elementary.
",article prove approximation theorem class deep shallow neural network analytic activation function elementary argument prove real complex network non polynomial activation closure class neural network coincide closure space polynomial closure characterize stone weierstrass theorem real case mergelyan theorem complex case real case prove approximation result network high dimensional harmonic activation orthogonally project linear map show fully connect residual network large depth polynomial activation function approximate polynomial certain width requirement proof entirely elementary
A Flexible Multi-Task Model for BERT Serving,"  In this demonstration, we present an efficient BERT-based multi-task (MT)
framework that is particularly suitable for iterative and incremental
development of the tasks. The proposed framework is based on the idea of
partial fine-tuning, i.e. only fine-tune some top layers of BERT while keep the
other layers frozen. For each task, we train independently a single-task (ST)
model using partial fine-tuning. Then we compress the task-specific layers in
each ST model using knowledge distillation. Those compressed ST models are
finally merged into one MT model so that the frozen layers of the former are
shared across the tasks. We exemplify our approach on eight GLUE tasks,
demonstrating that it is able to achieve both strong performance and
efficiency. We have implemented our method in the utterance understanding
system of XiaoAI, a commercial AI assistant developed by Xiaomi. We estimate
that our model reduces the overall serving cost by 86%.
",demonstration present efficient bert base multi task mt framework particularly suitable iterative incremental development task propose framework base idea partial fine tune fine tune top layer bert keep layer frozen task train independently single task st model use partial fine tune compress task specific layer st model use knowledge distillation compress st model finally merge one mt model frozen layer former share across task exemplify approach eight glue task demonstrate able achieve strong performance efficiency implement method utterance understanding system xiaoai commercial ai assistant developed xiaomi estimate model reduce overall serve cost 86
Adaptive Policies for Perimeter Surveillance Problems,"  Maximising the detection of intrusions is a fundamental and often critical
aim of perimeter surveillance. Commonly, this requires a decision-maker to
optimally allocate multiple searchers to segments of the perimeter. We consider
a scenario where the decision-maker may sequentially update the searchers'
allocation, learning from the observed data to improve decisions over time. In
this work we propose a formal model and solution methods for this sequential
perimeter surveillance problem. Our model is a combinatorial multi-armed bandit
(CMAB) with Poisson rewards and a novel filtered feedback mechanism - arising
from the failure to detect certain intrusions. Our solution method is an upper
confidence bound approach and we derive upper and lower bounds on its expected
performance. We prove that the gap between these bounds is of constant order,
and demonstrate empirically that our approach is more reliable in simulated
problems than competing algorithms.
",maximise detection intrusion fundamental often critical aim perimeter surveillance commonly require decision maker optimally allocate multiple searcher segment perimeter consider scenario decision maker may sequentially update searcher allocation learning observe datum improve decision time work propose formal model solution method sequential perimeter surveillance problem model combinatorial multi armed bandit cmab poisson reward novel filter feedback mechanism arise failure detect certain intrusion solution method upper confidence bind approach derive upper low bound expect performance prove gap bound constant order demonstrate empirically approach reliable simulated problem compete algorithm
Learning sparse relational transition models,"  We present a representation for describing transition models in complex
uncertain domains using relational rules. For any action, a rule selects a set
of relevant objects and computes a distribution over properties of just those
objects in the resulting state given their properties in the previous state. An
iterative greedy algorithm is used to construct a set of deictic references
that determine which objects are relevant in any given state. Feed-forward
neural networks are used to learn the transition distribution on the relevant
objects' properties. This strategy is demonstrated to be both more versatile
and more sample efficient than learning a monolithic transition model in a
simulated domain in which a robot pushes stacks of objects on a cluttered
table.
",present representation describe transition model complex uncertain domain use relational rule action rule select set relevant object compute distribution property object result state give property previous state iterative greedy algorithm use construct set deictic reference determine object relevant give state feed forward neural network use learn transition distribution relevant object property strategy demonstrate versatile sample efficient learn monolithic transition model simulate domain robot push stack object cluttered table
Privileged Zero-Shot AutoML,"  This work improves the quality of automated machine learning (AutoML) systems
by using dataset and function descriptions while significantly decreasing
computation time from minutes to milliseconds by using a zero-shot approach.
Given a new dataset and a well-defined machine learning task, humans begin by
reading a description of the dataset and documentation for the algorithms to be
used. This work is the first to use these textual descriptions, which we call
privileged information, for AutoML. We use a pre-trained Transformer model to
process the privileged text and demonstrate that using this information
improves AutoML performance. Thus, our approach leverages the progress of
unsupervised representation learning in natural language processing to provide
a significant boost to AutoML. We demonstrate that using only textual
descriptions of the data and functions achieves reasonable classification
performance, and adding textual descriptions to data meta-features improves
classification across tabular datasets. To achieve zero-shot AutoML we train a
graph neural network with these description embeddings and the data
meta-features. Each node represents a training dataset, which we use to predict
the best machine learning pipeline for a new test dataset in a zero-shot
fashion. Our zero-shot approach rapidly predicts a high-quality pipeline for a
supervised learning task and dataset. In contrast, most AutoML systems require
tens or hundreds of pipeline evaluations. We show that zero-shot AutoML reduces
running and prediction times from minutes to milliseconds, consistently across
datasets. By speeding up AutoML by orders of magnitude this work demonstrates
real-time AutoML.
",work improve quality automate machine learn automl system use dataset function description significantly decrease computation time minute millisecond use zero shot approach give new dataset well define machine learn task human begin read description dataset documentation algorithm use work first use textual description call privileged information automl use pre train transformer model process privileged text demonstrate use information improve automl performance thus approach leverage progress unsupervised representation learn natural language processing provide significant boost automl demonstrate use textual description data function achieve reasonable classification performance add textual description datum meta feature improve classification across tabular dataset achieve zero shot automl train graph neural network description embedding datum meta feature node represent train dataset use predict good machine learn pipeline new test dataset zero shot fashion zero shot approach rapidly predict high quality pipeline supervise learn task dataset contrast automl system require ten hundred pipeline evaluation show zero shot automl reduce run prediction time minute millisecond consistently across dataset speed automl order magnitude work demonstrate real time automl
"Solving Combinatorial Games using Products, Projections and
  Lexicographically Optimal Bases","  In order to find Nash-equilibria for two-player zero-sum games where each
player plays combinatorial objects like spanning trees, matchings etc, we
consider two online learning algorithms: the online mirror descent (OMD)
algorithm and the multiplicative weights update (MWU) algorithm. The OMD
algorithm requires the computation of a certain Bregman projection, that has
closed form solutions for simple convex sets like the Euclidean ball or the
simplex. However, for general polyhedra one often needs to exploit the general
machinery of convex optimization. We give a novel primal-style algorithm for
computing Bregman projections on the base polytopes of polymatroids. Next, in
the case of the MWU algorithm, although it scales logarithmically in the number
of pure strategies or experts $N$ in terms of regret, the algorithm takes time
polynomial in $N$; this especially becomes a problem when learning
combinatorial objects. We give a general recipe to simulate the multiplicative
weights update algorithm in time polynomial in their natural dimension. This is
useful whenever there exists a polynomial time generalized counting oracle
(even if approximate) over these objects. Finally, using the combinatorial
structure of symmetric Nash-equilibria (SNE) when both players play bases of
matroids, we show that these can be found with a single projection or convex
minimization (without using online learning).
",order find nash equilibria two player zero sum game player play combinatorial object like span tree matching etc consider two online learning algorithm online mirror descent omd algorithm multiplicative weights update mwu algorithm omd algorithm require computation certain bregman projection closed form solution simple convex set like euclidean ball simplex however general polyhedra one often need exploit general machinery convex optimization give novel primal style algorithm computing bregman projection base polytope polymatroid next case mwu algorithm although scale logarithmically number pure strategy expert n term regret algorithm take time polynomial n especially become problem learn combinatorial object give general recipe simulate multiplicative weight update algorithm time polynomial natural dimension useful whenever exist polynomial time generalize counting oracle even approximate object finally use combinatorial structure symmetric nash equilibria sne player play basis matroid show find single projection convex minimization without use online learning
"KO-PDE: Kernel Optimized Discovery of Partial Differential Equations
  with Varying Coefficients","  Partial differential equations (PDEs) fitting scientific data can represent
physical laws with explainable mechanisms for various mathematically-oriented
subjects. Most natural dynamics are expressed by PDEs with varying coefficients
(PDEs-VC), which highlights the importance of PDE discovery. Previous
algorithms can discover some simple instances of PDEs-VC but fail in the
discovery of PDEs with coefficients of higher complexity, as a result of
coefficient estimation inaccuracy. In this paper, we propose KO-PDE, a kernel
optimized regression method that incorporates the kernel density estimation of
adjacent coefficients to reduce the coefficient estimation error. KO-PDE can
discover PDEs-VC on which previous baselines fail and is more robust against
inevitable noise in data. In experiments, the PDEs-VC of seven challenging
spatiotemporal scientific datasets in fluid dynamics are all discovered by
KO-PDE, while the three baselines render false results in most cases. With
state-of-the-art performance, KO-PDE sheds light on the automatic description
of natural phenomenons using discovered PDEs in the real world.
",partial differential equation pde fitting scientific datum represent physical law explainable mechanism various mathematically orient subject natural dynamic express pde vary coefficient pde vc highlight importance pde discovery previous algorithm discover simple instance pde vc fail discovery pde coefficient high complexity result coefficient estimation inaccuracy paper propose ko pde kernel optimize regression method incorporate kernel density estimation adjacent coefficient reduce coefficient estimation error ko pde discover pde vc previous baseline fail robust inevitable noise datum experiment pde vc seven challenge spatiotemporal scientific dataset fluid dynamic discover ko pde three baseline render false result case state of the art performance ko pde shed light automatic description natural phenomenon use discover pde real world
Multimodal One-Shot Learning of Speech and Images,"  Imagine a robot is shown new concepts visually together with spoken tags,
e.g. ""milk"", ""eggs"", ""butter"". After seeing one paired audio-visual example per
class, it is shown a new set of unseen instances of these objects, and asked to
pick the ""milk"". Without receiving any hard labels, could it learn to match the
new continuous speech input to the correct visual instance? Although unimodal
one-shot learning has been studied, where one labelled example in a single
modality is given per class, this example motivates multimodal one-shot
learning. Our main contribution is to formally define this task, and to propose
several baseline and advanced models. We use a dataset of paired spoken and
visual digits to specifically investigate recent advances in Siamese
convolutional neural networks. Our best Siamese model achieves twice the
accuracy of a nearest neighbour model using pixel-distance over images and
dynamic time warping over speech in 11-way cross-modal matching.
",imagine robot show new concept visually together speak tag milk egg butter see one pair audio visual example per class show new set unseen instance object ask pick milk without receive hard label could learn match new continuous speech input correct visual instance although unimodal one shot learning study one label example single modality give per class example motivate multimodal one shot learn main contribution formally define task propose several baseline advanced model use dataset pair speak visual digit specifically investigate recent advance siamese convolutional neural network good siamese model achieve twice accuracy near neighbour model use pixel distance image dynamic time warp speech 11 way cross modal matching
"KF-LAX: Kronecker-factored curvature estimation for control variate
  optimization in reinforcement learning","  A key challenge for gradient based optimization methods in model-free
reinforcement learning is to develop an approach that is sample efficient and
has low variance. In this work, we apply Kronecker-factored curvature
estimation technique (KFAC) to a recently proposed gradient estimator for
control variate optimization, RELAX, to increase the sample efficiency of using
this gradient estimation method in reinforcement learning. The performance of
the proposed method is demonstrated on a synthetic problem and a set of three
discrete control task Atari games.
",key challenge gradient base optimization method model free reinforcement learning develop approach sample efficient low variance work apply kronecker factor curvature estimation technique kfac recently propose gradient estimator control variate optimization relax increase sample efficiency use gradient estimation method reinforcement learning performance propose method demonstrate synthetic problem set three discrete control task atari game
"FixyNN: Efficient Hardware for Mobile Computer Vision via Transfer
  Learning","  The computational demands of computer vision tasks based on state-of-the-art
Convolutional Neural Network (CNN) image classification far exceed the energy
budgets of mobile devices. This paper proposes FixyNN, which consists of a
fixed-weight feature extractor that generates ubiquitous CNN features, and a
conventional programmable CNN accelerator which processes a dataset-specific
CNN. Image classification models for FixyNN are trained end-to-end via transfer
learning, with the common feature extractor representing the transfered part,
and the programmable part being learnt on the target dataset. Experimental
results demonstrate FixyNN hardware can achieve very high energy efficiencies
up to 26.6 TOPS/W ($4.81 \times$ better than iso-area programmable
accelerator). Over a suite of six datasets we trained models via transfer
learning with an accuracy loss of $<1\%$ resulting in up to 11.2 TOPS/W -
nearly $2 \times$ more efficient than a conventional programmable CNN
accelerator of the same area.
",computational demand computer vision task base state of the art convolutional neural network cnn image classification far exceed energy budget mobile device paper propose fixynn consist fix weight feature extractor generate ubiquitous cnn feature conventional programmable cnn accelerator process dataset specific cnn image classification model fixynn train end to end via transfer learn common feature extractor represent transfered part programmable part learn target dataset experimental result demonstrate fixynn hardware achieve high energy efficiency well iso area programmable accelerator suite six dataset train model via transfer learn accuracy loss result nearly 2 efficient conventional programmable cnn accelerator area
Differentially Private ADMM for Distributed Medical Machine Learning,"  Due to massive amounts of data distributed across multiple locations,
distributed machine learning has attracted a lot of research interests.
Alternating Direction Method of Multipliers (ADMM) is a powerful method of
designing distributed machine learning algorithm, whereby each agent computes
over local datasets and exchanges computation results with its neighbor agents
in an iterative procedure. There exists significant privacy leakage during this
iterative process if the local data is sensitive. In this paper, we propose a
differentially private ADMM algorithm (P-ADMM) to provide dynamic
zero-concentrated differential privacy (dynamic zCDP), by inserting Gaussian
noise with linearly decaying variance. We prove that P-ADMM has the same
convergence rate compared to the non-private counterpart, i.e.,
$\mathcal{O}(1/K)$ with $K$ being the number of iterations and linear
convergence for general convex and strongly convex problems while providing
differentially private guarantee. Moreover, through our experiments performed
on real-world datasets, we empirically show that P-ADMM has the best-known
performance among the existing differentially private ADMM based algorithms.
",due massive amount datum distribute across multiple location distribute machine learning attract lot research interest alternate direction method multiplier admm powerful method designing distribute machine learn algorithm whereby agent compute local dataset exchange computation result neighbor agent iterative procedure exist significant privacy leakage iterative process local datum sensitive paper propose differentially private admm algorithm p admm provide dynamic zero concentrated differential privacy dynamic zcdp inserting gaussian noise linearly decay variance prove p admm convergence rate compare non private counterpart k number iteration linear convergence general convex strongly convex problem provide differentially private guarantee moreover experiment perform real world dataset empirically show p admm well know performance among exist differentially private admm base algorithm
"Multi-Target Multiple Instance Learning for Hyperspectral Target
  Detection","  In remote sensing, it is often challenging to acquire or collect a large
dataset that is accurately labeled. This difficulty is usually due to several
issues, including but not limited to the study site's spatial area and
accessibility, errors in the global positioning system (GPS), and mixed pixels
caused by an image's spatial resolution. We propose an approach, with two
variations, that estimates multiple target signatures from training samples
with imprecise labels: Multi-Target Multiple Instance Adaptive Cosine Estimator
(Multi-Target MI-ACE) and Multi-Target Multiple Instance Spectral Match Filter
(Multi-Target MI-SMF). The proposed methods address the problems above by
directly considering the multiple-instance, imprecisely labeled dataset. They
learn a dictionary of target signatures that optimizes detection against a
background using the Adaptive Cosine Estimator (ACE) and Spectral Match Filter
(SMF). Experiments were conducted to test the proposed algorithms using a
simulated hyperspectral dataset, the MUUFL Gulfport hyperspectral dataset
collected over the University of Southern Mississippi-Gulfpark Campus, and the
AVIRIS hyperspectral dataset collected over Santa Barbara County, California.
Both simulated and real hyperspectral target detection experiments show the
proposed algorithms are effective at learning target signatures and performing
target detection.
",remote sense often challenge acquire collect large dataset accurately label difficulty usually due several issue include limited study site spatial area accessibility error global positioning system gps mixed pixel cause image spatial resolution propose approach two variation estimate multiple target signature training sample imprecise label multi target multiple instance adaptive cosine estimator multi target mi ace multi target multiple instance spectral match filter multi target mi smf propose method address problem directly consider multiple instance imprecisely label dataset learn dictionary target signature optimize detection background use adaptive cosine estimator ace spectral match filter smf experiment conduct test propose algorithm use simulated hyperspectral dataset muufl gulfport hyperspectral dataset collect university southern mississippi gulfpark campus aviris hyperspectral dataset collect santa barbara county california simulate real hyperspectral target detection experiment show propose algorithm effective learn target signature perform target detection
"Deep Residual Learning for Accelerated MRI using Magnitude and Phase
  Networks","  Accelerated magnetic resonance (MR) scan acquisition with compressed sensing
(CS) and parallel imaging is a powerful method to reduce MR imaging scan time.
However, many reconstruction algorithms have high computational costs. To
address this, we investigate deep residual learning networks to remove aliasing
artifacts from artifact corrupted images. The proposed deep residual learning
networks are composed of magnitude and phase networks that are separately
trained. If both phase and magnitude information are available, the proposed
algorithm can work as an iterative k-space interpolation algorithm using
framelet representation. When only magnitude data is available, the proposed
approach works as an image domain post-processing algorithm. Even with strong
coherent aliasing artifacts, the proposed network successfully learned and
removed the aliasing artifacts, whereas current parallel and CS reconstruction
methods were unable to remove these artifacts. Comparisons using single and
multiple coil show that the proposed residual network provides good
reconstruction results with orders of magnitude faster computational time than
existing compressed sensing methods. The proposed deep learning framework may
have a great potential for accelerated MR reconstruction by generating accurate
results immediately.
",accelerate magnetic resonance mr scan acquisition compress sense cs parallel imaging powerful method reduce mr imaging scan time however many reconstruction algorithm high computational cost address investigate deep residual learning network remove aliasing artifact artifact corrupt image propose deep residual learning network compose magnitude phase network separately train phase magnitude information available propose algorithm work iterative k space interpolation algorithm use framelet representation magnitude datum available propose approach work image domain post processing algorithm even strong coherent aliasing artifact propose network successfully learn removed aliasing artifact whereas current parallel cs reconstruction method unable remove artifact comparison use single multiple coil show propose residual network provide good reconstruction result order magnitude fast computational time exist compress sense method propose deep learning framework may great potential accelerate mr reconstruction generate accurate result immediately
"Move-to-Data: A new Continual Learning approach with Deep CNNs,
  Application for image-class recognition","  In many real-life tasks of application of supervised learning approaches, all
the training data are not available at the same time. The examples are lifelong
image classification or recognition of environmental objects during interaction
of instrumented persons with their environment, enrichment of an
online-database with more images. It is necessary to pre-train the model at a
""training recording phase"" and then adjust it to the new coming data. This is
the task of incremental/continual learning approaches. Amongst different
problems to be solved by these approaches such as introduction of new
categories in the model, refining existing categories to sub-categories and
extending trained classifiers over them, ... we focus on the problem of
adjusting pre-trained model with new additional training data for existing
categories. We propose a fast continual learning layer at the end of the
neuronal network. Obtained results are illustrated on the opensource CIFAR
benchmark dataset. The proposed scheme yields similar performances as
retraining but with drastically lower computational cost.
",many real life task application supervise learning approach training datum available time example lifelong image classification recognition environmental object interaction instrument person environment enrichment online database image necessary pre train model training recording phase adjust new come datum task learn approach amongst different problem solve approach introduction new category model refine exist category sub category extend train classifier focus problem adjust pre trained model new additional training datum exist category propose fast continual learning layer end neuronal network obtain result illustrate opensource cifar benchmark dataset propose scheme yield similar performance retrain drastically low computational cost
Interlock-Free Multi-Aspect Rationalization for Text Classification,"  Explanation is important for text classification tasks. One prevalent type of
explanation is rationales, which are text snippets of input text that suffice
to yield the prediction and are meaningful to humans. A lot of research on
rationalization has been based on the selective rationalization framework,
which has recently been shown to be problematic due to the interlocking
dynamics. In this paper, we show that we address the interlocking problem in
the multi-aspect setting, where we aim to generate multiple rationales for
multiple outputs. More specifically, we propose a multi-stage training method
incorporating an additional self-supervised contrastive loss that helps to
generate more semantically diverse rationales. Empirical results on the beer
review dataset show that our method improves significantly the rationalization
performance.
",explanation important text classification task one prevalent type explanation rationale text snippet input text suffice yield prediction meaningful human lot research rationalization base selective rationalization framework recently show problematic due interlock dynamic paper show address interlock problem multi aspect set aim generate multiple rationale multiple output specifically propose multi stage training method incorporate additional self supervise contrastive loss help generate semantically diverse rationale empirical result beer review dataset show method improve significantly rationalization performance
ExSinGAN: Learning an Explainable Generative Model from a Single Image,"  Generating images from a single sample, as a newly developing branch of image
synthesis, has attracted extensive attention. In this paper, we formulate this
problem as sampling from the conditional distribution of a single image, and
propose a hierarchical framework that simplifies the learning of the intricate
conditional distributions through the successive learning of the distributions
about structure, semantics and texture, making the process of learning and
generation comprehensible. On this basis, we design ExSinGAN composed of three
cascaded GANs for learning an explainable generative model from a given image,
where the cascaded GANs model the distributions about structure, semantics and
texture successively. ExSinGAN is learned not only from the internal patches of
the given image as the previous works did, but also from the external prior
obtained by the GAN inversion technique. Benefiting from the appropriate
combination of internal and external information, ExSinGAN has a more powerful
capability of generation and competitive generalization ability for the image
manipulation tasks compared with prior works.
",generating image single sample newly develop branch image synthesis attract extensive attention paper formulate problem sample conditional distribution single image propose hierarchical framework simplifie learn intricate conditional distribution successive learning distribution structure semantic texture make process learn generation comprehensible basis design exsingan compose three cascade gan learn explainable generative model give image cascade gans model distribution structure semantic texture successively exsingan learn internal patch give image previous work also external prior obtain gan inversion technique benefit appropriate combination internal external information exsingan powerful capability generation competitive generalization ability image manipulation task compare prior work
An Active Approach for Model Interpretation,"  Model interpretation, or explanation of a machine learning classifier, aims
to extract generalizable knowledge from a trained classifier into a
human-understandable format, for various purposes such as model assessment,
debugging and trust. From a computaional viewpoint, it is formulated as
approximating the target classifier using a simpler interpretable model, such
as rule models like a decision set/list/tree. Often, this approximation is
handled as standard supervised learning and the only difference is that the
labels are provided by the target classifier instead of ground truth. This
paradigm is particularly popular because there exists a variety of well-studied
supervised algorithms for learning an interpretable classifier. However, we
argue that this paradigm is suboptimal for it does not utilize the unique
property of the model interpretation problem, that is, the ability to generate
synthetic instances and query the target classifier for their labels. We call
this the active-query property, suggesting that we should consider model
interpretation from an active learning perspective. Following this insight, we
argue that the active-query property should be employed when designing a model
interpretation algorithm, and that the generation of synthetic instances should
be integrated seamlessly with the algorithm that learns the model
interpretation. In this paper, we demonstrate that by doing so, it is possible
to achieve more faithful interpretation with simpler model complexity. As a
technical contribution, we present an active algorithm Active Decision Set
Induction (ADS) to learn a decision set, a set of if-else rules, for model
interpretation. ADS performs a local search over the space of all decision
sets. In every iteration, ADS computes confidence intervals for the value of
the objective function of all local actions and utilizes active-query to
determine the best one.
",model interpretation explanation machine learn classifier aim extract generalizable knowledge train classifier human understandable format various purpose model assessment debug trust computaional viewpoint formulate approximating target classifier use simple interpretable model rule model like decision often approximation handle standard supervised learning difference label provide target classifier instead grind truth paradigm particularly popular exist variety well study supervised algorithm learn interpretable classifier however argue paradigm suboptimal utilize unique property model interpretation problem ability generate synthetic instance query target classifier label call active query property suggesting consider model interpretation active learning perspective follow insight argue active query property employ designing model interpretation algorithm generation synthetic instance integrate seamlessly algorithm learn model interpretation paper demonstrate possible achieve faithful interpretation simple model complexity technical contribution present active algorithm active decision set induction ad learn decision set set if else rule model interpretation ad perform local search space decision set every iteration ad compute confidence interval value objective function local action utilize active query determine good one
"Meta-Regularization: An Approach to Adaptive Choice of the Learning Rate
  in Gradient Descent","  We propose \textit{Meta-Regularization}, a novel approach for the adaptive
choice of the learning rate in first-order gradient descent methods. Our
approach modifies the objective function by adding a regularization term on the
learning rate, and casts the joint updating process of parameters and learning
rates into a maxmin problem. Given any regularization term, our approach
facilitates the generation of practical algorithms. When
\textit{Meta-Regularization} takes the $\varphi$-divergence as a regularizer,
the resulting algorithms exhibit comparable theoretical convergence performance
with other first-order gradient-based algorithms. Furthermore, we theoretically
prove that some well-designed regularizers can improve the convergence
performance under the strong-convexity condition of the objective function.
Numerical experiments on benchmark problems demonstrate the effectiveness of
algorithms derived from some common $\varphi$-divergence in full batch as well
as online learning settings.
",propose meta regularization novel approach adaptive choice learn rate first order gradient descent method approach modify objective function add regularization term learning rate cast joint updating process parameter learn rate maxmin problem give regularization term approach facilitate generation practical algorithm meta regularization take -divergence regularizer result algorithm exhibit comparable theoretical convergence performance first order gradient base algorithm furthermore theoretically prove well design regularizer improve convergence performance strong convexity condition objective function numerical experiment benchmark problem demonstrate effectiveness algorithm derive common -divergence full batch well online learn setting
High-Performance FPGA-based Accelerator for Bayesian Neural Networks,"  Neural networks (NNs) have demonstrated their potential in a wide range of
applications such as image recognition, decision making or recommendation
systems. However, standard NNs are unable to capture their model uncertainty
which is crucial for many safety-critical applications including healthcare and
autonomous vehicles. In comparison, Bayesian neural networks (BNNs) are able to
express uncertainty in their prediction via a mathematical grounding.
Nevertheless, BNNs have not been as widely used in industrial practice, mainly
because of their expensive computational cost and limited hardware performance.
This work proposes a novel FPGA-based hardware architecture to accelerate BNNs
inferred through Monte Carlo Dropout. Compared with other state-of-the-art BNN
accelerators, the proposed accelerator can achieve up to 4 times higher energy
efficiency and 9 times better compute efficiency. Considering partial Bayesian
inference, an automatic framework is proposed, which explores the trade-off
between hardware and algorithmic performance. Extensive experiments are
conducted to demonstrate that our proposed framework can effectively find the
optimal points in the design space.
",neural network nns demonstrate potential wide range application image recognition decision make recommendation system however standard nn unable capture model uncertainty crucial many safety critical application include healthcare autonomous vehicle comparison bayesian neural network bnn able express uncertainty prediction via mathematical grounding nevertheless bnn widely use industrial practice mainly expensive computational cost limited hardware performance work propose novel fpga base hardware architecture accelerate bnns infer monte carlo dropout compare state of the art bnn accelerator propose accelerator achieve 4 time high energy efficiency 9 time well compute efficiency consider partial bayesian inference automatic framework propose explore trade off hardware algorithmic performance extensive experiment conduct demonstrate propose framework effectively find optimal point design space
Unsupervised Semantic Segmentation by Contrasting Object Mask Proposals,"  Being able to learn dense semantic representations of images without
supervision is an important problem in computer vision. However, despite its
significance, this problem remains rather unexplored, with a few exceptions
that considered unsupervised semantic segmentation on small-scale datasets with
a narrow visual domain. In this paper, we make a first attempt to tackle the
problem on datasets that have been traditionally utilized for the supervised
case. To achieve this, we introduce a two-step framework that adopts a
predetermined mid-level prior in a contrastive optimization objective to learn
pixel embeddings. This marks a large deviation from existing works that relied
on proxy tasks or end-to-end clustering. Additionally, we argue about the
importance of having a prior that contains information about objects, or their
parts, and discuss several possibilities to obtain such a prior in an
unsupervised manner.
  Experimental evaluation shows that our method comes with key advantages over
existing works. First, the learned pixel embeddings can be directly clustered
in semantic groups using K-Means on PASCAL. Under the fully unsupervised
setting, there is no precedent in solving the semantic segmentation task on
such a challenging benchmark. Second, our representations can improve over
strong baselines when transferred to new datasets, e.g. COCO and DAVIS. The
code is available.
",able learn dense semantic representation image without supervision important problem computer vision however despite significance problem remain rather unexplored exception consider unsupervised semantic segmentation small scale dataset narrow visual domain paper make first attempt tackle problem dataset traditionally utilize supervised case achieve introduce two step framework adopt predetermine mid level prior contrastive optimization objective learn pixel embedding mark large deviation exist work rely proxy task end to end clustering additionally argue importance prior contain information object part discuss several possibility obtain prior unsupervised manner experimental evaluation show method come key advantage exist work first learn pixel embedding directly cluster semantic group use k means pascal fully unsupervised set precedent solve semantic segmentation task challenge benchmark second representation improve strong baseline transfer new dataset coco davis code available
Batch Active Learning Using Determinantal Point Processes,"  Data collection and labeling is one of the main challenges in employing
machine learning algorithms in a variety of real-world applications with
limited data. While active learning methods attempt to tackle this issue by
labeling only the data samples that give high information, they generally
suffer from large computational costs and are impractical in settings where
data can be collected in parallel. Batch active learning methods attempt to
overcome this computational burden by querying batches of samples at a time. To
avoid redundancy between samples, previous works rely on some ad hoc
combination of sample quality and diversity. In this paper, we present a new
principled batch active learning method using Determinantal Point Processes, a
repulsive point process that enables generating diverse batches of samples. We
develop tractable algorithms to approximate the mode of a DPP distribution, and
provide theoretical guarantees on the degree of approximation. We further
demonstrate that an iterative greedy method for DPP maximization, which has
lower computational costs but worse theoretical guarantees, still gives
competitive results for batch active learning. Our experiments show the value
of our methods on several datasets against state-of-the-art baselines.
",data collection label one main challenge employ machine learning algorithm variety real world application limit datum active learning method attempt tackle issue label data sample give high information generally suffer large computational cost impractical setting datum collect parallel batch active learning method attempt overcome computational burden query batch sample time avoid redundancy sample previous work rely ad hoc combination sample quality diversity paper present new principle batch active learning method use determinantal point process repulsive point process enable generate diverse batch sample develop tractable algorithm approximate mode dpp distribution provide theoretical guarantee degree approximation demonstrate iterative greedy method dpp maximization low computational cost bad theoretical guarantee still give competitive result batch active learning experiment show value method several dataset state of the art baseline
Few-Shot Learning via Embedding Adaptation with Set-to-Set Functions,"  Learning with limited data is a key challenge for visual recognition. Many
few-shot learning methods address this challenge by learning an instance
embedding function from seen classes and apply the function to instances from
unseen classes with limited labels. This style of transfer learning is
task-agnostic: the embedding function is not learned optimally discriminative
with respect to the unseen classes, where discerning among them leads to the
target task. In this paper, we propose a novel approach to adapt the instance
embeddings to the target classification task with a set-to-set function,
yielding embeddings that are task-specific and are discriminative. We
empirically investigated various instantiations of such set-to-set functions
and observed the Transformer is most effective -- as it naturally satisfies key
properties of our desired model. We denote this model as FEAT (few-shot
embedding adaptation w/ Transformer) and validate it on both the standard
few-shot classification benchmark and four extended few-shot learning settings
with essential use cases, i.e., cross-domain, transductive, generalized
few-shot learning, and low-shot learning. It archived consistent improvements
over baseline models as well as previous methods and established the new
state-of-the-art results on two benchmarks.
",learn limited datum key challenge visual recognition many few shot learning method address challenge learn instance embed function see class apply function instance unseen class limit label style transfer learn task agnostic embed function learn optimally discriminative respect unseen class discern among lead target task paper propose novel approach adapt instance embedding target classification task set to set function yield embedding task specific discriminative empirically investigate various instantiation set to set function observe transformer effective naturally satisfy key property desire model denote model feat few shot embed adaptation transformer validate standard few shot classification benchmark four extended few shot learning setting essential use case cross domain transductive generalize few shot learn low shot learning archive consistent improvement baseline model well previous method establish new state of the art result two benchmark
"A Hybrid Objective Function for Robustness of Artificial Neural Networks
  -- Estimation of Parameters in a Mechanical System","  In several studies, hybrid neural networks have proven to be more robust
against noisy input data compared to plain data driven neural networks. We
consider the task of estimating parameters of a mechanical vehicle model based
on acceleration profiles. We introduce a convolutional neural network
architecture that is capable to predict the parameters for a family of vehicle
models that differ in the unknown parameters. We introduce a convolutional
neural network architecture that given sequential data predicts the parameters
of the underlying data's dynamics. This network is trained with two objective
functions. The first one constitutes a more naive approach that assumes that
the true parameters are known. The second objective incorporates the knowledge
of the underlying dynamics and is therefore considered as hybrid approach. We
show that in terms of robustness, the latter outperforms the first objective on
noisy input data.
",several study hybrid neural network prove robust noisy input datum compare plain datum drive neural network consider task estimate parameter mechanical vehicle model base acceleration profile introduce convolutional neural network architecture capable predict parameter family vehicle model differ unknown parameter introduce convolutional neural network architecture give sequential datum predict parameter underlie data dynamic network train two objective function first one constitute naive approach assume true parameter know second objective incorporate knowledge underlie dynamic therefore consider hybrid approach show term robustness latter outperform first objective noisy input datum
Ranking and synchronization from pairwise measurements via SVD,"  Given a measurement graph $G= (V,E)$ and an unknown signal $r \in
\mathbb{R}^n$, we investigate algorithms for recovering $r$ from pairwise
measurements of the form $r_i - r_j$; $\{i,j\} \in E$. This problem arises in a
variety of applications, such as ranking teams in sports data and time
synchronization of distributed networks. Framed in the context of ranking, the
task is to recover the ranking of $n$ teams (induced by $r$) given a small
subset of noisy pairwise rank offsets. We propose a simple SVD-based
algorithmic pipeline for both the problem of time synchronization and ranking.
We provide a detailed theoretical analysis in terms of robustness against both
sampling sparsity and noise perturbations with outliers, using results from
matrix perturbation and random matrix theory. Our theoretical findings are
complemented by a detailed set of numerical experiments on both synthetic and
real data, showcasing the competitiveness of our proposed algorithms with other
state-of-the-art methods.
",give measurement graph v e unknown signal r r investigate algorithm recover r pairwise measurement form r_i r_j e problem arise variety application rank team sport datum time synchronization distribute network frame context rank task recover rank n team induce r give small subset noisy pairwise rank offset propose simple svd base algorithmic pipeline problem time synchronization rank provide detailed theoretical analysis term robustness sampling sparsity noise perturbation outlier use result matrix perturbation random matrix theory theoretical finding complement detailed set numerical experiment synthetic real datum showcase competitiveness propose algorithm state of the art method
"FedBABU: Towards Enhanced Representation for Federated Image
  Classification","  Federated learning has evolved to improve a single global model under data
heterogeneity (as a curse) or to develop multiple personalized models using
data heterogeneity (as a blessing). However, little research has considered
both directions simultaneously. In this paper, we first investigate the
relationship between them by analyzing Federated Averaging at the client level
and determine that a better federated global model performance does not
constantly improve personalization. To elucidate the cause of this
personalization performance degradation problem, we decompose the entire
network into the body (extractor), which is related to universality, and the
head (classifier), which is related to personalization. We then point out that
this problem stems from training the head. Based on this observation, we
propose a novel federated learning algorithm, coined FedBABU, which only
updates the body of the model during federated training (i.e., the head is
randomly initialized and never updated), and the head is fine-tuned for
personalization during the evaluation process. Extensive experiments show
consistent performance improvements and an efficient personalization of
FedBABU. The code is available at https://github.com/jhoon-oh/FedBABU.
",federate learning evolve improve single global model datum heterogeneity curse develop multiple personalize model use datum heterogeneity blessing however little research consider direction simultaneously paper first investigate relationship analyze federate average client level determine well federate global model performance constantly improve personalization elucidate cause personalization performance degradation problem decompose entire network body extractor relate universality head classifier relate personalization point problem stem train head base observation propose novel federate learning algorithm coin fedbabu update body model federate training head randomly initialize never update head fine tune personalization evaluation process extensive experiment show consistent performance improvement efficient personalization fedbabu code available https
"Monocular 3D Pose Recovery via Nonconvex Sparsity with Theoretical
  Analysis","  For recovering 3D object poses from 2D images, a prevalent method is to
pre-train an over-complete dictionary $\mathcal D=\{B_i\}_i^D$ of 3D basis
poses. During testing, the detected 2D pose $Y$ is matched to dictionary by $Y
\approx \sum_i M_i B_i$ where $\{M_i\}_i^D=\{c_i \Pi R_i\}$, by estimating the
rotation $R_i$, projection $\Pi$ and sparse combination coefficients $c \in
\mathbb R_{+}^D$. In this paper, we propose non-convex regularization $H(c)$ to
learn coefficients $c$, including novel leaky capped $\ell_1$-norm
regularization (LCNR), \begin{align*} H(c)=\alpha \sum_{i } \min(|c_i|,\tau)+
\beta \sum_{i } \max(| c_i|,\tau), \end{align*} where $0\leq \beta \leq \alpha$
and $0<\tau$ is a certain threshold, so the invalid components smaller than
$\tau$ are composed with larger regularization and other valid components with
smaller regularization. We propose a multi-stage optimizer with convex
relaxation and ADMM. We prove that the estimation error $\mathcal L(l)$ decays
w.r.t. the stages $l$, \begin{align*} Pr\left(\mathcal L(l) < \rho^{l-1}
\mathcal L(0) + \delta \right) \geq 1- \epsilon, \end{align*} where $0< \rho
<1, 0<\delta, 0<\epsilon \ll 1$. Experiments on large 3D human datasets like
H36M are conducted to support our improvement upon previous approaches. To the
best of our knowledge, this is the first theoretical analysis in this line of
research, to understand how the recovery error is affected by fundamental
factors, e.g. dictionary size, observation noises, optimization times. We
characterize the trade-off between speed and accuracy towards real-time
inference in applications.
",recover 3d object pose 2d image prevalent method pre train over complete dictionary 3d basis pose testing detect 2d pose match dictionary m_i b_i c_i estimate rotation r_i projection sparse combination coefficient c r paper propose non convex regularization h c learn coefficient c include novel leaky cap -norm regularization lcnr align h c align 0 certain threshold invalid component smaller compose large regularization valid component small regularization propose multi stage optimizer convex relaxation admm prove estimation error l l decay stage l align l l l-1 l 0 1- align 0 1 0 0 1 experiment large 3d human dataset like h36 m conduct support improvement upon previous approach good knowledge first theoretical analysis line research understand recovery error affect fundamental factor dictionary size observation noise optimization time characterize trade off speed accuracy towards real time inference application
From Tensor Network Quantum States to Tensorial Recurrent Neural Networks,"We show that any matrix product state (MPS) can be exactly represented by a
recurrent neural network (RNN) with a linear memory update. We generalize this
RNN architecture to 2D lattices using a multilinear memory update. It supports
perfect sampling and wave function evaluation in polynomial time, and can
represent an area law of entanglement entropy. Numerical evidence shows that it
can encode the wave function using a bond dimension lower by orders of
magnitude when compared to MPS, with an accuracy that can be systematically
improved by increasing the bond dimension.",show matrix product state mp exactly represent recurrent neural network rnn linear memory update generalize rnn architecture 2d lattice use multilinear memory update support perfect sampling wave function evaluation polynomial time represent area law entanglement entropy numerical evidence show encode wave function use bond dimension low order magnitude compare mps accuracy systematically improve increase bond dimension
"An ADMM algorithm for solving a proximal bound-constrained quadratic
  program","  We consider a proximal operator given by a quadratic function subject to
bound constraints and give an optimization algorithm using the alternating
direction method of multipliers (ADMM). The algorithm is particularly efficient
to solve a collection of proximal operators that share the same quadratic form,
or if the quadratic program is the relaxation of a binary quadratic problem.
",consider proximal operator give quadratic function subject bind constraint give optimization algorithm use alternate direction method multiplier admm algorithm particularly efficient solve collection proximal operator share quadratic form quadratic program relaxation binary quadratic problem
"User-Guided Clustering in Heterogeneous Information Networks via
  Motif-Based Comprehensive Transcription","  Heterogeneous information networks (HINs) with rich semantics are ubiquitous
in real-world applications. For a given HIN, many reasonable clustering results
with distinct semantic meaning can simultaneously exist. User-guided clustering
is hence of great practical value for HINs where users provide labels to a
small portion of nodes. To cater to a broad spectrum of user guidance evidenced
by different expected clustering results, carefully exploiting the signals
residing in the data is potentially useful. Meanwhile, as one type of complex
networks, HINs often encapsulate higher-order interactions that reflect the
interlocked nature among nodes and edges. Network motifs, sometimes referred to
as meta-graphs, have been used as tools to capture such higher-order
interactions and reveal the many different semantics. We therefore approach the
problem of user-guided clustering in HINs with network motifs. In this process,
we identify the utility and importance of directly modeling higher-order
interactions without collapsing them to pairwise interactions. To achieve this,
we comprehensively transcribe the higher-order interaction signals to a series
of tensors via motifs and propose the MoCHIN model based on joint non-negative
tensor factorization. This approach applies to arbitrarily many, arbitrary
forms of HIN motifs. An inference algorithm with speed-up methods is also
proposed to tackle the challenge that tensor size grows exponentially as the
number of nodes in a motif increases. We validate the effectiveness of the
proposed method on two real-world datasets and three tasks, and MoCHIN
outperforms all baselines in three evaluation tasks under three different
metrics. Additional experiments demonstrated the utility of motifs and the
benefit of directly modeling higher-order information especially when user
guidance is limited.
",heterogeneous information network hin rich semantic ubiquitous real world application give hin many reasonable cluster result distinct semantic meaning simultaneously exist user guide clustering hence great practical value hin user provide label small portion node cater broad spectrum user guidance evidence different expect cluster result carefully exploit signal reside datum potentially useful meanwhile one type complex network hin often encapsulate high order interaction reflect interlock nature among node edge network motif sometimes refer meta graph use tool capture high order interaction reveal many different semantic therefore approach problem user guide cluster hin network motif process identify utility importance directly model high order interaction without collapse pairwise interaction achieve comprehensively transcribe high order interaction signal series tensor via motif propose mochin model base joint non negative tensor factorization approach apply arbitrarily many arbitrary form hin motif inference algorithm speed up method also propose tackle challenge tensor size grow exponentially number node motif increase validate effectiveness propose method two real world dataset three task mochin outperform baseline three evaluation task three different metric additional experiment demonstrate utility motif benefit directly model high order information especially user guidance limited
"Behavior-Guided Actor-Critic: Improving Exploration via Learning Policy
  Behavior Representation for Deep Reinforcement Learning","  In this work, we propose Behavior-Guided Actor-Critic (BAC), an off-policy
actor-critic deep RL algorithm. BAC mathematically formulates the behavior of
the policy through autoencoders by providing an accurate estimation of how
frequently each state-action pair was visited while taking into consideration
state dynamics that play a crucial role in determining the trajectories
produced by the policy. The agent is encouraged to change its behavior
consistently towards less-visited state-action pairs while attaining good
performance by maximizing the expected discounted sum of rewards, resulting in
an efficient exploration of the environment and good exploitation of all high
reward regions. One prominent aspect of our approach is that it is applicable
to both stochastic and deterministic actors in contrast to maximum entropy deep
reinforcement learning algorithms. Results show considerably better
performances of BAC when compared to several cutting-edge learning algorithms.
",work propose behavior guide actor critic bac off policy actor critic deep rl algorithm bac mathematically formulate behavior policy autoencoder provide accurate estimation frequently state action pair visit take consideration state dynamic play crucial role determine trajectory produce policy agent encourage change behavior consistently towards less visit state action pair attain good performance maximizing expect discount sum reward result efficient exploration environment good exploitation high reward region one prominent aspect approach applicable stochastic deterministic actor contrast maximum entropy deep reinforcement learning algorithm result show considerably well performance bac compare several cutting edge learning algorithm
"Federated Learning using Smart Contracts on Blockchains, based on Reward
  Driven Approach","  Over the recent years, Federated machine learning continues to gain interest
and momentum where there is a need to draw insights from data while preserving
the data provider's privacy. However, one among other existing challenges in
the adoption of federated learning has been the lack of fair, transparent and
universally agreed incentivization schemes for rewarding the federated learning
contributors. Smart contracts on a blockchain network provide transparent,
immutable and independently verifiable proofs by all participants of the
network. We leverage this open and transparent nature of smart contracts on a
blockchain to define incentivization rules for the contributors, which is based
on a novel scalar quantity - federated contribution. Such a smart contract
based reward-driven model has the potential to revolutionize the federated
learning adoption in enterprises. Our contribution is two-fold: first is to
show how smart contract based blockchain can be a very natural communication
channel for federated learning. Second, leveraging this infrastructure, we can
show how an intuitive measure of each agents' contribution can be built and
integrated with the life cycle of the training and reward process.
",recent year federate machine learning continue gain interest momentum need draw insight datum preserve datum provider privacy however one among exist challenge adoption federate learn lack fair transparent universally agree incentivization scheme reward federate learning contributor smart contract blockchain network provide transparent immutable independently verifiable proof participant network leverage open transparent nature smart contract blockchain define incentivization rule contributor base novel scalar quantity federate contribution smart contract base reward drive model potential revolutionize federated learn adoption enterprise contribution two fold first show smart contract base blockchain natural communication channel federate learn second leverage infrastructure show intuitive measure agent contribution build integrate life cycle training reward process
"Data Structures & Algorithms for Exact Inference in Hierarchical
  Clustering","  Hierarchical clustering is a fundamental task often used to discover
meaningful structures in data, such as phylogenetic trees, taxonomies of
concepts, subtypes of cancer, and cascades of particle decays in particle
physics. Typically approximate algorithms are used for inference due to the
combinatorial number of possible hierarchical clusterings. In contrast to
existing methods, we present novel dynamic-programming algorithms for
\emph{exact} inference in hierarchical clustering based on a novel trellis data
structure, and we prove that we can exactly compute the partition function,
maximum likelihood hierarchy, and marginal probabilities of sub-hierarchies and
clusters. Our algorithms scale in time and space proportional to the powerset
of $N$ elements which is super-exponentially more efficient than explicitly
considering each of the (2N-3)!! possible hierarchies. Also, for larger
datasets where our exact algorithms become infeasible, we introduce an
approximate algorithm based on a sparse trellis that compares well to other
benchmarks. Exact methods are relevant to data analyses in particle physics and
for finding correlations among gene expression in cancer genomics, and we give
examples in both areas, where our algorithms outperform greedy and beam search
baselines. In addition, we consider Dasgupta's cost with synthetic data.
",hierarchical clustering fundamental task often use discover meaningful structure datum phylogenetic tree taxonomy concept subtype cancer cascade particle decay particle physics typically approximate algorithm use inference due combinatorial number possible hierarchical clustering contrast exist method present novel dynamic program algorithm exact inference hierarchical clustering base novel trellis datum structure prove exactly compute partition function maximum likelihood hierarchy marginal probability sub hierarchy cluster algorithm scale time space proportional powerset n element super exponentially efficient explicitly consider 2n-3 possible hierarchy also large dataset exact algorithm become infeasible introduce approximate algorithm base sparse trellis compare well benchmark exact method relevant datum analyse particle physics find correlation among gene expression cancer genomic give example area algorithm outperform greedy beam search baseline addition consider dasgupta cost synthetic datum
"A novel multiclassSVM based framework to classify lithology from well
  logs: a real-world application","  Support vector machines (SVMs) have been recognized as a potential tool for
supervised classification analyses in different domains of research. In
essence, SVM is a binary classifier. Therefore, in case of a multiclass
problem, the problem is divided into a series of binary problems which are
solved by binary classifiers, and finally the classification results are
combined following either the one-against-one or one-against-all strategies. In
this paper, an attempt has been made to classify lithology using a multiclass
SVM based framework using well logs as predictor variables. Here, the lithology
is classified into four classes such as sand, shaly sand, sandy shale and shale
based on the relative values of sand and shale fractions as suggested by an
expert geologist. The available dataset consisting well logs (gamma ray,
neutron porosity, density, and P-sonic) and class information from four closely
spaced wells from an onshore hydrocarbon field is divided into training and
testing sets. We have used one-against-all strategy to combine the results of
multiple binary classifiers. The reported results established the superiority
of multiclass SVM compared to other classifiers in terms of classification
accuracy. The selection of kernel function and associated parameters has also
been investigated here. It can be envisaged from the results achieved in this
study that the proposed framework based on multiclass SVM can further be used
to solve classification problems. In future research endeavor, seismic
attributes can be introduced in the framework to classify the lithology
throughout a study area from seismic inputs.
",support vector machine svms recognize potential tool supervise classification analyse different domain research essence svm binary classifier therefore case multiclass problem problem divide series binary problem solve binary classifier finally classification result combine follow either one against one one against all strategy paper attempt make classify lithology use multiclass svm base framework use well log predictor variable lithology classify four class sand shaly sand sandy shale shale base relative value sand shale fraction suggest expert geologist available dataset consist well log gamma ray neutron porosity density p sonic class information four closely space well onshore hydrocarbon field divide training testing set use one against all strategy combine result multiple binary classifier report result establish superiority multiclass svm compare classifier term classification accuracy selection kernel function associate parameter also investigate envisage result achieve study propose framework base multiclass svm use solve classification problem future research endeavor seismic attribute introduce framework classify lithology throughout study area seismic input
On the Power of Differentiable Learning versus PAC and SQ Learning,"  We study the power of learning via mini-batch stochastic gradient descent
(SGD) on the population loss, and batch Gradient Descent (GD) on the empirical
loss, of a differentiable model or neural network, and ask what learning
problems can be learnt using these paradigms. We show that SGD and GD can
always simulate learning with statistical queries (SQ), but their ability to go
beyond that depends on the precision $\rho$ of the gradient calculations
relative to the minibatch size $b$ (for SGD) and sample size $m$ (for GD). With
fine enough precision relative to minibatch size, namely when $b \rho$ is small
enough, SGD can go beyond SQ learning and simulate any sample-based learning
algorithm and thus its learning power is equivalent to that of PAC learning;
this extends prior work that achieved this result for $b=1$. Similarly, with
fine enough precision relative to the sample size $m$, GD can also simulate any
sample-based learning algorithm based on $m$ samples. In particular, with
polynomially many bits of precision (i.e. when $\rho$ is exponentially small),
SGD and GD can both simulate PAC learning regardless of the mini-batch size. On
the other hand, when $b \rho^2$ is large enough, the power of SGD is equivalent
to that of SQ learning.
",study power learning via mini batch stochastic gradient descent sgd population loss batch gradient descent gd empirical loss differentiable model neural network ask learn problem learn use paradigms show sgd gd always simulate learn statistical query sq ability go beyond depend precision gradient calculation relative minibatch size b sgd sample size gd fine enough precision relative minibatch size namely b small enough sgd go beyond sq learn simulate sample base learning algorithm thus learn power equivalent pac learning extend prior work achieve result similarly fine enough precision relative sample size gd also simulate sample base learn algorithm base sample particular polynomially many bit precision exponentially small sgd gd simulate pac learning regardless mini batch size hand b large enough power sgd equivalent sq learning
Wasserstein Collaborative Filtering for Item Cold-start Recommendation,"  The item cold-start problem seriously limits the recommendation performance
of Collaborative Filtering (CF) methods when new items have either none or very
little interactions. To solve this issue, many modern Internet applications
propose to predict a new item's interaction from the possessing contents.
However, it is difficult to design and learn a map between the item's
interaction history and the corresponding contents. In this paper, we apply the
Wasserstein distance to address the item cold-start problem. Given item content
information, we can calculate the similarity between the interacted items and
cold-start ones, so that a user's preference on cold-start items can be
inferred by minimizing the Wasserstein distance between the distributions over
these two types of items. We further adopt the idea of CF and propose
Wasserstein CF (WCF) to improve the recommendation performance on cold-start
items. Experimental results demonstrate the superiority of WCF over
state-of-the-art approaches.
",item cold start problem seriously limit recommendation performance collaborative filtering cf method new item either none little interaction solve issue many modern internet application propose predict new item interaction possess content however difficult design learn map item interaction history correspond content paper apply wasserstein distance address item cold start problem give item content information calculate similarity interact item cold start one user preference cold start item infer minimize wasserstein distance distribution two type item adopt idea cf propose wasserstein cf wcf improve recommendation performance cold start item experimental result demonstrate superiority wcf state of the art approach
Prediction of Hereditary Cancers Using Neural Networks,"  Family history is a major risk factor for many types of cancer. Mendelian
risk prediction models translate family histories into cancer risk predictions
based on knowledge of cancer susceptibility genes. These models are widely used
in clinical practice to help identify high-risk individuals. Mendelian models
leverage the entire family history, but they rely on many assumptions about
cancer susceptibility genes that are either unrealistic or challenging to
validate due to low mutation prevalence. Training more flexible models, such as
neural networks, on large databases of pedigrees can potentially lead to
accuracy gains. In this paper, we develop a framework to apply neural networks
to family history data and investigate their ability to learn inherited
susceptibility to cancer. While there is an extensive literature on neural
networks and their state-of-the-art performance in many tasks, there is little
work applying them to family history data. We propose adaptations of
fully-connected neural networks and convolutional neural networks to pedigrees.
In data simulated under Mendelian inheritance, we demonstrate that our proposed
neural network models are able to achieve nearly optimal prediction
performance. Moreover, when the observed family history includes misreported
cancer diagnoses, neural networks are able to outperform the Mendelian BRCAPRO
model embedding the correct inheritance laws. Using a large dataset of over
200,000 family histories, the Risk Service cohort, we train prediction models
for future risk of breast cancer. We validate the models using data from the
Cancer Genetics Network.
",family history major risk factor many type cancer mendelian risk prediction model translate family history cancer risk prediction base knowledge cancer susceptibility gene model widely use clinical practice help identify high risk individual mendelian model leverage entire family history rely many assumption cancer susceptibility gene either unrealistic challenging validate due low mutation prevalence training flexible model neural network large database pedigree potentially lead accuracy gain paper develop framework apply neural network family history datum investigate ability learn inherit susceptibility cancer extensive literature neural network state of the art performance many task little work apply family history datum propose adaptation fully connect neural network convolutional neural network pedigree data simulated mendelian inheritance demonstrate propose neural network model able achieve nearly optimal prediction performance moreover observed family history include misreporte cancer diagnose neural network able outperform mendelian brcapro model embed correct inheritance law use large dataset family history risk service cohort train prediction model future risk breast cancer validate model use data cancer genetic network
"Is attention to bounding boxes all you need for pedestrian action
  prediction?","  The human driver is no longer the only one concerned with the complexity of
the driving scenarios. Autonomous vehicles (AV) are similarly becoming involved
in the process. Nowadays, the development of AVs in urban places raises
essential safety concerns for vulnerable road users (VRUs) such as pedestrians.
Therefore, to make the roads safer, it is critical to classify and predict the
pedestrians' future behavior. In this paper, we present a framework based on
multiple variations of the Transformer models able to infer predict the
pedestrian street-crossing decision-making based on the dynamics of its
initiated trajectory. We showed that using solely bounding boxes as input
features can outperform the previous state-of-the-art results by reaching a
prediction accuracy of 91\% and an F1-score of 0.83 on the PIE dataset. In
addition, we introduced a large-size simulated dataset (CP2A) using CARLA for
action prediction. Our model has similarly reached high accuracy (91\%) and
F1-score (0.91) on this dataset. Interestingly, we showed that pre-training our
Transformer model on the CP2A dataset and then fine-tuning it on the PIE
dataset is beneficial for the action prediction task. Finally, our model's
results are successfully supported by the ""human attention to bounding boxes""
experiment which we created to test humans ability for pedestrian action
prediction without the need for environmental context. The code for the dataset
and the models is available at:
https://github.com/linaashaji/Action_Anticipation
",human driver long one concerned complexity drive scenario autonomous vehicle av similarly become involved process nowadays development avs urban place raise essential safety concern vulnerable road user vrus pedestrian therefore make road safe critical classify predict pedestrian future behavior paper present framework base multiple variation transformer model able infer predict pedestrian street cross decision make base dynamic initiate trajectory show use solely bound box input feature outperform previous state of the art result reach prediction accuracy f1 score pie dataset addition introduce large size simulate dataset cp2a use carla action prediction model similarly reach high accuracy f1 score dataset interestingly show pre training transformer model cp2a dataset fine tune pie dataset beneficial action prediction task finally model result successfully support human attention bounding box experiment create test human ability pedestrian action prediction without need environmental context code dataset model available https
"Investigation on Data Adaptation Techniques for Neural Named Entity
  Recognition","  Data processing is an important step in various natural language processing
tasks. As the commonly used datasets in named entity recognition contain only a
limited number of samples, it is important to obtain additional labeled data in
an efficient and reliable manner. A common practice is to utilize large
monolingual unlabeled corpora. Another popular technique is to create synthetic
data from the original labeled data (data augmentation). In this work, we
investigate the impact of these two methods on the performance of three
different named entity recognition tasks.
",datum process important step various natural language processing task commonly use dataset name entity recognition contain limited number sample important obtain additional label datum efficient reliable manner common practice utilize large monolingual unlabeled corpora another popular technique create synthetic datum original label datum datum augmentation work investigate impact two method performance three different name entity recognition task
"Linear Regression with Distributed Learning: A Generalization Error
  Perspective","  Distributed learning provides an attractive framework for scaling the
learning task by sharing the computational load over multiple nodes in a
network. Here, we investigate the performance of distributed learning for
large-scale linear regression where the model parameters, i.e., the unknowns,
are distributed over the network. We adopt a statistical learning approach. In
contrast to works that focus on the performance on the training data, we focus
on the generalization error, i.e., the performance on unseen data. We provide
high-probability bounds on the generalization error for both isotropic and
correlated Gaussian data as well as sub-gaussian data. These results reveal the
dependence of the generalization performance on the partitioning of the model
over the network. In particular, our results show that the generalization error
of the distributed solution can be substantially higher than that of the
centralized solution even when the error on the training data is at the same
level for both the centralized and distributed approaches. Our numerical
results illustrate the performance with both real-world image data as well as
synthetic data.
",distribute learning provide attractive framework scaling learn task share computational load multiple node network investigate performance distribute learn large scale linear regression model parameter unknown distribute network adopt statistical learning approach contrast work focus performance training datum focus generalization error performance unseen datum provide high probability bound generalization error isotropic correlate gaussian datum well sub gaussian data result reveal dependence generalization performance partition model network particular result show generalization error distribute solution substantially high centralized solution even error training datum level centralize distribute approach numerical result illustrate performance real world image datum well synthetic datum
"GossipGraD: Scalable Deep Learning using Gossip Communication based
  Asynchronous Gradient Descent","  In this paper, we present GossipGraD - a gossip communication protocol based
Stochastic Gradient Descent (SGD) algorithm for scaling Deep Learning (DL)
algorithms on large-scale systems. The salient features of GossipGraD are: 1)
reduction in overall communication complexity from {\Theta}(log(p)) for p
compute nodes in well-studied SGD to O(1), 2) model diffusion such that compute
nodes exchange their updates (gradients) indirectly after every log(p) steps,
3) rotation of communication partners for facilitating direct diffusion of
gradients, 4) asynchronous distributed shuffle of samples during the
feedforward phase in SGD to prevent over-fitting, 5) asynchronous communication
of gradients for further reducing the communication cost of SGD and GossipGraD.
We implement GossipGraD for GPU and CPU clusters and use NVIDIA GPUs (Pascal
P100) connected with InfiniBand, and Intel Knights Landing (KNL) connected with
Aries network. We evaluate GossipGraD using well-studied dataset ImageNet-1K
(~250GB), and widely studied neural network topologies such as GoogLeNet and
ResNet50 (current winner of ImageNet Large Scale Visualization Research
Challenge (ILSVRC)). Our performance evaluation using both KNL and Pascal GPUs
indicates that GossipGraD can achieve perfect efficiency for these datasets and
their associated neural network topologies. Specifically, for ResNet50,
GossipGraD is able to achieve ~100% compute efficiency using 128 NVIDIA Pascal
P100 GPUs - while matching the top-1 classification accuracy published in
literature.
",paper present gossipgrad gossip communication protocol base stochastic gradient descent sgd algorithm scale deep learning dl algorithm large scale system salient feature gossipgrad 1 reduction overall communication complexity log p p compute node well study sgd 1 2 model diffusion compute nodes exchange update gradient indirectly every log p step 3 rotation communication partner facilitate direct diffusion gradient 4 asynchronous distribute shuffle sample feedforward phase sgd prevent over fit 5 asynchronous communication gradient reduce communication cost sgd gossipgrad implement gossipgrad gpu cpu cluster use nvidia gpus pascal p100 connect infiniband intel knight landing knl connect arie network evaluate gossipgrad use well study dataset imagenet-1k widely study neural network topology googlenet resnet50 current winner imagenet large scale visualization research challenge ilsvrc performance evaluation use knl pascal gpus indicate gossipgrad achieve perfect efficiency dataset associate neural network topology specifically resnet50 gossipgrad able achieve compute efficiency use 128 nvidia pascal p100 gpus matching top-1 classification accuracy publish literature
Learning Calibratable Policies using Programmatic Style-Consistency,"  We study the problem of controllable generation of long-term sequential
behaviors, where the goal is to calibrate to multiple behavior styles
simultaneously. In contrast to the well-studied areas of controllable
generation of images, text, and speech, there are two questions that pose
significant challenges when generating long-term behaviors: how should we
specify the factors of variation to control, and how can we ensure that the
generated behavior faithfully demonstrates combinatorially many styles? We
leverage programmatic labeling functions to specify controllable styles, and
derive a formal notion of style-consistency as a learning objective, which can
then be solved using conventional policy learning approaches. We evaluate our
framework using demonstrations from professional basketball players and agents
in the MuJoCo physics environment, and show that existing approaches that do
not explicitly enforce style-consistency fail to generate diverse behaviors
whereas our learned policies can be calibrated for up to 1024 distinct style
combinations.
",study problem controllable generation long term sequential behavior goal calibrate multiple behavior style simultaneously contrast well study area controllable generation image text speech two question pose significant challenge generate long term behavior specify factor variation control ensure generate behavior faithfully demonstrate combinatorially many style leverage programmatic labeling function specify controllable style derive formal notion style consistency learning objective solve use conventional policy learning approach evaluate framework use demonstration professional basketball player agent mujoco physics environment show exist approach explicitly enforce style consistency fail generate diverse behavior whereas learn policy calibrate 1024 distinct style combination
Towards Testing of Deep Learning Systems with Training Set Reduction,"  Testing the implementation of deep learning systems and their training
routines is crucial to maintain a reliable code base. Modern software
development employs processes, such as Continuous Integration, in which changes
to the software are frequently integrated and tested. However, testing the
training routines requires running them and fully training a deep learning
model can be resource-intensive, when using the full data set. Using only a
subset of the training data can improve test run time, but can also reduce its
effectiveness. We evaluate different ways for training set reduction and their
ability to mimic the characteristics of model training with the original full
data set. Our results underline the usefulness of training set reduction,
especially in resource-constrained environments.
",testing implementation deep learning system training routine crucial maintain reliable code base modern software development employ process continuous integration change software frequently integrate test however test training routine require run fully train deep learning model resource intensive use full datum set use subset training datum improve test run time also reduce effectiveness evaluate different way training set reduction ability mimic characteristic model train original full datum set result underline usefulness training set reduction especially resource constrain environment
Online Markov Decision Processes with Aggregate Bandit Feedback,"  We study a novel variant of online finite-horizon Markov Decision Processes
with adversarially changing loss functions and initially unknown dynamics. In
each episode, the learner suffers the loss accumulated along the trajectory
realized by the policy chosen for the episode, and observes aggregate bandit
feedback: the trajectory is revealed along with the cumulative loss suffered,
rather than the individual losses encountered along the trajectory. Our main
result is a computationally efficient algorithm with $O(\sqrt{K})$ regret for
this setting, where $K$ is the number of episodes.
  We establish this result via an efficient reduction to a novel bandit
learning setting we call Distorted Linear Bandits (DLB), which is a variant of
bandit linear optimization where actions chosen by the learner are
adversarially distorted before they are committed. We then develop a
computationally-efficient online algorithm for DLB for which we prove an
$O(\sqrt{T})$ regret bound, where $T$ is the number of time steps. Our
algorithm is based on online mirror descent with a self-concordant barrier
regularization that employs a novel increasing learning rate schedule.
",study novel variant online finite horizon markov decision process adversarially change loss function initially unknown dynamic episode learner suffer loss accumulate along trajectory realize policy choose episode observe aggregate bandit feedback trajectory reveal along cumulative loss suffer rather individual loss encounter along trajectory main result computationally efficient algorithm k regret set k number episode establish result via efficient reduction novel bandit learn set call distort linear bandit dlb variant bandit linear optimization action choose learner adversarially distort committed develop computationally efficient online algorithm dlb prove regret bind number time step algorithm base online mirror descent self concordant barrier regularization employ novel increase learn rate schedule
Learning Networks from Random Walk-Based Node Similarities,"  Digital presence in the world of online social media entails significant
privacy risks. In this work we consider a privacy threat to a social network in
which an attacker has access to a subset of random walk-based node
similarities, such as effective resistances (i.e., commute times) or
personalized PageRank scores. Using these similarities, the attacker's goal is
to infer as much information as possible about the underlying network,
including any remaining unknown pairwise node similarities and edges.
  For the effective resistance metric, we show that with just a small subset of
measurements, the attacker can learn a large fraction of edges in a social
network, even when the measurements are noisy. We also show that it is possible
to learn a graph which accurately matches the underlying network on all other
effective resistances. This second observation is interesting from a data
mining perspective, since it can be expensive to accurately compute all
effective resistances. As an alternative, our graphs learned from just a subset
of approximate effective resistances can be used as surrogates in a wide range
of applications that use effective resistances to probe graph structure,
including for graph clustering, node centrality evaluation, and anomaly
detection.
  We obtain our results by formalizing the graph learning objective
mathematically, using two optimization problems. One formulation is convex and
can be solved provably in polynomial time. The other is not, but we solve it
efficiently with projected gradient and coordinate descent. We demonstrate the
effectiveness of these methods on a number of social networks obtained from
Facebook. We also discuss how our methods can be generalized to other random
walk-based similarities, such as personalized PageRank. Our code is available
at https://github.com/cnmusco/graph-similarity-learning.
",digital presence world online social medium entail significant privacy risk work consider privacy threat social network attacker access subset random walk base node similarity effective resistance commute time personalize pagerank score use similarity attacker goal infer much information possible underlying network include remain unknown pairwise node similarity edge effective resistance metric show small subset measurement attacker learn large fraction edge social network even measurement noisy also show possible learn graph accurately match underlie network effective resistance second observation interesting data mining perspective since expensive accurately compute effective resistance alternative graph learn subset approximate effective resistance use surrogate wide range application use effective resistance probe graph structure include graph cluster node centrality evaluation anomaly detection obtain result formalize graph learning objective mathematically use two optimization problem one formulation convex solve provably polynomial time solve efficiently project gradient coordinate descent demonstrate effectiveness method number social network obtain facebook also discuss method generalize random walk base similarity personalize pagerank code available https
Comparing Sample-wise Learnability Across Deep Neural Network Models,"  Estimating the relative importance of each sample in a training set has
important practical and theoretical value, such as in importance sampling or
curriculum learning. This kind of focus on individual samples invokes the
concept of sample-wise learnability: How easy is it to correctly learn each
sample (cf. PAC learnability)? In this paper, we approach the sample-wise
learnability problem within a deep learning context. We propose a measure of
the learnability of a sample with a given deep neural network (DNN) model. The
basic idea is to train the given model on the training set, and for each
sample, aggregate the hits and misses over the entire training epochs. Our
experiments show that the sample-wise learnability measure collected this way
is highly linearly correlated across different DNN models (ResNet-20, VGG-16,
and MobileNet), suggesting that such a measure can provide deep general
insights on the data's properties. We expect our method to help develop better
curricula for training, and help us better understand the data itself.
",estimate relative importance sample training set important practical theoretical value importance sampling curriculum learn kind focus individual sample invoke concept sample wise learnability easy correctly learn sample cf pac learnability paper approach sample wise learnability problem within deep learn context propose measure learnability sample give deep neural network dnn model basic idea train give model training set sample aggregate hit miss entire training epoch experiment show sample wise learnability measure collect way highly linearly correlate across different dnn model resnet-20 vgg-16 mobilenet suggest measure provide deep general insight data property expect method help develop well curricula training help we well understand datum
Pure Exploration for Multi-Armed Bandit Problems,"  We consider the framework of stochastic multi-armed bandit problems and study
the possibilities and limitations of forecasters that perform an on-line
exploration of the arms. These forecasters are assessed in terms of their
simple regret, a regret notion that captures the fact that exploration is only
constrained by the number of available rounds (not necessarily known in
advance), in contrast to the case when the cumulative regret is considered and
when exploitation needs to be performed at the same time. We believe that this
performance criterion is suited to situations when the cost of pulling an arm
is expressed in terms of resources rather than rewards. We discuss the links
between the simple and the cumulative regret. One of the main results in the
case of a finite number of arms is a general lower bound on the simple regret
of a forecaster in terms of its cumulative regret: the smaller the latter, the
larger the former. Keeping this result in mind, we then exhibit upper bounds on
the simple regret of some forecasters. The paper ends with a study devoted to
continuous-armed bandit problems; we show that the simple regret can be
minimized with respect to a family of probability distributions if and only if
the cumulative regret can be minimized for it. Based on this equivalence, we
are able to prove that the separable metric spaces are exactly the metric
spaces on which these regrets can be minimized with respect to the family of
all probability distributions with continuous mean-payoff functions.
",consider framework stochastic multi armed bandit problem study possibility limitation forecaster perform on line exploration arm forecaster assess term simple regret regret notion capture fact exploration constrain number available round necessarily know advance contrast case cumulative regret consider exploitation need perform time believe performance criterion suit situation cost pull arm express term resource rather reward discuss link simple cumulative regret one main result case finite number arm general lower bind simple regret forecaster term cumulative regret small latter large former keeping result mind exhibit upper bound simple regret forecaster paper end study devoted continuous armed bandit problem show simple regret minimize respect family probability distribution cumulative regret minimize base equivalence able prove separable metric space exactly metric space regret minimize respect family probability distribution continuous mean payoff function
Legion: Best-First Concolic Testing,"  Concolic execution and fuzzing are two complementary coverage-based testing
techniques. How to achieve the best of both remains an open challenge. To
address this research problem, we propose and evaluate Legion. Legion
re-engineers the Monte Carlo tree search (MCTS) framework from the AI
literature to treat automated test generation as a problem of sequential
decision-making under uncertainty. Its best-first search strategy provides a
principled way to learn the most promising program states to investigate at
each search iteration, based on observed rewards from previous iterations.
Legion incorporates a form of directed fuzzing that we call approximate
path-preserving fuzzing (APPFuzzing) to investigate program states selected by
MCTS. APPFuzzing serves as the Monte Carlo simulation technique and is
implemented by extending prior work on constrained sampling. We evaluate Legion
against competitors on 2531 benchmarks from the coverage category of Test-Comp
2020, as well as measuring its sensitivity to hyperparameters, demonstrating
its effectiveness on a wide variety of input programs.
",concolic execution fuzze two complementary coverage base testing technique achieve good remain open challenge address research problem propose evaluate legion legion re engineer monte carlo tree search mct framework ai literature treat automate test generation problem sequential decision make uncertainty well first search strategy provide principled way learn promise program state investigate search iteration base observed reward previous iteration legion incorporate form direct fuzzing call approximate path preserve fuzzing appfuzzing investigate program state select mct appfuzze serve monte carlo simulation technique implement extend prior work constrain sample evaluate legion competitor 2531 benchmark coverage category test comp 2020 well measure sensitivity hyperparameter demonstrate effectiveness wide variety input program
Community Detection and Improved Detectability in Multiplex Networks,"  We investigate the widely encountered problem of detecting communities in
multiplex networks, such as social networks, with an unknown arbitrary
heterogeneous structure. To improve detectability, we propose a generative
model that leverages the multiplicity of a single community in multiple layers,
with no prior assumption on the relation of communities among different layers.
Our model relies on a novel idea of incorporating a large set of generic
localized community label constraints across the layers, in conjunction with
the celebrated Stochastic Block Model (SBM) in each layer. Accordingly, we
build a probabilistic graphical model over the entire multiplex network by
treating the constraints as Bayesian priors. We mathematically prove that these
constraints/priors promote existence of identical communities across layers
without introducing further correlation between individual communities. The
constraints are further tailored to render a sparse graphical model and the
numerically efficient Belief Propagation algorithm is subsequently employed. We
further demonstrate by numerical experiments that in the presence of consistent
communities between different layers, consistent communities are matched, and
the detectability is improved over a single layer. We compare our model with a
""correlated model"" which exploits the prior knowledge of community correlation
between layers. Similar detectability improvement is obtained under such a
correlation, even though our model relies on much milder assumptions than the
correlated model. Our model even shows a better detection performance over a
certain correlation and signal to noise ratio (SNR) range. In the absence of
community correlation, the correlation model naturally fails, while ours
maintains its performance.
",investigate widely encounter problem detect community multiplex network social network unknown arbitrary heterogeneous structure improve detectability propose generative model leverage multiplicity single community multiple layer prior assumption relation community among different layer model rely novel idea incorporate large set generic localize community label constraint across layer conjunction celebrate stochastic block model sbm layer accordingly build probabilistic graphical model entire multiplex network treat constraint bayesian prior mathematically prove promote existence identical community across layer without introduce correlation individual community constraint tailor render sparse graphical model numerically efficient belief propagation algorithm subsequently employ demonstrate numerical experiment presence consistent community different layer consistent community match detectability improve single layer compare model correlate model exploit prior knowledge community correlation layer similar detectability improvement obtain correlation even though model rely much milder assumption correlate model model even show well detection performance certain correlation signal noise ratio snr range absence community correlation correlation model naturally fail maintain performance
"Classification of Hand Movements from EEG using a Deep Attention-based
  LSTM Network","  Classifying limb movements using brain activity is an important task in
Brain-computer Interfaces (BCI) that has been successfully used in multiple
application domains, ranging from human-computer interaction to medical and
biomedical applications. This paper proposes a novel solution for
classification of left/right hand movement by exploiting a Long Short-Term
Memory (LSTM) network with attention mechanism to learn the
electroencephalogram (EEG) time-series information. To this end, a wide range
of time and frequency domain features are extracted from the EEG signals and
used to train an LSTM network to perform the classification task. We conduct
extensive experiments with the EEG Movement dataset and show that our proposed
solution our method achieves improvements over several benchmarks and
state-of-the-art methods in both intra-subject and cross-subject validation
schemes. Moreover, we utilize the proposed framework to analyze the information
as received by the sensors and monitor the activated regions of the brain by
tracking EEG topography throughout the experiments.
",classify limb movement use brain activity important task brain computer interface bci successfully use multiple application domain range human computer interaction medical biomedical application paper propose novel solution classification hand movement exploit long short term memory lstm network attention mechanism learn electroencephalogram eeg time series information end wide range time frequency domain feature extract eeg signal use train lstm network perform classification task conduct extensive experiment eeg movement dataset show propose solution method achieve improvement several benchmark state of the art method intra subject cross subject validation scheme moreover utilize propose framework analyze information receive sensor monitor activate region brain tracking eeg topography throughout experiment
"Temporal Abstractions-Augmented Temporally Contrastive Learning: An
  Alternative to the Laplacian in RL","  In reinforcement learning, the graph Laplacian has proved to be a valuable
tool in the task-agnostic setting, with applications ranging from skill
discovery to reward shaping. Recently, learning the Laplacian representation
has been framed as the optimization of a temporally-contrastive objective to
overcome its computational limitations in large (or continuous) state spaces.
However, this approach requires uniform access to all states in the state
space, overlooking the exploration problem that emerges during the
representation learning process. In this work, we propose an alternative method
that is able to recover, in a non-uniform-prior setting, the expressiveness and
the desired properties of the Laplacian representation. We do so by combining
the representation learning with a skill-based covering policy, which provides
a better training distribution to extend and refine the representation. We also
show that a simple augmentation of the representation objective with the
learned temporal abstractions improves dynamics-awareness and helps
exploration. We find that our method succeeds as an alternative to the
Laplacian in the non-uniform setting and scales to challenging continuous
control environments. Finally, even if our method is not optimized for skill
discovery, the learned skills can successfully solve difficult continuous
navigation tasks with sparse rewards, where standard skill discovery approaches
are no so effective.
",reinforcement learn graph laplacian prove valuable tool task agnostic setting application range skill discovery reward shape recently learn laplacian representation framed optimization temporally contrastive objective overcome computational limitation large continuous state space however approach require uniform access state state space overlook exploration problem emerge representation learning process work propose alternative method able recover non uniform prior set expressiveness desire property laplacian representation combine representation learn skill base covering policy provide well training distribution extend refine representation also show simple augmentation representation objective learn temporal abstraction improve dynamic awareness help exploration find method succeed alternative laplacian non uniform set scale challenge continuous control environment finally even method optimize skill discovery learn skill successfully solve difficult continuous navigation task sparse reward standard skill discovery approach effective
The supervised hierarchical Dirichlet process,"  We propose the supervised hierarchical Dirichlet process (sHDP), a
nonparametric generative model for the joint distribution of a group of
observations and a response variable directly associated with that whole group.
We compare the sHDP with another leading method for regression on grouped data,
the supervised latent Dirichlet allocation (sLDA) model. We evaluate our method
on two real-world classification problems and two real-world regression
problems. Bayesian nonparametric regression models based on the Dirichlet
process, such as the Dirichlet process-generalised linear models (DP-GLM) have
previously been explored; these models allow flexibility in modelling nonlinear
relationships. However, until now, Hierarchical Dirichlet Process (HDP)
mixtures have not seen significant use in supervised problems with grouped data
since a straightforward application of the HDP on the grouped data results in
learnt clusters that are not predictive of the responses. The sHDP solves this
problem by allowing for clusters to be learnt jointly from the group structure
and from the label assigned to each group.
",propose supervised hierarchical dirichlet process shdp nonparametric generative model joint distribution group observation response variable directly associate whole group compare shdp another lead method regression group datum supervise latent dirichlet allocation slda model evaluate method two real world classification problem two real world regression problem bayesian nonparametric regression model base dirichlet process dirichlet process generalise linear model dp glm previously explore model allow flexibility model nonlinear relationship however hierarchical dirichlet process hdp mixtures see significant use supervise problem group datum since straightforward application hdp group datum result learn cluster predictive response shdp solve problem allow cluster learn jointly group structure label assign group
Subject Membership Inference Attacks in Federated Learning,"Privacy in Federated Learning (FL) is studied at two different granularities:
item-level, which protects individual data points, and user-level, which
protects each user (participant) in the federation. Nearly all of the private
FL literature is dedicated to studying privacy attacks and defenses at these
two granularities. Recently, subject-level privacy has emerged as an
alternative privacy granularity to protect the privacy of individuals (data
subjects) whose data is spread across multiple (organizational) users in
cross-silo FL settings. An adversary might be interested in recovering private
information about these individuals (a.k.a. \emph{data subjects}) by attacking
the trained model. A systematic study of these patterns requires complete
control over the federation, which is impossible with real-world datasets. We
design a simulator for generating various synthetic federation configurations,
enabling us to study how properties of the data, model design and training, and
the federation itself impact subject privacy risk. We propose three attacks for
\emph{subject membership inference} and examine the interplay between all
factors within a federation that affect the attacks' efficacy. We also
investigate the effectiveness of Differential Privacy in mitigating this
threat. Our takeaways generalize to real-world datasets like FEMNIST, giving
credence to our findings.",privacy federate learning fl study two different granularity item level protect individual datum point user level protect user participant federation nearly private fl literature dedicate study privacy attack defense two granularity recently subject level privacy emerge alternative privacy granularity protect privacy individual datum subject whose datum spread across multiple organizational user cross silo fl setting adversary might interested recover private information individual datum subject attack train model systematic study pattern require complete control federation impossible real world dataset design simulator generate various synthetic federation configuration enable we study property datum model design training federation impact subject privacy risk propose three attack subject membership inference examine interplay factor within federation affect attack efficacy also investigate effectiveness differential privacy mitigating threat takeaway generalize real world dataset like femnist give credence finding
Learning Set-equivariant Functions with SWARM Mappings,"  In this work we propose a new neural network architecture that efficiently
implements and learns general purpose set-equivariant functions. Such a
function f maps a set of entities x = {x1, . . . , xn} from one domain to a set
of same cardinality y = f (x) = {y1, . . . , yn} in another domain regardless
of the ordering of the entities. The architecture is based on a gated recurrent
network which is iteratively applied to all entities individually and at the
same time syncs with the progression of the whole population. In reminiscence
to this pattern, which can be frequently observed in nature, we call our
approach SWARM mapping. Set-equivariant and generally permutation invariant
functions are important building blocks for many state of the art machine
learning approaches. Even in applications where the permutation invariance is
not of primary interest, as to be seen in the recent success of attention based
transformer models (Vaswani et. al. 2017). Accordingly, we demonstrate the
power and usefulness of SWARM mappings in different applications. We compare
the performance of our approach with another recently proposed set-equivariant
function, the Set Transformer (Lee et.al. 2018) and we demonstrate that models
solely based on SWARM layers gives state of the art results.
",work propose new neural network architecture efficiently implement learns general purpose set equivariant function function f maps set entity x x1 xn one domain set cardinality f x y1 yn another domain regardless order entity architecture base gate recurrent network iteratively apply entity individually time sync progression whole population reminiscence pattern frequently observe nature call approach swarm mapping set equivariant generally permutation invariant function important building block many state art machine learning approach even application permutation invariance primary interest see recent success attention base transformer model vaswani et al 2017 accordingly demonstrate power usefulness swarm mapping different application compare performance approach another recently propose set equivariant function set transformer lee 2018 demonstrate model solely base swarm layer give state art result
"Improved Variance-Aware Confidence Sets for Linear Bandits and Linear
  Mixture MDP","  This paper presents new \emph{variance-aware} confidence sets for linear
bandits and linear mixture Markov Decision Processes (MDPs). With the new
confidence sets, we obtain the follow regret bounds: For linear bandits, we
obtain an $\tilde{O}(poly(d)\sqrt{1 + \sum_{k=1}^{K}\sigma_k^2})$
data-dependent regret bound, where $d$ is the feature dimension, $K$ is the
number of rounds, and $\sigma_k^2$ is the \emph{unknown} variance of the reward
at the $k$-th round. This is the first regret bound that only scales with the
variance and the dimension but \emph{no explicit polynomial dependency on $K$}.
When variances are small, this bound can be significantly smaller than the
$\tilde{\Theta}\left(d\sqrt{K}\right)$ worst-case regret bound. For linear
mixture MDPs, we obtain an $\tilde{O}(poly(d, \log H)\sqrt{K})$ regret bound,
where $d$ is the number of base models, $K$ is the number of episodes, and $H$
is the planning horizon. This is the first regret bound that only scales
\emph{logarithmically} with $H$ in the reinforcement learning with linear
function approximation setting, thus \emph{exponentially improving} existing
results, and resolving an open problem in \citep{zhou2020nearly}. We develop
three technical ideas that may be of independent interest: 1) applications of
the peeling technique to both the input norm and the variance magnitude, 2) a
recursion-based estimator for the variance, and 3) a new convex potential lemma
that generalizes the seminal elliptical potential lemma.
",paper present new variance aware confidence set linear bandit linear mixture markov decision process mdps new confidence set obtain follow regret bound linear bandit obtain poly 1 k data dependent regret bind feature dimension k number round unknown variance reward k -th round first regret bind scale variance dimension explicit polynomial dependency k variance small bind significantly small k bad case regret bind linear mixture mdps obtain poly h k regret bind number base model k number episode h planning horizon first regret bind scale logarithmically h reinforcement learn linear function approximation set thus exponentially improve exist result resolve open problem zhou2020nearly develop three technical idea may independent interest 1 application peel technique input norm variance magnitude 2 recursion base estimator variance 3 new convex potential lemma generalize seminal elliptical potential lemma
Topology-Aware Graph Pooling Networks,"  Pooling operations have shown to be effective on computer vision and natural
language processing tasks. One challenge of performing pooling operations on
graph data is the lack of locality that is not well-defined on graphs. Previous
studies used global ranking methods to sample some of the important nodes, but
most of them are not able to incorporate graph topology. In this work, we
propose the topology-aware pooling (TAP) layer that explicitly considers graph
topology. Our TAP layer is a two-stage voting process that selects more
important nodes in a graph. It first performs local voting to generate scores
for each node by attending each node to its neighboring nodes. The scores are
generated locally such that topology information is explicitly considered. In
addition, graph topology is incorporated in global voting to compute the
importance score of each node globally in the entire graph. Altogether, the
final ranking score for each node is computed by combining its local and global
voting scores. To encourage better graph connectivity in the sampled graph, we
propose to add a graph connectivity term to the computation of ranking scores.
Results on graph classification tasks demonstrate that our methods achieve
consistently better performance than previous methods.
",pool operation show effective computer vision natural language processing task one challenge perform pool operation graph datum lack locality well define graph previous study use global ranking method sample important node able incorporate graph topology work propose topology aware pool tap layer explicitly consider graph topology tap layer two stage voting process select important node graph first perform local voting generate score node attend node neighboring node score generate locally topology information explicitly consider addition graph topology incorporate global voting compute importance score node globally entire graph altogether final rank score node compute combine local global voting score encourage well graph connectivity sample graph propose add graph connectivity term computation rank score result graph classification task demonstrate method achieve consistently well performance previous method
When Noise meets Chaos: Stochastic Resonance in Neurochaos Learning,"  Chaos and Noise are ubiquitous in the Brain. Inspired by the chaotic firing
of neurons and the constructive role of noise in neuronal models, we for the
first time connect chaos, noise and learning. In this paper, we demonstrate
Stochastic Resonance (SR) phenomenon in Neurochaos Learning (NL). SR manifests
at the level of a single neuron of NL and enables efficient subthreshold signal
detection. Furthermore, SR is shown to occur in single and multiple neuronal NL
architecture for classification tasks - both on simulated and real-world spoken
digit datasets. Intermediate levels of noise in neurochaos learning enables
peak performance in classification tasks thus highlighting the role of SR in AI
applications, especially in brain inspired learning architectures.
",chaos noise ubiquitous brain inspire chaotic firing neuron constructive role noise neuronal model first time connect chaos noise learn paper demonstrate stochastic resonance sr phenomenon neurochaos learn nl sr manifest level single neuron nl enable efficient subthreshold signal detection furthermore sr show occur single multiple neuronal nl architecture classification task simulate real world speak digit dataset intermediate level noise neurochaos learning enable peak performance classification task thus highlight role sr ai application especially brain inspire learning architecture
"A high-reproducibility and high-accuracy method for automated topic
  classification","  Much of human knowledge sits in large databases of unstructured text.
Leveraging this knowledge requires algorithms that extract and record metadata
on unstructured text documents. Assigning topics to documents will enable
intelligent search, statistical characterization, and meaningful
classification. Latent Dirichlet allocation (LDA) is the state-of-the-art in
topic classification. Here, we perform a systematic theoretical and numerical
analysis that demonstrates that current optimization techniques for LDA often
yield results which are not accurate in inferring the most suitable model
parameters. Adapting approaches for community detection in networks, we propose
a new algorithm which displays high-reproducibility and high-accuracy, and also
has high computational efficiency. We apply it to a large set of documents in
the English Wikipedia and reveal its hierarchical structure. Our algorithm
promises to make ""big data"" text analysis systems more reliable.
",much human knowledge sit large database unstructured text leverage knowledge require algorithm extract record metadata unstructured text document assign topic document enable intelligent search statistical characterization meaningful classification latent dirichlet allocation lda state of the art topic classification perform systematic theoretical numerical analysis demonstrate current optimization technique lda often yield result accurate infer suitable model parameter adapt approach community detection network propose new algorithm display high reproducibility high accuracy also high computational efficiency apply large set document english wikipedia reveal hierarchical structure algorithm promise make big data text analysis system reliable
Modelling and Mining of Patient Pathways: A Scoping Review,"The sequence of visits and procedures performed by the patient in the health
system, also known as the patient's pathway or trajectory, can reveal important
information about the clinical treatment adopted and the health service
provided. The rise of electronic health data availability made it possible to
assess the pathways of a large number of patients. Nevertheless, some
challenges also arose concerning how to synthesize these pathways and how to
mine them from the data, fostering a new field of research. The objective of
this review is to survey this new field of research, highlighting
representation models, mining techniques, methods of analysis, and examples of
case studies.",sequence visit procedure perform patient health system also know patient pathway trajectory reveal important information clinical treatment adopt health service provide rise electronic health datum availability make possible assess pathway large number patient nevertheless challenge also arise concern synthesize pathway mine datum foster new field research objective review survey new field research highlight representation model mining technique method analysis example case study
"Self-supervised Human Activity Recognition by Learning to Predict
  Cross-Dimensional Motion","  We propose the use of self-supervised learning for human activity recognition
with smartphone accelerometer data. Our proposed solution consists of two
steps. First, the representations of unlabeled input signals are learned by
training a deep convolutional neural network to predict a segment of
accelerometer values. Our model exploits a novel scheme to leverage past and
present motion in x and y dimensions, as well as past values of the z axis to
predict values in the z dimension. This cross-dimensional prediction approach
results in effective pretext training with which our model learns to extract
strong representations. Next, we freeze the convolution blocks and transfer the
weights to our downstream network aimed at human activity recognition. For this
task, we add a number of fully connected layers to the end of the frozen
network and train the added layers with labeled accelerometer signals to learn
to classify human activities. We evaluate the performance of our method on
three publicly available human activity datasets: UCI HAR, MotionSense, and
HAPT. The results show that our approach outperforms the existing methods and
sets new state-of-the-art results.
",propose use self supervise learn human activity recognition smartphone accelerometer datum propose solution consist two step first representation unlabeled input signal learn train deep convolutional neural network predict segment accelerometer value model exploit novel scheme leverage past present motion x dimension well past value z axis predict value z dimension cross dimensional prediction approach result effective pretext training model learn extract strong representation next freeze convolution block transfer weight downstream network aim human activity recognition task add number fully connect layer end frozen network train add layer label accelerometer signal learn classify human activity evaluate performance method three publicly available human activity dataset uci har motionsense hapt result show approach outperform exist method set new state of the art result
"Machine Learning Etudes in Astrophysics: Selection Functions for Mock
  Cluster Catalogs","  Making mock simulated catalogs is an important component of astrophysical
data analysis. Selection criteria for observed astronomical objects are often
too complicated to be derived from first principles. However the existence of
an observed group of objects is a well-suited problem for machine learning
classification. In this paper we use one-class classifiers to learn the
properties of an observed catalog of clusters of galaxies from ROSAT and to
pick clusters from mock simulations that resemble the observed ROSAT catalog.
We show how this method can be used to study the cross-correlations of thermal
Sunya'ev-Zeldovich signals with number density maps of X-ray selected cluster
catalogs. The method reduces the bias due to hand-tuning the selection function
and is readily scalable to large catalogs with a high-dimensional space of
astrophysical features.
",make mock simulate catalog important component astrophysical data analysis selection criterion observe astronomical object often complicate derive first principle however existence observe group object well suit problem machine learn classification paper use one class classifier learn property observe catalog cluster galaxy rosat pick cluster mock simulation resemble observe rosat catalog show method use study cross correlation thermal signal number density map x ray select cluster catalog method reduce bias due hand tune selection function readily scalable large catalog high dimensional space astrophysical feature
RPC Considered Harmful: Fast Distributed Deep Learning on RDMA,"  Deep learning emerges as an important new resource-intensive workload and has
been successfully applied in computer vision, speech, natural language
processing, and so on. Distributed deep learning is becoming a necessity to
cope with growing data and model sizes. Its computation is typically
characterized by a simple tensor data abstraction to model multi-dimensional
matrices, a data-flow graph to model computation, and iterative executions with
relatively frequent synchronizations, thereby making it substantially different
from Map/Reduce style distributed big data computation.
  RPC, commonly used as the communication primitive, has been adopted by
popular deep learning frameworks such as TensorFlow, which uses gRPC. We show
that RPC is sub-optimal for distributed deep learning computation, especially
on an RDMA-capable network. The tensor abstraction and data-flow graph, coupled
with an RDMA network, offers the opportunity to reduce the unnecessary overhead
(e.g., memory copy) without sacrificing programmability and generality. In
particular, from a data access point of view, a remote machine is abstracted
just as a ""device"" on an RDMA channel, with a simple memory interface for
allocating, reading, and writing memory regions. Our graph analyzer looks at
both the data flow graph and the tensors to optimize memory allocation and
remote data access using this interface. The result is up to 25 times speedup
in representative deep learning benchmarks against the standard gRPC in
TensorFlow and up to 169% improvement even against an RPC implementation
optimized for RDMA, leading to faster convergence in the training process.
",deep learning emerge important new resource intensive workload successfully apply computer vision speech natural language processing distribute deep learning become necessity cope grow datum model size computation typically characterize simple tensor datum abstraction model multi dimensional matrix data flow graph model computation iterative execution relatively frequent synchronization thereby make substantially different style distribute big datum computation rpc commonly use communication primitive adopt popular deep learning framework tensorflow use grpc show rpc sub optimal distribute deep learning computation especially rdma capable network tensor abstraction data flow graph couple rdma network offer opportunity reduce unnecessary overhead memory copy without sacrifice programmability generality particular datum access point view remote machine abstract device rdma channel simple memory interface allocate read write memory region graph analyzer look datum flow graph tensor optimize memory allocation remote datum access use interface result 25 time speedup representative deep learning benchmark standard grpc tensorflow 169 improvement even rpc implementation optimize rdma lead fast convergence training process
"Mitigating Memorization of Noisy Labels via Regularization between
  Representations","  Designing robust loss functions is popular in learning with noisy labels
while existing designs did not explicitly consider the overfitting property of
deep neural networks (DNNs). As a result, applying these losses may still
suffer from overfitting/memorizing noisy labels as training proceeds. In this
paper, we first theoretically analyze the memorization effect and show that a
lower-capacity model may perform better on noisy datasets. However, it is
non-trivial to design a neural network with the best capacity given an
arbitrary task. To circumvent this dilemma, instead of changing the model
architecture, we decouple DNNs into an encoder followed by a linear classifier
and propose to restrict the function space of a DNN by a representation
regularizer. Particularly, we require the distance between two self-supervised
features to be positively related to the distance between the corresponding two
supervised model outputs. Our proposed framework is easily extendable and can
incorporate many other robust loss functions to further improve performance.
Extensive experiments and theoretical analyses support our claims. Code is
available at github.com/UCSC-REAL/SelfSup_NoisyLabel.
",design robust loss function popular learn noisy label exist design explicitly consider overfitte property deep neural network dnn result apply loss may still suffer noisy label train proceed paper first theoretically analyze memorization effect show low capacity model may perform well noisy dataset however non trivial design neural network good capacity give arbitrary task circumvent dilemma instead change model architecture decouple dnn encoder follow linear classifier propose restrict function space dnn representation regularizer particularly require distance two self supervise feature positively relate distance correspond two supervise model output propose framework easily extendable incorporate many robust loss function improve performance extensive experiment theoretical analysis support claim code available
Video Diffusion Models,"  Generating temporally coherent high fidelity video is an important milestone
in generative modeling research. We make progress towards this milestone by
proposing a diffusion model for video generation that shows very promising
initial results. Our model is a natural extension of the standard image
diffusion architecture, and it enables jointly training from image and video
data, which we find to reduce the variance of minibatch gradients and speed up
optimization. To generate long and higher resolution videos we introduce a new
conditional sampling technique for spatial and temporal video extension that
performs better than previously proposed methods. We present the first results
on a large text-conditioned video generation task, as well as state-of-the-art
results on an established unconditional video generation benchmark.
Supplementary material is available at https://video-diffusion.github.io/
",generate temporally coherent high fidelity video important milestone generative modeling research make progress towards milestone propose diffusion model video generation show promise initial result model natural extension standard image diffusion architecture enable jointly train image video datum find reduce variance minibatch gradient speed optimization generate long high resolution video introduce new conditional sampling technique spatial temporal video extension perform well previously propose method present first result large text condition video generation task well state of the art result establish unconditional video generation benchmark supplementary material available https
"Distributionally Robust Recurrent Decoders with Random Network
  Distillation","  Neural machine learning models can successfully model language that is
similar to their training distribution, but they are highly susceptible to
degradation under distribution shift, which occurs in many practical
applications when processing out-of-domain (OOD) text. This has been attributed
to ""shortcut learning"": relying on weak correlations over arbitrary large
contexts.
  We propose a method based on OOD detection with Random Network Distillation
to allow an autoregressive language model to automatically disregard OOD
context during inference, smoothly transitioning towards a less expressive but
more robust model as the data becomes more OOD while retaining its full context
capability when operating in-distribution. We apply our method to a GRU
architecture, demonstrating improvements on multiple language modeling (LM)
datasets.
",neural machine learning model successfully model language similar training distribution highly susceptible degradation distribution shift occur many practical application process out of domain ood text attribute shortcut learn rely weak correlation arbitrary large contexts propose method base ood detection random network distillation allow autoregressive language model automatically disregard ood context inference smoothly transition towards less expressive robust model datum become ood retain full context capability operate in distribution apply method gru architecture demonstrate improvement multiple language model lm dataset
"Pareto Efficient Fairness in Supervised Learning: From Extraction to
  Tracing","  As algorithmic decision-making systems are becoming more pervasive, it is
crucial to ensure such systems do not become mechanisms of unfair
discrimination on the basis of gender, race, ethnicity, religion, etc.
Moreover, due to the inherent trade-off between fairness measures and accuracy,
it is desirable to learn fairness-enhanced models without significantly
compromising the accuracy. In this paper, we propose Pareto efficient Fairness
(PEF) as a suitable fairness notion for supervised learning, that can ensure
the optimal trade-off between overall loss and other fairness criteria. The
proposed PEF notion is definition-agnostic, meaning that any well-defined
notion of fairness can be reduced to the PEF notion. To efficiently find a PEF
classifier, we cast the fairness-enhanced classification as a bilevel
optimization problem and propose a gradient-based method that can guarantee the
solution belongs to the Pareto frontier with provable guarantees for convex and
non-convex objectives. We also generalize the proposed algorithmic solution to
extract and trace arbitrary solutions from the Pareto frontier for a given
preference over accuracy and fairness measures. This approach is generic and
can be generalized to any multicriteria optimization problem to trace points on
the Pareto frontier curve, which is interesting by its own right. We
empirically demonstrate the effectiveness of the PEF solution and the extracted
Pareto frontier on real-world datasets compared to state-of-the-art methods.
",algorithmic decision make system become pervasive crucial ensure system become mechanism unfair discrimination basis gender race ethnicity religion etc moreover due inherent trade off fairness measure accuracy desirable learn fairness enhance model without significantly compromise accuracy paper propose pareto efficient fairness pef suitable fairness notion supervise learning ensure optimal trade off overall loss fairness criterion propose pef notion definition agnostic meaning well define notion fairness reduce pef notion efficiently find pef classifier cast fairness enhance classification bilevel optimization problem propose gradient base method guarantee solution belong pareto frontier provable guarantee convex non convex objective also generalize propose algorithmic solution extract trace arbitrary solution pareto frontier give preference accuracy fairness measure approach generic generalized multicriteria optimization problem trace point pareto frontier curve interesting right empirically demonstrate effectiveness pef solution extract pareto frontier real world dataset compare state of the art method
"Optimizing Coordinative Schedules for Tanker Terminals: An Intelligent
  Large Spatial-Temporal Data-Driven Approach -- Part 1","  In this study, a novel coordinative scheduling optimization approach is
proposed to enhance port efficiency by reducing average wait time and
turnaround time. The proposed approach consists of enhanced particle swarm
optimization (ePSO) as kernel and augmented firefly algorithm (AFA) as global
optimal search. Two paradigm methods of the proposed approach are investigated,
which are batch method and rolling horizon method. The experimental results
show that both paradigm methods of proposed approach can effectively enhance
port efficiency. The average wait time could be significantly reduced by 86.0%
- 95.5%, and the average turnaround time could eventually save 38.2% - 42.4%
with respect to historical benchmarks. Moreover, the paradigm method of rolling
horizon could reduce to 20 mins on running time over 3-month datasets, rather
than 4 hrs on batch method at corresponding maximum performance.
",study novel coordinative scheduling optimization approach propose enhance port efficiency reduce average wait time turnaround time propose approach consist enhance particle swarm optimization epso kernel augment firefly algorithm afa global optimal search two paradigm method propose approach investigate batch method roll horizon method experimental result show paradigm method propose approach effectively enhance port efficiency average wait time could significantly reduce average turnaround time could eventually save respect historical benchmark moreover paradigm method rolling horizon could reduce 20 min run time 3 month dataset rather 4 hrs batch method correspond maximum performance
"Connecting ansatz expressibility to gradient magnitudes and barren
  plateaus","  Parameterized quantum circuits serve as ans\""{a}tze for solving variational
problems and provide a flexible paradigm for programming near-term quantum
computers. Ideally, such ans\""{a}tze should be highly expressive so that a
close approximation of the desired solution can be accessed. On the other hand,
the ansatz must also have sufficiently large gradients to allow for training.
Here, we derive a fundamental relationship between these two essential
properties: expressibility and trainability. This is done by extending the well
established barren plateau phenomenon, which holds for ans\""{a}tze that form
exact 2-designs, to arbitrary ans\""{a}tze. Specifically, we calculate the
variance in the cost gradient in terms of the expressibility of the ansatz, as
measured by its distance from being a 2-design. Our resulting bounds indicate
that highly expressive ans\""{a}tze exhibit flatter cost landscapes and
therefore will be harder to train. Furthermore, we provide numerics
illustrating the effect of expressiblity on gradient scalings, and we discuss
the implications for designing strategies to avoid barren plateaus.
",parameterize quantum circuit serve tze solve variational problem provide flexible paradigm programming near term quantum computer ideally tze highly expressive close approximation desire solution access hand ansatz must also sufficiently large gradient allow training derive fundamental relationship two essential property expressibility trainability done extend well establish barren plateau phenomenon hold tze form exact 2 design arbitrary tze specifically calculate variance cost gradient term expressibility ansatz measure distance 2 design result bound indicate highly expressive tze exhibit flatter cost landscape therefore hard train furthermore provide numeric illustrate effect expressiblity gradient scaling discuss implication design strategy avoid barren plateaus
"Unsupervised Video Decomposition using Spatio-temporal Iterative
  Inference","  Unsupervised multi-object scene decomposition is a fast-emerging problem in
representation learning. Despite significant progress in static scenes, such
models are unable to leverage important dynamic cues present in video. We
propose a novel spatio-temporal iterative inference framework that is powerful
enough to jointly model complex multi-object representations and explicit
temporal dependencies between latent variables across frames. This is achieved
by leveraging 2D-LSTM, temporally conditioned inference and generation within
the iterative amortized inference for posterior refinement. Our method improves
the overall quality of decompositions, encodes information about the objects'
dynamics, and can be used to predict trajectories of each object separately.
Additionally, we show that our model has a high accuracy even without color
information. We demonstrate the decomposition, segmentation, and prediction
capabilities of our model and show that it outperforms the state-of-the-art on
several benchmark datasets, one of which was curated for this work and will be
made publicly available.
",unsupervised multi object scene decomposition fast emerge problem representation learning despite significant progress static scene model unable leverage important dynamic cue present video propose novel spatio temporal iterative inference framework powerful enough jointly model complex multi object representation explicit temporal dependency latent variable across frame achieve leverage 2d lstm temporally condition inference generation within iterative amortize inference posterior refinement method improve overall quality decomposition encode information object dynamic use predict trajectory object separately additionally show model high accuracy even without color information demonstrate decomposition segmentation prediction capability model show outperform state of the art several benchmark dataset one curate work make publicly available
"Sense and Sensitivity Analysis: Simple Post-Hoc Analysis of Bias Due to
  Unobserved Confounding","  It is a truth universally acknowledged that an observed association without
known mechanism must be in want of a causal estimate. However, causal
estimation from observational data often relies on the (untestable) assumption
of `no unobserved confounding'. Violations of this assumption can induce bias
in effect estimates. In principle, such bias could invalidate or reverse the
conclusions of a study. However, in some cases, we might hope that the
influence of unobserved confounders is weak relative to a `large' estimated
effect, so the qualitative conclusions are robust to bias from unobserved
confounding. The purpose of this paper is to develop \emph{Austen plots}, a
sensitivity analysis tool to aid such judgments by making it easier to reason
about potential bias induced by unobserved confounding. We formalize
confounding strength in terms of how strongly the confounder influences
treatment assignment and outcome. For a target level of bias, an Austen plot
shows the minimum values of treatment and outcome influence required to induce
that level of bias. Domain experts can then make subjective judgments about
whether such strong confounders are plausible. To aid this judgment, the Austen
plot additionally displays the estimated influence strength of (groups of) the
observed covariates. Austen plots generalize the classic sensitivity analysis
approach of Imbens [Imb03]. Critically, Austen plots allow any approach for
modeling the observed data and producing the initial estimate. We illustrate
the tool by assessing biases for several real causal inference problems, using
a variety of machine learning approaches for the initial data analysis. Code is
available at https://github.com/anishazaveri/austen_plots
",truth universally acknowledge observed association without known mechanism must want causal estimate however causal estimation observational datum often rely untestable assumption unobserve confound violation assumption induce bias effect estimate principle bias could invalidate reverse conclusion study however case might hope influence unobserved confounder weak relative large estimate effect qualitative conclusion robust bias unobserve confound purpose paper develop austen plot sensitivity analysis tool aid judgment make easy reason potential bias induce unobserved confound formalize confound strength term strongly confounder influence treatment assignment outcome target level bias austen plot show minimum value treatment outcome influence require induce level bias domain expert make subjective judgment whether strong confounder plausible aid judgment austen plot additionally display estimate influence strength group observe covariate austen plot generalize classic sensitivity analysis approach imben imb03 critically austen plot allow approach modeling observe datum produce initial estimate illustrate tool assess bias several real causal inference problem use variety machine learning approach initial datum analysis code available https
"Strategically Efficient Exploration in Competitive Multi-agent
  Reinforcement Learning","  High sample complexity remains a barrier to the application of reinforcement
learning (RL), particularly in multi-agent systems. A large body of work has
demonstrated that exploration mechanisms based on the principle of optimism
under uncertainty can significantly improve the sample efficiency of RL in
single agent tasks. This work seeks to understand the role of optimistic
exploration in non-cooperative multi-agent settings. We will show that, in
zero-sum games, optimistic exploration can cause the learner to waste time
sampling parts of the state space that are irrelevant to strategic play, as
they can only be reached through cooperation between both players. To address
this issue, we introduce a formal notion of strategically efficient exploration
in Markov games, and use this to develop two strategically efficient learning
algorithms for finite Markov games. We demonstrate that these methods can be
significantly more sample efficient than their optimistic counterparts.
",high sample complexity remain barrier application reinforcement learning rl particularly multi agent system large body work demonstrate exploration mechanism base principle optimism uncertainty significantly improve sample efficiency rl single agent task work seeks understand role optimistic exploration non cooperative multi agent setting show zero sum games optimistic exploration cause learner waste time sample part state space irrelevant strategic play reach cooperation player address issue introduce formal notion strategically efficient exploration markov game use develop two strategically efficient learning algorithm finite markov games demonstrate method significantly sample efficient optimistic counterpart
Semi-Supervised Deep Ensembles for Blind Image Quality Assessment,"  Ensemble methods are generally regarded to be better than a single model if
the base learners are deemed to be ""accurate"" and ""diverse."" Here we
investigate a semi-supervised ensemble learning strategy to produce
generalizable blind image quality assessment models. We train a multi-head
convolutional network for quality prediction by maximizing the accuracy of the
ensemble (as well as the base learners) on labeled data, and the disagreement
(i.e., diversity) among them on unlabeled data, both implemented by the
fidelity loss. We conduct extensive experiments to demonstrate the advantages
of employing unlabeled data for BIQA, especially in model generalization and
failure identification.
",ensemble method generally regard well single model base learner deem accurate diverse investigate semi supervised ensemble learning strategy produce generalizable blind image quality assessment model train multi head convolutional network quality prediction maximize accuracy ensemble well base learner label datum disagreement diversity among unlabeled datum implement fidelity loss conduct extensive experiment demonstrate advantage employ unlabeled datum biqa especially model generalization failure identification
"MLComp: A Methodology for Machine Learning-based Performance Estimation
  and Adaptive Selection of Pareto-Optimal Compiler Optimization Sequences","  Embedded systems have proliferated in various consumer and industrial
applications with the evolution of Cyber-Physical Systems and the Internet of
Things. These systems are subjected to stringent constraints so that embedded
software must be optimized for multiple objectives simultaneously, namely
reduced energy consumption, execution time, and code size. Compilers offer
optimization phases to improve these metrics. However, proper selection and
ordering of them depends on multiple factors and typically requires expert
knowledge. State-of-the-art optimizers facilitate different platforms and
applications case by case, and they are limited by optimizing one metric at a
time, as well as requiring a time-consuming adaptation for different targets
through dynamic profiling.
  To address these problems, we propose the novel MLComp methodology, in which
optimization phases are sequenced by a Reinforcement Learning-based policy.
Training of the policy is supported by Machine Learning-based analytical models
for quick performance estimation, thereby drastically reducing the time spent
for dynamic profiling. In our framework, different Machine Learning models are
automatically tested to choose the best-fitting one. The trained Performance
Estimator model is leveraged to efficiently devise Reinforcement Learning-based
multi-objective policies for creating quasi-optimal phase sequences.
  Compared to state-of-the-art estimation models, our Performance Estimator
model achieves lower relative error (<2%) with up to 50x faster training time
over multiple platforms and application domains. Our Phase Selection Policy
improves execution time and energy consumption of a given code by up to 12% and
6%, respectively. The Performance Estimator and the Phase Selection Policy can
be trained efficiently for any target platform and application domain.
",embed system proliferate various consumer industrial application evolution cyber physical system internet thing system subject stringent constraint embed software must optimize multiple objective simultaneously namely reduce energy consumption execution time code size compiler offer optimization phase improve metric however proper selection ordering depend multiple factor typically require expert knowledge state of the art optimizer facilitate different platform application case case limit optimize one metric time well require time consume adaptation different target dynamic profiling address problem propose novel mlcomp methodology optimization phase sequence reinforcement learning base policy training policy support machine learning base analytical model quick performance estimation thereby drastically reduce time spend dynamic profiling framework different machine learning model automatically test choose well fit one train performance estimator model leverage efficiently devise reinforcement learning base multi objective policy create quasi optimal phase sequence compare state of the art estimation model performance estimator model achieve low relative error 2 50x fast training time multiple platform application domain phase selection policy improve execution time energy consumption give code 12 6 respectively performance estimator phase selection policy train efficiently target platform application domain
Revisit Policy Optimization in Matrix Form,"  In tabular case, when the reward and environment dynamics are known, policy
evaluation can be written as $\bm{V}_{\bm{\pi}} = (I - \gamma
P_{\bm{\pi}})^{-1} \bm{r}_{\bm{\pi}}$, where $P_{\bm{\pi}}$ is the state
transition matrix given policy ${\bm{\pi}}$ and $\bm{r}_{\bm{\pi}}$ is the
reward signal given ${\bm{\pi}}$. What annoys us is that $P_{\bm{\pi}}$ and
$\bm{r}_{\bm{\pi}}$ are both mixed with ${\bm{\pi}}$, which means every time
when we update ${\bm{\pi}}$, they will change together. In this paper, we
leverage the notation from \cite{wang2007dual} to disentangle ${\bm{\pi}}$ and
environment dynamics which makes optimization over policy more straightforward.
We show that policy gradient theorem \cite{sutton2018reinforcement} and TRPO
\cite{schulman2015trust} can be put into a more general framework and such
notation has good potential to be extended to model-based reinforcement
learning.
",tabular case reward environment dynamic know policy evaluation write v p -1 r p state transition matrix give policy r reward signal give annoy we p r mixed mean every time update change together paper leverage notation wang2007dual disentangle environment dynamic make optimization policy straightforward show policy gradient theorem sutton2018reinforcement trpo schulman2015trust put general framework notation good potential extend model base reinforcement learning
Cross-lingual Transfer of Monolingual Models,"  Recent studies in zero-shot cross-lingual learning using multilingual models
have falsified the previous hypothesis that shared vocabulary and joint
pre-training are the keys to cross-lingual generalization. Inspired by this
advancement, we introduce a cross-lingual transfer method for monolingual
models based on domain adaptation. We study the effects of such transfer from
four different languages to English. Our experimental results on GLUE show that
the transferred models outperform the native English model independently of the
source language. After probing the English linguistic knowledge encoded in the
representations before and after transfer, we find that semantic information is
retained from the source language, while syntactic information is learned
during transfer. Additionally, the results of evaluating the transferred models
in source language tasks reveal that their performance in the source domain
deteriorates after transfer.
",recent study zero shoot cross lingual learning use multilingual model falsify previous hypothesis share vocabulary joint pre training key cross lingual generalization inspire advancement introduce cross lingual transfer method monolingual model base domain adaptation study effect transfer four different language english experimental result glue show transfer model outperform native english model independently source language probe english linguistic knowledge encode representation transfer find semantic information retain source language syntactic information learn transfer additionally result evaluate transfer model source language task reveal performance source domain deteriorate transfer
"Explainable Artificial Intelligence for Process Mining: A General
  Overview and Application of a Novel Local Explanation Approach for Predictive
  Process Monitoring","  The contemporary process-aware information systems possess the capabilities
to record the activities generated during the process execution. To leverage
these process specific fine-granular data, process mining has recently emerged
as a promising research discipline. As an important branch of process mining,
predictive business process management, pursues the objective to generate
forward-looking, predictive insights to shape business processes. In this
study, we propose a conceptual framework sought to establish and promote
understanding of decision-making environment, underlying business processes and
nature of the user characteristics for developing explainable business process
prediction solutions. Consequently, with regard to the theoretical and
practical implications of the framework, this study proposes a novel local
post-hoc explanation approach for a deep learning classifier that is expected
to facilitate the domain experts in justifying the model decisions. In contrary
to alternative popular perturbation-based local explanation approaches, this
study defines the local regions from the validation dataset by using the
intermediate latent space representations learned by the deep neural networks.
To validate the applicability of the proposed explanation method, the real-life
process log data delivered by the Volvo IT Belgium's incident management system
are used.The adopted deep learning classifier achieves a good performance with
the Area Under the ROC Curve of 0.94. The generated local explanations are also
visualized and presented with relevant evaluation measures that are expected to
increase the users' trust in the black-box-model.
",contemporary process aware information system possess capability record activity generate process execution leverage process specific fine granular datum process mining recently emerge promising research discipline important branch process mining predictive business process management pursue objective generate forward look predictive insight shape business process study propose conceptual framework sought establish promote understand decision make environment underlie business process nature user characteristic develop explainable business process prediction solution consequently regard theoretical practical implication framework study propose novel local post hoc explanation approach deep learning classifier expect facilitate domain expert justify model decision contrary alternative popular perturbation base local explanation approach study define local region validation dataset use intermediate latent space representation learn deep neural network validate applicability propose explanation method real life process log datum deliver volvo belgium incident management system adopt deep learning classifier achieve good performance area roc curve generate local explanation also visualized present relevant evaluation measure expect increase user trust black box model
"Health Monitoring of Industrial machines using Scene-Aware Threshold
  Selection","  This paper presents an autoencoder based unsupervised approach to identify
anomaly in an industrial machine using sounds produced by the machine. The
proposed framework is trained using log-melspectrogram representations of the
sound signal. In classification, our hypothesis is that the reconstruction
error computed for an abnormal machine is larger than that of the a normal
machine, since only normal machine sounds are being used to train the
autoencoder. A threshold is chosen to discriminate between normal and abnormal
machines. However, the threshold changes as surrounding conditions vary. To
select an appropriate threshold irrespective of the surrounding, we propose a
scene classification framework, which can classify the underlying surrounding.
Hence, the threshold can be selected adaptively irrespective of the
surrounding. The experiment evaluation is performed on MIMII dataset for
industrial machines namely fan, pump, valve and slide rail. Our experiment
analysis shows that utilizing adaptive threshold, the performance improves
significantly as that obtained using the fixed threshold computed for a given
surrounding only.
",paper present autoencoder base unsupervised approach identify anomaly industrial machine use sound produce machine propose framework train use log melspectrogram representation sound signal classification hypothesis reconstruction error compute abnormal machine large normal machine since normal machine sound use train autoencoder threshold choose discriminate normal abnormal machine however threshold change surround condition vary select appropriate threshold irrespective surround propose scene classification framework classify underlie surround hence threshold select adaptively irrespective surround experiment evaluation perform mimii dataset industrial machine namely fan pump valve slide rail experiment analysis show utilize adaptive threshold performance improves significantly obtain use fix threshold compute give surround
The State of Sparse Training in Deep Reinforcement Learning,"The use of sparse neural networks has seen rapid growth in recent years,
particularly in computer vision. Their appeal stems largely from the reduced
number of parameters required to train and store, as well as in an increase in
learning efficiency. Somewhat surprisingly, there have been very few efforts
exploring their use in Deep Reinforcement Learning (DRL). In this work we
perform a systematic investigation into applying a number of existing sparse
training techniques on a variety of DRL agents and environments. Our results
corroborate the findings from sparse training in the computer vision domain -
sparse networks perform better than dense networks for the same parameter count
- in the DRL domain. We provide detailed analyses on how the various components
in DRL are affected by the use of sparse networks and conclude by suggesting
promising avenues for improving the effectiveness of sparse training methods,
as well as for advancing their use in DRL.",use sparse neural network see rapid growth recent year particularly computer vision appeal stem largely reduce number parameter require train store well increase learn efficiency somewhat surprisingly effort explore use deep reinforcement learn drl work perform systematic investigation apply number exist sparse training technique variety drl agent environment result corroborate finding sparse train computer vision domain sparse network perform well dense network parameter count drl domain provide detailed analysis various component drl affect use sparse network conclude suggest promising avenue improve effectiveness sparse training method well advance use drl
General Loss Bounds for Universal Sequence Prediction,"  The Bayesian framework is ideally suited for induction problems. The
probability of observing $x_t$ at time $t$, given past observations
$x_1...x_{t-1}$ can be computed with Bayes' rule if the true distribution $\mu$
of the sequences $x_1x_2x_3...$ is known. The problem, however, is that in many
cases one does not even have a reasonable estimate of the true distribution. In
order to overcome this problem a universal distribution $\xi$ is defined as a
weighted sum of distributions $\mu_i\inM$, where $M$ is any countable set of
distributions including $\mu$. This is a generalization of Solomonoff
induction, in which $M$ is the set of all enumerable semi-measures. Systems
which predict $y_t$, given $x_1...x_{t-1}$ and which receive loss $l_{x_t y_t}$
if $x_t$ is the true next symbol of the sequence are considered. It is proven
that using the universal $\xi$ as a prior is nearly as good as using the
unknown true distribution $\mu$. Furthermore, games of chance, defined as a
sequence of bets, observations, and rewards are studied. The time needed to
reach the winning zone is bounded in terms of the relative entropy of $\mu$ and
$\xi$. Extensions to arbitrary alphabets, partial and delayed prediction, and
more active systems are discussed.
",bayesian framework ideally suit induction problem probability observe x_t time give past observation x_1 x t-1 computed baye rule true distribution sequence x_1x_2x_3 know problem however many case one even reasonable estimate true distribution order overcome problem universal distribution define weighted sum distribution countable set distribution include generalization solomonoff induction set enumerable semi measure system predict y_t give x_1 x t-1 receive loss l x_t y_t x_t true next symbol sequence consider prove use universal prior nearly good use unknown true distribution furthermore game chance define sequence bet observation reward study time need reach win zone bound term relative entropy extension arbitrary alphabet partial delay prediction active system discuss
"Does anatomical contextual information improve 3D U-Net based brain
  tumor segmentation?","  Effective, robust, and automatic tools for brain tumor segmentation are
needed for the extraction of information useful in treatment planning from
magnetic resonance (MR) images. Context-aware artificial intelligence is an
emerging concept for the development of deep learning applications for
computer-aided medical image analysis. In this work, it is investigated whether
the addition of contextual information from the brain anatomy in the form of
white matter, gray matter, and cerebrospinal fluid masks and probability maps
improves U-Net-based brain tumor segmentation. The BraTS2020 dataset was used
to train and test two standard 3D U-Net models that, in addition to the
conventional MR image modalities, used the anatomical contextual information as
extra channels in the form of binary masks (CIM) or probability maps (CIP). A
baseline model (BLM) that only used the conventional MR image modalities was
also trained. The impact of adding contextual information was investigated in
terms of overall segmentation accuracy, model training time, domain
generalization, and compensation for fewer MR modalities available for each
subject. Results show that there is no statistically significant difference
when comparing Dice scores between the baseline model and the contextual
information models, even when comparing performances for high- and low-grade
tumors independently. Only in the case of compensation for fewer MR modalities
available for each subject did the addition of anatomical contextual
information significantly improve the segmentation of the whole tumor. Overall,
there is no overall significant improvement in segmentation performance when
using anatomical contextual information in the form of either binary masks or
probability maps as extra channels.
",effective robust automatic tool brain tumor segmentation need extraction information useful treatment plan magnetic resonance mr image context aware artificial intelligence emerge concept development deep learning application computer aid medical image analysis work investigate whether addition contextual information brain anatomy form white matter gray matter cerebrospinal fluid mask probability map improve u net base brain tumor segmentation brats2020 dataset use train test two standard 3d u net model addition conventional mr image modality use anatomical contextual information extra channel form binary mask cim probability maps cip baseline model blm use conventional mr image modality also train impact add contextual information investigate term overall segmentation accuracy model training time domain generalization compensation few mr modality available subject result show statistically significant difference compare dice score baseline model contextual information model even compare performance high- low grade tumor independently case compensation few mr modality available subject addition anatomical contextual information significantly improve segmentation whole tumor overall overall significant improvement segmentation performance use anatomical contextual information form either binary mask probability map extra channel
Decision Making with Side Information and Unbounded Loss Functions,"  We consider the problem of decision-making with side information and
unbounded loss functions. Inspired by probably approximately correct learning
model, we use a slightly different model that incorporates the notion of side
information in a more generic form to make it applicable to a broader class of
applications including parameter estimation and system identification. We
address sufficient conditions for consistent decision-making with exponential
convergence behavior. In this regard, besides a certain condition on the growth
function of the class of loss functions, it suffices that the class of loss
functions be dominated by a measurable function whose exponential Orlicz
expectation is uniformly bounded over the probabilistic model. Decay exponent,
decay constant, and sample complexity are discussed. Example applications to
method of moments, maximum likelihood estimation, and system identification are
illustrated, as well.
",consider problem decision make side information unbounded loss function inspire probably approximately correct learning model use slightly different model incorporate notion side information generic form make applicable broad class application include parameter estimation system identification address sufficient condition consistent decision make exponential convergence behavior regard besides certain condition growth function class loss function suffice class loss function dominate measurable function whose exponential orlicz expectation uniformly bound probabilistic model decay exponent decay constant sample complexity discuss example application method moment maximum likelihood estimation system identification illustrate well
"Determinant-free fermionic wave function using feed-forward neural
  networks","  We propose a general framework for finding the ground state of many-body
fermionic systems by using feed-forward neural networks. The anticommutation
relation for fermions is usually implemented to a variational wave function by
the Slater determinant (or Pfaffian), which is a computational bottleneck
because of the numerical cost of $O(N^3)$ for $N$ particles. We bypass this
bottleneck by explicitly calculating the sign changes associated with particle
exchanges in real space and using fully connected neural networks for
optimizing the rest parts of the wave function. This reduces the computational
cost to $O(N^2)$ or less. We show that the accuracy of the approximation can be
improved by optimizing the ""variance"" of the energy simultaneously with the
energy itself. We also find that a reweighting method in Monte Carlo sampling
can stabilize the calculation. These improvements can be applied to other
approaches based on variational Monte Carlo methods. Moreover, we show that the
accuracy can be further improved by using the symmetry of the system, the
representative states, and an additional neural network implementing a
generalized Gutzwiller-Jastrow factor. We demonstrate the efficiency of the
method by applying it to a two-dimensional Hubbard model.
",propose general framework find ground state many body fermionic system use feed forward neural network anticommutation relation fermion usually implement variational wave function slater determinant pfaffian computational bottleneck numerical cost n particle bypass bottleneck explicitly calculate sign change associate particle exchange real space use fully connect neural network optimize rest part wave function reduce computational cost less show accuracy approximation improve optimize variance energy simultaneously energy also find reweighte method monte carlo sampling stabilize calculation improvement apply approach base variational monte carlo method moreover show accuracy improve use symmetry system representative state additional neural network implement generalize gutzwiller jastrow factor demonstrate efficiency method apply two dimensional hubbard model
Mitigating Sampling Bias and Improving Robustness in Active Learning,"  This paper presents simple and efficient methods to mitigate sampling bias in
active learning while achieving state-of-the-art accuracy and model robustness.
We introduce supervised contrastive active learning by leveraging the
contrastive loss for active learning under a supervised setting. We propose an
unbiased query strategy that selects informative data samples of diverse
feature representations with our methods: supervised contrastive active
learning (SCAL) and deep feature modeling (DFM). We empirically demonstrate our
proposed methods reduce sampling bias, achieve state-of-the-art accuracy and
model calibration in an active learning setup with the query computation 26x
faster than Bayesian active learning by disagreement and 11x faster than
CoreSet. The proposed SCAL method outperforms by a big margin in robustness to
dataset shift and out-of-distribution.
",paper present simple efficient method mitigate sample bias active learning achieve state of the art accuracy model robustness introduce supervise contrastive active learning leverage contrastive loss active learning supervise set propose unbiased query strategy select informative datum sample diverse feature representation method supervise contrastive active learning scal deep feature modeling dfm empirically demonstrate propose method reduce sample bias achieve state of the art accuracy model calibration active learning setup query computation 26x fast bayesian active learning disagreement 11x fast coreset propose scal method outperform big margin robustness dataset shift out of distribution
"NSGANetV2: Evolutionary Multi-Objective Surrogate-Assisted Neural
  Architecture Search","  In this paper, we propose an efficient NAS algorithm for generating
task-specific models that are competitive under multiple competing objectives.
It comprises of two surrogates, one at the architecture level to improve sample
efficiency and one at the weights level, through a supernet, to improve
gradient descent training efficiency. On standard benchmark datasets (C10,
C100, ImageNet), the resulting models, dubbed NSGANetV2, either match or
outperform models from existing approaches with the search being orders of
magnitude more sample efficient. Furthermore, we demonstrate the effectiveness
and versatility of the proposed method on six diverse non-standard datasets,
e.g. STL-10, Flowers102, Oxford Pets, FGVC Aircrafts etc. In all cases,
NSGANetV2s improve the state-of-the-art (under mobile setting), suggesting that
NAS can be a viable alternative to conventional transfer learning approaches in
handling diverse scenarios such as small-scale or fine-grained datasets. Code
is available at https://github.com/mikelzc1990/nsganetv2
",paper propose efficient nas algorithm generate task specific model competitive multiple compete objective comprise two surrogate one architecture level improve sample efficiency one weight level supernet improve gradient descent training efficiency standard benchmark dataset c10 c100 imagenet result model dub nsganetv2 either match outperform model exist approach search order magnitude sample efficient furthermore demonstrate effectiveness versatility propose method six diverse non standard dataset stl-10 flowers102 oxford pet fgvc aircraft etc case nsganetv2s improve state of the art mobile setting suggest nas viable alternative conventional transfer learn approach handle diverse scenario small scale fine grain dataset code available https
Attending to Mathematical Language with Transformers,"  Mathematical expressions were generated, evaluated and used to train neural
network models based on the transformer architecture. The expressions and their
targets were analyzed as a character-level sequence transduction task in which
the encoder and decoder are built on attention mechanisms. Three models were
trained to understand and evaluate symbolic variables and expressions in
mathematics: (1) the self-attentive and feed-forward transformer without
recurrence or convolution, (2) the universal transformer with recurrence, and
(3) the adaptive universal transformer with recurrence and adaptive computation
time. The models respectively achieved test accuracies as high as 76.1%, 78.8%
and 84.9% in evaluating the expressions to match the target values. For the
cases inferred incorrectly, the results differed from the targets by only one
or two characters. The models notably learned to add, subtract and multiply
both positive and negative decimal numbers of variable digits assigned to
symbolic variables.
",mathematical expression generate evaluate use train neural network model base transformer architecture expression target analyze character level sequence transduction task encoder decoder build attention mechanism three model train understand evaluate symbolic variable expression mathematic 1 self attentive feed forward transformer without recurrence convolution 2 universal transformer recurrence 3 adaptive universal transformer recurrence adaptive computation time model respectively achieve test accuracy high evaluating expression match target value case infer incorrectly result differ target one two character model notably learn add subtract multiply positive negative decimal number variable digit assign symbolic variable
"RANK-NOSH: Efficient Predictor-Based Architecture Search via Non-Uniform
  Successive Halving","  Predictor-based algorithms have achieved remarkable performance in the Neural
Architecture Search (NAS) tasks. However, these methods suffer from high
computation costs, as training the performance predictor usually requires
training and evaluating hundreds of architectures from scratch. Previous works
along this line mainly focus on reducing the number of architectures required
to fit the predictor. In this work, we tackle this challenge from a different
perspective - improve search efficiency by cutting down the computation budget
of architecture training. We propose NOn-uniform Successive Halving (NOSH), a
hierarchical scheduling algorithm that terminates the training of
underperforming architectures early to avoid wasting budget. To effectively
leverage the non-uniform supervision signals produced by NOSH, we formulate
predictor-based architecture search as learning to rank with pairwise
comparisons. The resulting method - RANK-NOSH, reduces the search budget by ~5x
while achieving competitive or even better performance than previous
state-of-the-art predictor-based methods on various spaces and datasets.
",predictor base algorithm achieve remarkable performance neural architecture search nas task however method suffer high computation cost training performance predictor usually require training evaluate hundred architecture scratch previous work along line mainly focus reduce number architecture require fit predictor work tackle challenge different perspective improve search efficiency cut computation budget architecture training propose non uniform successive halve nosh hierarchical scheduling algorithm terminate training underperforming architecture early avoid waste budget effectively leverage non uniform supervision signal produce nosh formulate predictor base architecture search learn rank pairwise comparison result method rank nosh reduce search budget achieve competitive even well performance previous state of the art predictor base method various space dataset
"Contextual Sentence Analysis for the Sentiment Prediction on Financial
  Data","  Newsletters and social networks can reflect the opinion about the market and
specific stocks from the perspective of analysts and the general public on
products and/or services provided by a company. Therefore, sentiment analysis
of these texts can provide useful information to help investors trade in the
market. In this paper, a hierarchical stack of Transformers model is proposed
to identify the sentiment associated with companies and stocks, by predicting a
score (of data type real) in a range between -1 and +1. Specifically, we
fine-tuned a RoBERTa model to process headlines and microblogs and combined it
with additional Transformer layers to process the sentence analysis with
sentiment dictionaries to improve the sentiment analysis. We evaluated it on
financial data released by SemEval-2017 task 5 and our proposition outperformed
the best systems of SemEval-2017 task 5 and strong baselines. Indeed, the
combination of contextual sentence analysis with the financial and general
sentiment dictionaries provided useful information to our model and allowed it
to generate more reliable sentiment scores.
",newsletters social network reflect opinion market specific stock perspective analyst general public product service provide company therefore sentiment analysis text provide useful information help investor trade market paper hierarchical stack transformer model propose identify sentiment associate company stock predict score datum type real range -1 specifically fine tuned roberta model process headline microblog combine additional transformer layer process sentence analysis sentiment dictionary improve sentiment analysis evaluate financial datum release semeval-2017 task 5 proposition outperform good system semeval-2017 task 5 strong baseline indeed combination contextual sentence analysis financial general sentiment dictionary provide useful information model allow generate reliable sentiment score
"The Collective Knowledge project: making ML models more portable and
  reproducible with open APIs, reusable best practices and MLOps","  This article provides an overview of the Collective Knowledge technology (CK
or cKnowledge). CK attempts to make it easier to reproduce ML&systems research,
deploy ML models in production, and adapt them to continuously changing data
sets, models, research techniques, software, and hardware. The CK concept is to
decompose complex systems and ad-hoc research projects into reusable
sub-components with unified APIs, CLI, and JSON meta description. Such
components can be connected into portable workflows using DevOps principles
combined with reusable automation actions, software detection plugins, meta
packages, and exposed optimization parameters. CK workflows can automatically
plug in different models, data and tools from different vendors while building,
running and benchmarking research code in a unified way across diverse
platforms and environments. Such workflows also help to perform whole system
optimization, reproduce results, and compare them using public or private
scoreboards on the CK platform (https://cKnowledge.io). For example, the
modular CK approach was successfully validated with industrial partners to
automatically co-design and optimize software, hardware, and machine learning
models for reproducible and efficient object detection in terms of speed,
accuracy, energy, size, and other characteristics. The long-term goal is to
simplify and accelerate the development and deployment of ML models and systems
by helping researchers and practitioners to share and reuse their knowledge,
experience, best practices, artifacts, and techniques using open CK APIs.
",article provide overview collective knowledge technology ck cknowledge ck attempt make easy reproduce ml system research deploy ml model production adapt continuously change data set model research technique software hardware ck concept decompose complex system ad hoc research project reusable sub component unify apis cli json meta description component connect portable workflow use devop principle combine reusable automation action software detection plugin meta package expose optimization parameter ck workflow automatically plug different model datum tool different vendor build run benchmarke research code unify way across diverse platform environment workflow also help perform whole system optimization reproduce result compare use public private scoreboard ck platform https example modular ck approach successfully validate industrial partner automatically co design optimize software hardware machine learning model reproducible efficient object detection term speed accuracy energy size characteristic long term goal simplify accelerate development deployment ml model system help researcher practitioner share reuse knowledge experience good practice artifact technique use open ck apis
"On the Role of Spatial, Spectral, and Temporal Processing for DNN-based Non-linear Multi-channel Speech Enhancement","Employing deep neural networks (DNNs) to directly learn filters for
multi-channel speech enhancement has potentially two key advantages over a
traditional approach combining a linear spatial filter with an independent
tempo-spectral post-filter: 1) non-linear spatial filtering allows to overcome
potential restrictions originating from a linear processing model and 2) joint
processing of spatial and tempo-spectral information allows to exploit
interdependencies between different sources of information. A variety of
DNN-based non-linear filters have been proposed recently, for which good
enhancement performance is reported. However, little is known about the
internal mechanisms which turns network architecture design into a game of
chance. Therefore, in this paper, we perform experiments to better understand
the internal processing of spatial, spectral and temporal information by
DNN-based non-linear filters. On the one hand, our experiments in a difficult
speech extraction scenario confirm the importance of non-linear spatial
filtering, which outperforms an oracle linear spatial filter by 0.24 POLQA
score. On the other hand, we demonstrate that joint processing results in a
large performance gap of 0.4 POLQA score between network architectures
exploiting spectral versus temporal information besides spatial information.",employ deep neural network dnn directly learn filter multi channel speech enhancement potentially two key advantage traditional approach combine linear spatial filter independent tempo spectral post filter 1 non linear spatial filtering allow overcome potential restriction originate linear processing model 2 joint processing spatial tempo spectral information allow exploit interdependency different source information variety dnn base non linear filter propose recently good enhancement performance report however little know internal mechanism turn network architecture design game chance therefore paper perform experiment well understand internal processing spatial spectral temporal information dnn base non linear filter one hand experiment difficult speech extraction scenario confirm importance non linear spatial filtering outperform oracle linear spatial filter polqa score hand demonstrate joint processing result large performance gap polqa score network architecture exploit spectral versus temporal information besides spatial information
"On the Effective Number of Linear Regions in Shallow Univariate ReLU
  Networks: Convergence Guarantees and Implicit Bias","  We study the dynamics and implicit bias of gradient flow (GF) on univariate
ReLU neural networks with a single hidden layer in a binary classification
setting. We show that when the labels are determined by the sign of a target
network with $r$ neurons, with high probability over the initialization of the
network and the sampling of the dataset, GF converges in direction (suitably
defined) to a network achieving perfect training accuracy and having at most
$\mathcal{O}(r)$ linear regions, implying a generalization bound. Our result
may already hold for mild over-parameterization, where the width is
$\tilde{\mathcal{O}}(r)$ and independent of the sample size.
",study dynamic implicit bias gradient flow gf univariate relu neural network single hide layer binary classification set show label determine sign target network r neuron high probability initialization network sample dataset gf converge direction suitably define network achieve perfect training accuracy r linear region imply generalization bind result may already hold mild over parameterization width r independent sample size
A Unified Framework for Sparse Relaxed Regularized Regression: SR3,"  Regularized regression problems are ubiquitous in statistical modeling,
signal processing, and machine learning. Sparse regression in particular has
been instrumental in scientific model discovery, including compressed sensing
applications, variable selection, and high-dimensional analysis. We propose a
broad framework for sparse relaxed regularized regression, called SR3. The key
idea is to solve a relaxation of the regularized problem, which has three
advantages over the state-of-the-art: (1) solutions of the relaxed problem are
superior with respect to errors, false positives, and conditioning, (2)
relaxation allows extremely fast algorithms for both convex and nonconvex
formulations, and (3) the methods apply to composite regularizers such as total
variation (TV) and its nonconvex variants. We demonstrate the advantages of SR3
(computational efficiency, higher accuracy, faster convergence rates, greater
flexibility) across a range of regularized regression problems with synthetic
and real data, including applications in compressed sensing, LASSO, matrix
completion, TV regularization, and group sparsity. To promote reproducible
research, we also provide a companion MATLAB package that implements these
examples.
",regularize regression problem ubiquitous statistical modeling signal processing machine learn sparse regression particular instrumental scientific model discovery include compressed sensing application variable selection high dimensional analysis propose broad framework sparse relaxed regularized regression call sr3 key idea solve relaxation regularize problem three advantage state of the art 1 solution relax problem superior respect error false positive conditioning 2 relaxation allow extremely fast algorithm convex nonconvex formulation 3 method apply composite regularizer total variation tv nonconvex variant demonstrate advantage sr3 computational efficiency high accuracy fast convergence rate great flexibility across range regularize regression problem synthetic real datum include application compress sense lasso matrix completion tv regularization group sparsity promote reproducible research also provide companion matlab package implement example
Gradient descent follows the regularization path for general losses,"  Recent work across many machine learning disciplines has highlighted that
standard descent methods, even without explicit regularization, do not merely
minimize the training error, but also exhibit an implicit bias. This bias is
typically towards a certain regularized solution, and relies upon the details
of the learning process, for instance the use of the cross-entropy loss.
  In this work, we show that for empirical risk minimization over linear
predictors with arbitrary convex, strictly decreasing losses, if the risk does
not attain its infimum, then the gradient-descent path and the
algorithm-independent regularization path converge to the same direction
(whenever either converges to a direction). Using this result, we provide a
justification for the widely-used exponentially-tailed losses (such as the
exponential loss or the logistic loss): while this convergence to a direction
for exponentially-tailed losses is necessarily to the maximum-margin direction,
other losses such as polynomially-tailed losses may induce convergence to a
direction with a poor margin.
",recent work across many machine learn discipline highlight standard descent method even without explicit regularization merely minimize training error also exhibit implicit bias bias typically towards certain regularized solution rely upon detail learn process instance use cross entropy loss work show empirical risk minimization linear predictor arbitrary convex strictly decrease loss risk attain infimum gradient descent path algorithm independent regularization path converge direction whenever either converge direction use result provide justification widely use exponentially tail loss exponential loss logistic loss convergence direction exponentially tail loss necessarily maximum margin direction loss polynomially tail loss may induce convergence direction poor margin
"Finite Basis Physics-Informed Neural Networks (FBPINNs): a scalable
  domain decomposition approach for solving differential equations","  Recently, physics-informed neural networks (PINNs) have offered a powerful
new paradigm for solving problems relating to differential equations. Compared
to classical numerical methods PINNs have several advantages, for example their
ability to provide mesh-free solutions of differential equations and their
ability to carry out forward and inverse modelling within the same optimisation
problem. Whilst promising, a key limitation to date is that PINNs have
struggled to accurately and efficiently solve problems with large domains
and/or multi-scale solutions, which is crucial for their real-world
application. Multiple significant and related factors contribute to this issue,
including the increasing complexity of the underlying PINN optimisation problem
as the problem size grows and the spectral bias of neural networks. In this
work we propose a new, scalable approach for solving large problems relating to
differential equations called Finite Basis PINNs (FBPINNs). FBPINNs are
inspired by classical finite element methods, where the solution of the
differential equation is expressed as the sum of a finite set of basis
functions with compact support. In FBPINNs neural networks are used to learn
these basis functions, which are defined over small, overlapping subdomains.
FBINNs are designed to address the spectral bias of neural networks by using
separate input normalisation over each subdomain, and reduce the complexity of
the underlying optimisation problem by using many smaller neural networks in a
parallel divide-and-conquer approach. Our numerical experiments show that
FBPINNs are effective in solving both small and larger, multi-scale problems,
outperforming standard PINNs in both accuracy and computational resources
required, potentially paving the way to the application of PINNs on large,
real-world problems.
",recently physics inform neural network pinn offer powerful new paradigm solve problem relate differential equation compare classical numerical method pinn several advantage example ability provide mesh free solution differential equation ability carry forward inverse modelling within optimisation problem whilst promise key limitation date pinn struggle accurately efficiently solve problem large domain multi scale solution crucial real world application multiple significant relate factor contribute issue include increase complexity underlie pinn optimisation problem problem size grow spectral bias neural network work propose new scalable approach solve large problem relate differential equation call finite basis pinn fbpinns fbpinn inspire classical finite element method solution differential equation express sum finite set basis function compact support fbpinns neural network use learn basis function define small overlap subdomain fbinn design address spectral bias neural network use separate input normalisation subdomain reduce complexity underlie optimisation problem use many small neural network parallel divide and conquer approach numerical experiment show fbpinns effective solve small large multi scale problem outperform standard pinns accuracy computational resource require potentially pave way application pinn large real world problem
"Decision boundaries and convex hulls in the feature space that deep
  learning functions learn from images","  The success of deep neural networks in image classification and learning can
be partly attributed to the features they extract from images. It is often
speculated about the properties of a low-dimensional manifold that models
extract and learn from images. However, there is not sufficient understanding
about this low-dimensional space based on theory or empirical evidence. For
image classification models, their last hidden layer is the one where images of
each class is separated from other classes and it also has the least number of
features. Here, we develop methods and formulations to study that feature space
for any model. We study the partitioning of the domain in feature space,
identify regions guaranteed to have certain classifications, and investigate
its implications for the pixel space. We observe that geometric arrangements of
decision boundaries in feature space is significantly different compared to
pixel space, providing insights about adversarial vulnerabilities, image
morphing, extrapolation, ambiguity in classification, and the mathematical
understanding of image classification models.
",success deep neural network image classification learn partly attribute feature extract image often speculate property low dimensional manifold model extract learn image however sufficient understand low dimensional space base theory empirical evidence image classification model last hide layer one image class separate class also least number feature develop method formulation study feature space model study partition domain feature space identify region guarantee certain classification investigate implication pixel space observe geometric arrangement decision boundary feature space significantly different compare pixel space provide insight adversarial vulnerability image morph extrapolation ambiguity classification mathematical understand image classification model
Fooling SHAP with Stealthily Biased Sampling,"  SHAP explanations aim at identifying which features contribute the most to
the difference in model prediction at a specific input versus a background
distribution. Recent studies have shown that they can be manipulated by
malicious adversaries to produce arbitrary desired explanations. However,
existing attacks focus solely on altering the black-box model itself. In this
paper, we propose a complementary family of attacks that leave the model intact
and manipulate SHAP explanations using stealthily biased sampling of the data
points used to approximate expectations w.r.t the background distribution. In
the context of fairness audit, we show that our attack can reduce the
importance of a sensitive feature when explaining the difference in outcomes
between groups, while remaining undetected. These results highlight the
manipulability of SHAP explanations and encourage auditors to treat post-hoc
explanations with skepticism.
",shap explanation aim identify feature contribute difference model prediction specific input versus background distribution recent study show manipulate malicious adversary produce arbitrary desire explanation however exist attack focus solely alter black box model paper propose complementary family attack leave model intact manipulate shap explanation use stealthily biased sample datum point use approximate expectation background distribution context fairness audit show attack reduce importance sensitive feature explain difference outcome group remain undetected result highlight manipulability shap explanation encourage auditor treat post hoc explanation skepticism
Bayesian Active Learning for Wearable Stress and Affect Detection,"  In the recent past, psychological stress has been increasingly observed in
humans, and early detection is crucial to prevent health risks. Stress
detection using on-device deep learning algorithms has been on the rise owing
to advancements in pervasive computing. However, an important challenge that
needs to be addressed is handling unlabeled data in real-time via suitable
ground truthing techniques (like Active Learning), which should help establish
affective states (labels) while also selecting only the most informative data
points to query from an oracle. In this paper, we propose a framework with
capabilities to represent model uncertainties through approximations in
Bayesian Neural Networks using Monte-Carlo (MC) Dropout. This is combined with
suitable acquisition functions for active learning. Empirical results on a
popular stress and affect detection dataset experimented on a Raspberry Pi 2
indicate that our proposed framework achieves a considerable efficiency boost
during inference, with a substantially low number of acquired pool points
during active learning across various acquisition functions. Variation Ratios
achieves an accuracy of 90.38% which is comparable to the maximum test accuracy
achieved while training on about 40% lesser data.
",recent past psychological stress increasingly observe human early detection crucial prevent health risk stress detection use on device deep learning algorithm rise owe advancement pervasive computing however important challenge need address handle unlabeled datum real time via suitable ground truthing technique like active learning help establish affective state label also select informative datum point query oracle paper propose framework capability represent model uncertainty approximation bayesian neural network use monte carlo mc dropout combined suitable acquisition function active learn empirical result popular stress affect detection dataset experiment raspberry pi 2 indicate propose framework achieve considerable efficiency boost inference substantially low number acquire pool point active learning across various acquisition function variation ratio achieve accuracy comparable maximum test accuracy achieve train 40 less datum
Inter-Series Attention Model for COVID-19 Forecasting,"  COVID-19 pandemic has an unprecedented impact all over the world since early
2020. During this public health crisis, reliable forecasting of the disease
becomes critical for resource allocation and administrative planning. The
results from compartmental models such as SIR and SEIR are popularly referred
by CDC and news media. With more and more COVID-19 data becoming available, we
examine the following question: Can a direct data-driven approach without
modeling the disease spreading dynamics outperform the well referred
compartmental models and their variants? In this paper, we show the
possibility. It is observed that as COVID-19 spreads at different speed and
scale in different geographic regions, it is highly likely that similar
progression patterns are shared among these regions within different time
periods. This intuition lead us to develop a new neural forecasting model,
called Attention Crossing Time Series (\textbf{ACTS}), that makes forecasts via
comparing patterns across time series obtained from multiple regions. The
attention mechanism originally developed for natural language processing can be
leveraged and generalized to materialize this idea. Among 13 out of 18 testings
including forecasting newly confirmed cases, hospitalizations and deaths,
\textbf{ACTS} outperforms all the leading COVID-19 forecasters highlighted by
CDC.
",covid-19 pandemic unprecedented impact world since early public health crisis reliable forecasting disease become critical resource allocation administrative planning result compartmental model sir seir popularly refer cdc news medium covid-19 datum become available examine follow question direct data drive approach without modeling disease spread dynamic outperform well refer compartmental model variants paper show possibility observe covid-19 spread different speed scale different geographic region highly likely similar progression pattern share among region within different time period intuition lead we develop new neural forecasting model call attention crossing time series act make forecast via compare pattern across time series obtain multiple region attention mechanism originally develop natural language processing leverage generalized materialize idea among 13 18 testing include forecast newly confirm case hospitalization death act outperform lead covid-19 forecaster highlight cdc
"T-SVDNet: Exploring High-Order Prototypical Correlations for
  Multi-Source Domain Adaptation","  Most existing domain adaptation methods focus on adaptation from only one
source domain, however, in practice there are a number of relevant sources that
could be leveraged to help improve performance on target domain. We propose a
novel approach named T-SVDNet to address the task of Multi-source Domain
Adaptation (MDA), which is featured by incorporating Tensor Singular Value
Decomposition (T-SVD) into a neural network's training pipeline. Overall,
high-order correlations among multiple domains and categories are fully
explored so as to better bridge the domain gap. Specifically, we impose
Tensor-Low-Rank (TLR) constraint on a tensor obtained by stacking up a group of
prototypical similarity matrices, aiming at capturing consistent data structure
across different domains. Furthermore, to avoid negative transfer brought by
noisy source data, we propose a novel uncertainty-aware weighting strategy to
adaptively assign weights to different source domains and samples based on the
result of uncertainty estimation. Extensive experiments conducted on public
benchmarks demonstrate the superiority of our model in addressing the task of
MDA compared to state-of-the-art methods.
",exist domain adaptation method focus adaptation one source domain however practice number relevant source could leveraged help improve performance target domain propose novel approach name t svdnet address task multi source domain adaptation mda feature incorporate tensor singular value decomposition t svd neural network training pipeline overall high order correlation among multiple domain category fully explore well bridge domain gap specifically impose tensor low rank tlr constraint tensor obtain stack group prototypical similarity matrix aim capture consistent datum structure across different domain furthermore avoid negative transfer bring noisy source datum propose novel uncertainty aware weighting strategy adaptively assign weight different source domain sample base result uncertainty estimation extensive experiment conduct public benchmark demonstrate superiority model address task mda compare state of the art method
"An Unsupervised Generative Neural Approach for InSAR Phase Filtering and
  Coherence Estimation","  Phase filtering and pixel quality (coherence) estimation is critical in
producing Digital Elevation Models (DEMs) from Interferometric Synthetic
Aperture Radar (InSAR) images, as it removes spatial inconsistencies (residues)
and immensely improves the subsequent unwrapping. Large amount of InSAR data
facilitates Wide Area Monitoring (WAM) over geographical regions. Advances in
parallel computing have accelerated Convolutional Neural Networks (CNNs),
giving them advantages over human performance on visual pattern recognition,
which makes CNNs a good choice for WAM. Nevertheless, this research is largely
unexplored. We thus propose ""GenInSAR"", a CNN-based generative model for joint
phase filtering and coherence estimation, that directly learns the InSAR data
distribution. GenInSAR's unsupervised training on satellite and simulated noisy
InSAR images outperforms other five related methods in total residue reduction
(over 16.5% better on average) with less over-smoothing/artefacts around branch
cuts. GenInSAR's Phase, and Coherence Root-Mean-Squared-Error and Phase Cosine
Error have average improvements of 0.54, 0.07, and 0.05 respectively compared
to the related methods.
",phase filtering pixel quality coherence estimation critical produce digital elevation model dem interferometric synthetic aperture radar insar image remove spatial inconsistency residue immensely improve subsequent unwrapping large amount insar datum facilitate wide area monitor wam geographical region advance parallel computing accelerate convolutional neural network cnn give advantage human performance visual pattern recognition make cnns good choice wam nevertheless research largely unexplored thus propose geninsar cnn base generative model joint phase filtering coherence estimation directly learn insar datum distribution geninsar unsupervised training satellite simulate noisy insar image outperform five relate method total residue reduction well average less around branch cut geninsar phase coherence root mean square error phase cosine error average improvement respectively compare relate method
"MuseMorphose: Full-Song and Fine-Grained Music Style Transfer with One
  Transformer VAE","  Transformers and variational autoencoders (VAE) have been extensively
employed for symbolic (e.g., MIDI) domain music generation. While the former
boast an impressive capability in modeling long sequences, the latter allow
users to willingly exert control over different parts (e.g., bars) of the music
to be generated. In this paper, we are interested in bringing the two together
to construct a single model that exhibits both strengths. The task is split
into two steps. First, we equip Transformer decoders with the ability to accept
segment-level, time-varying conditions during sequence generation.
Subsequently, we combine the developed and tested in-attention decoder with a
Transformer encoder, and train the resulting MuseMorphose model with the VAE
objective to achieve style transfer of long musical pieces, in which users can
specify musical attributes including rhythmic intensity and polyphony (i.e.,
harmonic fullness) they desire, down to the bar level. Experiments show that
MuseMorphose outperforms recurrent neural network (RNN) based baselines on
numerous widely-used metrics for style transfer tasks.
",transformer variational autoencoder vae extensively employ symbolic midi domain music generation former boast impressive capability modeling long sequence latter allow user willingly exert control different part bar music generate paper interested bring two together construct single model exhibit strength task split two step first equip transformer decoder ability accept segment level time vary condition sequence generation subsequently combine develop test in attention decoder transformer encoder train result musemorphose model vae objective achieve style transfer long musical piece user specify musical attribute include rhythmic intensity polyphony harmonic fullness desire bar level experiment show musemorphose outperform recurrent neural network rnn base baseline numerous widely use metric style transfer task
"See, Attend and Brake: An Attention-based Saliency Map Prediction Model
  for End-to-End Driving","  Visual perception is the most critical input for driving decisions. In this
study, our aim is to understand relationship between saliency and driving
decisions. We present a novel attention-based saliency map prediction model for
making braking decisions This approach constructs a holistic model to the
driving task and can be extended for other driving decisions like steering and
acceleration. The proposed model is a deep neural network model that feeds
extracted features from input image to a recurrent neural network with an
attention mechanism. Then predicted saliency map is used to make braking
decision. We trained and evaluated using driving attention dataset BDD-A, and
saliency dataset CAT2000.
",visual perception critical input driving decision study aim understand relationship saliency drive decision present novel attention base saliency map prediction model make braking decision approach construct holistic model drive task extend driving decision like steering acceleration propose model deep neural network model feed extract feature input image recurrent neural network attention mechanism predict saliency map use make brake decision train evaluate use drive attention dataset bdd a saliency dataset cat2000
"PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of
  Generative Models","  The primary aim of single-image super-resolution is to construct
high-resolution (HR) images from corresponding low-resolution (LR) inputs. In
previous approaches, which have generally been supervised, the training
objective typically measures a pixel-wise average distance between the
super-resolved (SR) and HR images. Optimizing such metrics often leads to
blurring, especially in high variance (detailed) regions. We propose an
alternative formulation of the super-resolution problem based on creating
realistic SR images that downscale correctly. We present an algorithm
addressing this problem, PULSE (Photo Upsampling via Latent Space Exploration),
which generates high-resolution, realistic images at resolutions previously
unseen in the literature. It accomplishes this in an entirely self-supervised
fashion and is not confined to a specific degradation operator used during
training, unlike previous methods (which require supervised training on
databases of LR-HR image pairs). Instead of starting with the LR image and
slowly adding detail, PULSE traverses the high-resolution natural image
manifold, searching for images that downscale to the original LR image. This is
formalized through the ""downscaling loss,"" which guides exploration through the
latent space of a generative model. By leveraging properties of
high-dimensional Gaussians, we restrict the search space to guarantee realistic
outputs. PULSE thereby generates super-resolved images that both are realistic
and downscale correctly. We show proof of concept of our approach in the domain
of face super-resolution (i.e., face hallucination). We also present a
discussion of the limitations and biases of the method as currently implemented
with an accompanying model card with relevant metrics. Our method outperforms
state-of-the-art methods in perceptual quality at higher resolutions and scale
factors than previously possible.
",primary aim single image super resolution construct high resolution hr image correspond low resolution lr input previous approach generally supervise training objective typically measure pixel wise average distance super resolved sr hr image optimize metric often lead blur especially high variance detailed region propose alternative formulation super resolution problem base create realistic sr image downscale correctly present algorithm address problem pulse photo upsampling via latent space exploration generate high resolution realistic image resolution previously unseen literature accomplish entirely self supervise fashion confine specific degradation operator use training unlike previous method require supervised training database lr hr image pair instead start lr image slowly add detail pulse traverse high resolution natural image manifold searching image downscale original lr image formalize downscaling loss guide exploration latent space generative model leverage property high dimensional gaussian restrict search space guarantee realistic output pulse thereby generate super resolve image realistic downscale correctly show proof concept approach domain face super resolution face hallucination also present discussion limitation bias method currently implement accompany model card relevant metric method outperform state of the art method perceptual quality high resolution scale factor previously possible
"A Comprehensive Survey and Performance Analysis of Activation Functions
  in Deep Learning","  Neural networks have shown tremendous growth in recent years to solve
numerous problems. Various types of neural networks have been introduced to
deal with different types of problems. However, the main goal of any neural
network is to transform the non-linearly separable input data into more
linearly separable abstract features using a hierarchy of layers. These layers
are combinations of linear and nonlinear functions. The most popular and common
non-linearity layers are activation functions (AFs), such as Logistic Sigmoid,
Tanh, ReLU, ELU, Swish and Mish. In this paper, a comprehensive overview and
survey is presented for AFs in neural networks for deep learning. Different
classes of AFs such as Logistic Sigmoid and Tanh based, ReLU based, ELU based,
and Learning based are covered. Several characteristics of AFs such as output
range, monotonicity, and smoothness are also pointed out. A performance
comparison is also performed among 18 state-of-the-art AFs with different
networks on different types of data. The insights of AFs are presented to
benefit the researchers for doing further research and practitioners to select
among different choices. The code used for experimental comparison is released
at: \url{https://github.com/shivram1987/ActivationFunctions}.
",neural network show tremendous growth recent year solve numerous problem various type neural network introduce deal different type problem however main goal neural network transform non linearly separable input datum linearly separable abstract feature use hierarchy layer layer combination linear nonlinear function popular common non linearity layer activation function afs logistic sigmoid tanh relu elu swish mish paper comprehensive overview survey present afs neural network deep learn different class afs logistic sigmoid tanh base relu base elu base learning base cover several characteristic afs output range monotonicity smoothness also point performance comparison also perform among 18 state of the art afs different network different type data insight afs present benefit researcher research practitioner select among different choice code use experimental comparison release https
"CrossCount: A Deep Learning System for Device-free Human Counting using
  WiFi","  Counting humans is an essential part of many people-centric applications. In
this paper, we propose CrossCount: an accurate deep-learning-based human count
estimator that uses a single WiFi link to estimate the human count in an area
of interest. The main idea is to depend on the temporal link-blockage pattern
as a discriminant feature that is more robust to wireless channel noise than
the signal strength, hence delivering a ubiquitous and accurate human counting
system. As part of its design, CrossCount addresses a number of deep learning
challenges such as class imbalance and training data augmentation for enhancing
the model generalizability. Implementation and evaluation of CrossCount in
multiple testbeds show that it can achieve a human counting accuracy to within
a maximum of 2 persons 100% of the time. This highlights the promise of
CrossCount as a ubiquitous crowd estimator with non-labour-intensive data
collection from off-the-shelf devices.
",count human essential part many people centric application paper propose crosscount accurate deep learn base human count estimator use single wifi link estimate human count area interest main idea depend temporal link blockage pattern discriminant feature robust wireless channel noise signal strength hence deliver ubiquitous accurate human counting system part design crosscount address number deep learning challenge class imbalance training datum augmentation enhance model generalizability implementation evaluation crosscount multiple testbed show achieve human counting accuracy within maximum 2 person 100 time highlight promise crosscount ubiquitous crowd estimator non labour intensive data collection off the shelf device
"Two Stage Transformer Model for COVID-19 Fake News Detection and Fact
  Checking","  The rapid advancement of technology in online communication via social media
platforms has led to a prolific rise in the spread of misinformation and fake
news. Fake news is especially rampant in the current COVID-19 pandemic, leading
to people believing in false and potentially harmful claims and stories.
Detecting fake news quickly can alleviate the spread of panic, chaos and
potential health hazards. We developed a two stage automated pipeline for
COVID-19 fake news detection using state of the art machine learning models for
natural language processing. The first model leverages a novel fact checking
algorithm that retrieves the most relevant facts concerning user claims about
particular COVID-19 claims. The second model verifies the level of truth in the
claim by computing the textual entailment between the claim and the true facts
retrieved from a manually curated COVID-19 dataset. The dataset is based on a
publicly available knowledge source consisting of more than 5000 COVID-19 false
claims and verified explanations, a subset of which was internally annotated
and cross-validated to train and evaluate our models. We evaluate a series of
models based on classical text-based features to more contextual Transformer
based models and observe that a model pipeline based on BERT and ALBERT for the
two stages respectively yields the best results.
",rapid advancement technology online communication via social medium platform lead prolific rise spread misinformation fake news fake news especially rampant current covid-19 pandemic lead people believe false potentially harmful claim story detect fake news quickly alleviate spread panic chaos potential health hazard develop two stage automate pipeline covid-19 fake news detection use state art machine learning model natural language processing first model leverage novel fact check algorithm retrieve relevant fact concern user claim particular covid-19 claim second model verifie level truth claim compute textual entailment claim true fact retrieve manually curate covid-19 dataset dataset base publicly available knowledge source consist 5000 covid-19 false claim verify explanation subset internally annotate cross validated train evaluate model evaluate series model base classical text base feature contextual transformer base model observe model pipeline base bert albert two stage respectively yield good result
Transmodal Analysis of Neural Signals,"  Localizing neuronal activity in the brain, both in time and in space, is a
central challenge to advance the understanding of brain function. Because of
the inability of any single neuroimaging techniques to cover all aspects at
once, there is a growing interest to combine signals from multiple modalities
in order to benefit from the advantages of each acquisition method. Due to the
complexity and unknown parameterization of any suggested complete model of BOLD
response in functional magnetic resonance imaging (fMRI), the development of a
reliable ultimate fusion approach remains difficult. But besides the primary
goal of superior temporal and spatial resolution, conjoint analysis of data
from multiple imaging modalities can alternatively be used to segregate neural
information from physiological and acquisition noise. In this paper we suggest
a novel methodology which relies on constructing a quantifiable mapping of data
from one modality (electroencephalography; EEG) into another (fMRI), called
transmodal analysis of neural signals (TRANSfusion). TRANSfusion attempts to
map neural data embedded within the EEG signal into its reflection in fMRI
data. Assessing the mapping performance on unseen data allows to localize brain
areas where a significant portion of the signal could be reliably
reconstructed, hence the areas neural activity of which is reflected in both
EEG and fMRI data. Consecutive analysis of the learnt model allows to localize
areas associated with specific frequency bands of EEG, or areas functionally
related (connected or coherent) to any given EEG sensor. We demonstrate the
performance of TRANSfusion on artificial and real data from an auditory
experiment. We further speculate on possible alternative uses: cross-modal data
filtering and EEG-driven interpolation of fMRI signals to obtain arbitrarily
high temporal sampling of BOLD.
",localize neuronal activity brain time space central challenge advance understand brain function inability single neuroimage technique cover aspect grow interest combine signal multiple modality order benefit advantage acquisition method due complexity unknown parameterization suggest complete model bold response functional magnetic resonance imaging fmri development reliable ultimate fusion approach remain difficult besides primary goal superior temporal spatial resolution conjoint analysis datum multiple imaging modality alternatively use segregate neural information physiological acquisition noise paper suggest novel methodology rely construct quantifiable mapping datum one modality electroencephalography eeg another fmri call transmodal analysis neural signal transfusion transfusion attempt map neural datum embed within eeg signal reflection fmri datum assess mapping performance unseen datum allow localize brain area significant portion signal could reliably reconstruct hence area neural activity reflect eeg fmri datum consecutive analysis learn model allow localize area associate specific frequency band eeg area functionally relate connect coherent give eeg sensor demonstrate performance transfusion artificial real datum auditory experiment speculate possible alternative use cross modal datum filter eeg drive interpolation fmri signal obtain arbitrarily high temporal sampling bold
Sparsification for Sums of Exponentials and its Algorithmic Applications,"  Many works in signal processing and learning theory operate under the
assumption that the underlying model is simple, e.g. that a signal is
approximately $k$-Fourier-sparse or that a distribution can be approximated by
a mixture model that has at most $k$ components. However the problem of fitting
the parameters of such a model becomes more challenging when the
frequencies/components are too close together.
  In this work we introduce new methods for sparsifying sums of exponentials
and give various algorithmic applications. First we study Fourier-sparse
interpolation without a frequency gap, where Chen et al. gave an algorithm for
finding an $\epsilon$-approximate solution which uses $k' = \mbox{poly}(k, \log
1/\epsilon)$ frequencies. Second, we study learning Gaussian mixture models in
one dimension without a separation condition. Kernel density estimators give an
$\epsilon$-approximation that uses $k' = O(k/\epsilon^2)$ components. These
methods both output models that are much more complex than what we started out
with. We show how to post-process to reduce the number of
frequencies/components down to $k' = \widetilde{O}(k)$, which is optimal up to
logarithmic factors. Moreover we give applications to model selection. In
particular, we give the first algorithms for approximately (and robustly)
determining the number of components in a Gaussian mixture model that work
without a separation condition.
",many work signal processing learning theory operate assumption underlie model simple signal approximately k -fourier sparse distribution approximate mixture model k component however problem fitting parameter model become challenge close together work introduce new method sparsifye sum exponential give various algorithmic application first study fourier sparse interpolation without frequency gap chen et al give algorithm find -approximate solution use k poly k frequency second study learn gaussian mixture model one dimension without separation condition kernel density estimator give -approximation use k component method output model much complex start show post process reduce number k k optimal logarithmic factor moreover give application model selection particular give first algorithm approximately robustly determine number component gaussian mixture model work without separation condition
"A hemodynamic decomposition model for detecting cognitive load using
  functional near-infrared spectroscopy","  In the current paper, we introduce a parametric data-driven model for
functional near-infrared spectroscopy that decomposes a signal into a series of
independent, rescaled, time-shifted, hemodynamic basis functions. Each
decomposed waveform retains relevant biological information about the expected
hemodynamic behavior. The model is also presented along with an efficient
iterative estimation method to improve the computational speed. Our hemodynamic
decomposition model (HDM) extends the canonical model for instances when a) the
external stimuli are unknown, or b) when the assumption of a direct
relationship between the experimental stimuli and the hemodynamic responses
cannot hold. We also argue that the proposed approach can be potentially
adopted as a feature transformation method for machine learning purposes. By
virtue of applying our devised HDM to a cognitive load classification task on
fNIRS signals, we have achieved an accuracy of 86.20%+-2.56% using six channels
in the frontal cortex, and 86.34%+-2.81% utilizing only the AFpz channel also
located in the frontal area. In comparison, state-of-the-art time-spectral
transformations only yield 64.61%+-3.03% and 37.8%+-2.96% under identical
experimental settings.
",current paper introduce parametric data drive model functional near infrared spectroscopy decompose signal series independent rescale time shift hemodynamic basis function decompose waveform retain relevant biological information expect hemodynamic behavior model also present along efficient iterative estimation method improve computational speed hemodynamic decomposition model hdm extend canonical model instance external stimulus unknown b assumption direct relationship experimental stimulus hemodynamic response hold also argue propose approach potentially adopt feature transformation method machine learn purpose virtue applying devise hdm cognitive load classification task fnir signal achieve accuracy use six channel frontal cortex utilize afpz channel also locate frontal area comparison state of the art time spectral transformation yield identical experimental setting
Deep AutoRegressive Networks,"  We introduce a deep, generative autoencoder capable of learning hierarchies
of distributed representations from data. Successive deep stochastic hidden
layers are equipped with autoregressive connections, which enable the model to
be sampled from quickly and exactly via ancestral sampling. We derive an
efficient approximate parameter estimation method based on the minimum
description length (MDL) principle, which can be seen as maximising a
variational lower bound on the log-likelihood, with a feedforward neural
network implementing approximate inference. We demonstrate state-of-the-art
generative performance on a number of classic data sets: several UCI data sets,
MNIST and Atari 2600 games.
",introduce deep generative autoencoder capable learning hierarchy distribute representation datum successive deep stochastic hide layer equip autoregressive connection enable model sample quickly exactly via ancestral sampling derive efficient approximate parameter estimation method base minimum description length mdl principle see maximise variational lower bind log likelihood feedforward neural network implement approximate inference demonstrate state of the art generative performance number classic datum set several uci datum set mnist atari 2600 game
Maximum Density Divergence for Domain Adaptation,"  Unsupervised domain adaptation addresses the problem of transferring
knowledge from a well-labeled source domain to an unlabeled target domain where
the two domains have distinctive data distributions. Thus, the essence of
domain adaptation is to mitigate the distribution divergence between the two
domains. The state-of-the-art methods practice this very idea by either
conducting adversarial training or minimizing a metric which defines the
distribution gaps. In this paper, we propose a new domain adaptation method
named Adversarial Tight Match (ATM) which enjoys the benefits of both
adversarial training and metric learning. Specifically, at first, we propose a
novel distance loss, named Maximum Density Divergence (MDD), to quantify the
distribution divergence. MDD minimizes the inter-domain divergence (""match"" in
ATM) and maximizes the intra-class density (""tight"" in ATM). Then, to address
the equilibrium challenge issue in adversarial domain adaptation, we consider
leveraging the proposed MDD into adversarial domain adaptation framework. At
last, we tailor the proposed MDD as a practical learning loss and report our
ATM. Both empirical evaluation and theoretical analysis are reported to verify
the effectiveness of the proposed method. The experimental results on four
benchmarks, both classical and large-scale, show that our method is able to
achieve new state-of-the-art performance on most evaluations. Codes and
datasets used in this paper are available at {\it github.com/lijin118/ATM}.
",unsupervised domain adaptation address problem transfer knowledge well label source domain unlabeled target domain two domain distinctive data distribution thus essence domain adaptation mitigate distribution divergence two domain state of the art method practice idea either conduct adversarial training minimize metric define distribution gap paper propose new domain adaptation method name adversarial tight match atm enjoy benefit adversarial training metric learning specifically first propose novel distance loss name maximum density divergence mdd quantify distribution divergence mdd minimize inter domain divergence match atm maximize intra class density tight atm address equilibrium challenge issue adversarial domain adaptation consider leverage propose mdd adversarial domain adaptation framework last tailor propose mdd practical learning loss report atm empirical evaluation theoretical analysis report verify effectiveness propose method experimental result four benchmark classical large scale show method able achieve new state of the art performance evaluation code dataset use paper available
"Split and Expand: An inference-time improvement for Weakly Supervised
  Cell Instance Segmentation","  We consider the problem of segmenting cell nuclei instances from Hematoxylin
and Eosin (H&E) stains with weak supervision. While most recent works focus on
improving the segmentation quality, this is usually insufficient for instance
segmentation of cell instances clumped together or with a small size. In this
work, we propose a two-step post-processing procedure, Split and Expand, that
directly improves the conversion of segmentation maps to instances. In the
Split step, we split clumps of cells from the segmentation map into individual
cell instances with the guidance of cell-center predictions through Gaussian
Mixture Model clustering. In the Expand step, we find missing small cells using
the cell-center predictions (which tend to capture small cells more
consistently as they are trained using reliable point annotations), and utilize
Layer-wise Relevance Propagation (LRP) explanation results to expand those
cell-center predictions into cell instances. Our Split and Expand
post-processing procedure is training-free and is executed at inference-time
only. To further improve the performance of our method, a feature re-weighting
loss based on LRP is proposed. We test our procedure on the MoNuSeg and TNBC
datasets and show that our proposed method provides statistically significant
improvements on object-level metrics. Our code will be made available.
",consider problem segment cell nucleus instance hematoxylin eosin h e stain weak supervision recent work focus improve segmentation quality usually insufficient instance segmentation cell instance clump together small size work propose two step post processing procedure split expand directly improve conversion segmentation map instance split step split clump cell segmentation map individual cell instance guidance cell center prediction gaussian mixture model cluster expand step find miss small cell use cell center prediction tend capture small cell consistently train use reliable point annotation utilize layer wise relevance propagation lrp explanation result expand cell center prediction cell instance split expand post processing procedure training free execute inference time improve performance method feature re weighting loss base lrp propose test procedure monuseg tnbc dataset show propose method provide statistically significant improvement object level metric code make available
Self-supervised Adversarial Training,"  Recent work has demonstrated that neural networks are vulnerable to
adversarial examples. To escape from the predicament, many works try to harden
the model in various ways, in which adversarial training is an effective way
which learns robust feature representation so as to resist adversarial attacks.
Meanwhile, the self-supervised learning aims to learn robust and semantic
embedding from data itself. With these views, we introduce self-supervised
learning to against adversarial examples in this paper. Specifically, the
self-supervised representation coupled with k-Nearest Neighbour is proposed for
classification. To further strengthen the defense ability, self-supervised
adversarial training is proposed, which maximizes the mutual information
between the representations of original examples and the corresponding
adversarial examples. Experimental results show that the self-supervised
representation outperforms its supervised version in respect of robustness and
self-supervised adversarial training can further improve the defense ability
efficiently.
",recent work demonstrate neural network vulnerable adversarial example escape predicament many work try harden model various way adversarial training effective way learn robust feature representation resist adversarial attack meanwhile self supervise learning aim learn robust semantic embed datum view introduce self supervise learn adversarial example paper specifically self supervise representation couple k near neighbour propose classification strengthen defense ability self supervise adversarial training propose maximize mutual information representation original example correspond adversarial example experimental result show self supervise representation outperform supervise version respect robustness self supervise adversarial training improve defense ability efficiently
Discretely Indexed Flows,"  In this paper we propose Discretely Indexed flows (DIF) as a new tool for
solving variational estimation problems. Roughly speaking, DIF are built as an
extension of Normalizing Flows (NF), in which the deterministic transport
becomes stochastic, and more precisely discretely indexed. Due to the discrete
nature of the underlying additional latent variable, DIF inherit the good
computational behavior of NF: they benefit from both a tractable density as
well as a straightforward sampling scheme, and can thus be used for the dual
problems of Variational Inference (VI) and of Variational density estimation
(VDE). On the other hand, DIF can also be understood as an extension of mixture
density models, in which the constant mixture weights are replaced by flexible
functions. As a consequence, DIF are better suited for capturing distributions
with discontinuities, sharp edges and fine details, which is a main advantage
of this construction. Finally we propose a methodology for constructiong DIF in
practice, and see that DIF can be sequentially cascaded, and cascaded with NF.
",paper propose discretely index flow dif new tool solve variational estimation problem roughly speak dif build extension normalizing flow nf deterministic transport become stochastic precisely discretely index due discrete nature underlie additional latent variable dif inherit good computational behavior nf benefit tractable density well straightforward sampling scheme thus use dual problem variational inference vi variational density estimation vde hand dif also understand extension mixture density model constant mixture weight replace flexible function consequence dif well suited capturing distribution discontinuity sharp edge fine detail main advantage construction finally propose methodology constructiong dif practice see dif sequentially cascade cascade nf
"NeuSaver: Neural Adaptive Power Consumption Optimization for Mobile
  Video Streaming","  Video streaming services strive to support high-quality videos at higher
resolutions and frame rates to improve the quality of experience (QoE).
However, high-quality videos consume considerable amounts of energy on mobile
devices. This paper proposes NeuSaver, which reduces the power consumption of
mobile devices when streaming videos by applying an adaptive frame rate to each
video chunk without compromising user experience. NeuSaver generates an optimal
policy that determines the appropriate frame rate for each video chunk using
reinforcement learning (RL). The RL model automatically learns the policy that
maximizes the QoE goals based on previous observations. NeuSaver also uses an
asynchronous advantage actor-critic algorithm to reinforce the RL model quickly
and robustly. Streaming servers that support NeuSaver preprocesses videos into
segments with various frame rates, which is similar to the process of creating
videos with multiple bit rates in dynamic adaptive streaming over HTTP.
NeuSaver utilizes the commonly used H.264 video codec. We evaluated NeuSaver in
various experiments and a user study through four video categories along with
the state-of-the-art model. Our experiments showed that NeuSaver effectively
reduces the power consumption of mobile devices when streaming video by an
average of 16.14% and up to 23.12% while achieving high QoE.
",video stream service strive support high quality video high resolution frame rate improve quality experience qoe however high quality video consume considerable amount energy mobile device paper propose neusaver reduce power consumption mobile device stream video apply adaptive frame rate video chunk without compromise user experience neusaver generate optimal policy determine appropriate frame rate video chunk use reinforcement learning rl rl model automatically learn policy maximize qoe goal base previous observation neusaver also use asynchronous advantage actor critic algorithm reinforce rl model quickly robustly stream server support neusaver preprocesse video segment various frame rate similar process create video multiple bit rate dynamic adaptive streaming http neusaver utilize commonly use video codec evaluate neusaver various experiment user study four video category along state of the art model experiment show neusaver effectively reduce power consumption mobile device stream video average achieve high qoe
UAV-Aided Multi-Community Federated Learning,"In this work, we investigate the problem of an online trajectory design for
an Unmanned Aerial Vehicle (UAV) in a Federated Learning (FL) setting where
several different communities exist, each defined by a unique task to be
learned. In this setting, spatially distributed devices belonging to each
community collaboratively contribute towards training their community model via
wireless links provided by the UAV. Accordingly, the UAV acts as a mobile
orchestrator coordinating the transmissions and the learning schedule among the
devices in each community, intending to accelerate the learning process of all
tasks. We propose a heuristic metric as a proxy for the training performance of
the different tasks. Capitalizing on this metric, a surrogate objective is
defined which enables us to jointly optimize the UAV trajectory and the
scheduling of the devices by employing convex optimization techniques and graph
theory. The simulations illustrate the out-performance of our solution when
compared to other handpicked static and mobile UAV deployment baselines.",work investigate problem online trajectory design unmanned aerial vehicle uav federate learn fl set several different community exist define unique task learn set spatially distribute device belong community collaboratively contribute towards training community model via wireless link provide uav accordingly uav act mobile orchestrator coordinating transmission learn schedule among device community intend accelerate learning process task propose heuristic metric proxy training performance different task capitalize metric surrogate objective define enable we jointly optimize uav trajectory scheduling device employ convex optimization technique graph theory simulation illustrate out performance solution compare handpicke static mobile uav deployment baseline
"On the effect of the activation function on the distribution of hidden
  nodes in a deep network","  We analyze the joint probability distribution on the lengths of the vectors
of hidden variables in different layers of a fully connected deep network, when
the weights and biases are chosen randomly according to Gaussian distributions,
and the input is in $\{ -1, 1\}^N$. We show that, if the activation function
$\phi$ satisfies a minimal set of assumptions, satisfied by all activation
functions that we know that are used in practice, then, as the width of the
network gets large, the `length process' converges in probability to a length
map that is determined as a simple function of the variances of the random
weights and biases, and the activation function $\phi$. We also show that this
convergence may fail for $\phi$ that violate our assumptions.
",analyze joint probability distribution length vector hide variable different layer fully connect deep network weight bias choose randomly accord gaussian distribution input -1 show activation function satisfie minimal set assumption satisfied activation function know use practice width network get large length process converge probability length map determine simple function variance random weight bias activation function also show convergence may fail violate assumption
Dynamic Routing Networks,"  The deployment of deep neural networks in real-world applications is mostly
restricted by their high inference costs. Extensive efforts have been made to
improve the accuracy with expert-designed or algorithm-searched architectures.
However, the incremental improvement is typically achieved with increasingly
more expensive models that only a small portion of input instances really need.
Inference with a static architecture that processes all input instances via the
same transformation would thus incur unnecessary computational costs.
Therefore, customizing the model capacity in an instance-aware manner is much
needed for higher inference efficiency. In this paper, we propose Dynamic
Routing Networks (DRNets), which support efficient instance-aware inference by
routing the input instance to only necessary transformation branches selected
from a candidate set of branches for each connection between transformation
nodes. The branch selection is dynamically determined via the corresponding
branch importance weights, which are first generated from lightweight
hypernetworks (RouterNets) and then recalibrated with Gumbel-Softmax before the
selection. Extensive experiments show that DRNets can reduce a substantial
amount of parameter size and FLOPs during inference with prediction performance
comparable to state-of-the-art architectures.
",deployment deep neural network real world application mostly restrict high inference cost extensive effort make improve accuracy expert design algorithm search architecture however incremental improvement typically achieve increasingly expensive model small portion input instance really need inference static architecture process input instance via transformation would thus incur unnecessary computational cost therefore customize model capacity instance aware manner much need high inference efficiency paper propose dynamic routing network drnet support efficient instance aware inference route input instance necessary transformation branch select candidate set branch connection transformation node branch selection dynamically determine via correspond branch importance weight first generate lightweight hypernetwork routernet recalibrate gumbel softmax selection extensive experiment show drnet reduce substantial amount parameter size flop inference prediction performance comparable state of the art architecture
"Channel-wise Hessian Aware trace-Weighted Quantization of Neural
  Networks","  Second-order information has proven to be very effective in determining the
redundancy of neural network weights and activations. Recent paper proposes to
use Hessian traces of weights and activations for mixed-precision quantization
and achieves state-of-the-art results. However, prior works only focus on
selecting bits for each layer while the redundancy of different channels within
a layer also differ a lot. This is mainly because the complexity of determining
bits for each channel is too high for original methods. Here, we introduce
Channel-wise Hessian Aware trace-Weighted Quantization (CW-HAWQ). CW-HAWQ uses
Hessian trace to determine the relative sensitivity order of different channels
of activations and weights. What's more, CW-HAWQ proposes to use deep
Reinforcement learning (DRL) Deep Deterministic Policy Gradient (DDPG)-based
agent to find the optimal ratios of different quantization bits and assign bits
to channels according to the Hessian trace order. The number of states in
CW-HAWQ is much smaller compared with traditional AutoML based mix-precision
methods since we only need to search ratios for the quantization bits. Compare
CW-HAWQ with state-of-the-art shows that we can achieve better results for
multiple networks.
",second order information prove effective determine redundancy neural network weight activation recent paper propose use hessian trace weights activation mixed precision quantization achieve state of the art result however prior work focus select bit layer redundancy different channel within layer also differ lot mainly complexity determine bit channel high original method introduce channel wise hessian aware trace weight quantization cw hawq cw hawq use hessian trace determine relative sensitivity order different channel activation weight cw hawq propose use deep reinforcement learn drl deep deterministic policy gradient ddpg -based agent find optimal ratio different quantization bit assign bit channel accord hessian trace order number state cw hawq much small compare traditional automl base mix precision method since need search ratio quantization bit compare cw hawq state of the art show achieve well result multiple network
Reinforcement Learning for Nested Polar Code Construction,"  In this paper, we model nested polar code construction as a Markov decision
process (MDP), and tackle it with advanced reinforcement learning (RL)
techniques. First, an MDP environment with state, action, and reward is defined
in the context of polar coding. Specifically, a state represents the
construction of an $(N,K)$ polar code, an action specifies its reduction to an
$(N,K-1)$ subcode, and reward is the decoding performance. A neural network
architecture consisting of both policy and value networks is proposed to
generate actions based on the observed states, aiming at maximizing the overall
rewards. A loss function is defined to trade off between exploitation and
exploration. To further improve learning efficiency and quality, an `integrated
learning' paradigm is proposed. It first employs a genetic algorithm to
generate a population of (sub-)optimal polar codes for each $(N,K)$, and then
uses them as prior knowledge to refine the policy in RL. Such a paradigm is
shown to accelerate the training process, and converge at better performances.
Simulation results show that the proposed learning-based polar constructions
achieve comparable, or even better, performances than the state of the art
under successive cancellation list (SCL) decoders. Last but not least, this is
achieved without exploiting any expert knowledge from polar coding theory in
the learning algorithms.
",paper model nest polar code construction markov decision process mdp tackle advanced reinforcement learning rl technique first mdp environment state action reward define context polar coding specifically state represent construction n k polar code action specifie reduction n k-1 subcode reward decode performance neural network architecture consist policy value network propose generate action base observed state aim maximize overall reward loss function define trade exploitation exploration improve learn efficiency quality integrate learning paradigm propose first employ genetic algorithm generate population sub- optimal polar code n k use prior knowledge refine policy rl paradigm show accelerate training process converge well performance simulation result show propose learning base polar construction achieve comparable even well performance state art successive cancellation list scl decoder last least achieve without exploit expert knowledge polar coding theory learn algorithm
Revisiting Multiple Instance Neural Networks,"  Recently neural networks and multiple instance learning are both attractive
topics in Artificial Intelligence related research fields. Deep neural networks
have achieved great success in supervised learning problems, and multiple
instance learning as a typical weakly-supervised learning method is effective
for many applications in computer vision, biometrics, nature language
processing, etc. In this paper, we revisit the problem of solving multiple
instance learning problems using neural networks. Neural networks are appealing
for solving multiple instance learning problem. The multiple instance neural
networks perform multiple instance learning in an end-to-end way, which take a
bag with various number of instances as input and directly output bag label.
All of the parameters in a multiple instance network are able to be optimized
via back-propagation. We propose a new multiple instance neural network to
learn bag representations, which is different from the existing multiple
instance neural networks that focus on estimating instance label. In addition,
recent tricks developed in deep learning have been studied in multiple instance
networks, we find deep supervision is effective for boosting bag classification
accuracy. In the experiments, the proposed multiple instance networks achieve
state-of-the-art or competitive performance on several MIL benchmarks.
Moreover, it is extremely fast for both testing and training, e.g., it takes
only 0.0003 second to predict a bag and a few seconds to train on a MIL
datasets on a moderate CPU.
",recently neural network multiple instance learn attractive topic artificial intelligence relate research field deep neural network achieve great success supervise learning problem multiple instance learn typical weakly supervise learning method effective many application computer vision biometric nature language processing etc paper revisit problem solve multiple instance learn problem use neural network neural network appeal solve multiple instance learn problem multiple instance neural network perform multiple instance learn end to end way take bag various number instance input directly output bag label parameter multiple instance network able optimize via back propagation propose new multiple instance neural network learn bag representation different exist multiple instance neural network focus estimate instance label addition recent trick develop deep learning study multiple instance network find deep supervision effective boost bag classification accuracy experiment propose multiple instance network achieve state of the art competitive performance several mil benchmark moreover extremely fast testing training take second predict bag second train mil dataset moderate cpu
"On Discrimination Discovery and Removal in Ranked Data using Causal
  Graph","  Predictive models learned from historical data are widely used to help
companies and organizations make decisions. However, they may digitally
unfairly treat unwanted groups, raising concerns about fairness and
discrimination. In this paper, we study the fairness-aware ranking problem
which aims to discover discrimination in ranked datasets and reconstruct the
fair ranking. Existing methods in fairness-aware ranking are mainly based on
statistical parity that cannot measure the true discriminatory effect since
discrimination is causal. On the other hand, existing methods in causal-based
anti-discrimination learning focus on classification problems and cannot be
directly applied to handle the ranked data. To address these limitations, we
propose to map the rank position to a continuous score variable that represents
the qualification of the candidates. Then, we build a causal graph that
consists of both the discrete profile attributes and the continuous score. The
path-specific effect technique is extended to the mixed-variable causal graph
to identify both direct and indirect discrimination. The relationship between
the path-specific effects for the ranked data and those for the binary decision
is theoretically analyzed. Finally, algorithms for discovering and removing
discrimination from a ranked dataset are developed. Experiments using the real
dataset show the effectiveness of our approaches.
",predictive model learn historical datum widely use help company organization make decision however may digitally unfairly treat unwanted group raise concern fairness discrimination paper study fairness aware ranking problem aims discover discrimination rank dataset reconstruct fair rank exist method fairness aware ranking mainly base statistical parity measure true discriminatory effect since discrimination causal hand exist method causal base anti discrimination learn focus classification problem directly apply handle rank datum address limitation propose map rank position continuous score variable represent qualification candidate build causal graph consist discrete profile attribute continuous score path specific effect technique extend mixed variable causal graph identify direct indirect discrimination relationship path specific effect rank datum binary decision theoretically analyze finally algorithm discover remove discrimination rank dataset develop experiment use real dataset show effectiveness approach
"Unveiling the semantic structure of text documents using paragraph-aware
  Topic Models","  Classic Topic Models are built under the Bag Of Words assumption, in which
word position is ignored for simplicity. Besides, symmetric priors are
typically used in most applications. In order to easily learn topics with
different properties among the same corpus, we propose a new line of work in
which the paragraph structure is exploited. Our proposal is based on the
following assumption: in many text document corpora there are formal
constraints shared across all the collection, e.g. sections. When this
assumption is satisfied, some paragraphs may be related to general concepts
shared by all documents in the corpus, while others would contain the genuine
description of documents. Assuming each paragraph can be semantically more
general, specific, or hybrid, we look for ways to measure this, transferring
this distinction to topics and being able to learn what we call specific and
general topics. Experiments show that this is a proper methodology to highlight
certain paragraphs in structured documents at the same time we learn
interesting and more diverse topics.
",classic topic model build bag word assumption word position ignore simplicity besides symmetric prior typically use application order easily learn topic different property among corpus propose new line work paragraph structure exploit proposal base follow assumption many text document corpora formal constraint share across collection section assumption satisfied paragraph may relate general concept share document corpus other would contain genuine description document assume paragraph semantically general specific hybrid look way measure transfer distinction topic able learn call specific general topic experiment show proper methodology highlight certain paragraph structure document time learn interesting diverse topic
"Measurement-based Admission Control in Sliced Networks: A Best Arm
  Identification Approach","  In sliced networks, the shared tenancy of slices requires adaptive admission
control of data flows, based on measurements of network resources. In this
paper, we investigate the design of measurement-based admission control
schemes, deciding whether a new data flow can be admitted and in this case, on
which slice. The objective is to devise a joint measurement and decision
strategy that returns a correct decision (e.g., the least loaded slice) with a
certain level of confidence while minimizing the measurement cost (the number
of measurements made before committing to the decision). We study the design of
such strategies for several natural admission criteria specifying what a
correct decision is. For each of these criteria, using tools from best arm
identification in bandits, we first derive an explicit information-theoretical
lower bound on the cost of any algorithm returning the correct decision with
fixed confidence. We then devise a joint measurement and decision strategy
achieving this theoretical limit. We compare empirically the measurement costs
of these strategies, and compare them both to the lower bounds as well as a
naive measurement scheme. We find that our algorithm significantly outperforms
the naive scheme (by a factor $2-8$).
",slice network share tenancy slice require adaptive admission control datum flow base measurement network resource paper investigate design measurement base admission control scheme decide whether new data flow admit case slice objective devise joint measurement decision strategy return correct decision least load slice certain level confidence minimize measurement cost number measurement make commit decision study design strategy several natural admission criterion specify correct decision criterion use tool good arm identification bandit first derive explicit information theoretical lower bind cost algorithm return correct decision fix confidence devise joint measurement decision strategy achieve theoretical limit compare empirically measurement cost strategy compare low bound well naive measurement scheme find algorithm significantly outperform naive scheme factor 2 8
"Should I Stay or Should I Go: Coordinating Biological Needs with
  Continuously-updated Assessments of the Environment","  This paper presents Wanderer, a model of how autonomous adaptive systems
coordinate internal biological needs with moment-by-moment assessments of the
probabilities of events in the external world. The extent to which Wanderer
moves about or explores its environment reflects the relative activations of
two competing motivational sub-systems: one represents the need to acquire
energy and it excites exploration, and the other represents the need to avoid
predators and it inhibits exploration. The environment contains food,
predators, and neutral stimuli. Wanderer responds to these events in a way that
is adaptive in the short turn, and reassesses the probabilities of these events
so that it can modify its long term behaviour appropriately. When food appears,
Wanderer be-comes satiated and exploration temporarily decreases. When a
predator appears, Wanderer both decreases exploration in the short term, and
becomes more ""cautious"" about exploring in the future. Wanderer also forms
associations between neutral features and salient ones (food and predators)
when they are present at the same time, and uses these associations to guide
its behaviour.
",paper present wanderer model autonomous adaptive system coordinate internal biological need moment by moment assessment probability event external world extent wanderer move explore environment reflect relative activation two compete motivational sub system one represent need acquire energy excite exploration represent need avoid predator inhibit exploration environment contain food predator neutral stimulus wanderer respond event way adaptive short turn reassesse probability event modify long term behaviour appropriately food appear wanderer be come satiate exploration temporarily decrease predator appear wanderer decrease exploration short term become cautious explore future wanderer also form association neutral feature salient one food predator present time use association guide behaviour
PROFHIT: Probabilistic Robust Forecasting for Hierarchical Time-series,"  Probabilistic hierarchical time-series forecasting is an important variant of
time-series forecasting, where the goal is to model and forecast multivariate
time-series that have underlying hierarchical relations. Most methods focus on
point predictions and do not provide well-calibrated probabilistic forecasts
distributions. Recent state-of-art probabilistic forecasting methods also
impose hierarchical relations on point predictions and samples of distribution
which does not account for coherency of forecast distributions. Previous works
also silently assume that datasets are always consistent with given
hierarchical relations and do not adapt to real-world datasets that show
deviation from this assumption. We close both these gaps and propose PROFHIT,
which is a fully probabilistic hierarchical forecasting model that jointly
models forecast distribution of entire hierarchy. PROFHIT uses a flexible
probabilistic Bayesian approach and introduces a novel Distributional Coherency
regularization to learn from hierarchical relations for entire forecast
distribution that enables robust and calibrated forecasts as well as adapt to
datasets of varying hierarchical consistency. On evaluating PROFHIT over wide
range of datasets, we observed 41-88% better performance in accuracy and
calibration. Due to modeling the coherency over full distribution, we observed
that PROFHIT can robustly provide reliable forecasts even if up to 10% of input
time-series data is missing where other methods' performance severely degrade
by over 70%.
",probabilistic hierarchical time series forecast important variant time series forecasting goal model forecast multivariate time series underlie hierarchical relation method focus point prediction provide well calibrate probabilistic forecast distribution recent state of art probabilistic forecasting method also impose hierarchical relation point prediction sample distribution account coherency forecast distribution previous work also silently assume dataset always consistent give hierarchical relation adapt real world dataset show deviation assumption close gap propose profhit fully probabilistic hierarchical forecasting model jointly model forecast distribution entire hierarchy profhit use flexible probabilistic bayesian approach introduce novel distributional coherency regularization learn hierarchical relation entire forecast distribution enable robust calibrate forecast well adapt dataset vary hierarchical consistency evaluate profhit wide range dataset observe 41 88 well performance accuracy calibration due modeling coherency full distribution observe profhit robustly provide reliable forecast even 10 input time series datum miss method performance severely degrade 70
"On-line Active Reward Learning for Policy Optimisation in Spoken
  Dialogue Systems","  The ability to compute an accurate reward function is essential for
optimising a dialogue policy via reinforcement learning. In real-world
applications, using explicit user feedback as the reward signal is often
unreliable and costly to collect. This problem can be mitigated if the user's
intent is known in advance or data is available to pre-train a task success
predictor off-line. In practice neither of these apply for most real world
applications. Here we propose an on-line learning framework whereby the
dialogue policy is jointly trained alongside the reward model via active
learning with a Gaussian process model. This Gaussian process operates on a
continuous space dialogue representation generated in an unsupervised fashion
using a recurrent neural network encoder-decoder. The experimental results
demonstrate that the proposed framework is able to significantly reduce data
annotation costs and mitigate noisy user feedback in dialogue policy learning.
",ability compute accurate reward function essential optimising dialogue policy via reinforcement learn real world application use explicit user feedback reward signal often unreliable costly collect problem mitigate user intent know advance datum available pre train task success predictor off line practice neither apply real world application propose on line learning framework whereby dialogue policy jointly train alongside reward model via active learn gaussian process model gaussian process operate continuous space dialogue representation generate unsupervised fashion use recurrent neural network encoder decoder experimental result demonstrate propose framework able significantly reduce data annotation cost mitigate noisy user feedback dialogue policy learning
"Transfer Learning Based Efficient Traffic Prediction with Limited
  Training Data","  Efficient prediction of internet traffic is an essential part of Self
Organizing Network (SON) for ensuring proactive management. There are many
existing solutions for internet traffic prediction with higher accuracy using
deep learning. But designing individual predictive models for each service
provider in the network is challenging due to data heterogeneity, scarcity, and
abnormality. Moreover, the performance of the deep sequence model in network
traffic prediction with limited training data has not been studied extensively
in the current works. In this paper, we investigated and evaluated the
performance of the deep transfer learning technique in traffic prediction with
inadequate historical data leveraging the knowledge of our pre-trained model.
First, we used a comparatively larger real-world traffic dataset for source
domain prediction based on five different deep sequence models: Recurrent
Neural Network (RNN), Long Short-Term Memory (LSTM), LSTM Encoder-Decoder
(LSTM_En_De), LSTM_En_De with Attention layer (LSTM_En_De_Atn), and Gated
Recurrent Unit (GRU). Then, two best-performing models, LSTM_En_De and
LSTM_En_De_Atn, from the source domain with an accuracy of 96.06% and 96.05%
are considered for the target domain prediction. Finally, four smaller traffic
datasets collected for four particular sources and destination pairs are used
in the target domain to compare the performance of the standard learning and
transfer learning in terms of accuracy and execution time. According to our
experimental result, transfer learning helps to reduce the execution time for
most cases, while the model's accuracy is improved in transfer learning with a
larger training session.
",efficient prediction internet traffic essential part self organize network son ensure proactive management many exist solution internet traffic prediction high accuracy use deep learning design individual predictive model service provider network challenge due datum heterogeneity scarcity abnormality moreover performance deep sequence model network traffic prediction limited training datum study extensively current work paper investigate evaluate performance deep transfer learn technique traffic prediction inadequate historical datum leverage knowledge pre trained model first use comparatively large real world traffic dataset source domain prediction base five different deep sequence model recurrent neural network rnn long short term memory lstm lstm encoder decoder lstm_en_de lstm_en_de attention layer lstm_en_de_atn gate recurrent unit gru two well perform model lstm_en_de lstm_en_de_atn source domain accuracy consider target domain prediction finally four small traffic dataset collect four particular source destination pair use target domain compare performance standard learn transfer learn term accuracy execution time accord experimental result transfer learning help reduce execution time case model accuracy improve transfer learn large training session
"Syntactic Question Abstraction and Retrieval for Data-Scarce Semantic
  Parsing","  Deep learning approaches to semantic parsing require a large amount of
labeled data, but annotating complex logical forms is costly. Here, we propose
Syntactic Question Abstraction and Retrieval (SQAR), a method to build a neural
semantic parser that translates a natural language (NL) query to a SQL logical
form (LF) with less than 1,000 annotated examples. SQAR first retrieves a
logical pattern from the train data by computing the similarity between NL
queries and then grounds a lexical information on the retrieved pattern in
order to generate the final LF. We validate SQAR by training models using
various small subsets of WikiSQL train data achieving up to 4.9% higher LF
accuracy compared to the previous state-of-the-art models on WikiSQL test set.
We also show that by using query-similarity to retrieve logical pattern, SQAR
can leverage a paraphrasing dataset achieving up to 5.9% higher LF accuracy
compared to the case where SQAR is trained by using only WikiSQL data. In
contrast to a simple pattern classification approach, SQAR can generate unseen
logical patterns upon the addition of new examples without re-training the
model. We also discuss an ideal way to create cost efficient and robust train
datasets when the data distribution can be approximated under a data-hungry
setting.
",deep learning approach semantic parsing require large amount label datum annotate complex logical form costly propose syntactic question abstraction retrieval sqar method build neural semantic parser translate natural language nl query sql logical form lf less annotated example sqar first retrieve logical pattern train datum computing similarity nl query ground lexical information retrieve pattern order generate final lf validate sqar training model use various small subset wikisql train datum achieve high lf accuracy compare previous state of the art model wikisql test set also show use query similarity retrieve logical pattern sqar leverage paraphrase dataset achieve high lf accuracy compare case sqar train use wikisql datum contrast simple pattern classification approach sqar generate unseen logical pattern upon addition new example without re training model also discuss ideal way create cost efficient robust train dataset datum distribution approximate data hungry setting
"Detecting Anomalies within Time Series using Local Neural
  Transformations","  We develop a new method to detect anomalies within time series, which is
essential in many application domains, reaching from self-driving cars,
finance, and marketing to medical diagnosis and epidemiology. The method is
based on self-supervised deep learning that has played a key role in
facilitating deep anomaly detection on images, where powerful image
transformations are available. However, such transformations are widely
unavailable for time series. Addressing this, we develop Local Neural
Transformations(LNT), a method learning local transformations of time series
from data. The method produces an anomaly score for each time step and thus can
be used to detect anomalies within time series. We prove in a theoretical
analysis that our novel training objective is more suitable for transformation
learning than previous deep Anomaly detection(AD) methods. Our experiments
demonstrate that LNT can find anomalies in speech segments from the LibriSpeech
data set and better detect interruptions to cyber-physical systems than
previous work. Visualization of the learned transformations gives insight into
the type of transformations that LNT learns.
",develop new method detect anomaly within time series essential many application domain reach self drive car finance marketing medical diagnosis epidemiology method base self supervise deep learning play key role facilitate deep anomaly detection image powerful image transformation available however transformation widely unavailable time series address develop local neural transformation lnt method learn local transformation time series datum method produce anomaly score time step thus use detect anomaly within time series prove theoretical analysis novel training objective suitable transformation learn previous deep anomaly detection ad method experiment demonstrate lnt find anomaly speech segment librispeech datum set well detect interruption cyber physical system previous work visualization learn transformation give insight type transformation lnt learn
"Unsupervised Numerical Reasoning to Extract Phenotypes from Clinical
  Text by Leveraging External Knowledge","  Extracting phenotypes from clinical text has been shown to be useful for a
variety of clinical use cases such as identifying patients with rare diseases.
However, reasoning with numerical values remains challenging for phenotyping in
clinical text, for example, temperature 102F representing Fever. Current
state-of-the-art phenotyping models are able to detect general phenotypes, but
perform poorly when they detect phenotypes requiring numerical reasoning. We
present a novel unsupervised methodology leveraging external knowledge and
contextualized word embeddings from ClinicalBERT for numerical reasoning in a
variety of phenotypic contexts. Comparing against unsupervised benchmarks, it
shows a substantial performance improvement with absolute gains on generalized
Recall and F1 scores up to 79% and 71%, respectively. In the supervised
setting, it also surpasses the performance of alternative approaches with
absolute gains on generalized Recall and F1 scores up to 70% and 44%,
respectively.
",extract phenotype clinical text show useful variety clinical use case identify patient rare disease however reason numerical value remains challenge phenotype clinical text example temperature 102f represent fever current state of the art phenotyping model able detect general phenotype perform poorly detect phenotype require numerical reasoning present novel unsupervised methodology leverage external knowledge contextualize word embedding clinicalbert numerical reasoning variety phenotypic context compare unsupervised benchmark show substantial performance improvement absolute gain generalize recall f1 score 79 71 respectively supervise setting also surpass performance alternative approach absolute gain generalize recall f1 score 70 44 respectively
"Comparative Study between Adversarial Networks and Classical Techniques
  for Speech Enhancement","  Speech enhancement is a crucial task for several applications. Among the most
explored techniques are the Wiener filter and the LogMMSE, but approaches
exploring deep learning adapted to this task, such as SEGAN, have presented
relevant results. This study compared the performance of the mentioned
techniques in 85 noise conditions regarding quality, intelligibility, and
distortion; and concluded that classical techniques continue to exhibit
superior results for most scenarios, but, in severe noise scenarios, SEGAN
performed better and with lower variance.
",speech enhancement crucial task several application among explore technique wiener filter logmmse approach explore deep learning adapt task segan present relevant result study compare performance mention technique 85 noise condition regard quality intelligibility distortion conclude classical technique continue exhibit superior result scenario severe noise scenario segan perform well low variance
CoachNet: An Adversarial Sampling Approach for Reinforcement Learning,"  Despite the recent successes of reinforcement learning in games and robotics,
it is yet to become broadly practical. Sample efficiency and unreliable
performance in rare but challenging scenarios are two of the major obstacles.
Drawing inspiration from the effectiveness of deliberate practice for achieving
expert-level human performance, we propose a new adversarial sampling approach
guided by a failure predictor named ""CoachNet"". CoachNet is trained online
along with the agent to predict the probability of failure. This probability is
then used in a stochastic sampling process to guide the agent to more
challenging episodes. This way, instead of wasting time on scenarios that the
agent has already mastered, training is focused on the agent's ""weak spots"". We
present the design of CoachNet, explain its underlying principles, and
empirically demonstrate its effectiveness in improving sample efficiency and
test-time robustness in common continuous control tasks.
",despite recent success reinforcement learning game robotic yet become broadly practical sample efficiency unreliable performance rare challenge scenario two major obstacle draw inspiration effectiveness deliberate practice achieve expert level human performance propose new adversarial sample approach guide failure predictor name coachnet coachnet train online along agent predict probability failure probability use stochastic sampling process guide agent challenge episode way instead waste time scenario agent already master training focus agent weak spot present design coachnet explain underlying principle empirically demonstrate effectiveness improve sample efficiency test time robustness common continuous control task
"On the Tradeoff between Energy, Precision, and Accuracy in Federated
  Quantized Neural Networks","  Deploying federated learning (FL) over wireless networks with
resource-constrained devices requires balancing between accuracy, energy
efficiency, and precision. Prior art on FL often requires devices to train deep
neural networks (DNNs) using a 32-bit precision level for data representation
to improve accuracy. However, such algorithms are impractical for
resource-constrained devices since DNNs could require execution of millions of
operations. Thus, training DNNs with a high precision level incurs a high
energy cost for FL. In this paper, a quantized FL framework, that represents
data with a finite level of precision in both local training and uplink
transmission, is proposed. Here, the finite level of precision is captured
through the use of quantized neural networks (QNNs) that quantize weights and
activations in fixed-precision format. In the considered FL model, each device
trains its QNN and transmits a quantized training result to the base station.
Energy models for the local training and the transmission with the quantization
are rigorously derived. An energy minimization problem is formulated with
respect to the level of precision while ensuring convergence. To solve the
problem, we first analytically derive the FL convergence rate and use a line
search method. Simulation results show that our FL framework can reduce energy
consumption by up to 53% compared to a standard FL model. The results also shed
light on the tradeoff between precision, energy, and accuracy in FL over
wireless networks.
",deploy federated learning fl wireless network resource constrain device require balance accuracy energy efficiency precision prior art fl often require device train deep neural network dnn use 32 bit precision level data representation improve accuracy however algorithm impractical resource constrain device since dnn could require execution million operation thus train dnn high precision level incur high energy cost fl paper quantize fl framework represent datum finite level precision local training uplink transmission propose finite level precision capture use quantize neural network qnn quantize weight activation fix precision format consider fl model device train qnn transmit quantize training result base station energy model local training transmission quantization rigorously derive energy minimization problem formulate respect level precision ensure convergence solve problem first analytically derive fl convergence rate use line search method simulation result show fl framework reduce energy consumption 53 compare standard fl model result also shed light tradeoff precision energy accuracy fl wireless network
"Feature Selection Based on Sparse Neural Network Layer with Normalizing
  Constraints","  Feature selection is important step in machine learning since it has shown to
improve prediction accuracy while depressing the curse of dimensionality of
high dimensional data. The neural networks have experienced tremendous success
in solving many nonlinear learning problems. Here, we propose new
neural-network based feature selection approach that introduces two constrains,
the satisfying of which leads to sparse FS layer. We have performed extensive
experiments on synthetic and real world data to evaluate performance of the
proposed FS. In experiments we focus on the high dimension, low sample size
data since those represent the main challenge for feature selection. The
results confirm that proposed Feature Selection Based on Sparse Neural Network
Layer with Normalizing Constraints (SNEL-FS) is able to select the important
features and yields superior performance compared to other conventional FS
methods.
",feature selection important step machine learning since show improve prediction accuracy depress curse dimensionality high dimensional datum neural network experience tremendous success solve many nonlinear learn problem propose new neural network base feature selection approach introduce two constrain satisfy lead sparse fs layer perform extensive experiment synthetic real world datum evaluate performance propose f experiment focus high dimension low sample size datum since represent main challenge feature selection result confirm propose feature selection base sparse neural network layer normalizing constraints snel fs able select important feature yield superior performance compare conventional fs method
Adaptive MCMC via Combining Local Samplers,"  Markov chain Monte Carlo (MCMC) methods are widely used in machine learning.
One of the major problems with MCMC is the question of how to design chains
that mix fast over the whole state space; in particular, how to select the
parameters of an MCMC algorithm. Here we take a different approach and,
similarly to parallel MCMC methods, instead of trying to find a single chain
that samples from the whole distribution, we combine samples from several
chains run in parallel, each exploring only parts of the state space (e.g., a
few modes only). The chains are prioritized based on kernel Stein discrepancy,
which provides a good measure of performance locally. The samples from the
independent chains are combined using a novel technique for estimating the
probability of different regions of the sample space. Experimental results
demonstrate that the proposed algorithm may provide significant speedups in
different sampling problems. Most importantly, when combined with the
state-of-the-art NUTS algorithm as the base MCMC sampler, our method remained
competitive with NUTS on sampling from unimodal distributions, while
significantly outperforming state-of-the-art competitors on synthetic
multimodal problems as well as on a challenging sensor localization task.
",markov chain monte carlo mcmc method widely use machine learn one major problem mcmc question design chain mix fast whole state space particular select parameter mcmc algorithm take different approach similarly parallel mcmc method instead try find single chain sample whole distribution combine sample several chain run parallel explore part state space mode chain prioritize base kernel stein discrepancy provide good measure performance locally sample independent chain combine use novel technique estimating probability different region sample space experimental result demonstrate propose algorithm may provide significant speedup different sampling problem importantly combine state of the art nut algorithm base mcmc sampler method remain competitive nuts sample unimodal distribution significantly outperform state of the art competitor synthetic multimodal problem well challenge sensor localization task
"Runtime Concurrency Control and Operation Scheduling for High
  Performance Neural Network Training","  Training neural network often uses a machine learning framework such as
TensorFlow and Caffe2. These frameworks employ a dataflow model where the NN
training is modeled as a directed graph composed of a set of nodes. Operations
in neural network training are typically implemented by the frameworks as
primitives and represented as nodes in the dataflow graph. Training NN models
in a dataflow-based machine learning framework involves a large number of
fine-grained operations. Those operations have diverse memory access patterns
and computation intensity. How to manage and schedule those operations is
challenging, because we have to decide the number of threads to run each
operation (concurrency control) and schedule those operations for good hardware
utilization and system throughput.
  In this paper, we extend an existing runtime system (the TensorFlow runtime)
to enable automatic concurrency control and scheduling of operations. We
explore performance modeling to predict the performance of operations with
various thread-level parallelism. Our performance model is highly accurate and
lightweight. Leveraging the performance model, our runtime system employs a set
of scheduling strategies that co-run operations to improve hardware utilization
and system throughput. Our runtime system demonstrates a big performance
benefit. Comparing with using the recommended configurations for concurrency
control and operation scheduling in TensorFlow, our approach achieves 33%
performance (execution time) improvement on average (up to 49%) for three
neural network models, and achieves high performance closing to the optimal one
manually obtained by the user.
",training neural network often use machine learn framework tensorflow caffe2 framework employ dataflow model nn training model direct graph compose set node operation neural network training typically implement framework primitive represent node dataflow graph training nn model dataflow base machine learn framework involve large number fine grain operation operation diverse memory access pattern computation intensity manage schedule operation challenge decide number thread run operation concurrency control schedule operation good hardware utilization system throughput paper extend exist runtime system tensorflow runtime enable automatic concurrency control scheduling operation explore performance modeling predict performance operation various thread level parallelism performance model highly accurate lightweight leverage performance model runtime system employ set scheduling strategy co run operation improve hardware utilization system throughput runtime system demonstrate big performance benefit compare use recommend configuration concurrency control operation scheduling tensorflow approach achieve 33 performance execution time improvement average 49 three neural network model achieve high performance close optimal one manually obtain user
"Resource Management and Security Scheme of ICPSs and IoT Based on VNE
  Algorithm","  The development of Intelligent Cyber-Physical Systems (ICPSs) in virtual
network environment is facing severe challenges. On the one hand, the Internet
of things (IoT) based on ICPSs construction needs a large amount of reasonable
network resources support. On the other hand, ICPSs are facing severe network
security problems. The integration of ICPSs and network virtualization (NV) can
provide more efficient network resource support and security guarantees for IoT
users. Based on the above two problems faced by ICPSs, we propose a virtual
network embedded (VNE) algorithm with computing, storage resources and security
constraints to ensure the rationality and security of resource allocation in
ICPSs. In particular, we use reinforcement learning (RL) method as a means to
improve algorithm performance. We extract the important attribute
characteristics of underlying network as the training environment of RL agent.
Agent can derive the optimal node embedding strategy through training, so as to
meet the requirements of ICPSs for resource management and security. The
embedding of virtual links is based on the breadth first search (BFS) strategy.
Therefore, this is a comprehensive two-stage RL-VNE algorithm considering the
constraints of computing, storage and security three-dimensional resources.
Finally, we design a large number of simulation experiments from the
perspective of typical indicators of VNE algorithms. The experimental results
effectively illustrate the effectiveness of the algorithm in the application of
ICPSs.
",development intelligent cyber physical system icpss virtual network environment face severe challenge one hand internet thing iot base icpss construction need large amount reasonable network resource support hand icpss face severe network security problem integration icpss network virtualization nv provide efficient network resource support security guarantee iot user base two problem face icpss propose virtual network embed vne algorithm computing storage resource security constraint ensure rationality security resource allocation icpss particular use reinforcement learning rl method mean improve algorithm performance extract important attribute characteristic underlie network training environment rl agent agent derive optimal node embed strategy training meet requirement icpss resource management security embed virtual link base breadth first search bfs strategy therefore comprehensive two stage rl vne algorithm consider constraint compute storage security three dimensional resource finally design large number simulation experiment perspective typical indicator vne algorithm experimental result effectively illustrate effectiveness algorithm application icpss
"Challenges and Opportunities in Rapid Epidemic Information Propagation
  with Live Knowledge Aggregation from Social Media","  A rapidly evolving situation such as the COVID-19 pandemic is a significant
challenge for AI/ML models because of its unpredictability. %The most reliable
indicator of the pandemic spreading has been the number of test positive cases.
However, the tests are both incomplete (due to untested asymptomatic cases) and
late (due the lag from the initial contact event, worsening symptoms, and test
results). Social media can complement physical test data due to faster and
higher coverage, but they present a different challenge: significant amounts of
noise, misinformation and disinformation. We believe that social media can
become good indicators of pandemic, provided two conditions are met. The first
(True Novelty) is the capture of new, previously unknown, information from
unpredictably evolving situations. The second (Fact vs. Fiction) is the
distinction of verifiable facts from misinformation and disinformation. Social
media information that satisfy those two conditions are called live knowledge.
We apply evidence-based knowledge acquisition (EBKA) approach to collect,
filter, and update live knowledge through the integration of social media
sources with authoritative sources. Although limited in quantity, the reliable
training data from authoritative sources enable the filtering of misinformation
as well as capturing truly new information. We describe the EDNA/LITMUS tools
that implement EBKA, integrating social media such as Twitter and Facebook with
authoritative sources such as WHO and CDC, creating and updating live knowledge
on the COVID-19 pandemic.
",rapidly evolve situation covid-19 pandemic significant challenge model unpredictability reliable indicator pandemic spread number test positive case however test incomplete due untested asymptomatic case late due lag initial contact event worsen symptom test result social medium complement physical test datum due fast high coverage present different challenge significant amount noise misinformation disinformation believe social medium become good indicator pandemic provide two condition meet first true novelty capture new previously unknown information unpredictably evolve situation second fact fiction distinction verifiable fact misinformation disinformation social media information satisfy two condition call live knowledge apply evidence base knowledge acquisition ebka approach collect filter update live knowledge integration social medium source authoritative source although limit quantity reliable training datum authoritative source enable filtering misinformation well capture truly new information describe tool implement ebka integrate social medium twitter facebook authoritative source cdc create update live knowledge covid-19 pandemic
"SNeCT: Scalable network constrained Tucker decomposition for integrative
  multi-platform data analysis","  Motivation: How do we integratively analyze large-scale multi-platform
genomic data that are high dimensional and sparse? Furthermore, how can we
incorporate prior knowledge, such as the association between genes, in the
analysis systematically? Method: To solve this problem, we propose a Scalable
Network Constrained Tucker decomposition method we call SNeCT. SNeCT adopts
parallel stochastic gradient descent approach on the proposed parallelizable
network constrained optimization function. SNeCT decomposition is applied to
tensor constructed from large scale multi-platform multi-cohort cancer data,
PanCan12, constrained on a network built from PathwayCommons database. Results:
The decomposed factor matrices are applied to stratify cancers, to search for
top-k similar patients, and to illustrate how the matrices can be used for
personalized interpretation. In the stratification test, combined twelve-cohort
data is clustered to form thirteen subclasses. The thirteen subclasses have a
high correlation to tissue of origin in addition to other interesting
observations, such as clear separation of OV cancers to two groups, and high
clinical correlation within subclusters formed in cohorts BRCA and UCEC. In the
top-k search, a new patient's genomic profile is generated and searched against
existing patients based on the factor matrices. The similarity of the top-k
patient to the query is high for 23 clinical features, including
estrogen/progesterone receptor statuses of BRCA patients with average precision
value ranges from 0.72 to 0.86 and from 0.68 to 0.86, respectively. We also
provide an illustration of how the factor matrices can be used for
interpretable personalized analysis of each patient.
",motivation integratively analyze large scale multi platform genomic datum high dimensional sparse furthermore incorporate prior knowledge association gene analysis systematically method solve problem propose scalable network constrain tucker decomposition method call snect snect adopt parallel stochastic gradient descent approach propose parallelizable network constrain optimization function snect decomposition apply tensor construct large scale multi platform multi cohort cancer datum pancan12 constrain network build pathwaycommon database result decompose factor matrix apply stratify cancer search top k similar patient illustrate matrix use personalized interpretation stratification test combine twelve cohort datum cluster form thirteen subclass thirteen subclass high correlation tissue origin addition interesting observation clear separation ov cancer two group high clinical correlation within subcluster form cohort brca ucec top k search new patient genomic profile generate search exist patient base factor matrix similarity top k patient query high 23 clinical feature include receptor status brca patient average precision value range respectively also provide illustration factor matrix use interpretable personalize analysis patient
"Temporal Autoencoder with U-Net Style Skip-Connections for Frame
  Prediction","  Finding sustainable and novel solutions to predict city-wide mobility
behaviour is an ever-growing problem given increased urban complexity and
growing populations. This paper seeks to address this by describing a traffic
frame prediction approach that uses Convolutional LSTMs to create a Temporal
Autoencoder with U-Net style skip-connections that marry together recurrent and
traditional computer vision techniques to capture spatio-temporal dependencies
at different scales without losing topological details of a given city.
Utilisation of Cyclical Learning Rates is also presented, improving training
efficiency by achieving lower loss scores in fewer epochs than standard
approaches.
",find sustainable novel solution predict city wide mobility behaviour ever grow problem give increase urban complexity growing population paper seek address describe traffic frame prediction approach use convolutional lstms create temporal autoencoder u net style skip connection marry together recurrent traditional computer vision technique capture spatio temporal dependency different scale without lose topological detail give city utilisation cyclical learning rate also present improve training efficiency achieve low loss score few epoch standard approach
"Bellman Error Based Feature Generation using Random Projections on
  Sparse Spaces","  We address the problem of automatic generation of features for value function
approximation. Bellman Error Basis Functions (BEBFs) have been shown to improve
the error of policy evaluation with function approximation, with a convergence
rate similar to that of value iteration. We propose a simple, fast and robust
algorithm based on random projections to generate BEBFs for sparse feature
spaces. We provide a finite sample analysis of the proposed method, and prove
that projections logarithmic in the dimension of the original space are enough
to guarantee contraction in the error. Empirical results demonstrate the
strength of this method.
",address problem automatic generation feature value function approximation bellman error basis function bebfs show improve error policy evaluation function approximation convergence rate similar value iteration propose simple fast robust algorithm base random projection generate bebfs sparse feature space provide finite sample analysis propose method prove projection logarithmic dimension original space enough guarantee contraction error empirical result demonstrate strength method
"Improving Video Instance Segmentation by Light-weight Temporal
  Uncertainty Estimates","  Instance segmentation with neural networks is an essential task in
environment perception. In many works, it has been observed that neural
networks can predict false positive instances with high confidence values and
true positives with low ones. Thus, it is important to accurately model the
uncertainties of neural networks in order to prevent safety issues and foster
interpretability. In applications such as automated driving, the reliability of
neural networks is of highest interest. In this paper, we present a
time-dynamic approach to model uncertainties of instance segmentation networks
and apply this to the detection of false positives as well as the estimation of
prediction quality. The availability of image sequences in online applications
allows for tracking instances over multiple frames. Based on an instances
history of shape and uncertainty information, we construct temporal
instance-wise aggregated metrics. The latter are used as input to
post-processing models that estimate the prediction quality in terms of
instance-wise intersection over union. The proposed method only requires a
readily trained neural network (that may operate on single frames) and video
sequence input. In our experiments, we further demonstrate the use of the
proposed method by replacing the traditional score value from object detection
and thereby improving the overall performance of the instance segmentation
network.
",instance segmentation neural network essential task environment perception many work observe neural network predict false positive instance high confidence value true positive low one thus important accurately model uncertainty neural network order prevent safety issue foster interpretability application automate driving reliability neural network high interest paper present time dynamic approach model uncertaintie instance segmentation network apply detection false positive well estimation prediction quality availability image sequence online application allow tracking instance multiple frame base instance history shape uncertainty information construct temporal instance wise aggregate metric latter use input post processing model estimate prediction quality term instance wise intersection union propose method require readily train neural network may operate single frame video sequence input experiment demonstrate use propose method replace traditional score value object detection thereby improve overall performance instance segmentation network
"Learning to Discretize: Solving 1D Scalar Conservation Laws via Deep
  Reinforcement Learning","  Conservation laws are considered to be fundamental laws of nature. It has
broad applications in many fields, including physics, chemistry, biology,
geology, and engineering. Solving the differential equations associated with
conservation laws is a major branch in computational mathematics. The recent
success of machine learning, especially deep learning in areas such as computer
vision and natural language processing, has attracted a lot of attention from
the community of computational mathematics and inspired many intriguing works
in combining machine learning with traditional methods. In this paper, we are
the first to view numerical PDE solvers as an MDP and to use (deep) RL to learn
new solvers. As proof of concept, we focus on 1-dimensional scalar conservation
laws. We deploy the machinery of deep reinforcement learning to train a policy
network that can decide on how the numerical solutions should be approximated
in a sequential and spatial-temporal adaptive manner. We will show that the
problem of solving conservation laws can be naturally viewed as a sequential
decision-making process, and the numerical schemes learned in such a way can
easily enforce long-term accuracy. Furthermore, the learned policy network is
carefully designed to determine a good local discrete approximation based on
the current state of the solution, which essentially makes the proposed method
a meta-learning approach. In other words, the proposed method is capable of
learning how to discretize for a given situation mimicking human experts.
Finally, we will provide details on how the policy network is trained, how well
it performs compared with some state-of-the-art numerical solvers such as WENO
schemes, and supervised learning based approach L3D and PINN, and how well it
generalizes.
",conservation law consider fundamental law nature broad application many field include physics chemistry biology geology engineering solve differential equation associate conservation law major branch computational mathematic recent success machine learn especially deep learning area computer vision natural language processing attract lot attention community computational mathematic inspire many intriguing work combine machine learn traditional method paper first view numerical pde solver mdp use deep rl learn new solver proof concept focus 1 dimensional scalar conservation law deploy machinery deep reinforcement learn train policy network decide numerical solution approximate sequential spatial temporal adaptive manner show problem solve conservation law naturally view sequential decision make process numerical scheme learn way easily enforce long term accuracy furthermore learn policy network carefully design determine good local discrete approximation base current state solution essentially make propose method meta learn approach word propose method capable learning discretize give situation mimic human expert finally provide detail policy network train well perform compare state of the art numerical solver weno scheme supervise learning base approach l3d pinn well generalize
"Pedestrian Detection with Spatially Pooled Features and Structured
  Ensemble Learning","  Many typical applications of object detection operate within a prescribed
false-positive range. In this situation the performance of a detector should be
assessed on the basis of the area under the ROC curve over that range, rather
than over the full curve, as the performance outside the range is irrelevant.
This measure is labelled as the partial area under the ROC curve (pAUC). We
propose a novel ensemble learning method which achieves a maximal detection
rate at a user-defined range of false positive rates by directly optimizing the
partial AUC using structured learning.
  In order to achieve a high object detection performance, we propose a new
approach to extract low-level visual features based on spatial pooling.
Incorporating spatial pooling improves the translational invariance and thus
the robustness of the detection process. Experimental results on both synthetic
and real-world data sets demonstrate the effectiveness of our approach, and we
show that it is possible to train state-of-the-art pedestrian detectors using
the proposed structured ensemble learning method with spatially pooled
features. The result is the current best reported performance on the
Caltech-USA pedestrian detection dataset.
",many typical application object detection operate within prescribe false positive range situation performance detector assess basis area roc curve range rather full curve performance outside range irrelevant measure label partial area roc curve pauc propose novel ensemble learning method achieve maximal detection rate user define range false positive rate directly optimize partial auc use structured learning order achieve high object detection performance propose new approach extract low level visual feature base spatial pooling incorporate spatial pooling improve translational invariance thus robustness detection process experimental result synthetic real world datum set demonstrate effectiveness approach show possible train state of the art pedestrian detector use propose structure ensemble learning method spatially pool feature result current well report performance caltech usa pedestrian detection dataset
"Confident Clustering via PCA Compression Ratio and Its Application to
  Single-cell RNA-seq Analysis","  Unsupervised clustering algorithms for vectors has been widely used in the
area of machine learning. Many applications, including the biological data we
studied in this paper, contain some boundary datapoints which show combination
properties of two underlying clusters and could lower the performance of the
traditional clustering algorithms. We develop a confident clustering method
aiming to diminish the influence of these datapoints and improve the clustering
results. Concretely, for a list of datapoints, we give two clustering results.
The first-round clustering attempts to classify only pure vectors with high
confidence. Based on it, we classify more vectors with less confidence in the
second round. We validate our algorithm on single-cell RNA-seq data, which is a
powerful and widely used tool in biology area. Our confident clustering shows a
high accuracy on our tested datasets. In addition, unlike traditional
clustering methods in single-cell analysis, the confident clustering shows high
stability under different choices of parameters.
",unsupervised clustering algorithm vector widely use area machine learn many application include biological datum study paper contain boundary datapoint show combination property two underlying cluster could lower performance traditional clustering algorithm develop confident clustering method aim diminish influence datapoint improve cluster result concretely list datapoint give two clustering result first round cluster attempt classify pure vector high confidence base classify vector less confidence second round validate algorithm single cell rna seq datum powerful widely use tool biology area confident clustering show high accuracy test dataset addition unlike traditional clustering method single cell analysis confident clustering show high stability different choice parameter
Writer Independent Offline Signature Recognition Using Ensemble Learning,"  The area of Handwritten Signature Verification has been broadly researched in
the last decades, but remains an open research problem. In offline (static)
signature verification, the dynamic information of the signature writing
process is lost, and it is difficult to design good feature extractors that can
distinguish genuine signatures and skilled forgeries. This verification task is
even harder in writer independent scenarios which is undeniably fiscal for
realistic cases. In this paper, we have proposed an Ensemble model for offline
writer, independent signature verification task with Deep learning. We have
used two CNNs for feature extraction, after that RGBT for classification &
Stacking to generate final prediction vector. We have done extensive
experiments on various datasets from various sources to maintain a variance in
the dataset. We have achieved the state of the art performance on various
datasets.
",area handwritten signature verification broadly research last decade remain open research problem offline static signature verification dynamic information signature writing process lose difficult design good feature extractor distinguish genuine signature skilled forgery verification task even hard writer independent scenario undeniably fiscal realistic case paper propose ensemble model offline writer independent signature verification task deep learning use two cnn feature extraction rgbt classification stack generate final prediction vector do extensive experiment various dataset various source maintain variance dataset achieve state art performance various dataset
"Probabilistic Combination of Classifier and Cluster Ensembles for
  Non-transductive Learning","  Unsupervised models can provide supplementary soft constraints to help
classify new target data under the assumption that similar objects in the
target set are more likely to share the same class label. Such models can also
help detect possible differences between training and target distributions,
which is useful in applications where concept drift may take place. This paper
describes a Bayesian framework that takes as input class labels from existing
classifiers (designed based on labeled data from the source domain), as well as
cluster labels from a cluster ensemble operating solely on the target data to
be classified, and yields a consensus labeling of the target data. This
framework is particularly useful when the statistics of the target data drift
or change from those of the training data. We also show that the proposed
framework is privacy-aware and allows performing distributed learning when
data/models have sharing restrictions. Experiments show that our framework can
yield superior results to those provided by applying classifier ensembles only.
",unsupervised model provide supplementary soft constraint help classify new target datum assumption similar object target set likely share class label model also help detect possible difference training target distribution useful application concept drift may take place paper describe bayesian framework take input class label exist classifier design base label datum source domain well cluster label cluster ensemble operate solely target datum classify yield consensus labeling target datum framework particularly useful statistic target datum drift change training datum also show propose framework privacy aware allow perform distribute learning share restriction experiment show framework yield superior result provide apply classifier ensemble
ML-LBM: Machine Learning Aided Flow Simulation in Porous Media,"  Simulation of fluid flow in porous media has many applications, from the
micro-scale (cell membranes, filters, rocks) to macro-scale (groundwater,
hydrocarbon reservoirs, and geothermal) and beyond. Direct simulation of flow
in porous media requires significant computational resources to solve within
reasonable timeframes. An integrated method combining predictions of fluid flow
(fast, limited accuracy) with direct flow simulation (slow, high accuracy) is
outlined. In the tortuous flow paths of porous media, Deep Learning techniques
based on Convolutional Neural Networks (CNNs) are shown to give an accurate
estimate of the steady state velocity fields (in all axes), and by extension,
the macro-scale permeability. This estimate can be used as-is, or as initial
conditions in direct simulation to reach a fully accurate result in a fraction
of the compute time. A Gated U-Net Convolutional Neural Network is trained on a
datasets of 2D and 3D porous media generated by correlated fields, with their
steady state velocity fields calculated from direct LBM simulation. Sensitivity
analysis indicates that network accuracy is dependent on (1) the tortuosity of
the domain, (2) the size of convolution filters, (3) the use of distance maps
as input, (4) the use of mass conservation loss functions. Permeability
estimation from these predicted fields reaches over 90\% accuracy for 80\% of
cases. It is further shown that these velocity fields are error prone when used
for solute transport simulation. Using the predicted velocity fields as initial
conditions is shown to accelerate direct flow simulation to physically true
steady state conditions an order of magnitude less compute time. Using Deep
Learning predictions (or potentially any other approximation method) to
accelerate flow simulation to steady state in complex pore structures shows
promise as a technique push the boundaries fluid flow modelling.
",simulation fluid flow porous medium many application micro scale cell membrane filter rock macro scale groundwater hydrocarbon reservoir geothermal beyond direct simulation flow porous medium require significant computational resource solve within reasonable timeframe integrate method combine prediction fluid flow fast limited accuracy direct flow simulation slow high accuracy outline tortuous flow path porous medium deep learning technique base convolutional neural network cnn show give accurate estimate steady state velocity field axis extension macro scale permeability estimate use as be initial condition direct simulation reach fully accurate result fraction compute time gate u net convolutional neural network train dataset 2d 3d porous medium generate correlate field steady state velocity field calculate direct lbm simulation sensitivity analysis indicate network accuracy dependent 1 tortuosity domain 2 size convolution filter 3 use distance map input 4 use mass conservation loss function permeability estimation predict field reach accuracy case show velocity field error prone use solute transport simulation use predict velocity field initial condition show accelerate direct flow simulation physically true steady state condition order magnitude less compute time use deep learning prediction potentially approximation method accelerate flow simulation steady state complex pore structure show promise technique push boundary fluid flow modelling
Incorporating Voice Instructions in Model-Based Reinforcement Learning for Self-Driving Cars,"This paper presents a novel approach that supports natural language voice
instructions to guide deep reinforcement learning (DRL) algorithms when
training self-driving cars. DRL methods are popular approaches for autonomous
vehicle (AV) agents. However, most existing methods are sample- and
time-inefficient and lack a natural communication channel with the human
expert. In this paper, how new human drivers learn from human coaches motivates
us to study new ways of human-in-the-loop learning and a more natural and
approachable training interface for the agents. We propose incorporating
natural language voice instructions (NLI) in model-based deep reinforcement
learning to train self-driving cars. We evaluate the proposed method together
with a few state-of-the-art DRL methods in the CARLA simulator. The results
show that NLI can help ease the training process and significantly boost the
agents' learning speed.",paper present novel approach support natural language voice instruction guide deep reinforcement learn drl algorithm train self drive car drl method popular approach autonomous vehicle av agent however exist method sample- time inefficient lack natural communication channel human expert paper new human driver learn human coach motivate we study new way human in the loop learn natural approachable training interface agent propose incorporate natural language voice instruction nli model base deep reinforcement learning train self drive car evaluate propose method together state of the art drl method carla simulator result show nli help ease training process significantly boost agent learn speed
"SHARKS: Smart Hacking Approaches for RisK Scanning in Internet-of-Things
  and Cyber-Physical Systems based on Machine Learning","  Cyber-physical systems (CPS) and Internet-of-Things (IoT) devices are
increasingly being deployed across multiple functionalities, ranging from
healthcare devices and wearables to critical infrastructures, e.g., nuclear
power plants, autonomous vehicles, smart cities, and smart homes. These devices
are inherently not secure across their comprehensive software, hardware, and
network stacks, thus presenting a large attack surface that can be exploited by
hackers. In this article, we present an innovative technique for detecting
unknown system vulnerabilities, managing these vulnerabilities, and improving
incident response when such vulnerabilities are exploited. The novelty of this
approach lies in extracting intelligence from known real-world CPS/IoT attacks,
representing them in the form of regular expressions, and employing machine
learning (ML) techniques on this ensemble of regular expressions to generate
new attack vectors and security vulnerabilities. Our results show that 10 new
attack vectors and 122 new vulnerability exploits can be successfully generated
that have the potential to exploit a CPS or an IoT ecosystem. The ML
methodology achieves an accuracy of 97.4% and enables us to predict these
attacks efficiently with an 87.2% reduction in the search space. We demonstrate
the application of our method to the hacking of the in-vehicle network of a
connected car. To defend against the known attacks and possible novel exploits,
we discuss a defense-in-depth mechanism for various classes of attacks and the
classification of data targeted by such attacks. This defense mechanism
optimizes the cost of security measures based on the sensitivity of the
protected resource, thus incentivizing its adoption in real-world CPS/IoT by
cybersecurity practitioners.
",cyber physical system cp internet of thing iot device increasingly deploy across multiple functionality range healthcare device wearable critical infrastructure nuclear power plant autonomous vehicle smart city smart home device inherently secure across comprehensive software hardware network stack thus present large attack surface exploit hacker article present innovative technique detect unknown system vulnerability manage vulnerability improve incident response vulnerability exploit novelty approach lie extract intelligence know real world attack represent form regular expression employ machine learn ml technique ensemble regular expression generate new attack vector security vulnerability result show 10 new attack vector 122 new vulnerability exploit successfully generate potential exploit cp iot ecosystem ml methodology achieve accuracy enable we predict attack efficiently reduction search space demonstrate application method hack in vehicle network connect car defend know attack possible novel exploit discuss defense in depth mechanism various class attack classification datum target attack defense mechanism optimize cost security measure base sensitivity protect resource thus incentivize adoption real world cybersecurity practitioner
"IconQA: A New Benchmark for Abstract Diagram Understanding and Visual
  Language Reasoning","  Current visual question answering (VQA) tasks mainly consider answering
human-annotated questions for natural images. However, aside from natural
images, abstract diagrams with semantic richness are still understudied in
visual understanding and reasoning research. In this work, we introduce a new
challenge of Icon Question Answering (IconQA) with the goal of answering a
question in an icon image context. We release IconQA, a large-scale dataset
that consists of 107,439 questions and three sub-tasks: multi-image-choice,
multi-text-choice, and filling-in-the-blank. The IconQA dataset is inspired by
real-world diagram word problems that highlight the importance of abstract
diagram understanding and comprehensive cognitive reasoning. Thus, IconQA
requires not only perception skills like object recognition and text
understanding, but also diverse cognitive reasoning skills, such as geometric
reasoning, commonsense reasoning, and arithmetic reasoning. To facilitate
potential IconQA models to learn semantic representations for icon images, we
further release an icon dataset Icon645 which contains 645,687 colored icons on
377 classes. We conduct extensive user studies and blind experiments and
reproduce a wide range of advanced VQA methods to benchmark the IconQA task.
Also, we develop a strong IconQA baseline Patch-TRM that applies a pyramid
cross-modal Transformer with input diagram embeddings pre-trained on the icon
dataset. IconQA and Icon645 are available at https://iconqa.github.io.
",current visual question answer vqa task mainly consider answer human annotate question natural image however aside natural image abstract diagram semantic richness still understudied visual understanding reasoning research work introduce new challenge icon question answer iconqa goal answer question icon image context release iconqa large scale dataset consist question three sub tasks multi image choice multi text choice filling in the blank iconqa dataset inspire real world diagram word problem highlight importance abstract diagram understand comprehensive cognitive reasoning thus iconqa require perception skill like object recognition text understanding also diverse cognitive reasoning skill geometric reasoning commonsense reason arithmetic reasoning facilitate potential iconqa model learn semantic representation icon image release icon dataset icon645 contain color icon 377 class conduct extensive user study blind experiment reproduce wide range advanced vqa method benchmark iconqa task also develop strong iconqa baseline patch trm apply pyramid cross modal transformer input diagram embedding pre train icon dataset iconqa icon645 available https
"Neuroevolution deep learning architecture search for estimation of river
  surface elevation from photogrammetric Digital Surface Models","  Development of the new methods of surface water observation is crucial in the
perspective of increasingly frequent extreme hydrological events related to
global warming and increasing demand for water. Orthophotos and digital surface
models (DSMs) obtained using UAV photogrammetry can be used to determine the
Water Surface Elevation (WSE) of a river. However, this task is difficult due
to disturbances of the water surface on DSMs caused by limitations of
photogrammetric algorithms. In this study, machine learning was used to extract
a WSE value from disturbed photogrammetric data. A brand new dataset has been
prepared specifically for this purpose by hydrology and photogrammetry experts.
The new method is an important step toward automating water surface level
measurements with high spatial and temporal resolution. Such data can be used
to validate and calibrate of hydrological, hydraulic and hydrodynamic models
making hydrological forecasts more accurate, in particular predicting extreme
and dangerous events such as floods or droughts. For our knowledge this is the
first approach in which dataset was created for this purpose and deep learning
models were used for this task. Additionally, neuroevolution algorithm was set
to explore different architectures to find local optimal models and
non-gradient search was performed to fine-tune the model parameters. The
achieved results have better accuracy compared to manual methods of determining
WSE from photogrammetric DSMs.
",development new method surface water observation crucial perspective increasingly frequent extreme hydrological event relate global warming increase demand water orthophoto digital surface model dsm obtain use uav photogrammetry use determine water surface elevation wse river however task difficult due disturbance water surface dsm cause limitation photogrammetric algorithm study machine learning use extract wse value disturb photogrammetric datum brand new dataset prepare specifically purpose hydrology photogrammetry expert new method important step toward automate water surface level measurement high spatial temporal resolution datum use validate calibrate hydrological hydraulic hydrodynamic model make hydrological forecast accurate particular predict extreme dangerous event flood drought knowledge first approach dataset create purpose deep learning model use task additionally neuroevolution algorithm set explore different architecture find local optimal model non gradient search perform fine tune model parameter achieve result well accuracy compare manual method determine wse photogrammetric dsm
Prune and Replace NAS,"  While recent NAS algorithms are thousands of times faster than the pioneering
works, it is often overlooked that they use fewer candidate operations,
resulting in a significantly smaller search space. We present PR-DARTS, a NAS
algorithm that discovers strong network configurations in a much larger search
space and a single day. A small candidate operation pool is used, from which
candidates are progressively pruned and replaced with better performing ones.
Experiments on CIFAR-10 and CIFAR-100 achieve 2.51% and 15.53% test error,
respectively, despite searching in a space where each cell has 150 times as
many possible configurations than in the DARTS baseline. Code is available at
https://github.com/cogsys-tuebingen/prdarts
",recent nas algorithm thousand time fast pioneer work often overlook use few candidate operation result significantly small search space present pr dart nas algorithm discover strong network configuration much large search space single day small candidate operation pool use candidate progressively prune replace well perform one experiment cifar-10 cifar-100 achieve test error respectively despite search space cell 150 time many possible configuration dart baseline code available https
Generalizing Hamiltonian Monte Carlo with Neural Networks,"  We present a general-purpose method to train Markov chain Monte Carlo
kernels, parameterized by deep neural networks, that converge and mix quickly
to their target distribution. Our method generalizes Hamiltonian Monte Carlo
and is trained to maximize expected squared jumped distance, a proxy for mixing
speed. We demonstrate large empirical gains on a collection of simple but
challenging distributions, for instance achieving a 106x improvement in
effective sample size in one case, and mixing when standard HMC makes no
measurable progress in a second. Finally, we show quantitative and qualitative
gains on a real-world task: latent-variable generative modeling. We release an
open source TensorFlow implementation of the algorithm.
",present general purpose method train markov chain monte carlo kernels parameterize deep neural network converge mix quickly target distribution method generalize hamiltonian monte carlo train maximize expect square jump distance proxy mix speed demonstrate large empirical gain collection simple challenging distribution instance achieve 106x improvement effective sample size one case mix standard hmc make measurable progress second finally show quantitative qualitative gain real world task latent variable generative modeling release open source tensorflow implementation algorithm
Mixture of Regression Experts in fMRI Encoding,"  fMRI semantic category understanding using linguistic encoding models attempt
to learn a forward mapping that relates stimuli to the corresponding brain
activation. Classical encoding models use linear multi-variate methods to
predict the brain activation (all voxels) given the stimulus. However, these
methods essentially assume multiple regions as one large uniform region or
several independent regions, ignoring connections among them. In this paper, we
present a mixture of experts-based model where a group of experts captures
brain activity patterns related to particular regions of interest (ROI) and
also show the discrimination across different experts. The model is trained
word stimuli encoded as 25-dimensional feature vectors as input and the
corresponding brain responses as output. Given a new word (25-dimensional
feature vector), it predicts the entire brain activation as the linear
combination of multiple experts brain activations. We argue that each expert
learns a certain region of brain activations corresponding to its category of
words, which solves the problem of identifying the regions with a simple
encoding model. We showcase that proposed mixture of experts-based model indeed
learns region-based experts to predict the brain activations with high spatial
accuracy.
",fmri semantic category understanding use linguistic encoding model attempt learn forward mapping relate stimulus correspond brain activation classical encoding model use linear multi variate method predict brain activation voxel give stimulus however method essentially assume multiple region one large uniform region several independent region ignore connection among paper present mixture expert base model group expert capture brain activity pattern relate particular region interest roi also show discrimination across different expert model train word stimulus encode 25 dimensional feature vector input correspond brain response output give new word 25 dimensional feature vector predict entire brain activation linear combination multiple expert brain activation argue expert learn certain region brain activation correspond category word solve problem identify region simple encoding model showcase propose mixture expert base model indeed learn region base expert predict brain activation high spatial accuracy
"Building a great multi-lingual teacher with sparsely-gated mixture of
  experts for speech recognition","  The sparsely-gated Mixture of Experts (MoE) can magnify a network capacity
with a little computational complexity. In this work, we investigate how
multi-lingual Automatic Speech Recognition (ASR) networks can be scaled up with
a simple routing algorithm in order to achieve better accuracy. More
specifically, we apply the sparsely-gated MoE technique to two types of
networks: Sequence-to-Sequence Transformer (S2S-T) and Transformer Transducer
(T-T). We demonstrate through a set of ASR experiments on multiple language
data that the MoE networks can reduce the relative word error rates by 16.3%
and 4.6% with the S2S-T and T-T, respectively. Moreover, we thoroughly
investigate the effect of the MoE on the T-T architecture in various
conditions: streaming mode, non-streaming mode, the use of language ID and the
label decoder with the MoE.
",sparsely gate mixture expert moe magnify network capacity little computational complexity work investigate multi lingual automatic speech recognition asr networks scale simple route algorithm order achieve well accuracy specifically apply sparsely gate moe technique two type network sequence to sequence transformer s2s t transformer transducer t t demonstrate set asr experiment multiple language datum moe network reduce relative word error rate s2s t t t respectively moreover thoroughly investigate effect moe t t architecture various condition stream mode non streaming mode use language i d label decoder moe
"Contrastive Multiple Correspondence Analysis (cMCA): Using Contrastive
  Learning to Identify Latent Subgroups in Political Parties","  Scaling methods have long been utilized to simplify and cluster
high-dimensional data. However, the latent spaces derived from these methods
are sometimes uninformative or unable to identify significant differences in
the data. To tackle this common issue, we adopt an emerging analysis approach
called contrastive learning. We contribute to this emerging field by extending
its ideas to multiple correspondence analysis (MCA) in order to enable an
analysis of data often encountered by social scientists -- namely binary,
ordinal, and nominal variables. We demonstrate the utility of contrastive MCA
(cMCA) by analyzing three different surveys of voters in Europe, Japan, and the
United States. Our results suggest that, first, cMCA can identify substantively
important dimensions and divisions among (sub)groups that are overlooked by
traditional methods; second, for certain cases, cMCA can still derive latent
traits that generalize across and apply to multiple groups in the dataset;
finally, when data is high-dimensional and unstructured, cMCA provides
objective heuristics, above and beyond the standard results, enabling more
complex subgroup analysis.
",scale method long utilize simplify cluster high dimensional datum however latent space derive method sometimes uninformative unable identify significant difference datum tackle common issue adopt emerge analysis approach call contrastive learning contribute emerge field extend idea multiple correspondence analysis mca order enable analysis datum often encounter social scientist namely binary ordinal nominal variable demonstrate utility contrastive mca cmca analyze three different survey voter europe japan united states result suggest first cmca identify substantively important dimension division among sub group overlook traditional method second certain case cmca still derive latent trait generalize across apply multiple group dataset finally data high dimensional unstructured cmca provide objective heuristic beyond standard result enable complex subgroup analysis
"Graph-augmented Convolutional Networks on Drug-Drug Interactions
  Prediction","  We propose an end-to-end model to predict drug-drug interactions (DDIs) by
employing graph-augmented convolutional networks. And this is implemented by
combining graph CNN with an attentive pooling network to extract structural
relations between drug pairs and make DDI predictions. The experiment results
suggest a desirable performance achieving ROC at 0.988, F1-score at 0.956, and
AUPR at 0.986. Besides, the model can tell how the two DDI drugs interact
structurally by varying colored atoms. And this may be helpful for drug design
during drug discovery.
",propose end to end model predict drug drug interaction ddi employ graph augment convolutional network implement combine graph cnn attentive pool network extract structural relation drug pair make ddi prediction experiment result suggest desirable performance achieve roc f1 score aupr besides model tell two ddi drug interact structurally vary colored atom may helpful drug design drug discovery
"Every Local Minimum Value is the Global Minimum Value of Induced Model
  in Non-convex Machine Learning","  For nonconvex optimization in machine learning, this article proves that
every local minimum achieves the globally optimal value of the perturbable
gradient basis model at any differentiable point. As a result, nonconvex
machine learning is theoretically as supported as convex machine learning with
a handcrafted basis in terms of the loss at differentiable local minima, except
in the case when a preference is given to the handcrafted basis over the
perturbable gradient basis. The proofs of these results are derived under mild
assumptions. Accordingly, the proven results are directly applicable to many
machine learning models, including practical deep neural networks, without any
modification of practical methods. Furthermore, as special cases of our general
results, this article improves or complements several state-of-the-art
theoretical results on deep neural networks, deep residual networks, and
overparameterized deep neural networks with a unified proof technique and novel
geometric insights. A special case of our results also contributes to the
theoretical foundation of representation learning.
",nonconvex optimization machine learning article prove every local minimum achieve globally optimal value perturbable gradient basis model differentiable point result nonconvex machine learning theoretically support convex machine learning handcraft basis term loss differentiable local minima except case preference give handcraft basis perturbable gradient basis proof result derive mild assumption accordingly prove result directly applicable many machine learning model include practical deep neural network without modification practical method furthermore special case general result article improve complement several state of the art theoretical result deep neural network deep residual network overparameterize deep neural network unify proof technique novel geometric insight special case result also contribute theoretical foundation representation learning
Efficient Subspace Search in Data Streams,"  In the real world, data streams are ubiquitous -- think of network traffic or
sensor data. Mining patterns, e.g., outliers or clusters, from such data must
take place in real time. This is challenging because (1) streams often have
high dimensionality, and (2) the data characteristics may change over time.
Existing approaches tend to focus on only one aspect, either high
dimensionality or the specifics of the streaming setting. For static data, a
common approach to deal with high dimensionality -- known as subspace search --
extracts low-dimensional, `interesting' projections (subspaces), in which
patterns are easier to find. In this paper, we address both Challenge (1) and
(2) by generalising subspace search to data streams. Our approach, Streaming
Greedy Maximum Random Deviation (SGMRD), monitors interesting subspaces in
high-dimensional data streams. It leverages novel multivariate dependency
estimators and monitoring techniques based on bandit theory. We show that the
benefits of SGMRD are twofold: (i) It monitors subspaces efficiently, and (ii)
this improves the results of downstream data mining tasks, such as outlier
detection. Our experiments, performed against synthetic and real-world data,
demonstrate that SGMRD outperforms its competitors by a large margin.
",real world datum stream ubiquitous think network traffic sensor datum mining pattern outlier cluster datum must take place real time challenge 1 stream often high dimensionality 2 datum characteristic may change time exist approach tend focus one aspect either high dimensionality specific streaming set static datum common approach deal high dimensionality know subspace search extract low dimensional interesting projection subspace pattern easy find paper address challenge 1 2 generalise subspace search datum stream approach stream greedy maximum random deviation sgmrd monitor interesting subspace high dimensional data stream leverage novel multivariate dependency estimator monitor technique base bandit theory show benefit sgmrd twofold monitor subspace efficiently ii improve result downstream datum mining task outli detection experiment perform synthetic real world datum demonstrate sgmrd outperform competitor large margin
Unsupervised Learning and Exploration of Reachable Outcome Space,"  Performing Reinforcement Learning in sparse rewards settings, with very
little prior knowledge, is a challenging problem since there is no signal to
properly guide the learning process. In such situations, a good search strategy
is fundamental. At the same time, not having to adapt the algorithm to every
single problem is very desirable. Here we introduce TAXONS, a Task Agnostic
eXploration of Outcome spaces through Novelty and Surprise algorithm. Based on
a population-based divergent-search approach, it learns a set of diverse
policies directly from high-dimensional observations, without any task-specific
information. TAXONS builds a repertoire of policies while training an
autoencoder on the high-dimensional observation of the final state of the
system to build a low-dimensional outcome space. The learned outcome space,
combined with the reconstruction error, is used to drive the search for new
policies. Results show that TAXONS can find a diverse set of controllers,
covering a good part of the ground-truth outcome space, while having no
information about such space.
",perform reinforcement learning sparse reward setting little prior knowledge challenge problem since signal properly guide learning process situation good search strategy fundamental time adapt algorithm every single problem desirable introduce taxon task agnostic exploration outcome space novelty surprise algorithm base population base divergent search approach learn set diverse policy directly high dimensional observation without task specific information taxon builds repertoire policy train autoencoder high dimensional observation final state system build low dimensional outcome space learn outcome space combine reconstruction error use drive search new policy result show taxon find diverse set controller cover good part ground truth outcome space information space
"Provable Alternating Gradient Descent for Non-negative Matrix
  Factorization with Strong Correlations","  Non-negative matrix factorization is a basic tool for decomposing data into
the feature and weight matrices under non-negativity constraints, and in
practice is often solved in the alternating minimization framework. However, it
is unclear whether such algorithms can recover the ground-truth feature matrix
when the weights for different features are highly correlated, which is common
in applications. This paper proposes a simple and natural alternating gradient
descent based algorithm, and shows that with a mild initialization it provably
recovers the ground-truth in the presence of strong correlations. In most
interesting cases, the correlation can be in the same order as the highest
possible. Our analysis also reveals its several favorable features including
robustness to noise. We complement our theoretical results with empirical
studies on semi-synthetic datasets, demonstrating its advantage over several
popular methods in recovering the ground-truth.
",non negative matrix factorization basic tool decompose datum feature weight matrix non negativity constraint practice often solve alternate minimization framework however unclear whether algorithm recover ground truth feature matrix weight different feature highly correlate common application paper propose simple natural alternate gradient descent base algorithm show mild initialization provably recover ground truth presence strong correlation interesting case correlation order high possible analysis also reveal several favorable feature include robustness noise complement theoretical result empirical study semi synthetic dataset demonstrate advantage several popular method recover ground truth
Learning Privately with Labeled and Unlabeled Examples,"  A private learner is an algorithm that given a sample of labeled individual
examples outputs a generalizing hypothesis while preserving the privacy of each
individual. In 2008, Kasiviswanathan et al. (FOCS 2008) gave a generic
construction of private learners, in which the sample complexity is (generally)
higher than what is needed for non-private learners. This gap in the sample
complexity was then further studied in several followup papers, showing that
(at least in some cases) this gap is unavoidable. Moreover, those papers
considered ways to overcome the gap, by relaxing either the privacy or the
learning guarantees of the learner.
  We suggest an alternative approach, inspired by the (non-private) models of
semi-supervised learning and active-learning, where the focus is on the sample
complexity of labeled examples whereas unlabeled examples are of a
significantly lower cost. We consider private semi-supervised learners that
operate on a random sample, where only a (hopefully small) portion of this
sample is labeled. The learners have no control over which of the sample
elements are labeled. Our main result is that the labeled sample complexity of
private learners is characterized by the VC dimension.
  We present two generic constructions of private semi-supervised learners. The
first construction is of learners where the labeled sample complexity is
proportional to the VC dimension of the concept class, however, the unlabeled
sample complexity of the algorithm is as big as the representation length of
domain elements. Our second construction presents a new technique for
decreasing the labeled sample complexity of a given private learner, while
roughly maintaining its unlabeled sample complexity. In addition, we show that
in some settings the labeled sample complexity does not depend on the privacy
parameters of the learner.
",private learner algorithm give sample label individual example output generalize hypothesis preserve privacy individual 2008 kasiviswanathan et al focs 2008 give generic construction private learner sample complexity generally higher need non private learner gap sample complexity study several followup paper show least case gap unavoidable moreover paper consider way overcome gap relax either privacy learning guarantee learner suggest alternative approach inspire non private model semi supervised learn active learn focus sample complexity label example whereas unlabeled example significantly low cost consider private semi supervised learner operate random sample hopefully small portion sample label learner control sample element label main result label sample complexity private learner characterize vc dimension present two generic construction private semi supervised learner first construction learner label sample complexity proportional vc dimension concept class however unlabeled sample complexity algorithm big representation length domain element second construction present new technique decrease label sample complexity give private learner roughly maintain unlabeled sample complexity addition show setting label sample complexity depend privacy parameter learner
"Evaluating the Robustness of Bayesian Neural Networks Against Different
  Types of Attacks","  To evaluate the robustness gain of Bayesian neural networks on image
classification tasks, we perform input perturbations, and adversarial attacks
to the state-of-the-art Bayesian neural networks, with a benchmark CNN model as
reference. The attacks are selected to simulate signal interference and
cyberattacks towards CNN-based machine learning systems. The result shows that
a Bayesian neural network achieves significantly higher robustness against
adversarial attacks generated against a deterministic neural network model,
without adversarial training. The Bayesian posterior can act as the safety
precursor of ongoing malicious activities. Furthermore, we show that the
stochastic classifier after the deterministic CNN extractor has sufficient
robustness enhancement rather than a stochastic feature extractor before the
stochastic classifier. This advises on utilizing stochastic layers in building
decision-making pipelines within a safety-critical domain.
",evaluate robustness gain bayesian neural network image classification task perform input perturbation adversarial attack state of the art bayesian neural networks benchmark cnn model reference attack select simulate signal interference cyberattack towards cnn base machine learn system result show bayesian neural network achieve significantly high robustness adversarial attack generate deterministic neural network model without adversarial training bayesian posterior act safety precursor ongoing malicious activity furthermore show stochastic classifier deterministic cnn extractor sufficient robustness enhancement rather stochastic feature extractor stochastic classifier advise utilize stochastic layer build decision make pipeline within safety critical domain
Deep Features for CBIR with Scarce Data using Hebbian Learning,"Features extracted from Deep Neural Networks (DNNs) have proven to be very
effective in the context of Content Based Image Retrieval (CBIR). In recent
work, biologically inspired \textit{Hebbian} learning algorithms have shown
promises for DNN training. In this contribution, we study the performance of
such algorithms in the development of feature extractors for CBIR tasks.
Specifically, we consider a semi-supervised learning strategy in two steps:
first, an unsupervised pre-training stage is performed using Hebbian learning
on the image dataset; second, the network is fine-tuned using supervised
Stochastic Gradient Descent (SGD) training. For the unsupervised pre-training
stage, we explore the nonlinear Hebbian Principal Component Analysis (HPCA)
learning rule. For the supervised fine-tuning stage, we assume sample
efficiency scenarios, in which the amount of labeled samples is just a small
fraction of the whole dataset. Our experimental analysis, conducted on the
CIFAR10 and CIFAR100 datasets shows that, when few labeled samples are
available, our Hebbian approach provides relevant improvements compared to
various alternative methods.",feature extract deep neural network dnn prove effective context content base image retrieval cbir recent work biologically inspire hebbian learning algorithm show promise dnn training contribution study performance algorithm development feature extractor cbir task specifically consider semi supervised learning strategy two step first unsupervised pre training stage perform use hebbian learn image dataset second network fine tune use supervised stochastic gradient descent sgd training unsupervised pre training stage explore nonlinear hebbian principal component analysis hpca learning rule supervise fine tuning stage assume sample efficiency scenario amount label sample small fraction whole dataset experimental analysis conduct cifar10 cifar100 dataset show label sample available hebbian approach provide relevant improvement compare various alternative method
"Evaluating Adversarial Attacks on ImageNet: A Reality Check on
  Misclassification Classes","  Although ImageNet was initially proposed as a dataset for performance
benchmarking in the domain of computer vision, it also enabled a variety of
other research efforts. Adversarial machine learning is one such research
effort, employing deceptive inputs to fool models in making wrong predictions.
To evaluate attacks and defenses in the field of adversarial machine learning,
ImageNet remains one of the most frequently used datasets. However, a topic
that is yet to be investigated is the nature of the classes into which
adversarial examples are misclassified. In this paper, we perform a detailed
analysis of these misclassification classes, leveraging the ImageNet class
hierarchy and measuring the relative positions of the aforementioned type of
classes in the unperturbed origins of the adversarial examples. We find that
$71\%$ of the adversarial examples that achieve model-to-model adversarial
transferability are misclassified into one of the top-5 classes predicted for
the underlying source images. We also find that a large subset of untargeted
misclassifications are, in fact, misclassifications into semantically similar
classes. Based on these findings, we discuss the need to take into account the
ImageNet class hierarchy when evaluating untargeted adversarial successes.
Furthermore, we advocate for future research efforts to incorporate categorical
information.
",although imagenet initially propose dataset performance benchmarke domain computer vision also enable variety research effort adversarial machine learn one research effort employ deceptive input fool model make wrong prediction evaluate attack defense field adversarial machine learn imagenet remain one frequently use dataset however topic yet investigate nature class adversarial example misclassifie paper perform detailed analysis misclassification class leverage imagenet class hierarchy measure relative position aforementione type class unperturbed origin adversarial example find adversarial example achieve model to model adversarial transferability misclassifie one top-5 class predict underlying source image also find large subset untargeted misclassification fact misclassification semantically similar class base finding discuss need take account imagenet class hierarchy evaluate untargeted adversarial success furthermore advocate future research effort incorporate categorical information
ABCinML: Anticipatory Bias Correction in Machine Learning Applications,"  The idealization of a static machine-learned model, trained once and deployed
forever, is not practical. As input distributions change over time, the model
will not only lose accuracy, any constraints to reduce bias against a protected
class may fail to work as intended. Thus, researchers have begun to explore
ways to maintain algorithmic fairness over time. One line of work focuses on
dynamic learning: retraining after each batch, and the other on robust learning
which tries to make algorithms robust against all possible future changes.
Dynamic learning seeks to reduce biases soon after they have occurred and
robust learning often yields (overly) conservative models. We propose an
anticipatory dynamic learning approach for correcting the algorithm to mitigate
bias before it occurs. Specifically, we make use of anticipations regarding the
relative distributions of population subgroups (e.g., relative ratios of male
and female applicants) in the next cycle to identify the right parameters for
an importance weighing fairness approach. Results from experiments over
multiple real-world datasets suggest that this approach has promise for
anticipatory bias correction.
",idealization static machine learn model train deploy forever practical input distribution change time model lose accuracy constraint reduce bias protect class may fail work intend thus researcher begin explore way maintain algorithmic fairness time one line work focus dynamic learning retrain batch robust learning try make algorithm robust possible future change dynamic learning seek reduce bias soon occur robust learning often yield overly conservative model propose anticipatory dynamic learning approach correct algorithm mitigate bias occurs specifically make use anticipation regard relative distribution population subgroup relative ratio male female applicant next cycle identify right parameter importance weigh fairness approach result experiment multiple real world dataset suggest approach promise anticipatory bias correction
Over-the-Air Federated Learning with Joint Adaptive Computation and Power Control,"This paper considers over-the-air federated learning (OTA-FL). OTA-FL
exploits the superposition property of the wireless medium, and performs model
aggregation over the air for free. Thus, it can greatly reduce the
communication cost incurred in communicating model updates from the edge
devices. In order to fully utilize this advantage while providing comparable
learning performance to conventional federated learning that presumes model
aggregation via noiseless channels, we consider the joint design of
transmission scaling and the number of local iterations at each round, given
the power constraint at each edge device. We first characterize the training
error due to such channel noise in OTA-FL by establishing a fundamental lower
bound for general functions with Lipschitz-continuous gradients. Then, by
introducing an adaptive transceiver power scaling scheme, we propose an
over-the-air federated learning algorithm with joint adaptive computation and
power control (ACPC-OTA-FL). We provide the convergence analysis for
ACPC-OTA-FL in training with non-convex objective functions and heterogeneous
data. We show that the convergence rate of ACPC-OTA-FL matches that of FL with
noise-free communications.",paper consider over the air federate learn ota fl ota fl exploit superposition property wireless medium perform model aggregation air free thus greatly reduce communication cost incur communicating model update edge device order fully utilize advantage provide comparable learning performance conventional federated learn presume model aggregation via noiseless channel consider joint design transmission scaling number local iteration round give power constraint edge device first characterize training error due channel noise ota fl establish fundamental lower bind general function lipschitz continuous gradient introduce adaptive transceiver power scale scheme propose over the air federate learning algorithm joint adaptive computation power control acpc ota fl provide convergence analysis acpc ota fl training non convex objective function heterogeneous datum show convergence rate acpc ota fl match fl noise free communication
Multiclass MinMax Rank Aggregation,"  We introduce a new family of minmax rank aggregation problems under two
distance measures, the Kendall {\tau} and the Spearman footrule. As the
problems are NP-hard, we proceed to describe a number of constant-approximation
algorithms for solving them. We conclude with illustrative applications of the
aggregation methods on the Mallows model and genomic data.
",introduce new family minmax rank aggregation problem two distance measure kendall spearman footrule problem np hard proceed describe number constant approximation algorithm solve conclude illustrative application aggregation method mallow model genomic datum
"Disentanglement Learning for Variational Autoencoders Applied to
  Audio-Visual Speech Enhancement","  Recently, the standard variational autoencoder has been successfully used to
learn a probabilistic prior over speech signals, which is then used to perform
speech enhancement. Variational autoencoders have then been conditioned on a
label describing a high-level speech attribute (e.g. speech activity) that
allows for a more explicit control of speech generation. However, the label is
not guaranteed to be disentangled from the other latent variables, which
results in limited performance improvements compared to the standard
variational autoencoder. In this work, we propose to use an adversarial
training scheme for variational autoencoders to disentangle the label from the
other latent variables. At training, we use a discriminator that competes with
the encoder of the variational autoencoder. Simultaneously, we also use an
additional encoder that estimates the label for the decoder of the variational
autoencoder, which proves to be crucial to learn disentanglement. We show the
benefit of the proposed disentanglement learning when a voice activity label,
estimated from visual data, is used for speech enhancement.
",recently standard variational autoencoder successfully use learn probabilistic prior speech signal use perform speech enhancement variational autoencoder condition label describe high level speech attribute speech activity allow explicit control speech generation however label guarantee disentangle latent variable result limited performance improvement compare standard variational autoencoder work propose use adversarial training scheme variational autoencoder disentangle label latent variable training use discriminator compete encoder variational autoencoder simultaneously also use additional encoder estimate label decoder variational autoencoder prove crucial learn disentanglement show benefit propose disentanglement learn voice activity label estimate visual datum use speech enhancement
"Representation of binary classification trees with binary features by
  quantum circuits","  We propose a quantum representation of binary classification trees with
binary features based on a probabilistic approach. By using the quantum
computer as a processor for probability distributions, a probabilistic
traversal of the decision tree can be realized via measurements of a quantum
circuit. We describe how tree inductions and the prediction of class labels of
query data can be integrated into this framework. An on-demand sampling method
enables predictions with a constant number of classical memory slots,
independent of the tree depth. We experimentally study our approach using both
a quantum computing simulator and actual IBM quantum hardware. To our
knowledge, this is the first realization of a decision tree classifier on a
quantum device.
",propose quantum representation binary classification tree binary feature base probabilistic approach use quantum computer processor probability distribution probabilistic traversal decision tree realize via measurement quantum circuit describe tree induction prediction class label query datum integrate framework on demand sampling method enable prediction constant number classical memory slot independent tree depth experimentally study approach use quantum computing simulator actual ibm quantum hardware knowledge first realization decision tree classifier quantum device
Teaching the Machine to Explain Itself using Domain Knowledge,"  Machine Learning (ML) has been increasingly used to aid humans to make better
and faster decisions. However, non-technical humans-in-the-loop struggle to
comprehend the rationale behind model predictions, hindering trust in
algorithmic decision-making systems. Considerable research work on AI
explainability attempts to win back trust in AI systems by developing
explanation methods but there is still no major breakthrough. At the same time,
popular explanation methods (e.g., LIME, and SHAP) produce explanations that
are very hard to understand for non-data scientist persona. To address this, we
present JOEL, a neural network-based framework to jointly learn a
decision-making task and associated explanations that convey domain knowledge.
JOEL is tailored to human-in-the-loop domain experts that lack deep technical
ML knowledge, providing high-level insights about the model's predictions that
very much resemble the experts' own reasoning. Moreover, we collect the domain
feedback from a pool of certified experts and use it to ameliorate the model
(human teaching), hence promoting seamless and better suited explanations.
Lastly, we resort to semantic mappings between legacy expert systems and domain
taxonomies to automatically annotate a bootstrap training set, overcoming the
absence of concept-based human annotations. We validate JOEL empirically on a
real-world fraud detection dataset. We show that JOEL can generalize the
explanations from the bootstrap dataset. Furthermore, obtained results indicate
that human teaching can further improve the explanations prediction quality by
approximately $13.57\%$.
",machine learning ml increasingly use aid human make well fast decision however non technical human in the loop struggle comprehend rationale behind model prediction hinder trust algorithmic decision make system considerable research work ai explainability attempt win back trust ai system develop explanation method still major breakthrough time popular explanation method lime shap produce explanation hard understand non data scientist persona address present joel neural network base framework jointly learn decision make task associate explanation convey domain knowledge joel tailor human in the loop domain expert lack deep technical ml knowledge provide high level insight model prediction much resemble expert reasoning moreover collect domain feedback pool certify expert use ameliorate model human teaching hence promote seamless well suited explanation lastly resort semantic mapping legacy expert system domain taxonomy automatically annotate bootstrap training set overcome absence concept base human annotation validate joel empirically real world fraud detection dataset show joel generalize explanation bootstrap dataset furthermore obtain result indicate human teaching improve explanation prediction quality approximately
A Fast Transformer-based General-Purpose Lossless Compressor,"  Deep-learning-based compressor has received interests recently due to much
improved compression ratio. However, modern approaches suffer from long
execution time. To ease this problem, this paper targets on cutting down the
execution time of deep-learning-based compressors. Building
history-dependencies sequentially (e.g., recurrent neural networks) is
responsible for long inference latency. Instead, we introduce transformer into
deep learning compressors to build history-dependencies in parallel. However,
existing transformer is too heavy in computation and incompatible to
compression tasks.
  This paper proposes a fast general-purpose lossless compressor, TRACE, by
designing a compression-friendly structure based on a single-layer transformer.
We first design a new metric to advise the selection part of compression model
structures. Byte-grouping and Shared-ffn schemes are further proposed to fully
utilize the capacity of the single-layer transformer. These features allow
TRACE to achieve competitive compression ratio and a much faster speed. In
addition, we further accelerate the compression procedure by designing a
controller to reduce the parameter updating overhead. Experiments show that
TRACE achieves an overall $\sim$3x speedup while keeps a comparable compression
ratio to the state-of-the-art compressors. The source code for TRACE and links
to the datasets are available at
https://github.com/mynotwo/A-Fast-Transformer-based-General-Purpose-LosslessCompressor.
",deep learn base compressor receive interest recently due much improve compression ratio however modern approach suffer long execution time ease problem paper target cut execution time deep learn base compressor build history dependency sequentially recurrent neural network responsible long inference latency instead introduce transformer deep learning compressor build history dependency parallel however exist transformer heavy computation incompatible compression task paper propose fast general purpose lossless compressor trace design compression friendly structure base single layer transformer first design new metric advise selection part compression model structure byte group share ffn scheme propose fully utilize capacity single layer transformer feature allow trace achieve competitive compression ratio much fast speed addition accelerate compression procedure design controller reduce parameter update overhead experiment show trace achieve overall 3x speedup keep comparable compression ratio state of the art compressor source code trace link dataset available https
"CodeReef: an open platform for portable MLOps, reusable automation
  actions and reproducible benchmarking","  We present CodeReef - an open platform to share all the components necessary
to enable cross-platform MLOps (MLSysOps), i.e. automating the deployment of ML
models across diverse systems in the most efficient way. We also introduce the
CodeReef solution - a way to package and share models as non-virtualized,
portable, customizable and reproducible archive files. Such ML packages include
JSON meta description of models with all dependencies, Python APIs, CLI actions
and portable workflows necessary to automatically build, benchmark, test and
customize models across diverse platforms, AI frameworks, libraries, compilers
and datasets. We demonstrate several CodeReef solutions to automatically build,
run and measure object detection based on SSD-Mobilenets, TensorFlow and COCO
dataset from the latest MLPerf inference benchmark across a wide range of
platforms from Raspberry Pi, Android phones and IoT devices to data centers.
Our long-term goal is to help researchers share their new techniques as
production-ready packages along with research papers to participate in
collaborative and reproducible benchmarking, compare the different
ML/software/hardware stacks and select the most efficient ones on a Pareto
frontier using online CodeReef dashboards.
",present codereef open platform share component necessary enable cross platform mlop mlsysop automate deployment ml model across diverse system efficient way also introduce codereef solution way package share model non virtualize portable customizable reproducible archive file ml package include json meta description model dependencie python apis cli action portable workflow necessary automatically build benchmark test customize model across diverse platform ai framework library compiler dataset demonstrate several codereef solution automatically build run measure object detection base ssd mobilenet tensorflow coco dataset late mlperf inference benchmark across wide range platform raspberry pi android phone iot device data center long term goal help researcher share new technique production ready package along research paper participate collaborative reproducible benchmarke compare different stack select efficient one pareto frontier use online codereef dashboard
"Gradient Regularized Contrastive Learning for Continual Domain
  Adaptation","  Human beings can quickly adapt to environmental changes by leveraging
learning experience. However, the poor ability of adapting to dynamic
environments remains a major challenge for AI models. To better understand this
issue, we study the problem of continual domain adaptation, where the model is
presented with a labeled source domain and a sequence of unlabeled target
domains. There are two major obstacles in this problem: domain shifts and
catastrophic forgetting. In this work, we propose Gradient Regularized
Contrastive Learning to solve the above obstacles. At the core of our method,
gradient regularization plays two key roles: (1) enforces the gradient of
contrastive loss not to increase the supervised training loss on the source
domain, which maintains the discriminative power of learned features; (2)
regularizes the gradient update on the new domain not to increase the
classification loss on the old target domains, which enables the model to adapt
to an in-coming target domain while preserving the performance of previously
observed domains. Hence our method can jointly learn both semantically
discriminative and domain-invariant features with labeled source domain and
unlabeled target domains. The experiments on Digits, DomainNet and
Office-Caltech benchmarks demonstrate the strong performance of our approach
when compared to the state-of-the-art.
",human being quickly adapt environmental change leverage learn experience however poor ability adapt dynamic environment remain major challenge ai model well understand issue study problem continual domain adaptation model present label source domain sequence unlabeled target domain two major obstacle problem domain shift catastrophic forgetting work propose gradient regularize contrastive learning solve obstacle core method gradient regularization play two key role 1 enforce gradient contrastive loss increase supervise training loss source domain maintain discriminative power learn feature 2 regularize gradient update new domain increase classification loss old target domain enable model adapt in come target domain preserve performance previously observe domain hence method jointly learn semantically discriminative domain invariant feature label source domain unlabeled target domain experiment digit domainnet office caltech benchmark demonstrate strong performance approach compare state of the art
"Bayesian Cycle-Consistent Generative Adversarial Networks via
  Marginalizing Latent Sampling","  Recent techniques built on Generative Adversarial Networks (GANs), such as
Cycle-Consistent GANs, are able to learn mappings among different domains built
from unpaired datasets, through min-max optimization games between generators
and discriminators. However, it remains challenging to stabilize the training
process and thus cyclic models fall into mode collapse accompanied by the
success of discriminator. To address this problem, we propose an novel Bayesian
cyclic model and an integrated cyclic framework for inter-domain mappings. The
proposed method motivated by Bayesian GAN explores the full posteriors of
cyclic model via sampling latent variables and optimizes the model with maximum
a posteriori (MAP) estimation. Hence, we name it Bayesian CycleGAN. In
addition, original CycleGAN cannot generate diversified results. But it is
feasible for Bayesian framework to diversify generated images by replacing
restricted latent variables in inference process. We evaluate the proposed
Bayesian CycleGAN on multiple benchmark datasets, including Cityscapes, Maps,
and Monet2photo. The proposed method improve the per-pixel accuracy by 15% for
the Cityscapes semantic segmentation task within origin framework and improve
20% within the proposed integrated framework, showing better resilience to
imbalance confrontation. The diversified results of Monet2Photo style transfer
also demonstrate its superiority over original cyclic model. We provide codes
for all of our experiments in https://github.com/ranery/Bayesian-CycleGAN.
",recent technique build generative adversarial network gan cycle consistent gan able learn mapping among different domain build unpaired dataset min max optimization game generator discriminator however remain challenge stabilize training process thus cyclic model fall mode collapse accompany success discriminator address problem propose novel bayesian cyclic model integrate cyclic framework inter domain mapping propose method motivate bayesian gin explore full posterior cyclic model via sample latent variable optimize model maximum posteriori map estimation hence name bayesian cyclegan addition original cyclegan generate diversify result feasible bayesian framework diversify generate image replace restricted latent variable inference process evaluate propose bayesian cyclegan multiple benchmark dataset include cityscape maps monet2photo propose method improve per pixel accuracy 15 cityscape semantic segmentation task within origin framework improve 20 within propose integrate framework show well resilience imbalance confrontation diversify result monet2photo style transfer also demonstrate superiority original cyclic model provide code experiment https
An Ensemble SVM-based Approach for Voice Activity Detection,"  Voice activity detection (VAD), used as the front end of speech enhancement,
speech and speaker recognition algorithms, determines the overall accuracy and
efficiency of the algorithms. Therefore, a VAD with low complexity and high
accuracy is highly desirable for speech processing applications. In this paper,
we propose a novel training method on large dataset for supervised
learning-based VAD system using support vector machine (SVM). Despite of high
classification accuracy of support vector machines (SVM), trivial SVM is not
suitable for classification of large data sets needed for a good VAD system
because of high training complexity. To overcome this problem, a novel
ensemble-based approach using SVM has been proposed in this paper.The
performance of the proposed ensemble structure has been compared with a
feedforward neural network (NN). Although NN performs better than single
SVM-based VAD trained on a small portion of the training data, ensemble SVM
gives accuracy comparable to neural network-based VAD. Ensemble SVM and NN give
88.74% and 86.28% accuracy respectively whereas the stand-alone SVM shows
57.05% accuracy on average on the test dataset.
",voice activity detection vad use front end speech enhancement speech speaker recognition algorithm determine overall accuracy efficiency algorithm therefore vad low complexity high accuracy highly desirable speech processing application paper propose novel training method large dataset supervise learning base vad system use support vector machine svm despite high classification accuracy support vector machine svm trivial svm suitable classification large datum set need good vad system high training complexity overcome problem novel ensemble base approach use svm propose performance propose ensemble structure compare feedforward neural network nn although nn perform well single svm base vad train small portion training datum ensemble svm give accuracy comparable neural network base vad ensemble svm nn give accuracy respectively whereas stand alone svm show accuracy average test dataset
"Optimized Potential Initialization for Low-latency Spiking Neural
  Networks","  Spiking Neural Networks (SNNs) have been attached great importance due to the
distinctive properties of low power consumption, biological plausibility, and
adversarial robustness. The most effective way to train deep SNNs is through
ANN-to-SNN conversion, which have yielded the best performance in deep network
structure and large-scale datasets. However, there is a trade-off between
accuracy and latency. In order to achieve high precision as original ANNs, a
long simulation time is needed to match the firing rate of a spiking neuron
with the activation value of an analog neuron, which impedes the practical
application of SNN. In this paper, we aim to achieve high-performance converted
SNNs with extremely low latency (fewer than 32 time-steps). We start by
theoretically analyzing ANN-to-SNN conversion and show that scaling the
thresholds does play a similar role as weight normalization. Instead of
introducing constraints that facilitate ANN-to-SNN conversion at the cost of
model capacity, we applied a more direct way by optimizing the initial membrane
potential to reduce the conversion loss in each layer. Besides, we demonstrate
that optimal initialization of membrane potentials can implement expected
error-free ANN-to-SNN conversion. We evaluate our algorithm on the CIFAR-10,
CIFAR-100 and ImageNet datasets and achieve state-of-the-art accuracy, using
fewer time-steps. For example, we reach top-1 accuracy of 93.38\% on CIFAR-10
with 16 time-steps. Moreover, our method can be applied to other ANN-SNN
conversion methodologies and remarkably promote performance when the time-steps
is small.
",spike neural network snn attach great importance due distinctive property low power consumption biological plausibility adversarial robustness effective way train deep snn ann to snn conversion yield good performance deep network structure large scale dataset however trade off accuracy latency order achieve high precision original ann long simulation time need match firing rate spike neuron activation value analog neuron impede practical application snn paper aim achieve high performance convert snn extremely low latency few 32 time step start theoretically analyze ann to snn conversion show scale threshold play similar role weight normalization instead introduce constraint facilitate ann to snn conversion cost model capacity apply direct way optimize initial membrane potential reduce conversion loss layer besides demonstrate optimal initialization membrane potential implement expect error free ann to snn conversion evaluate algorithm cifar-10 cifar-100 imagenet dataset achieve state of the art accuracy use few time step example reach top-1 accuracy cifar-10 16 time step moreover method applied ann snn conversion methodology remarkably promote performance time step small
Deep Learning with Dynamic Computation Graphs,"  Neural networks that compute over graph structures are a natural fit for
problems in a variety of domains, including natural language (parse trees) and
cheminformatics (molecular graphs). However, since the computation graph has a
different shape and size for every input, such networks do not directly support
batched training or inference. They are also difficult to implement in popular
deep learning libraries, which are based on static data-flow graphs. We
introduce a technique called dynamic batching, which not only batches together
operations between different input graphs of dissimilar shape, but also between
different nodes within a single input graph. The technique allows us to create
static graphs, using popular libraries, that emulate dynamic computation graphs
of arbitrary shape and size. We further present a high-level library of
compositional blocks that simplifies the creation of dynamic graph models.
Using the library, we demonstrate concise and batch-wise parallel
implementations for a variety of models from the literature.
",neural network compute graph structure natural fit problem variety domain include natural language parse tree cheminformatic molecular graph however since computation graph different shape size every input network directly support batch training inference also difficult implement popular deep learning library base static data flow graph introduce technique call dynamic batching batch together operation different input graph dissimilar shape also different node within single input graph technique allow we create static graph use popular library emulate dynamic computation graph arbitrary shape size present high level library compositional block simplify creation dynamic graph model use library demonstrate concise batch wise parallel implementation variety model literature
Learning Expanding Graphs for Signal Interpolation,"  Performing signal processing over graphs requires knowledge of the underlying
fixed topology. However, graphs often grow in size with new nodes appearing
over time, whose connectivity is typically unknown; hence, making more
challenging the downstream tasks in applications like cold start
recommendation. We address such a challenge for signal interpolation at the
incoming nodes blind to the topological connectivity of the specific node.
Specifically, we propose a stochastic attachment model for incoming nodes
parameterized by the attachment probabilities and edge weights. We estimate
these parameters in a data-driven fashion by relying only on the attachment
behaviour of earlier incoming nodes with the goal of interpolating the signal
value. We study the non-convexity of the problem at hand, derive conditions
when it can be marginally convexified, and propose an alternating projected
descent approach between estimating the attachment probabilities and the edge
weights. Numerical experiments with synthetic and real data dealing in cold
start collaborative filtering corroborate our findings.
",perform signal processing graph require knowledge underlie fix topology however graph often grow size new node appear time whose connectivity typically unknown hence make challenge downstream task application like cold start recommendation address challenge signal interpolation incoming node blind topological connectivity specific node specifically propose stochastic attachment model incoming node parameterize attachment probability edge weight estimate parameter data drive fashion rely attachment behaviour early incoming node goal interpolate signal value study non convexity problem hand derive condition marginally convexifie propose alternate project descent approach estimate attachment probability edge weight numerical experiment synthetic real datum deal cold start collaborative filtering corroborate finding
"Increasing Trustworthiness of Deep Neural Networks via Accuracy
  Monitoring","  Inference accuracy of deep neural networks (DNNs) is a crucial performance
metric, but can vary greatly in practice subject to actual test datasets and is
typically unknown due to the lack of ground truth labels. This has raised
significant concerns with trustworthiness of DNNs, especially in
safety-critical applications. In this paper, we address trustworthiness of DNNs
by using post-hoc processing to monitor the true inference accuracy on a user's
dataset. Concretely, we propose a neural network-based accuracy monitor model,
which only takes the deployed DNN's softmax probability output as its input and
directly predicts if the DNN's prediction result is correct or not, thus
leading to an estimate of the true inference accuracy. The accuracy monitor
model can be pre-trained on a dataset relevant to the target application of
interest, and only needs to actively label a small portion (1% in our
experiments) of the user's dataset for model transfer. For estimation
robustness, we further employ an ensemble of monitor models based on the
Monte-Carlo dropout method. We evaluate our approach on different deployed DNN
models for image classification and traffic sign detection over multiple
datasets (including adversarial samples). The result shows that our accuracy
monitor model provides a close-to-true accuracy estimation and outperforms the
existing baseline methods.
",inference accuracy deep neural network dnn crucial performance metric vary greatly practice subject actual test dataset typically unknown due lack ground truth label raise significant concern trustworthiness dnn especially safety critical application paper address trustworthiness dnn use post hoc processing monitor true inference accuracy user dataset concretely propose neural network base accuracy monitor model take deploy dnn softmax probability output input directly predict dnn prediction result correct thus leading estimate true inference accuracy accuracy monitor model pre train dataset relevant target application interest need actively label small portion 1 experiment user dataset model transfer estimation robustness employ ensemble monitor model base monte carlo dropout method evaluate approach different deploy dnn model image classification traffic sign detection multiple dataset include adversarial sample result show accuracy monitor model provide close to true accuracy estimation outperform exist baseline method
FastNet,"  Inception and the Resnet family of Convolutional Neural Network
archi-tectures have broken records in the past few years, but recent state of
the art models have also incurred very high computational cost in terms of
training, inference and model size. Making the deployment of these models on
Edge devices, impractical. In light of this, we present a new novel
architecture that is designed for high computational efficiency on both GPUs
and CPUs, and is highly suited for deployment on Mobile Applications, Smart
Cameras, Iot devices and controllers as well as low cost drones. Our
architecture boasts competitive accuracies on standard Datasets even
out-performing the original Resnet. We present below the motivation for this
research, the architecture of the network, single test accuracies on CIFAR 10
and CIFAR 100 , a detailed comparison with other well-known architectures and
link to an implementation in Keras.
",inception resnet family convolutional neural network archi tecture broken record past year recent state art model also incur high computational cost term train inference model size make deployment model edge device impractical light present new novel architecture design high computational efficiency gpus cpus highly suited deployment mobile application smart camera iot device controller well low cost drone architecture boast competitive accuracy standard dataset even out perform original resnet present motivation research architecture network single test accuracy cifar 10 cifar 100 detailed comparison well know architecture link implementation keras
"On Bottleneck Features for Text-Dependent Speaker Verification Using
  X-vectors","  Applying x-vectors for speaker verification has recently attracted great
interest, with the focus being on text-independent speaker verification. In
this paper, we study x-vectors for text-dependent speaker verification (TD-SV),
which remains unexplored. We further investigate the impact of the different
bottleneck (BN) features on the performance of x-vectors, including the
recently-introduced time-contrastive-learning (TCL) BN features and
phone-discriminant BN features. TCL is a weakly supervised learning approach
that constructs training data by uniformly partitioning each utterance into a
predefined number of segments and then assigning each segment a class label
depending on their position in the utterance. We also compare TD-SV performance
for different modeling techniques, including the Gaussian mixture
models-universal background model (GMM-UBM), i-vector, and x-vector.
Experiments are conducted on the RedDots 2016 challenge database. It is found
that the type of features has a marginal impact on the performance of x-vectors
with the TCL BN feature achieving the lowest equal error rate, while the impact
of features is significant for i-vector and GMM-UBM. The fusion of x-vector and
i-vector systems gives a large gain in performance. The GMM-UBM technique shows
its advantage for TD-SV using short utterances.
",apply x vectors speaker verification recently attract great interest focus text independent speaker verification paper study x vectors text dependent speaker verification td sv remain unexplored investigate impact different bottleneck bn feature performance x vector include recently introduce time contrastive learning tcl bn feature phone discriminant bn feature tcl weakly supervised learning approach construct training datum uniformly partition utterance predefine number segment assign segment class label depend position utterance also compare td sv performance different modeling technique include gaussian mixture model universal background model gmm ubm i vector x vector experiment conduct reddot 2016 challenge database find type feature marginal impact performance x vector tcl bn feature achieve low equal error rate impact feature significant i vector gmm ubm fusion x vector i vector system give large gain performance gmm ubm technique show advantage td sv use short utterance
Clustering Structure of Microstructure Measures,"  This paper builds the clustering model of measures of market microstructure
features which are popular in predicting stock returns. In a 10-second
time-frequency, we study the clustering structure of different measures to find
out the best ones for predicting. In this way, we can predict more accurately
with a limited number of predictors, which removes the noise and makes the
model more interpretable.
",paper build cluster model measure market microstructure feature popular predict stock return 10 second time frequency study cluster structure different measure find good one predict way predict accurately limited number predictor remove noise make model interpretable
"Learning to recognize touch gestures: recurrent vs. convolutional
  features and dynamic sampling","  We propose a fully automatic method for learning gestures on big touch
devices in a potentially multi-user context. The goal is to learn general
models capable of adapting to different gestures, user styles and hardware
variations (e.g. device sizes, sampling frequencies and regularities).
  Based on deep neural networks, our method features a novel dynamic sampling
and temporal normalization component, transforming variable length gestures
into fixed length representations while preserving finger/surface contact
transitions, that is, the topology of the signal. This sequential
representation is then processed with a convolutional model capable, unlike
recurrent networks, of learning hierarchical representations with different
levels of abstraction.
  To demonstrate the interest of the proposed method, we introduce a new touch
gestures dataset with 6591 gestures performed by 27 people, which is, up to our
knowledge, the first of its kind: a publicly available multi-touch gesture
dataset for interaction.
  We also tested our method on a standard dataset of symbolic touch gesture
recognition, the MMG dataset, outperforming the state of the art and reporting
close to perfect performance.
",propose fully automatic method learn gesture big touch device potentially multi user context goal learn general model capable adapt different gesture user style hardware variation device size sample frequency regularity base deep neural network method feature novel dynamic sample temporal normalization component transform variable length gesture fix length representation preserve contact transition topology signal sequential representation process convolutional model capable unlike recurrent network learn hierarchical representation different level abstraction demonstrate interest propose method introduce new touch gesture dataset 6591 gesture perform 27 people knowledge first kind publicly available multi touch gesture dataset interaction also test method standard dataset symbolic touch gesture recognition mmg dataset outperform state art report close perfect performance
Pairwise Feedback for Data Programming,"  The scalability of the labeling process and the attainable quality of labels
have become limiting factors for many applications of machine learning. The
programmatic creation of labeled datasets via the synthesis of noisy heuristics
provides a promising avenue to address this problem. We propose to improve
modeling of latent class variables in the programmatic creation of labeled
datasets by incorporating pairwise feedback into the process. We discuss the
ease with which such pairwise feedback can be obtained or generated in many
application domains. Our experiments show that even a small number of sources
of pairwise feedback can substantially improve the quality of the posterior
estimate of the latent class variable.
",scalability labeling process attainable quality label become limit factor many application machine learn programmatic creation label dataset via synthesis noisy heuristic provide promise avenue address problem propose improve modeling latent class variable programmatic creation label dataset incorporate pairwise feedback process discuss ease pairwise feedback obtain generate many application domain experiment show even small number source pairwise feedback substantially improve quality posterior estimate latent class variable
Grid HTM: Hierarchical Temporal Memory for Anomaly Detection in Videos,"  The interest for video anomaly detection systems has gained traction for the
past few years. The current approaches use deep learning to perform anomaly
detection in videos, but this approach has multiple problems. For starters,
deep learning in general has issues with noise, concept drift, explainability,
and training data volumes. Additionally, anomaly detection in itself is a
complex task and faces challenges such as unknowness, heterogeneity, and class
imbalance. Anomaly detection using deep learning is therefore mainly
constrained to generative models such as generative adversarial networks and
autoencoders due to their unsupervised nature, but even they suffer from
general deep learning issues and are hard to train properly. In this paper, we
explore the capabilities of the Hierarchical Temporal Memory (HTM) algorithm to
perform anomaly detection in videos, as it has favorable properties such as
noise tolerance and online learning which combats concept drift. We introduce a
novel version of HTM, namely, Grid HTM, which is an HTM-based architecture
specifically for anomaly detection in complex videos such as surveillance
footage.
",interest video anomaly detection system gain traction past year current approach use deep learning perform anomaly detection video approach multiple problem starter deep learn general issue noise concept drift explainability training datum volume additionally anomaly detection complex task face challenge unknowness heterogeneity class imbalance anomaly detection use deep learning therefore mainly constrain generative model generative adversarial network autoencoder due unsupervised nature even suffer general deep learning issue hard train properly paper explore capabilitie hierarchical temporal memory htm algorithm perform anomaly detection video favorable property noise tolerance online learn combat concept drift introduce novel version htm namely grid htm htm base architecture specifically anomaly detection complex video surveillance footage
"Explainable Deep Learning for Video Recognition Tasks: A Framework &
  Recommendations","  The popularity of Deep Learning for real-world applications is ever-growing.
With the introduction of high performance hardware, applications are no longer
limited to image recognition. With the introduction of more complex problems
comes more and more complex solutions, and the increasing need for explainable
AI. Deep Neural Networks for Video tasks are amongst the most complex models,
with at least twice the parameters of their Image counterparts. However,
explanations for these models are often ill-adapted to the video domain. The
current work in explainability for video models is still overshadowed by Image
techniques, while Video Deep Learning itself is quickly gaining on methods for
still images. This paper seeks to highlight the need for explainability methods
designed with video deep learning models, and by association spatio-temporal
input in mind, by first illustrating the cutting edge for video deep learning,
and then noting the scarcity of research into explanations for these methods.
",popularity deep learn real world application ever grow introduction high performance hardware application long limit image recognition introduction complex problem come complex solution increase need explainable ai deep neural network video task amongst complex model least twice parameter image counterpart however explanation model often ill adapt video domain current work explainability video model still overshadow image technique video deep learning quickly gain method still image paper seek highlight need explainability method design video deep learning model association spatio temporal input mind first illustrate cut edge video deep learning note scarcity research explanation method
Quantization based Fast Inner Product Search,"  We propose a quantization based approach for fast approximate Maximum Inner
Product Search (MIPS). Each database vector is quantized in multiple subspaces
via a set of codebooks, learned directly by minimizing the inner product
quantization error. Then, the inner product of a query to a database vector is
approximated as the sum of inner products with the subspace quantizers.
Different from recently proposed LSH approaches to MIPS, the database vectors
and queries do not need to be augmented in a higher dimensional feature space.
We also provide a theoretical analysis of the proposed approach, consisting of
the concentration results under mild assumptions. Furthermore, if a small
sample of example queries is given at the training time, we propose a modified
codebook learning procedure which further improves the accuracy. Experimental
results on a variety of datasets including those arising from deep neural
networks show that the proposed approach significantly outperforms the existing
state-of-the-art.
",propose quantization base approach fast approximate maximum inner product search mips database vector quantize multiple subspace via set codebook learn directly minimize inner product quantization error inner product query database vector approximate sum inner product subspace quantizer different recently propose lsh approach mips database vector query need augmented high dimensional feature space also provide theoretical analysis propose approach consist concentration result mild assumption furthermore small sample example query give training time propose modify codebook learn procedure improve accuracy experimental result variety dataset include arise deep neural network show propose approach significantly outperform exist state of the art
"Train Flat, Then Compress: Sharpness-Aware Minimization Learns More
  Compressible Models","  Model compression by way of parameter pruning, quantization, or distillation
has recently gained popularity as an approach for reducing the computational
requirements of modern deep neural network models for NLP. Pruning unnecessary
parameters has emerged as a simple and effective method for compressing large
models that is compatible with a wide variety of contemporary off-the-shelf
hardware (unlike quantization), and that requires little additional training
(unlike distillation). Pruning approaches typically take a large, accurate
model as input, then attempt to discover a smaller subnetwork of that model
capable of achieving end-task accuracy comparable to the full model. Inspired
by previous work suggesting a connection between simpler, more generalizable
models and those that lie within flat basins in the loss landscape, we propose
to directly optimize for flat minima while performing task-specific pruning,
which we hypothesize should lead to simpler parameterizations and thus more
compressible models. In experiments combining sharpness-aware minimization with
both iterative magnitude pruning and structured pruning approaches, we show
that optimizing for flat minima consistently leads to greater compressibility
of parameters compared to standard Adam optimization when fine-tuning BERT
models, leading to higher rates of compression with little to no loss in
accuracy on the GLUE classification benchmark.
",model compression way parameter prune quantization distillation recently gain popularity approach reduce computational requirement modern deep neural network model nlp prune unnecessary parameter emerge simple effective method compress large model compatible wide variety contemporary off the shelf hardware unlike quantization require little additional training unlike distillation pruning approach typically take large accurate model input attempt discover small subnetwork model capable achieve end task accuracy comparable full model inspire previous work suggest connection simple generalizable model lie within flat basin loss landscape propose directly optimize flat minima perform task specific pruning hypothesize lead simple parameterization thus compressible model experiment combine sharpness aware minimization iterative magnitude prune structured pruning approach show optimize flat minima consistently lead great compressibility parameter compare standard adam optimization fine tune bert model lead high rate compression little loss accuracy glue classification benchmark
"Survey of Recent Multi-Agent Reinforcement Learning Algorithms Utilizing
  Centralized Training","  Much work has been dedicated to the exploration of Multi-Agent Reinforcement
Learning (MARL) paradigms implementing a centralized learning with
decentralized execution (CLDE) approach to achieve human-like collaboration in
cooperative tasks. Here, we discuss variations of centralized training and
describe a recent survey of algorithmic approaches. The goal is to explore how
different implementations of information sharing mechanism in centralized
learning may give rise to distinct group coordinated behaviors in multi-agent
systems performing cooperative tasks.
",much work dedicated exploration multi agent reinforcement learn marl paradigm implement centralized learning decentralize execution clde approach achieve human like collaboration cooperative task discuss variation centralize training describe recent survey algorithmic approach goal explore different implementation information sharing mechanism centralize learning may give rise distinct group coordinate behavior multi agent system perform cooperative task
"Zero-shot Sequence Labeling: Transferring Knowledge from Sentences to
  Tokens","  Can attention- or gradient-based visualization techniques be used to infer
token-level labels for binary sequence tagging problems, using networks trained
only on sentence-level labels? We construct a neural network architecture based
on soft attention, train it as a binary sentence classifier and evaluate
against token-level annotation on four different datasets. Inferring token
labels from a network provides a method for quantitatively evaluating what the
model is learning, along with generating useful feedback in assistance systems.
Our results indicate that attention-based methods are able to predict
token-level labels more accurately, compared to gradient-based methods,
sometimes even rivaling the supervised oracle network.
",attention- gradient base visualization technique use infer token level label binary sequence tagging problem use network train sentence level label construct neural network architecture base soft attention train binary sentence classifier evaluate token level annotation four different dataset infer token label network provide method quantitatively evaluate model learn along generate useful feedback assistance system result indicate attention base method able predict token level label accurately compare gradient base method sometimes even rival supervised oracle network
"Improvements to short-term weather prediction with
  recurrent-convolutional networks","  The Weather4cast 2021 competition gave the participants a task of predicting
the time evolution of two-dimensional fields of satellite-based meteorological
data. This paper describes the author's efforts, after initial success in the
first stage of the competition, to improve the model further in the second
stage. The improvements consisted of a shallower model variant that is
competitive against the deeper version, adoption of the AdaBelief optimizer,
improved handling of one of the predicted variables where the training set was
found not to represent the validation set well, and ensembling multiple models
to improve the results further. The largest quantitative improvements to the
competition metrics can be attributed to the increased amount of training data
available in the second stage of the competition, followed by the effects of
model ensembling. Qualitative results show that the model can predict the time
evolution of the fields, including the motion of the fields over time, starting
with sharp predictions for the immediate future and blurring of the outputs in
later frames to account for the increased uncertainty.
",weather4cast 2021 competition give participant task predict time evolution two dimensional field satellite base meteorological datum paper describe author effort initial success first stage competition improve model second stage improvement consist shallow model variant competitive deep version adoption adabelief optimizer improve handle one predict variable train set find represent validation set well ensemble multiple model improve result large quantitative improvement competition metric attribute increase amount training datum available second stage competition follow effect model ensemble qualitative result show model predict time evolution field include motion field time start sharp prediction immediate future blur output later frame account increase uncertainty
WeightScale: Interpreting Weight Change in Neural Networks,"  Interpreting the learning dynamics of neural networks can provide useful
insights into how networks learn and the development of better training and
design approaches. We present an approach to interpret learning in neural
networks by measuring relative weight change on a per layer basis and
dynamically aggregating emerging trends through combination of dimensionality
reduction and clustering which allows us to scale to very deep networks. We use
this approach to investigate learning in the context of vision tasks across a
variety of state-of-the-art networks and provide insights into the learning
behavior of these networks, including how task complexity affects layer-wise
learning in deeper layers of networks.
",interpret learn dynamic neural network provide useful insight network learn development well training design approach present approach interpret learn neural network measure relative weight change per layer basis dynamically aggregate emerge trend combination dimensionality reduction clustering allow we scale deep network use approach investigate learn context vision task across variety state of the art network provide insight learn behavior network include task complexity affect layer wise learn deep layer network
"Approximation Algorithms for Sparse Best Rank-1 Approximation to
  Higher-Order Tensors","  Sparse tensor best rank-1 approximation (BR1Approx), which is a sparsity
generalization of the dense tensor BR1Approx, and is a higher-order extension
of the sparse matrix BR1Approx, is one of the most important problems in sparse
tensor decomposition and related problems arising from statistics and machine
learning. By exploiting the multilinearity as well as the sparsity structure of
the problem, four approximation algorithms are proposed, which are easily
implemented, of low computational complexity, and can serve as initial
procedures for iterative algorithms. In addition, theoretically guaranteed
worst-case approximation lower bounds are proved for all the algorithms. We
provide numerical experiments on synthetic and real data to illustrate the
effectiveness of the proposed algorithms.
",sparse tensor good rank-1 approximation br1approx sparsity generalization dense tensor br1approx high order extension sparse matrix br1approx one important problem sparse tensor decomposition relate problem arise statistic machine learning exploit multilinearity well sparsity structure problem four approximation algorithm propose easily implement low computational complexity serve initial procedure iterative algorithm addition theoretically guarantee bad case approximation low bound prove algorithm provide numerical experiment synthetic real datum illustrate effectiveness propose algorithm
"Variable Importance Clouds: A Way to Explore Variable Importance for the
  Set of Good Models","  Variable importance is central to scientific studies, including the social
sciences and causal inference, healthcare, and other domains. However, current
notions of variable importance are often tied to a specific predictive model.
This is problematic: what if there were multiple well-performing predictive
models, and a specific variable is important to some of them and not to others?
In that case, we may not be able to tell from a single well-performing model
whether a variable is always important in predicting the outcome. Rather than
depending on variable importance for a single predictive model, we would like
to explore variable importance for all approximately-equally-accurate
predictive models. This work introduces the concept of a variable importance
cloud, which maps every variable to its importance for every good predictive
model. We show properties of the variable importance cloud and draw connections
to other areas of statistics. We introduce variable importance diagrams as a
projection of the variable importance cloud into two dimensions for
visualization purposes. Experiments with criminal justice, marketing data, and
image classification tasks illustrate how variables can change dramatically in
importance for approximately-equally-accurate predictive models
",variable importance central scientific study include social science causal inference healthcare domain however current notion variable importance often tie specific predictive model problematic multiple well perform predictive model specific variable important other case may able tell single well perform model whether variable always important predict outcome rather depend variable importance single predictive model would like explore variable importance approximately equally accurate predictive model work introduce concept variable importance cloud map every variable importance every good predictive model show property variable importance cloud draw connection area statistic introduce variable importance diagrams projection variable importance cloud two dimension visualization purpose experiment criminal justice marketing datum image classification task illustrate variable change dramatically importance approximately equally accurate predictive model
Reparameterizable Subset Sampling via Continuous Relaxations,"  Many machine learning tasks require sampling a subset of items from a
collection based on a parameterized distribution. The Gumbel-softmax trick can
be used to sample a single item, and allows for low-variance reparameterized
gradients with respect to the parameters of the underlying distribution.
However, stochastic optimization involving subset sampling is typically not
reparameterizable. To overcome this limitation, we define a continuous
relaxation of subset sampling that provides reparameterization gradients by
generalizing the Gumbel-max trick. We use this approach to sample subsets of
features in an instance-wise feature selection task for model interpretability,
subsets of neighbors to implement a deep stochastic k-nearest neighbors model,
and sub-sequences of neighbors to implement parametric t-SNE by directly
comparing the identities of local neighbors. We improve performance in all
these tasks by incorporating subset sampling in end-to-end training.
",many machine learn task require sample subset item collection base parameterized distribution gumbel softmax trick use sample single item allow low variance reparameterize gradient respect parameter underlie distribution however stochastic optimization involve subset sample typically reparameterizable overcome limitation define continuous relaxation subset sampling provide reparameterization gradient generalize gumbel max trick use approach sample subset feature instance wise feature selection task model interpretability subset neighbor implement deep stochastic k near neighbor model sub sequence neighbor implement parametric t sne directly compare identity local neighbor improve performance task incorporate subset sample end to end training
Automatic Differentiation Variational Inference,"  Probabilistic modeling is iterative. A scientist posits a simple model, fits
it to her data, refines it according to her analysis, and repeats. However,
fitting complex models to large data is a bottleneck in this process. Deriving
algorithms for new models can be both mathematically and computationally
challenging, which makes it difficult to efficiently cycle through the steps.
To this end, we develop automatic differentiation variational inference (ADVI).
Using our method, the scientist only provides a probabilistic model and a
dataset, nothing else. ADVI automatically derives an efficient variational
inference algorithm, freeing the scientist to refine and explore many models.
ADVI supports a broad class of models-no conjugacy assumptions are required. We
study ADVI across ten different models and apply it to a dataset with millions
of observations. ADVI is integrated into Stan, a probabilistic programming
system; it is available for immediate use.
",probabilistic modeling iterative scientist posit simple model fit datum refine accord analysis repeat however fitting complex model large datum bottleneck process derive algorithm new model mathematically computationally challenging make difficult efficiently cycle step end develop automatic differentiation variational inference advi use method scientist provide probabilistic model dataset nothing else advi automatically derive efficient variational inference algorithm freeing scientist refine explore many model advi support broad class model no conjugacy assumption require study advi across ten different model apply dataset million observation advi integrate stan probabilistic programming system available immediate use
Agnostic Physics-Driven Deep Learning,"  This work establishes that a physical system can perform statistical learning
without gradient computations, via an Agnostic Equilibrium Propagation
(Aeqprop) procedure that combines energy minimization, homeostatic control, and
nudging towards the correct response. In Aeqprop, the specifics of the system
do not have to be known: the procedure is based only on external manipulations,
and produces a stochastic gradient descent without explicit gradient
computations. Thanks to nudging, the system performs a true, order-one gradient
step for each training sample, in contrast with order-zero methods like
reinforcement or evolutionary strategies, which rely on trial and error. This
procedure considerably widens the range of potential hardware for statistical
learning to any system with enough controllable parameters, even if the details
of the system are poorly known. Aeqprop also establishes that in natural
(bio)physical systems, genuine gradient-based statistical learning may result
from generic, relatively simple mechanisms, without backpropagation and its
requirement for analytic knowledge of partial derivatives.
",work establish physical system perform statistical learning without gradient computation via agnostic equilibrium propagation aeqprop procedure combine energy minimization homeostatic control nudge towards correct response aeqprop specific system know procedure base external manipulation produce stochastic gradient descent without explicit gradient computation thanks nudge system perform true order one gradient step training sample contrast order zero method like reinforcement evolutionary strategy rely trial error procedure considerably widen range potential hardware statistical learning system enough controllable parameter even detail system poorly know aeqprop also establish natural bio physical system genuine gradient base statistical learning may result generic relatively simple mechanism without backpropagation requirement analytic knowledge partial derivative
"Grounded Graph Decoding Improves Compositional Generalization in
  Question Answering","  Question answering models struggle to generalize to novel compositions of
training patterns, such to longer sequences or more complex test structures.
Current end-to-end models learn a flat input embedding which can lose input
syntax context. Prior approaches improve generalization by learning permutation
invariant models, but these methods do not scale to more complex train-test
splits. We propose Grounded Graph Decoding, a method to improve compositional
generalization of language representations by grounding structured predictions
with an attention mechanism. Grounding enables the model to retain syntax
information from the input in thereby significantly improving generalization
over complex inputs. By predicting a structured graph containing conjunctions
of query clauses, we learn a group invariant representation without making
assumptions on the target domain. Our model significantly outperforms
state-of-the-art baselines on the Compositional Freebase Questions (CFQ)
dataset, a challenging benchmark for compositional generalization in question
answering. Moreover, we effectively solve the MCD1 split with 98% accuracy.
",question answer model struggle generalize novel composition train pattern long sequence complex test structure current end to end model learn flat input embed lose input syntax context prior approach improve generalization learn permutation invariant model method scale complex train test split propose ground graph decode method improve compositional generalization language representation ground structured prediction attention mechanism ground enable model retain syntax information input thereby significantly improve generalization complex input predict structured graph contain conjunction query clause learn group invariant representation without make assumption target domain model significantly outperform state of the art baseline compositional freebase question cfq dataset challenge benchmark compositional generalization question answer moreover effectively solve mcd1 split 98 accuracy
"Practical applicability of deep neural networks for overlapping speaker
  separation","  This paper examines the applicability in realistic scenarios of two deep
learning based solutions to the overlapping speaker separation problem.
Firstly, we present experiments that show that these methods are applicable for
a broad range of languages. Further experimentation indicates limited
performance loss for untrained languages, when these have common features with
the trained language(s). Secondly, it investigates how the methods deal with
realistic background noise and proposes some modifications to better cope with
these disturbances. The deep learning methods that will be examined are deep
clustering and deep attractor networks.
",paper examine applicability realistic scenario two deep learning base solution overlap speaker separation problem firstly present experiment show method applicable broad range language experimentation indicate limited performance loss untraine language common feature train language secondly investigate method deal realistic background noise propose modification well cope disturbance deep learning method examine deep clustering deep attractor network
"Mixed Integer Linear Programming for Feature Selection in Support Vector
  Machine","  This work focuses on support vector machine (SVM) with feature selection. A
MILP formulation is proposed for the problem. The choice of suitable features
to construct the separating hyperplanes has been modelled in this formulation
by including a budget constraint that sets in advance a limit on the number of
features to be used in the classification process. We propose both an exact and
a heuristic procedure to solve this formulation in an efficient way. Finally,
the validation of the model is done by checking it with some well-known data
sets and comparing it with classical classification methods.
",work focus support vector machine svm feature selection milp formulation propose problem choice suitable feature construct separate hyperplane model formulation include budget constraint set advance limit number feature use classification process propose exact heuristic procedure solve formulation efficient way finally validation model do check well know data set compare classical classification method
Hide-and-Seek: A Template for Explainable AI,"  Lack of transparency has been the Achilles heal of Neural Networks and their
wider adoption in industry. Despite significant interest this shortcoming has
not been adequately addressed. This study proposes a novel framework called
Hide-and-Seek (HnS) for training Interpretable Neural Networks and establishes
a theoretical foundation for exploring and comparing similar ideas. Extensive
experimentation indicates that a high degree of interpretability can be imputed
into Neural Networks, without sacrificing their predictive power.
",lack transparency achille heal neural network wide adoption industry despite significant interest shortcoming adequately address study propose novel framework call hide and seek hns training interpretable neural network establish theoretical foundation explore compare similar idea extensive experimentation indicate high degree interpretability impute neural network without sacrifice predictive power
"Predicting drug-target interaction using 3D structure-embedded graph
  representations from graph neural networks","  Accurate prediction of drug-target interaction (DTI) is essential for in
silico drug design. For the purpose, we propose a novel approach for predicting
DTI using a GNN that directly incorporates the 3D structure of a protein-ligand
complex. We also apply a distance-aware graph attention algorithm with gate
augmentation to increase the performance of our model. As a result, our model
shows better performance than docking and other deep learning methods for both
virtual screening and pose prediction. In addition, our model can reproduce the
natural population distribution of active molecules and inactive molecules.
",accurate prediction drug target interaction dti essential silico drug design purpose propose novel approach predict dti use gnn directly incorporate 3d structure protein ligand complex also apply distance aware graph attention algorithm gate augmentation increase performance model result model show well performance dock deep learning method virtual screening pose prediction addition model reproduce natural population distribution active molecule inactive molecule
DadaGP: A Dataset of Tokenized GuitarPro Songs for Sequence Models,"  Originating in the Renaissance and burgeoning in the digital era, tablatures
are a commonly used music notation system which provides explicit
representations of instrument fingerings rather than pitches. GuitarPro has
established itself as a widely used tablature format and software enabling
musicians to edit and share songs for musical practice, learning, and
composition. In this work, we present DadaGP, a new symbolic music dataset
comprising 26,181 song scores in the GuitarPro format covering 739 musical
genres, along with an accompanying tokenized format well-suited for generative
sequence models such as the Transformer. The tokenized format is inspired by
event-based MIDI encodings, often used in symbolic music generation models. The
dataset is released with an encoder/decoder which converts GuitarPro files to
tokens and back. We present results of a use case in which DadaGP is used to
train a Transformer-based model to generate new songs in GuitarPro format. We
discuss other relevant use cases for the dataset (guitar-bass transcription,
music style transfer and artist/genre classification) as well as ethical
implications. DadaGP opens up the possibility to train GuitarPro score
generators, fine-tune models on custom data, create new styles of music,
AI-powered songwriting apps, and human-AI improvisation.
",originate renaissance burgeon digital era tablature commonly use music notation system provide explicit representation instrument fingering rather pitch guitarpro establish widely use tablature format software enable musician edit share song musical practice learn composition work present dadagp new symbolic music dataset comprise song score guitarpro format cover 739 musical genre along accompany tokenized format well suit generative sequence model transformer tokenized format inspire event base midi encoding often use symbolic music generation model dataset release convert guitarpro file token back present result use case dadagp use train transformer base model generate new song guitarpro format discuss relevant use case dataset guitar bass transcription music style transfer classification well ethical implication dadagp open possibility train guitarpro score generator fine tune model custom datum create new style music ai power songwriting app human ai improvisation
Optimal Time-Series Motifs,"  Motifs are the most repetitive/frequent patterns of a time-series. The
discovery of motifs is crucial for practitioners in order to understand and
interpret the phenomena occurring in sequential data. Currently, motifs are
searched among series sub-sequences, aiming at selecting the most frequently
occurring ones. Search-based methods, which try out series sub-sequence as
motif candidates, are currently believed to be the best methods in finding the
most frequent patterns.
  However, this paper proposes an entirely new perspective in finding motifs.
We demonstrate that searching is non-optimal since the domain of motifs is
restricted, and instead we propose a principled optimization approach able to
find optimal motifs. We treat the occurrence frequency as a function and
time-series motifs as its parameters, therefore we \textit{learn} the optimal
motifs that maximize the frequency function. In contrast to searching, our
method is able to discover the most repetitive patterns (hence optimal), even
in cases where they do not explicitly occur as sub-sequences. Experiments on
several real-life time-series datasets show that the motifs found by our method
are highly more frequent than the ones found through searching, for exactly the
same distance threshold.
",motifs pattern time series discovery motif crucial practitioner order understand interpret phenomena occur sequential datum currently motif search among series sub sequence aim select frequently occur one search base method try series sub sequence motif candidate currently believe good method find frequent pattern however paper propose entirely new perspective find motif demonstrate search non optimal since domain motif restrict instead propose principled optimization approach able find optimal motif treat occurrence frequency function time series motif parameter therefore learn optimal motif maximize frequency function contrast search method able discover repetitive pattern hence optimal even case explicitly occur sub sequence experiment several real life time series dataset show motif find method highly frequent one find search exactly distance threshold
SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards,"  Learning to imitate expert behavior from demonstrations can be challenging,
especially in environments with high-dimensional, continuous observations and
unknown dynamics. Supervised learning methods based on behavioral cloning (BC)
suffer from distribution shift: because the agent greedily imitates
demonstrated actions, it can drift away from demonstrated states due to error
accumulation. Recent methods based on reinforcement learning (RL), such as
inverse RL and generative adversarial imitation learning (GAIL), overcome this
issue by training an RL agent to match the demonstrations over a long horizon.
Since the true reward function for the task is unknown, these methods learn a
reward function from the demonstrations, often using complex and brittle
approximation techniques that involve adversarial training. We propose a simple
alternative that still uses RL, but does not require learning a reward
function. The key idea is to provide the agent with an incentive to match the
demonstrations over a long horizon, by encouraging it to return to demonstrated
states upon encountering new, out-of-distribution states. We accomplish this by
giving the agent a constant reward of r=+1 for matching the demonstrated action
in a demonstrated state, and a constant reward of r=0 for all other behavior.
Our method, which we call soft Q imitation learning (SQIL), can be implemented
with a handful of minor modifications to any standard Q-learning or off-policy
actor-critic algorithm. Theoretically, we show that SQIL can be interpreted as
a regularized variant of BC that uses a sparsity prior to encourage
long-horizon imitation. Empirically, we show that SQIL outperforms BC and
achieves competitive results compared to GAIL, on a variety of image-based and
low-dimensional tasks in Box2D, Atari, and MuJoCo.
",learning imitate expert behavior demonstration challenge especially environment high dimensional continuous observation unknown dynamic supervise learning method base behavioral cloning bc suffer distribution shift agent greedily imitate demonstrate action drift away demonstrate state due error accumulation recent method base reinforcement learning rl inverse rl generative adversarial imitation learning gail overcome issue training rl agent match demonstration long horizon since true reward function task unknown method learn reward function demonstration often use complex brittle approximation technique involve adversarial training propose simple alternative still use rl require learn reward function key idea provide agent incentive match demonstration long horizon encourage return demonstrate state upon encounter new out of distribution state accomplish give agent constant reward matching demonstrate action demonstrate state constant reward behavior method call soft q imitation learning sqil implement handful minor modification standard q learn off policy actor critic algorithm theoretically show sqil interpret regularize variant bc use sparsity prior encourage long horizon imitation empirically show sqil outperform bc achieve competitive result compare gail variety image base low dimensional task box2d atari mujoco
Random projections of random manifolds,"  Interesting data often concentrate on low dimensional smooth manifolds inside
a high dimensional ambient space. Random projections are a simple, powerful
tool for dimensionality reduction of such data. Previous works have studied
bounds on how many projections are needed to accurately preserve the geometry
of these manifolds, given their intrinsic dimensionality, volume and curvature.
However, such works employ definitions of volume and curvature that are
inherently difficult to compute. Therefore such theory cannot be easily tested
against numerical simulations to understand the tightness of the proven bounds.
We instead study typical distortions arising in random projections of an
ensemble of smooth Gaussian random manifolds. We find explicitly computable,
approximate theoretical bounds on the number of projections required to
accurately preserve the geometry of these manifolds. Our bounds, while
approximate, can only be violated with a probability that is exponentially
small in the ambient dimension, and therefore they hold with high probability
in cases of practical interest. Moreover, unlike previous work, we test our
theoretical bounds against numerical experiments on the actual geometric
distortions that typically occur for random projections of random smooth
manifolds. We find our bounds are tighter than previous results by several
orders of magnitude.
",interesting datum often concentrate low dimensional smooth manifold inside high dimensional ambient space random projection simple powerful tool dimensionality reduction datum previous work study bound many projection need accurately preserve geometry manifold give intrinsic dimensionality volume curvature however works employ definition volume curvature inherently difficult compute therefore theory easily test numerical simulation understand tightness prove bound instead study typical distortion arise random projection ensemble smooth gaussian random manifold find explicitly computable approximate theoretical bound number projection required accurately preserve geometry manifold bounds approximate violate probability exponentially small ambient dimension therefore hold high probability case practical interest moreover unlike previous work test theoretical bound numerical experiment actual geometric distortion typically occur random projection random smooth manifold find bound tight previous result several order magnitude
"Learning with Safety Constraints: Sample Complexity of Reinforcement
  Learning for Constrained MDPs","  Many physical systems have underlying safety considerations that require that
the policy employed ensures the satisfaction of a set of constraints. The
analytical formulation usually takes the form of a Constrained Markov Decision
Process (CMDP). We focus on the case where the CMDP is unknown, and RL
algorithms obtain samples to discover the model and compute an optimal
constrained policy. Our goal is to characterize the relationship between safety
constraints and the number of samples needed to ensure a desired level of
accuracy -- both objective maximization and constraint satisfaction -- in a PAC
sense. We explore two classes of RL algorithms, namely, (i) a generative model
based approach, wherein samples are taken initially to estimate a model, and
(ii) an online approach, wherein the model is updated as samples are obtained.
Our main finding is that compared to the best known bounds of the unconstrained
regime, the sample complexity of constrained RL algorithms are increased by a
factor that is logarithmic in the number of constraints, which suggests that
the approach may be easily utilized in real systems.
",many physical system underlie safety consideration require policy employ ensure satisfaction set constraint analytical formulation usually take form constrain markov decision process cmdp focus case cmdp unknown rl algorithm obtain sample discover model compute optimal constrain policy goal characterize relationship safety constraint number sample need ensure desire level accuracy objective maximization constraint satisfaction pac sense explore two class rl algorithm namely generative model base approach wherein sample take initially estimate model ii online approach wherein model update sample obtain main finding compare well know bound unconstrained regime sample complexity constrain rl algorithm increase factor logarithmic number constraint suggest approach may easily utilize real system
End-to-end Kernel Learning via Generative Random Fourier Features,"  Random Fourier features (RFFs) provide a promising way for kernel learning in
a spectral case. Current RFFs-based kernel learning methods usually work in a
two-stage way. In the first-stage process, learning the optimal feature map is
often formulated as a target alignment problem, which aims to align the learned
kernel with the pre-defined target kernel (usually the ideal kernel). In the
second-stage process, a linear learner is conducted with respect to the mapped
random features. Nevertheless, the pre-defined kernel in target alignment is
not necessarily optimal for the generalization of the linear learner. Instead,
in this paper, we consider a one-stage process that incorporates the kernel
learning and linear learner into a unifying framework. To be specific, a
generative network via RFFs is devised to implicitly learn the kernel, followed
by a linear classifier parameterized as a full-connected layer. Then the
generative network and the classifier are jointly trained by solving the
empirical risk minimization (ERM) problem to reach a one-stage solution. This
end-to-end scheme naturally allows deeper features, in correspondence to a
multi-layer structure, and shows superior generalization performance over the
classical two-stage, RFFs-based methods in real-world classification tasks.
Moreover, inspired by the randomized resampling mechanism of the proposed
method, its enhanced adversarial robustness is investigated and experimentally
verified.
",random fourier feature rff provide promising way kernel learn spectral case current rff base kernel learn method usually work two stage way first stage process learn optimal feature map often formulate target alignment problem aim align learn kernel pre defined target kernel usually ideal kernel second stage process linear learner conduct respect map random feature nevertheless pre define kernel target alignment necessarily optimal generalization linear learner instead paper consider one stage process incorporate kernel learn linear learner unifying framework specific generative network via rff devise implicitly learn kernel follow linear classifier parameterized full connect layer generative network classifier jointly train solve empirical risk minimization erm problem reach one stage solution end to end scheme naturally allow deep feature correspondence multi layer structure show superior generalization performance classical two stage rff base method real world classification task moreover inspire randomized resampling mechanism propose method enhance adversarial robustness investigate experimentally verify
Content-based Music Similarity with Triplet Networks,"  We explore the feasibility of using triplet neural networks to embed songs
based on content-based music similarity. Our network is trained using triplets
of songs such that two songs by the same artist are embedded closer to one
another than to a third song by a different artist. We compare two models that
are trained using different ways of picking this third song: at random vs.
based on shared genre labels. Our experiments are conducted using songs from
the Free Music Archive and use standard audio features. The initial results
show that shallow Siamese networks can be used to embed music for a simple
artist retrieval task.
",explore feasibility use triplet neural network embed song base content base music similarity network train use triplet song two song artist embed close one another third song different artist compare two model train use different way pick third song random base shared genre label experiment conduct use song free music archive use standard audio feature initial result show shallow siamese network use embed music simple artist retrieval task
"Provable Benefit of Multitask Representation Learning in Reinforcement
  Learning","  As representation learning becomes a powerful technique to reduce sample
complexity in reinforcement learning (RL) in practice, theoretical
understanding of its advantage is still limited. In this paper, we
theoretically characterize the benefit of representation learning under the
low-rank Markov decision process (MDP) model. We first study multitask low-rank
RL (as upstream training), where all tasks share a common representation, and
propose a new multitask reward-free algorithm called REFUEL. REFUEL learns both
the transition kernel and the near-optimal policy for each task, and outputs a
well-learned representation for downstream tasks. Our result demonstrates that
multitask representation learning is provably more sample-efficient than
learning each task individually, as long as the total number of tasks is above
a certain threshold. We then study the downstream RL in both online and offline
settings, where the agent is assigned with a new task sharing the same
representation as the upstream tasks. For both online and offline settings, we
develop a sample-efficient algorithm, and show that it finds a near-optimal
policy with the suboptimality gap bounded by the sum of the estimation error of
the learned representation in upstream and a vanishing term as the number of
downstream samples becomes large. Our downstream results of online and offline
RL further capture the benefit of employing the learned representation from
upstream as opposed to learning the representation of the low-rank model
directly. To the best of our knowledge, this is the first theoretical study
that characterizes the benefit of representation learning in exploration-based
reward-free multitask RL for both upstream and downstream tasks.
",representation learning become powerful technique reduce sample complexity reinforcement learning rl practice theoretical understanding advantage still limited paper theoretically characterize benefit representation learn low rank markov decision process mdp model first study multitask low rank rl upstream training task share common representation propose new multitask reward free algorithm call refuel refuel learns transition kernel near optimal policy task output well learn representation downstream task result demonstrate multitask representation learn provably sample efficient learning task individually long total number task certain threshold study downstream rl online offline setting agent assign new task share representation upstream task online offline setting develop sample efficient algorithm show find near optimal policy suboptimality gap bound sum estimation error learn representation upstream vanish term number downstream sample become large downstream result online offline rl capture benefit employing learn representation upstream oppose learn representation low rank model directly good knowledge first theoretical study characterize benefit representation learn exploration base reward free multitask rl upstream downstream task
Transformer for Partial Differential Equations' Operator Learning,"  Data-driven learning of partial differential equations' solution operators
has recently emerged as a promising paradigm for approximating the underlying
solutions. The solution operators are usually parameterized by deep learning
models that are built upon problem-specific inductive biases. An example is a
convolutional or a graph neural network that exploits the local grid structure
where functions' values are sampled. The attention mechanism, on the other
hand, provides a flexible way to implicitly exploit the patterns within inputs,
and furthermore, relationship between arbitrary query locations and inputs. In
this work, we present an attention-based framework for data-driven operator
learning, which we term Operator Transformer (OFormer). Our framework is built
upon self-attention, cross-attention, and a set of point-wise multilayer
perceptrons (MLPs), and thus it makes few assumptions on the sampling pattern
of the input function or query locations. We show that the proposed framework
is competitive on standard benchmark problems and can flexibly be adapted to
randomly sampled input.
",data drive learn partial differential equation solution operator recently emerge promise paradigm approximate underlie solution solution operator usually parameterize deep learning model build upon problem specific inductive bias example convolutional graph neural network exploit local grid structure function value sample attention mechanism hand provide flexible way implicitly exploit pattern within input furthermore relationship arbitrary query location input work present attention base framework data drive operator learn term operator transformer oform framework build upon self attention cross attention set point wise multilayer perceptron mlp thus make assumption sample pattern input function query location show propose framework competitive standard benchmark problem flexibly adapt randomly sample input
"The Power of Factorial Powers: New Parameter settings for (Stochastic)
  Optimization","  The convergence rates for convex and non-convex optimization methods depend
on the choice of a host of constants, including step sizes, Lyapunov function
constants and momentum constants. In this work we propose the use of factorial
powers as a flexible tool for defining constants that appear in convergence
proofs. We list a number of remarkable properties that these sequences enjoy,
and show how they can be applied to convergence proofs to simplify or improve
the convergence rates of the momentum method, accelerated gradient and the
stochastic variance reduced method (SVRG).
",convergence rate convex non convex optimization method depend choice host constant include step size lyapunov function constant momentum constant work propose use factorial power flexible tool define constant appear convergence proof list number remarkable property sequence enjoy show apply convergence proof simplify improve convergence rate momentum method accelerate gradient stochastic variance reduce method svrg
"Machine Learning Product State Distributions from Initial Reactant
  States for a Reactive Atom-Diatom Collision System","  A machine learned (ML) model for predicting product state distributions from
specific initial states (state-to-distribution or STD) for reactive atom-diatom
collisions is presented and quantitatively tested for the N($^4$S)+O$_{2}$(X$^3
\Sigma_{\rm g}^{-}$) $\rightarrow$ NO(X$^2\Pi$) +O($^3$P) reaction. The
reference data set for training the neural network (NN) consists of final state
distributions determined from explicit quasi-classical trajectory (QCT)
simulations for $\sim 2000$ initial conditions. Overall, the prediction
accuracy as quantified by the root-mean-squared difference $(\sim 0.003)$ and
the $R^2$ $(\sim 0.99)$ between the reference QCT and predictions of the STD
model is high for the test set and off-grid state specific initial conditions
and for initial conditions drawn from reactant state distributions
characterized by translational, rotational and vibrational temperatures.
Compared with a more coarse grained distribution-to-distribution (DTD) model
evaluated on the same initial state distributions, the STD model shows
comparable performance with the additional benefit of the state resolution in
the reactant preparation. Starting from specific initial states also leads to a
more diverse range of final state distributions which requires a more
expressive neural network to be used compared with DTD. Direct comparison
between explicit QCT simulations, the STD model, and the widely used
Larsen-Borgnakke (LB) model shows that the STD model is quantitative whereas
the LB model is qualitative at best for rotational distributions $P(j')$ and
fails for vibrational distributions $P(v')$. As such the STD model can be
well-suited for simulating nonequilibrium high-speed flows, e.g., using the
direct simulation Monte Carlo method.
",machine learn ml model predict product state distribution specific initial state state to distribution std reactive atom diatom collision present quantitatively test n 2 x g x p reaction reference datum set training neural network nn consist final state distribution determine explicit quasi classical trajectory qct simulation 2000 initial condition overall prediction accuracy quantify root mean square difference reference qct prediction std model high test set off grid state specific initial condition initial condition draw reactant state distribution characterize translational rotational vibrational temperature compare coarse grain distribution to distribution dtd model evaluate initial state distribution std model show comparable performance additional benefit state resolution reactant preparation start specific initial state also lead diverse range final state distribution require expressive neural network use compare dtd direct comparison explicit qct simulation std model widely use larsen borgnakke lb model show std model quantitative whereas lb model qualitative good rotational distribution p j fail vibrational distribution p v std model well suit simulating nonequilibrium high speed flow use direct simulation monte carlo method
Spectral-Spatial Diffusion Geometry for Hyperspectral Image Clustering,"  An unsupervised learning algorithm to cluster hyperspectral image (HSI) data
is proposed that exploits spatially-regularized random walks. Markov diffusions
are defined on the space of HSI spectra with transitions constrained to near
spatial neighbors. The explicit incorporation of spatial regularity into the
diffusion construction leads to smoother random processes that are more adapted
for unsupervised machine learning than those based on spectra alone. The
regularized diffusion process is subsequently used to embed the
high-dimensional HSI into a lower dimensional space through diffusion
distances. Cluster modes are computed using density estimation and diffusion
distances, and all other points are labeled according to these modes. The
proposed method has low computational complexity and performs competitively
against state-of-the-art HSI clustering algorithms on real data. In particular,
the proposed spatial regularization confers an empirical advantage over
non-regularized methods.
",unsupervised learn algorithm cluster hyperspectral image hsi datum propose exploit spatially regularize random walk markov diffusion define space hsi spectra transition constrain near spatial neighbor explicit incorporation spatial regularity diffusion construction lead smoother random process adapt unsupervised machine learning base spectra alone regularized diffusion process subsequently use embe high dimensional hsi low dimensional space diffusion distance cluster mode compute use density estimation diffusion distance point label accord mode propose method low computational complexity perform competitively state of the art hsi cluster algorithm real datum particular propose spatial regularization confer empirical advantage non regularized method
"Invariant Information Clustering for Unsupervised Image Classification
  and Segmentation","  We present a novel clustering objective that learns a neural network
classifier from scratch, given only unlabelled data samples. The model
discovers clusters that accurately match semantic classes, achieving
state-of-the-art results in eight unsupervised clustering benchmarks spanning
image classification and segmentation. These include STL10, an unsupervised
variant of ImageNet, and CIFAR10, where we significantly beat the accuracy of
our closest competitors by 6.6 and 9.5 absolute percentage points respectively.
The method is not specialised to computer vision and operates on any paired
dataset samples; in our experiments we use random transforms to obtain a pair
from each image. The trained network directly outputs semantic labels, rather
than high dimensional representations that need external processing to be
usable for semantic clustering. The objective is simply to maximise mutual
information between the class assignments of each pair. It is easy to implement
and rigorously grounded in information theory, meaning we effortlessly avoid
degenerate solutions that other clustering methods are susceptible to. In
addition to the fully unsupervised mode, we also test two semi-supervised
settings. The first achieves 88.8% accuracy on STL10 classification, setting a
new global state-of-the-art over all existing methods (whether supervised,
semi-supervised or unsupervised). The second shows robustness to 90% reductions
in label coverage, of relevance to applications that wish to make use of small
amounts of labels. github.com/xu-ji/IIC
",present novel cluster objective learn neural network classifier scratch give unlabelled data sample model discover cluster accurately match semantic class achieve state of the art result eight unsupervised clustering benchmark span image classification segmentation include stl10 unsupervised variant imagenet cifar10 significantly beat accuracy close competitor absolute percentage point respectively method specialised computer vision operate pair dataset sample experiment use random transform obtain pair image train network directly output semantic label rather high dimensional representation need external processing usable semantic cluster objective simply maximise mutual information class assignment pair easy implement rigorously ground information theory mean effortlessly avoid degenerate solution cluster method susceptible addition fully unsupervised mode also test two semi supervised setting first achieve accuracy stl10 classification set new global state of the art exist method whether supervise semi supervised unsupervised second show robustness 90 reduction label coverage relevance application wish make use small amount label
"Towards Quantized Model Parallelism for Graph-Augmented MLPs Based on
  Gradient-Free ADMM framework","  The Graph Augmented Multi-layer Perceptron (GA-MLP) model is an attractive
alternative to Graph Neural Networks (GNNs). This is because it is resistant to
the over-smoothing problem, and deeper GA-MLP models yield better performance.
GA-MLP models are traditionally optimized by the Stochastic Gradient Descent
(SGD). However, SGD suffers from the layer dependency problem, which prevents
the gradients of different layers of GA-MLP models from being calculated in
parallel. In this paper, we propose a parallel deep learning Alternating
Direction Method of Multipliers (pdADMM) framework to achieve model
parallelism: parameters in each layer of GA-MLP models can be updated in
parallel. The extended pdADMM-Q algorithm reduces communication cost by
utilizing the quantization technique. Theoretical convergence to a critical
point of the pdADMM algorithm and the pdADMM-Q algorithm is provided with a
sublinear convergence rate $o(1/k)$. Extensive experiments in six benchmark
datasets demonstrate that the pdADMM can lead to high speedup, and outperforms
all the existing state-of-the-art comparison methods.
",graph augmented multi layer perceptron ga mlp model attractive alternative graph neural network gnns resistant over smooth problem deep ga mlp model yield well performance ga mlp model traditionally optimize stochastic gradient descent sgd however sgd suffer layer dependency problem prevent gradient different layer ga mlp model calculate parallel paper propose parallel deep learning alternate direction method multiplier pdadmm framework achieve model parallelism parameter layer ga mlp model update parallel extended pdadmm q algorithm reduce communication cost utilize quantization technique theoretical convergence critical point pdadmm algorithm pdadmm q algorithm provide sublinear convergence rate extensive experiment six benchmark dataset demonstrate pdadmm lead high speedup outperform exist state of the art comparison method
Sentence Embeddings for Russian NLU,"  We investigate the performance of sentence embeddings models on several tasks
for the Russian language. In our comparison, we include such tasks as multiple
choice question answering, next sentence prediction, and paraphrase
identification. We employ FastText embeddings as a baseline and compare it to
ELMo and BERT embeddings. We conduct two series of experiments, using both
unsupervised (i.e., based on similarity measure only) and supervised approaches
for the tasks. Finally, we present datasets for multiple choice question
answering and next sentence prediction in Russian.
",investigate performance sentence embedding model several task russian language comparison include task multiple choice question answer next sentence prediction paraphrase identification employ fasttext embedding baseline compare elmo bert embedding conduct two series experiment use unsupervised base similarity measure supervise approach task finally present dataset multiple choice question answer next sentence prediction russian
Survey on Semantic Stereo Matching / Semantic Depth Estimation,"  Stereo matching is one of the widely used techniques for inferring depth from
stereo images owing to its robustness and speed. It has become one of the major
topics of research since it finds its applications in autonomous driving,
robotic navigation, 3D reconstruction, and many other fields. Finding pixel
correspondences in non-textured, occluded and reflective areas is the major
challenge in stereo matching. Recent developments have shown that semantic cues
from image segmentation can be used to improve the results of stereo matching.
Many deep neural network architectures have been proposed to leverage the
advantages of semantic segmentation in stereo matching. This paper aims to give
a comparison among the state of art networks both in terms of accuracy and in
terms of speed which are of higher importance in real-time applications.
",stereo match one widely use technique infer depth stereo image owe robustness speed become one major topic research since find application autonomous drive robotic navigation 3d reconstruction many field find pixel correspondence non textured occluded reflective area major challenge stereo matching recent development show semantic cue image segmentation use improve result stereo matching many deep neural network architecture propose leverage advantage semantic segmentation stereo matching paper aim give comparison among state art network term accuracy term speed high importance real time application
SurFree: a fast surrogate-free black-box attack,"  Machine learning classifiers are critically prone to evasion attacks.
Adversarial examples are slightly modified inputs that are then misclassified,
while remaining perceptively close to their originals. Last couple of years
have witnessed a striking decrease in the amount of queries a black box attack
submits to the target classifier, in order to forge adversarials. This
particularly concerns the black-box score-based setup, where the attacker has
access to top predicted probabilites: the amount of queries went from to
millions of to less than a thousand. This paper presents SurFree, a geometrical
approach that achieves a similar drastic reduction in the amount of queries in
the hardest setup: black box decision-based attacks (only the top-1 label is
available). We first highlight that the most recent attacks in that setup,
HSJA, QEBA and GeoDA all perform costly gradient surrogate estimations. SurFree
proposes to bypass these, by instead focusing on careful trials along diverse
directions, guided by precise indications of geometrical properties of the
classifier decision boundaries. We motivate this geometric approach before
performing a head-to-head comparison with previous attacks with the amount of
queries as a first class citizen. We exhibit a faster distortion decay under
low query amounts (few hundreds to a thousand), while remaining competitive at
higher query budgets.
",machine learn classifier critically prone evasion attack adversarial example slightly modify input misclassifie remain perceptively close original last couple year witness strike decrease amount query black box attack submit target classifier order forge adversarial particularly concern black box score base setup attacker access top predict probabilite amount query go million less thousand paper present surfree geometrical approach achieve similar drastic reduction amount query hard setup black box decision base attack top-1 label available first highlight recent attack setup hsja qeba geoda perform costly gradient surrogate estimation surfree propose bypass instead focus careful trial along diverse direction guide precise indication geometrical property classifier decision boundary motivate geometric approach perform head to head comparison previous attack amount query first class citizen exhibit fast distortion decay low query amount hundreds thousand remain competitive high query budget
Sim-Env: Decoupling OpenAI Gym Environments from Simulation Models,"  Reinforcement learning (RL) is one of the most active fields of AI research.
Despite the interest demonstrated by the research community in reinforcement
learning, the development methodology still lags behind, with a severe lack of
standard APIs to foster the development of RL applications. OpenAI Gym is
probably the most used environment to develop RL applications and simulations,
but most of the abstractions proposed in such a framework are still assuming a
semi-structured methodology. This is particularly relevant for agent-based
models whose purpose is to analyse adaptive behaviour displayed by
self-learning agents in the simulation. In order to bridge this gap, we present
a workflow and tools for the decoupled development and maintenance of
multi-purpose agent-based models and derived single-purpose reinforcement
learning environments, enabling the researcher to swap out environments with
ones representing different perspectives or different reward models, all while
keeping the underlying domain model intact and separate. The Sim-Env Python
library generates OpenAI-Gym-compatible reinforcement learning environments
that use existing or purposely created domain models as their simulation
back-ends. Its design emphasizes ease-of-use, modularity and code separation.
",reinforcement learning rl one active field ai research despite interest demonstrate research community reinforcement learning development methodology still lag behind severe lack standard apis foster development rl application openai gym probably use environment develop rl application simulation abstraction propose framework still assume semi structured methodology particularly relevant agent base model whose purpose analyse adaptive behaviour display self learn agent simulation order bridge gap present workflow tool decouple development maintenance multi purpose agent base model derive single purpose reinforcement learning environment enable researcher swap environment one represent different perspective different reward model keep underlie domain model intact separate sim env python library generate openai gym compatible reinforcement learning environment use exist purposely create domain model simulation back end design emphasize ease of use modularity code separation
Selecting the rank of truncated SVD by Maximum Approximation Capacity,"  Truncated Singular Value Decomposition (SVD) calculates the closest rank-$k$
approximation of a given input matrix. Selecting the appropriate rank $k$
defines a critical model order choice in most applications of SVD. To obtain a
principled cut-off criterion for the spectrum, we convert the underlying
optimization problem into a noisy channel coding problem. The optimal
approximation capacity of this channel controls the appropriate strength of
regularization to suppress noise. In simulation experiments, this information
theoretic method to determine the optimal rank competes with state-of-the art
model selection techniques.
",truncated singular value decomposition svd calculate close rank- k approximation give input matrix select appropriate rank k define critical model order choice application svd obtain principle cut off criterion spectrum convert underlie optimization problem noisy channel code problem optimal approximation capacity channel control appropriate strength regularization suppress noise simulation experiment information theoretic method determine optimal rank compete state of the art model selection technique
"Capturing dynamics of post-earnings-announcement drift using genetic
  algorithm-optimised supervised learning","  While Post-Earnings-Announcement Drift (PEAD) is one of the most studied
stock market anomalies, the current literature is often limited in explaining
this phenomenon by a small number of factors using simpler regression methods.
In this paper, we use a machine learning based approach instead, and aim to
capture the PEAD dynamics using data from a large group of stocks and a wide
range of both fundamental and technical factors. Our model is built around the
Extreme Gradient Boosting (XGBoost) and uses a long list of engineered input
features based on quarterly financial announcement data from 1,106 companies in
the Russell 1000 index between 1997 and 2018. We perform numerous experiments
on PEAD predictions and analysis and have the following contributions to the
literature. First, we show how Post-Earnings-Announcement Drift can be analysed
using machine learning methods and demonstrate such methods' prowess in
producing credible forecasting on the drift direction. It is the first time
PEAD dynamics are studied using XGBoost. We show that the drift direction is in
fact driven by different factors for stocks from different industrial sectors
and in different quarters and XGBoost is effective in understanding the
changing drivers. Second, we show that an XGBoost well optimised by a Genetic
Algorithm can help allocate out-of-sample stocks to form portfolios with higher
positive returns to long and portfolios with lower negative returns to short, a
finding that could be adopted in the process of developing market neutral
strategies. Third, we show how theoretical event-driven stock strategies have
to grapple with ever changing market prices in reality, reducing their
effectiveness. We present a tactic to remedy the difficulty of buying into a
moving market when dealing with PEAD signals.
",post earning announcement drift pead one study stock market anomaly current literature often limit explain phenomenon small number factor use simple regression method paper use machine learn base approach instead aim capture pead dynamic use datum large group stock wide range fundamental technical factor model build around extreme gradient boost xgboost use long list engineer input feature base quarterly financial announcement datum company russell 1000 index 1997 perform numerous experiment pead prediction analysis follow contribution literature first show post earning announcement drift analyse use machine learn method demonstrate method prowess produce credible forecasting drift direction first time pead dynamic study use xgboost show drift direction fact drive different factor stock different industrial sector different quarter xgboost effective understanding change driver second show xgboost well optimise genetic algorithm help allocate out of sample stock form portfolio high positive return long portfolio low negative return short finding could adopt process develop market neutral strategy third show theoretical event drive stock strategy grapple ever change market price reality reduce effectiveness present tactic remedy difficulty buy move market deal pead signal
Communicative Capital for Prosthetic Agents,"  This work presents an overarching perspective on the role that machine
intelligence can play in enhancing human abilities, especially those that have
been diminished due to injury or illness. As a primary contribution, we develop
the hypothesis that assistive devices, and specifically artificial arms and
hands, can and should be viewed as agents in order for us to most effectively
improve their collaboration with their human users. We believe that increased
agency will enable more powerful interactions between human users and next
generation prosthetic devices, especially when the sensorimotor space of the
prosthetic technology greatly exceeds the conventional control and
communication channels available to a prosthetic user. To more concretely
examine an agency-based view on prosthetic devices, we propose a new schema for
interpreting the capacity of a human-machine collaboration as a function of
both the human's and machine's degrees of agency. We then introduce the idea of
communicative capital as a way of thinking about the communication resources
developed by a human and a machine during their ongoing interaction. Using this
schema of agency and capacity, we examine the benefits and disadvantages of
increasing the agency of a prosthetic limb. To do so, we present an analysis of
examples from the literature where building communicative capital has enabled a
progression of fruitful, task-directed interactions between prostheses and
their human users. We then describe further work that is needed to concretely
evaluate the hypothesis that prostheses are best thought of as agents. The
agent-based viewpoint developed in this article significantly extends current
thinking on how best to support the natural, functional use of increasingly
complex prosthetic enhancements, and opens the door for more powerful
interactions between humans and their assistive technologies.
",work present overarch perspective role machine intelligence play enhance human ability especially diminish due injury illness primary contribution develop hypothesis assistive device specifically artificial arm hand view agent order we effectively improve collaboration human user believe increase agency enable powerful interaction human user next generation prosthetic device especially sensorimotor space prosthetic technology greatly exceed conventional control communication channel available prosthetic user concretely examine agency base view prosthetic device propose new schema interpret capacity human machine collaboration function human machine degree agency introduce idea communicative capital way think communication resource develop human machine ongoing interaction use schema agency capacity examine benefit disadvantage increase agency prosthetic limb present analysis example literature build communicative capital enable progression fruitful task direct interaction prosthesis human user describe work need concretely evaluate hypothesis prosthesis well think agent agent base viewpoint develop article significantly extend current thinking good support natural functional use increasingly complex prosthetic enhancement open door powerful interaction human assistive technology
Deep Neural Networks for Estimation and Inference,"  We study deep neural networks and their use in semiparametric inference. We
establish novel rates of convergence for deep feedforward neural nets. Our new
rates are sufficiently fast (in some cases minimax optimal) to allow us to
establish valid second-step inference after first-step estimation with deep
learning, a result also new to the literature. Our estimation rates and
semiparametric inference results handle the current standard architecture:
fully connected feedforward neural networks (multi-layer perceptrons), with the
now-common rectified linear unit activation function and a depth explicitly
diverging with the sample size. We discuss other architectures as well,
including fixed-width, very deep networks. We establish nonasymptotic bounds
for these deep nets for a general class of nonparametric regression-type loss
functions, which includes as special cases least squares, logistic regression,
and other generalized linear models. We then apply our theory to develop
semiparametric inference, focusing on causal parameters for concreteness, such
as treatment effects, expected welfare, and decomposition effects. Inference in
many other semiparametric contexts can be readily obtained. We demonstrate the
effectiveness of deep learning with a Monte Carlo analysis and an empirical
application to direct mail marketing.
",study deep neural network use semiparametric inference establish novel rate convergence deep feedforward neural net new rate sufficiently fast case minimax optimal allow we establish valid second step inference first step estimation deep learning result also new literature estimation rate semiparametric inference result handle current standard architecture fully connect feedforward neural network multi layer perceptron now common rectified linear unit activation function depth explicitly diverge sample size discuss architecture well include fix width deep network establish nonasymptotic bound deep net general class nonparametric regression type loss function include special case least square logistic regression generalize linear model apply theory develop semiparametric inference focus causal parameter concreteness treatment effect expect welfare decomposition effect inference many semiparametric contexts readily obtain demonstrate effectiveness deep learning monte carlo analysis empirical application direct mail marketing
Rethinking the Hyperparameters for Fine-tuning,"  Fine-tuning from pre-trained ImageNet models has become the de-facto standard
for various computer vision tasks. Current practices for fine-tuning typically
involve selecting an ad-hoc choice of hyperparameters and keeping them fixed to
values normally used for training from scratch. This paper re-examines several
common practices of setting hyperparameters for fine-tuning. Our findings are
based on extensive empirical evaluation for fine-tuning on various transfer
learning benchmarks. (1) While prior works have thoroughly investigated
learning rate and batch size, momentum for fine-tuning is a relatively
unexplored parameter. We find that the value of momentum also affects
fine-tuning performance and connect it with previous theoretical findings. (2)
Optimal hyperparameters for fine-tuning, in particular, the effective learning
rate, are not only dataset dependent but also sensitive to the similarity
between the source domain and target domain. This is in contrast to
hyperparameters for training from scratch. (3) Reference-based regularization
that keeps models close to the initial model does not necessarily apply for
""dissimilar"" datasets. Our findings challenge common practices of fine-tuning
and encourages deep learning practitioners to rethink the hyperparameters for
fine-tuning.
",fine tune pre trained imagenet model become de facto standard various computer vision task current practice fine tuning typically involve select ad hoc choice hyperparameter keep fix value normally use training scratch paper re examine several common practice set hyperparameter fine tuning finding base extensive empirical evaluation fine tuning various transfer learning benchmark 1 prior work thoroughly investigate learn rate batch size momentum fine tune relatively unexplored parameter find value momentum also affect fine tuning performance connect previous theoretical finding 2 optimal hyperparameter fine tune particular effective learning rate dataset dependent also sensitive similarity source domain target domain contrast hyperparameter training scratch 3 reference base regularization keep model close initial model necessarily apply dissimilar dataset finding challenge common practice fine tuning encourage deep learning practitioner rethink hyperparameter fine tuning
CFDNet: a deep learning-based accelerator for fluid simulations,"  CFD is widely used in physical system design and optimization, where it is
used to predict engineering quantities of interest, such as the lift on a plane
wing or the drag on a motor vehicle. However, many systems of interest are
prohibitively expensive for design optimization, due to the expense of
evaluating CFD simulations. To render the computation tractable, reduced-order
or surrogate models are used to accelerate simulations while respecting the
convergence constraints provided by the higher-fidelity solution. This paper
introduces CFDNet -- a physical simulation and deep learning coupled framework,
for accelerating the convergence of Reynolds Averaged Navier-Stokes
simulations. CFDNet is designed to predict the primary physical properties of
the fluid including velocity, pressure, and eddy viscosity using a single
convolutional neural network at its core. We evaluate CFDNet on a variety of
use-cases, both extrapolative and interpolative, where test geometries are
observed/not-observed during training. Our results show that CFDNet meets the
convergence constraints of the domain-specific physics solver while
outperforming it by 1.9 - 7.4x on both steady laminar and turbulent flows.
Moreover, we demonstrate the generalization capacity of CFDNet by testing its
prediction on new geometries unseen during training. In this case, the approach
meets the CFD convergence criterion while still providing significant speedups
over traditional domain-only models.
",cfd widely use physical system design optimization use predict engineering quantity interest lift plane wing drag motor vehicle however many system interest prohibitively expensive design optimization due expense evaluate cfd simulation render computation tractable reduced order surrogate model use accelerate simulation respect convergence constraint provide high fidelity solution paper introduce cfdnet physical simulation deep learning couple framework accelerate convergence reynold average navi stoke simulation cfdnet design predict primary physical property fluid include velocity pressure eddy viscosity use single convolutional neural network core evaluate cfdnet variety use case extrapolative interpolative test geometry training result show cfdnet meet convergence constraint domain specific physics solver outperform steady laminar turbulent flow moreover demonstrate generalization capacity cfdnet testing prediction new geometry unseen training case approach meet cfd convergence criterion still provide significant speedup traditional domain only model
RedSync : Reducing Synchronization Traffic for Distributed Deep Learning,"  Data parallelism has become a dominant method to scale Deep Neural Network
(DNN) training across multiple nodes. Since synchronizing a large number of
gradients of the local model can be a bottleneck for large-scale distributed
training, compressing communication data has gained widespread attention
recently. Among several recent proposed compression algorithms, Residual
Gradient Compression (RGC) is one of the most successful approaches---it can
significantly compress the transmitting message size (0.1\% of the gradient
size) of each node and still achieve correct accuracy and the same convergence
speed. However, the literature on compressing deep networks focuses almost
exclusively on achieving good theoretical compression rate, while the
efficiency of RGC in real distributed implementation has been less
investigated. In this paper, we develop an RGC-based system that is able to
reduce the end-to-end training time on real-world multi-GPU systems. Our
proposed design called RedSync, which introduces a set of optimizations to
reduce communication bandwidth requirement while introducing limited overhead.
We evaluate the performance of RedSync on two different multiple GPU platforms,
including 128 GPUs of a supercomputer and an 8-GPU server. Our test cases
include image classification tasks on Cifar10 and ImageNet, and language
modeling tasks on Penn Treebank and Wiki2 datasets. For DNNs featured with high
communication to computation ratio, which have long been considered with poor
scalability, RedSync brings significant performance improvements.
",data parallelism become dominant method scale deep neural network dnn training across multiple node since synchronize large number gradient local model bottleneck large scale distribute training compress communication datum gain widespread attention recently among several recent propose compression algorithm residual gradient compression rgc one successful approach -it significantly compress transmit message size gradient size node still achieve correct accuracy convergence speed however literature compress deep network focuses almost exclusively achieve good theoretical compression rate efficiency rgc real distribute implementation less investigate paper develop rgc base system able reduce end to end training time real world multi gpu system propose design call redsync introduce set optimization reduce communication bandwidth requirement introduce limited overhead evaluate performance redsync two different multiple gpu platform include 128 gpu supercomputer 8 gpu server test case include image classification task cifar10 imagenet language modeling task penn treebank wiki2 dataset dnn feature high communication computation ratio long consider poor scalability redsync bring significant performance improvement
ACMo: Angle-Calibrated Moment Methods for Stochastic Optimization,"  Due to its simplicity and outstanding ability to generalize, stochastic
gradient descent (SGD) is still the most widely used optimization method
despite its slow convergence. Meanwhile, adaptive methods have attracted rising
attention of optimization and machine learning communities, both for the
leverage of life-long information and for the profound and fundamental
mathematical theory. Taking the best of both worlds is the most exciting and
challenging question in the field of optimization for machine learning. Along
this line, we revisited existing adaptive gradient methods from a novel
perspective, refreshing understanding of second moments. Our new perspective
empowers us to attach the properties of second moments to the first moment
iteration, and to propose a novel first moment optimizer,
\emph{Angle-Calibrated Moment method} (\method). Our theoretical results show
that \method is able to achieve the same convergence rate as mainstream
adaptive methods. Furthermore, extensive experiments on CV and NLP tasks
demonstrate that \method has a comparable convergence to SOTA Adam-type
optimizers, and gains a better generalization performance in most cases.
",due simplicity outstanding ability generalize stochastic gradient descent sgd still widely use optimization method despite slow convergence meanwhile adaptive method attract rise attention optimization machine learn community leverage life long information profound fundamental mathematical theory take good world exciting challenging question field optimization machine learn along line revisit exist adaptive gradient method novel perspective refresh understanding second moment new perspective empower we attach property second moment first moment iteration propose novel first moment optimizer angle calibrate moment method theoretical result show able achieve convergence rate mainstream adaptive method furthermore extensive experiment cv nlp task demonstrate comparable convergence sota adam type optimizer gain well generalization performance case
Learning Extreme Hummingbird Maneuvers on Flapping Wing Robots,"  Biological studies show that hummingbirds can perform extreme aerobatic
maneuvers during fast escape. Given a sudden looming visual stimulus at hover,
a hummingbird initiates a fast backward translation coupled with a 180-degree
yaw turn, which is followed by instant posture stabilization in just under 10
wingbeats. Consider the wingbeat frequency of 40Hz, this aggressive maneuver is
carried out in just 0.2 seconds. Inspired by the hummingbirds' near-maximal
performance during such extreme maneuvers, we developed a flight control
strategy and experimentally demonstrated that such maneuverability can be
achieved by an at-scale 12-gram hummingbird robot equipped with just two
actuators. The proposed hybrid control policy combines model-based nonlinear
control with model-free reinforcement learning. We use model-based nonlinear
control for nominal flight control, as the dynamic model is relatively accurate
for these conditions. However, during extreme maneuver, the modeling error
becomes unmanageable. A model-free reinforcement learning policy trained in
simulation was optimized to 'destabilize' the system and maximize the
performance during maneuvering. The hybrid policy manifests a maneuver that is
close to that observed in hummingbirds. Direct simulation-to-real transfer is
achieved, demonstrating the hummingbird-like fast evasive maneuvers on the
at-scale hummingbird robot.
",biological study show hummingbird perform extreme aerobatic maneuver fast escape give sudden loom visual stimulus hover hummingbird initiate fast backward translation couple 180 degree yaw turn follow instant posture stabilization 10 wingbeat consider wingbeat frequency 40hz aggressive maneuver carry second inspire hummingbird near maximal performance extreme maneuver develop flight control strategy experimentally demonstrate maneuverability achieve at scale 12 gram hummingbird robot equip two actuator propose hybrid control policy combine model base nonlinear control model free reinforcement learning use model base nonlinear control nominal flight control dynamic model relatively accurate condition however extreme maneuver modeling error become unmanageable model free reinforcement learning policy train simulation optimize system maximize performance maneuver hybrid policy manifest maneuver close observed hummingbird direct simulation to real transfer achieve demonstrate hummingbird like fast evasive maneuver at scale hummingbird robot
Edge-Cloud Polarization and Collaboration: A Comprehensive Survey for AI,"  Influenced by the great success of deep learning via cloud computing and the
rapid development of edge chips, research in artificial intelligence (AI) has
shifted to both of the computing paradigms, i.e., cloud computing and edge
computing. In recent years, we have witnessed significant progress in
developing more advanced AI models on cloud servers that surpass traditional
deep learning models owing to model innovations (e.g., Transformers, Pretrained
families), explosion of training data and soaring computing capabilities.
However, edge computing, especially edge and cloud collaborative computing, are
still in its infancy to announce their success due to the resource-constrained
IoT scenarios with very limited algorithms deployed. In this survey, we conduct
a systematic review for both cloud and edge AI. Specifically, we are the first
to set up the collaborative learning mechanism for cloud and edge modeling with
a thorough review of the architectures that enable such mechanism. We also
discuss potentials and practical experiences of some on-going advanced edge AI
topics including pretraining models, graph neural networks and reinforcement
learning. Finally, we discuss the promising directions and challenges in this
field.
",influence great success deep learning via cloud compute rapid development edge chip research artificial intelligence ai shift compute paradigms cloud compute edge computing recent year witness significant progress develop advanced ai model cloud server surpass traditional deep learning model owe model innovation transformer pretraine family explosion training datum soar compute capability however edge computing especially edge cloud collaborative computing still infancy announce success due resource constrain iot scenario limited algorithm deploy survey conduct systematic review cloud edge ai specifically first set collaborative learning mechanism cloud edge model thorough review architecture enable mechanism also discuss potential practical experience on go advanced edge ai topic include pretraine model graph neural network reinforcement learning finally discuss promise direction challenge field
Overparameterization of deep ResNet: zero loss and mean-field analysis,"  Finding parameters in a deep neural network (NN) that fit training data is a
nonconvex optimization problem, but a basic first-order optimization method
(gradient descent) finds a global optimizer with perfect fit (zero-loss) in
many practical situations. We examine this phenomenon for the case of Residual
Neural Networks (ResNet) with smooth activation functions in a limiting regime
in which both the number of layers (depth) and the number of weights in each
layer (width) go to infinity. First, we use a mean-field-limit argument to
prove that the gradient descent for parameter training becomes a gradient flow
for a probability distribution that is characterized by a partial differential
equation (PDE) in the large-NN limit. Next, we show that under certain
assumptions, the solution to the PDE converges in the training time to a
zero-loss solution. Together, these results suggest that the training of the
ResNet gives a near-zero loss if the ResNet is large enough. We give estimates
of the depth and width needed to reduce the loss below a given threshold, with
high probability.
",find parameter deep neural network nn fit training datum nonconvex optimization problem basic first order optimization method gradient descent find global optimizer perfect fit zero loss many practical situation examine phenomenon case residual neural network resnet smooth activation function limit regime number layer depth number weight layer width go infinity first use mean field limit argument prove gradient descent parameter training become gradient flow probability distribution characterize partial differential equation pde large nn limit next show certain assumption solution pde converge training time zero loss solution together result suggest training resnet give near zero loss resnet large enough give estimate depth width need reduce loss give threshold high probability
Variational Wasserstein Barycenters with c-Cyclical Monotonicity,"  Wasserstein barycenter, built on the theory of optimal transport, provides a
powerful framework to aggregate probability distributions, and it has
increasingly attracted great attention within the machine learning community.
However, it suffers from severe computational burden, especially for high
dimensional and continuous settings. To this end, we develop a novel continuous
approximation method for the Wasserstein barycenters problem given sample
access to the input distributions. The basic idea is to introduce a variational
distribution as the approximation of the true continuous barycenter, so as to
frame the barycenters computation problem as an optimization problem, where
parameters of the variational distribution adjust the proxy distribution to be
similar to the barycenter. Leveraging the variational distribution, we
construct a tractable dual formulation for the regularized Wasserstein
barycenter problem with c-cyclical monotonicity, which can be efficiently
solved by stochastic optimization. We provide theoretical analysis on
convergence and demonstrate the practical effectiveness of our method on real
applications of subset posterior aggregation and synthetic data.
",wasserstein barycenter build theory optimal transport provide powerful framework aggregate probability distribution increasingly attract great attention within machine learning community however suffer severe computational burden especially high dimensional continuous setting end develop novel continuous approximation method wasserstein barycenter problem give sample access input distribution basic idea introduce variational distribution approximation true continuous barycenter frame barycenter computation problem optimization problem parameter variational distribution adjust proxy distribution similar barycenter leverage variational distribution construct tractable dual formulation regularize wasserstein barycenter problem c cyclical monotonicity efficiently solve stochastic optimization provide theoretical analysis convergence demonstrate practical effectiveness method real application subset posterior aggregation synthetic datum
Near Neighbor: Who is the Fairest of Them All?,"  $\newcommand{\ball}{\mathbb{B}}\newcommand{\dsQ}{{\mathcal{Q}}}\newcommand{\dsS}{{\mathcal{S}}}$In
this work we study a fair variant of the near neighbor problem. Namely, given a
set of $n$ points $P$ and a parameter $r$, the goal is to preprocess the
points, such that given a query point $q$, any point in the $r$-neighborhood of
the query, i.e., $\ball(q,r)$, have the same probability of being reported as
the near neighbor.
  We show that LSH based algorithms can be made fair, without a significant
loss in efficiency. Specifically, we show an algorithm that reports a point in
the $r$-neighborhood of a query $q$ with almost uniform probability. The query
time is proportional to $O\bigl( \mathrm{dns}(q.r) \dsQ(n,c) \bigr)$, and its
space is $O(\dsS(n,c))$, where $\dsQ(n,c)$ and $\dsS(n,c)$ are the query time
and space of an LSH algorithm for $c$-approximate near neighbor, and
$\mathrm{dns}(q,r)$ is a function of the local density around $q$.
  Our approach works more generally for sampling uniformly from a
sub-collection of sets of a given collection and can be used in a few other
applications. Finally, we run experiments to show performance of our approach
on real data.
",b q work study fair variant near neighbor problem namely give set n point p parameter r goal preprocess point give query point q point r -neighborhood query q r probability report near neighbor show lsh base algorithm make fair without significant loss efficiency specifically show algorithm report point r -neighborhood query q almost uniform probability query time proportional dns n c space n c n c n c query time space lsh algorithm c -approximate near neighbor dns q r function local density around q approach work generally sample uniformly sub collection set give collection use application finally run experiment show performance approach real datum
Are Hyperbolic Representations in Graphs Created Equal?,"  Recently there was an increasing interest in applications of graph neural
networks in non-Euclidean geometry; however, are non-Euclidean representations
always useful for graph learning tasks? For different problems such as node
classification and link prediction we compute hyperbolic embeddings and
conclude that for tasks that require global prediction consistency it might be
useful to use non-Euclidean embeddings, while for other tasks Euclidean models
are superior. To do so we first fix an issue of the existing models associated
with the optimization process at zero curvature. Current hyperbolic models deal
with gradients at the origin in ad-hoc manner, which is inefficient and can
lead to numerical instabilities. We solve the instabilities of
kappa-Stereographic model at zero curvature cases and evaluate the approach of
embedding graphs into the manifold in several graph representation learning
tasks.
",recently increase interest application graph neural network non euclidean geometry however non euclidean representation always useful graph learn task different problem node classification link prediction compute hyperbolic embedding conclude task require global prediction consistency might useful use non euclidean embedding task euclidean model superior first fix issue exist model associate optimization process zero curvature current hyperbolic model deal gradient origin ad hoc manner inefficient lead numerical instability solve instability kappa stereographic model zero curvature case evaluate approach embed graph manifold several graph representation learn task
Neural Markov Logic Networks,"  We introduce neural Markov logic networks (NMLNs), a statistical relational
learning system that borrows ideas from Markov logic. Like Markov logic
networks (MLNs), NMLNs are an exponential-family model for modelling
distributions over possible worlds, but unlike MLNs, they do not rely on
explicitly specified first-order logic rules. Instead, NMLNs learn an implicit
representation of such rules as a neural network that acts as a potential
function on fragments of the relational structure. Similarly to many neural
symbolic methods, NMLNs can exploit embeddings of constants but, unlike them,
NMLNs work well also in their absence. This is extremely important for
predicting in settings other than the transductive one. We showcase the
potential of NMLNs on knowledge-base completion, triple classification and on
generation of molecular (graph) data.
",introduce neural markov logic network nmlns statistical relational learn system borrow idea markov logic like markov logic network mlns nmlns exponential family model modelling distribution possible world unlike mlns rely explicitly specify first order logic rule instead nmlns learn implicit representation rule neural network act potential function fragment relational structure similarly many neural symbolic method nmlns exploit embedding constant unlike nmlns work well also absence extremely important predict setting transductive one showcase potential nmlns knowledge base completion triple classification generation molecular graph datum
WMRB: Learning to Rank in a Scalable Batch Training Approach,"  We propose a new learning to rank algorithm, named Weighted Margin-Rank Batch
loss (WMRB), to extend the popular Weighted Approximate-Rank Pairwise loss
(WARP). WMRB uses a new rank estimator and an efficient batch training
algorithm. The approach allows more accurate item rank approximation and
explicit utilization of parallel computation to accelerate training. In three
item recommendation tasks, WMRB consistently outperforms WARP and other
baselines. Moreover, WMRB shows clear time efficiency advantages as data scale
increases.
",propose new learning rank algorithm name weight margin rank batch loss wmrb extend popular weight approximate rank pairwise loss warp wmrb use new rank estimator efficient batch training algorithm approach allow accurate item rank approximation explicit utilization parallel computation accelerate train three item recommendation task wmrb consistently outperform warp baseline moreover wmrb show clear time efficiency advantage datum scale increase
"Small nonlinearities in activation functions create bad local minima in
  neural networks","  We investigate the loss surface of neural networks. We prove that even for
one-hidden-layer networks with ""slightest"" nonlinearity, the empirical risks
have spurious local minima in most cases. Our results thus indicate that in
general ""no spurious local minima"" is a property limited to deep linear
networks, and insights obtained from linear networks may not be robust.
Specifically, for ReLU(-like) networks we constructively prove that for almost
all practical datasets there exist infinitely many local minima. We also
present a counterexample for more general activations (sigmoid, tanh, arctan,
ReLU, etc.), for which there exists a bad local minimum. Our results make the
least restrictive assumptions relative to existing results on spurious local
optima in neural networks. We complete our discussion by presenting a
comprehensive characterization of global optimality for deep linear networks,
which unifies other results on this topic.
",investigate loss surface neural network prove even one hide layer network slight nonlinearity empirical risk spurious local minima case result thus indicate general spurious local minima property limit deep linear network insight obtain linear network may robust specifically relu -like network constructively prove almost practical dataset exist infinitely many local minima also present counterexample general activation sigmoid tanh arctan relu etc exist bad local minimum result make least restrictive assumption relative exist result spurious local optima neural network complete discussion present comprehensive characterization global optimality deep linear network unify result topic
When Machine Learning Meets Quantum Computers: A Case Study,"  Along with the development of AI democratization, the machine learning
approach, in particular neural networks, has been applied to wide-range
applications. In different application scenarios, the neural network will be
accelerated on the tailored computing platform. The acceleration of neural
networks on classical computing platforms, such as CPU, GPU, FPGA, ASIC, has
been widely studied; however, when the scale of the application consistently
grows up, the memory bottleneck becomes obvious, widely known as memory-wall.
In response to such a challenge, advanced quantum computing, which can
represent 2^N states with N quantum bits (qubits), is regarded as a promising
solution. It is imminent to know how to design the quantum circuit for
accelerating neural networks. Most recently, there are initial works studying
how to map neural networks to actual quantum processors. To better understand
the state-of-the-art design and inspire new design methodology, this paper
carries out a case study to demonstrate an end-to-end implementation. On the
neural network side, we employ the multilayer perceptron to complete image
classification tasks using the standard and widely used MNIST dataset. On the
quantum computing side, we target IBM Quantum processors, which can be
programmed and simulated by using IBM Qiskit. This work targets the
acceleration of the inference phase of a trained neural network on the quantum
processor. Along with the case study, we will demonstrate the typical procedure
for mapping neural networks to quantum circuits.
",along development ai democratization machine learn approach particular neural network apply wide range application different application scenario neural network accelerate tailor computing platform acceleration neural network classical computing platform cpu gpu fpga asic widely study however scale application consistently grow memory bottleneck become obvious widely know memory wall response challenge advanced quantum computing represent state n quantum bit qubit regard promising solution imminent know design quantum circuit accelerate neural network recently initial work study map neural network actual quantum processor well understand state of the art design inspire new design methodology paper carry case study demonstrate end to end implementation neural network side employ multilayer perceptron complete image classification task use standard widely use mnist dataset quantum compute side target ibm quantum processor program simulate use ibm qiskit work target acceleration inference phase train neural network quantum processor along case study demonstrate typical procedure mapping neural network quantum circuit
Impact of Learning Rate on Noise Resistant Property of Deep Learning Models,"The interest in analog computation has grown tremendously in recent years due
to its fast computation speed and excellent energy efficiency, which is very
important for edge and IoT devices in the sub-watt power envelope for deep
learning inferencing. However, significant performance degradation suffered by
deep learning models due to the inherent noise present in the analog
computation can limit their use in mission-critical applications. Hence, there
is a need to understand the impact of critical model hyperparameters choice on
the resulting model noise-resistant property. This need is critical as the
insight obtained can be used to design deep learning models that are robust to
analog noise. In this paper, the impact of the learning rate, a critical design
choice, on the noise-resistant property is investigated. The study is achieved
by first training deep learning models using different learning rates.
Thereafter, the models are injected with analog noise and the noise-resistant
property of the resulting models is examined by measuring the performance
degradation due to the analog noise. The results showed there exists a sweet
spot of learning rate values that achieves a good balance between model
prediction performance and model noise-resistant property. Furthermore, the
theoretical justification of the observed phenomenon is provided.",interest analog computation grow tremendously recent year due fast computation speed excellent energy efficiency important edge iot device sub watt power envelope deep learning inference however significant performance degradation suffer deep learning model due inherent noise present analog computation limit use mission critical application hence need understand impact critical model hyperparameter choice result model noise resistant property need critical insight obtain use design deep learning model robust analog noise paper impact learn rate critical design choice noise resistant property investigate study achieve first train deep learning model use different learning rate thereafter model inject analog noise noise resistant property result model examine measure performance degradation due analog noise result show exist sweet spot learning rate value achieve good balance model prediction performance model noise resistant property furthermore theoretical justification observe phenomenon provide
Episodic Memory Deep Q-Networks,"  Reinforcement learning (RL) algorithms have made huge progress in recent
years by leveraging the power of deep neural networks (DNN). Despite the
success, deep RL algorithms are known to be sample inefficient, often requiring
many rounds of interaction with the environments to obtain satisfactory
performance. Recently, episodic memory based RL has attracted attention due to
its ability to latch on good actions quickly. In this paper, we present a
simple yet effective biologically inspired RL algorithm called Episodic Memory
Deep Q-Networks (EMDQN), which leverages episodic memory to supervise an agent
during training. Experiments show that our proposed method can lead to better
sample efficiency and is more likely to find good policies. It only requires
1/5 of the interactions of DQN to achieve many state-of-the-art performances on
Atari games, significantly outperforming regular DQN and other episodic memory
based RL algorithms.
",reinforcement learning rl algorithm make huge progress recent year leverage power deep neural network dnn despite success deep rl algorithm know sample inefficient often require many round interaction environment obtain satisfactory performance recently episodic memory base rl attract attention due ability latch good action quickly paper present simple yet effective biologically inspire rl algorithm call episodic memory deep q network emdqn leverage episodic memory supervise agent training experiment show propose method lead well sample efficiency likely find good policy require interaction dqn achieve many state of the art performance atari game significantly outperform regular dqn episodic memory base rl algorithm
Learning from Conditional Distributions via Dual Embeddings,"  Many machine learning tasks, such as learning with invariance and policy
evaluation in reinforcement learning, can be characterized as problems of
learning from conditional distributions. In such problems, each sample $x$
itself is associated with a conditional distribution $p(z|x)$ represented by
samples $\{z_i\}_{i=1}^M$, and the goal is to learn a function $f$ that links
these conditional distributions to target values $y$. These learning problems
become very challenging when we only have limited samples or in the extreme
case only one sample from each conditional distribution. Commonly used
approaches either assume that $z$ is independent of $x$, or require an
overwhelmingly large samples from each conditional distribution.
  To address these challenges, we propose a novel approach which employs a new
min-max reformulation of the learning from conditional distribution problem.
With such new reformulation, we only need to deal with the joint distribution
$p(z,x)$. We also design an efficient learning algorithm, Embedding-SGD, and
establish theoretical sample complexity for such problems. Finally, our
numerical experiments on both synthetic and real-world datasets show that the
proposed approach can significantly improve over the existing algorithms.
",many machine learn task learn invariance policy evaluation reinforcement learning characterize problem learn conditional distribution problem sample x associate conditional distribution p represent sample goal learn function f link conditional distribution target value learn problem become challenge limited sample extreme case one sample conditional distribution commonly use approach either assume z independent x require overwhelmingly large sample conditional distribution address challenge propose novel approach employ new min max reformulation learn conditional distribution problem new reformulation need deal joint distribution p z x also design efficient learn algorithm embed sgd establish theoretical sample complexity problem finally numerical experiment synthetic real world dataset show propose approach significantly improve exist algorithm
Extractor-Based Time-Space Lower Bounds for Learning,"  A matrix $M: A \times X \rightarrow \{-1,1\}$ corresponds to the following
learning problem: An unknown element $x \in X$ is chosen uniformly at random. A
learner tries to learn $x$ from a stream of samples, $(a_1, b_1), (a_2, b_2)
\ldots$, where for every $i$, $a_i \in A$ is chosen uniformly at random and
$b_i = M(a_i,x)$.
  Assume that $k,\ell, r$ are such that any submatrix of $M$ of at least
$2^{-k} \cdot |A|$ rows and at least $2^{-\ell} \cdot |X|$ columns, has a bias
of at most $2^{-r}$. We show that any learning algorithm for the learning
problem corresponding to $M$ requires either a memory of size at least
$\Omega\left(k \cdot \ell \right)$, or at least $2^{\Omega(r)}$ samples. The
result holds even if the learner has an exponentially small success probability
(of $2^{-\Omega(r)}$).
  In particular, this shows that for a large class of learning problems, any
learning algorithm requires either a memory of size at least $\Omega\left((\log
|X|) \cdot (\log |A|)\right)$ or an exponential number of samples, achieving a
tight $\Omega\left((\log |X|) \cdot (\log |A|)\right)$ lower bound on the size
of the memory, rather than a bound of $\Omega\left(\min\left\{(\log
|X|)^2,(\log |A|)^2\right\}\right)$ obtained in previous works [R17,MM17b].
  Moreover, our result implies all previous memory-samples lower bounds, as
well as a number of new applications.
  Our proof builds on [R17] that gave a general technique for proving
memory-samples lower bounds.
",matrix x correspond follow learn problem unknown element x x choose uniformly random learner try learn x stream sample a_1 b_1 a_2 b_2 every a_i choose uniformly random b_i a_i x assume k r submatrix least -k row least column bias -r show learn algorithm learn problem correspond require either memory size least k least r sample result hold even learner exponentially small success probability r particular show large class learning problem learn algorithm require either memory size least exponential number sample achieve tight lower bind size memory rather bind obtain previous work r17 mm17b moreover result imply previous memory sample low bound well number new application proof build r17 give general technique prove memory sample low bound
"Plannable Approximations to MDP Homomorphisms: Equivariance under
  Actions","  This work exploits action equivariance for representation learning in
reinforcement learning. Equivariance under actions states that transitions in
the input space are mirrored by equivalent transitions in latent space, while
the map and transition functions should also commute. We introduce a
contrastive loss function that enforces action equivariance on the learned
representations. We prove that when our loss is zero, we have a homomorphism of
a deterministic Markov Decision Process (MDP). Learning equivariant maps leads
to structured latent spaces, allowing us to build a model on which we plan
through value iteration. We show experimentally that for deterministic MDPs,
the optimal policy in the abstract MDP can be successfully lifted to the
original MDP. Moreover, the approach easily adapts to changes in the goal
states. Empirically, we show that in such MDPs, we obtain better
representations in fewer epochs compared to representation learning approaches
using reconstructions, while generalizing better to new goals than model-free
approaches.
",work exploit action equivariance representation learn reinforcement learn equivariance action state transition input space mirror equivalent transition latent space map transition function also commute introduce contrastive loss function enforce action equivariance learn representation prove loss zero homomorphism deterministic markov decision process mdp learn equivariant map lead structured latent space allow we build model plan value iteration show experimentally deterministic mdps optimal policy abstract mdp successfully lift original mdp moreover approach easily adapt change goal state empirically show mdps obtain well representation few epoch compare representation learning approach use reconstruction generalize well new goal model free approach
"Understanding Limitation of Two Symmetrized Orders by Worst-case
  Complexity","  Update order is one of the major design choices of block decomposition
algorithms. There are at least two classes of deterministic update orders:
nonsymmetric (e.g. cyclic order) and symmetric (e.g. Gaussian back substitution
or symmetric Gauss-Seidel). Recently, Coordinate Descent (CD) with cyclic order
was shown to be $O(n^2)$ times slower than randomized versions in the
worst-case. A natural question arises: can the symmetrized orders achieve
faster convergence rates than the cyclic order, or even getting close to the
randomized versions? In this paper, we give a negative answer to this question.
We show that both Gaussian back substitution (GBS) and symmetric Gauss-Seidel
(sGS) suffer from the same slow convergence issue as the cyclic order in the
worst case. In particular, we prove that for unconstrained problems, both
GBS-CD and sGS-CD can be $O(n^2)$ times slower than R-CD. Despite unconstrained
problems, we also empirically study linearly constrained problems with
quadratic objective: we empirically demonstrate that the convergence speed of
GBS-ADMM and sGS-ADMM can be roughly $O(n^2)$ times slower than randomly
permuted ADMM.
",update order one major design choice block decomposition algorithm least two class deterministic update order nonsymmetric cyclic order symmetric gaussian back substitution symmetric gauss seidel recently coordinate descent cd cyclic order show time slow randomized version bad case natural question arise symmetrize order achieve fast convergence rate cyclic order even get close randomized version paper give negative answer question show gaussian back substitution gbs symmetric gauss seidel sgs suffer slow convergence issue cyclic order worst case particular prove unconstrained problem gbs cd sgs cd time slow r cd despite unconstrained problem also empirically study linearly constrain problem quadratic objective empirically demonstrate convergence speed gbs admm sgs admm roughly time slow randomly permuted admm
"Predicting Water Temperature Dynamics of Unmonitored Lakes with Meta
  Transfer Learning","  Most environmental data come from a minority of well-monitored sites. An
ongoing challenge in the environmental sciences is transferring knowledge from
monitored sites to unmonitored sites. Here, we demonstrate a novel transfer
learning framework that accurately predicts depth-specific temperature in
unmonitored lakes (targets) by borrowing models from well-monitored lakes
(sources). This method, Meta Transfer Learning (MTL), builds a meta-learning
model to predict transfer performance from candidate source models to targets
using lake attributes and candidates' past performance. We constructed source
models at 145 well-monitored lakes using calibrated process-based modeling (PB)
and a recently developed approach called process-guided deep learning (PGDL).
We applied MTL to either PB or PGDL source models (PB-MTL or PGDL-MTL,
respectively) to predict temperatures in 305 target lakes treated as
unmonitored in the Upper Midwestern United States. We show significantly
improved performance relative to the uncalibrated process-based General Lake
Model, where the median RMSE for the target lakes is $2.52^{\circ}C$. PB-MTL
yielded a median RMSE of $2.43^{\circ}C$; PGDL-MTL yielded $2.16^{\circ}C$; and
a PGDL-MTL ensemble of nine sources per target yielded $1.88^{\circ}C$. For
sparsely monitored target lakes, PGDL-MTL often outperformed PGDL models
trained on the target lakes themselves. Differences in maximum depth between
the source and target were consistently the most important predictors. Our
approach readily scales to thousands of lakes in the Midwestern United States,
demonstrating that MTL with meaningful predictor variables and high-quality
source models is a promising approach for many kinds of unmonitored systems and
environmental variables.
",environmental datum come minority well monitor site ongoing challenge environmental science transfer knowledge monitor site unmonitored site demonstrate novel transfer learning framework accurately predict depth specific temperature unmonitored lake target borrowing model well monitor lake source method meta transfer learn mtl build meta learn model predict transfer performance candidate source model target use lake attribute candidate past performance construct source model 145 well monitor lake use calibrate process base modeling pb recently develop approach call process guide deep learning pgdl apply mtl either pb pgdl source model pb mtl pgdl mtl respectively predict temperature 305 target lake treat unmonitored upper midwestern united states show significantly improve performance relative uncalibrate process base general lake model median rmse target lake c pb mtl yield median rmse c pgdl mtl yield c pgdl mtl ensemble nine source per target yield c sparsely monitor target lake pgdl mtl often outperform pgdl model train target lake difference maximum depth source target consistently important predictor approach readily scale thousand lake midwestern united states demonstrate mtl meaningful predictor variable high quality source model promise approach many kind unmonitored system environmental variable
"Spectral Clustering: An empirical study of Approximation Algorithms and
  its Application to the Attrition Problem","  Clustering is the problem of separating a set of objects into groups (called
clusters) so that objects within the same cluster are more similar to each
other than to those in different clusters. Spectral clustering is a now
well-known method for clustering which utilizes the spectrum of the data
similarity matrix to perform this separation. Since the method relies on
solving an eigenvector problem, it is computationally expensive for large
datasets. To overcome this constraint, approximation methods have been
developed which aim to reduce running time while maintaining accurate
classification. In this article, we summarize and experimentally evaluate
several approximation methods for spectral clustering. From an applications
standpoint, we employ spectral clustering to solve the so-called attrition
problem, where one aims to identify from a set of employees those who are
likely to voluntarily leave the company from those who are not. Our study sheds
light on the empirical performance of existing approximate spectral clustering
methods and shows the applicability of these methods in an important business
optimization related problem.
",cluster problem separate set object group call cluster object within cluster similar different cluster spectral cluster well know method cluster utilize spectrum datum similarity matrix perform separation since method rely solve eigenvector problem computationally expensive large dataset overcome constraint approximation method develop aim reduce run time maintain accurate classification article summarize experimentally evaluate several approximation method spectral clustering application standpoint employ spectral clustering solve so call attrition problem one aim identify set employee likely voluntarily leave company study shed light empirical performance exist approximate spectral cluster method show applicability method important business optimization relate problem
"Structural Solutions to Dynamic Scheduling for Multimedia Transmission
  in Unknown Wireless Environments","  In this paper, we propose a systematic solution to the problem of scheduling
delay-sensitive media data for transmission over time-varying wireless
channels. We first formulate the dynamic scheduling problem as a Markov
decision process (MDP) that explicitly considers the users' heterogeneous
multimedia data characteristics (e.g. delay deadlines, distortion impacts and
dependencies etc.) and time-varying channel conditions, which are not
simultaneously considered in state-of-the-art packet scheduling algorithms.
This formulation allows us to perform foresighted decisions to schedule
multiple data units for transmission at each time in order to optimize the
long-term utilities of the multimedia applications. The heterogeneity of the
media data enables us to express the transmission priorities between the
different data units as a priority graph, which is a directed acyclic graph
(DAG). This priority graph provides us with an elegant structure to decompose
the multi-data unit foresighted decision at each time into multiple single-data
unit foresighted decisions which can be performed sequentially, from the high
priority data units to the low priority data units, thereby significantly
reducing the computation complexity. When the statistical knowledge of the
multimedia data characteristics and channel conditions is unknown a priori, we
develop a low-complexity online learning algorithm to update the value
functions which capture the impact of the current decision on the future
utility. The simulation results show that the proposed solution significantly
outperforms existing state-of-the-art scheduling solutions.
",paper propose systematic solution problem scheduling delay sensitive medium datum transmission time vary wireless channel first formulate dynamic scheduling problem markov decision process mdp explicitly consider user heterogeneous multimedia data characteristic delay deadline distortion impact dependency etc time vary channel condition simultaneously consider state of the art packet scheduling algorithm formulation allow we perform foresighte decision schedule multiple datum unit transmission time order optimize long term utility multimedia application heterogeneity medium datum enable we express transmission priority different data unit priority graph direct acyclic graph dag priority graph provide we elegant structure decompose multi data unit foresighte decision time multiple single data unit foresighte decision perform sequentially high priority datum unit low priority datum unit thereby significantly reduce computation complexity statistical knowledge multimedia data characteristic channel condition unknown priori develop low complexity online learning algorithm update value function capture impact current decision future utility simulation result show propose solution significantly outperform exist state of the art scheduling solution
"Explainable nonlinear modelling of multiple time series with invertible
  neural networks","  A method for nonlinear topology identification is proposed, based on the
assumption that a collection of time series are generated in two steps: i) a
vector autoregressive process in a latent space, and ii) a nonlinear,
component-wise, monotonically increasing observation mapping. The latter
mappings are assumed invertible, and are modelled as shallow neural networks,
so that their inverse can be numerically evaluated, and their parameters can be
learned using a technique inspired in deep learning. Due to the function
inversion, the back-propagation step is not straightforward, and this paper
explains the steps needed to calculate the gradients applying implicit
differentiation. Whereas the model explainability is the same as that for
linear VAR processes, preliminary numerical tests show that the prediction
error becomes smaller.
",method nonlinear topology identification propose base assumption collection time series generate two step vector autoregressive process latent space ii nonlinear component wise monotonically increase observation mapping latter mapping assume invertible model shallow neural network inverse numerically evaluate parameter learn use technique inspire deep learning due function inversion back propagation step straightforward paper explain step need calculate gradient apply implicit differentiation whereas model explainability linear var process preliminary numerical test show prediction error become small
Dehaze-GLCGAN: Unpaired Single Image De-hazing via Adversarial Training,"  Single image de-hazing is a challenging problem, and it is far from solved.
Most current solutions require paired image datasets that include both hazy
images and their corresponding haze-free ground-truth images. However, in
reality, lighting conditions and other factors can produce a range of haze-free
images that can serve as ground truth for a hazy image, and a single ground
truth image cannot capture that range. This limits the scalability and
practicality of paired image datasets in real-world applications. In this
paper, we focus on unpaired single image de-hazing and we do not rely on the
ground truth image or physical scattering model. We reduce the image de-hazing
problem to an image-to-image translation problem and propose a dehazing
Global-Local Cycle-consistent Generative Adversarial Network (Dehaze-GLCGAN).
Generator network of Dehaze-GLCGAN combines an encoder-decoder architecture
with residual blocks to better recover the haze free scene. We also employ a
global-local discriminator structure to deal with spatially varying haze.
Through ablation study, we demonstrate the effectiveness of different factors
in the performance of the proposed network. Our extensive experiments over
three benchmark datasets show that our network outperforms previous work in
terms of PSNR and SSIM while being trained on smaller amount of data compared
to other methods.
",single image de hazing challenging problem far solve current solution require paired image dataset include hazy image correspond haze free ground truth image however reality lighting condition factor produce range haze free image serve ground truth hazy image single ground truth image capture range limit scalability practicality pair image dataset real world application paper focus unpaire single image de hazing rely ground truth image physical scattering model reduce image de hazing problem image to image translation problem propose dehaze global local cycle consistent generative adversarial network dehaze glcgan generator network dehaze glcgan combine encoder decoder architecture residual block well recover haze free scene also employ global local discriminator structure deal spatially vary haze ablation study demonstrate effectiveness different factor performance propose network extensive experiment three benchmark dataset show network outperform previous work term psnr ssim train small amount datum compare method
CryptoCredit: Securely Training Fair Models,"  When developing models for regulated decision making, sensitive features like
age, race and gender cannot be used and must be obscured from model developers
to prevent bias. However, the remaining features still need to be tested for
correlation with sensitive features, which can only be done with the knowledge
of those features. We resolve this dilemma using a fully homomorphic encryption
scheme, allowing model developers to train linear regression and logistic
regression models and test them for possible bias without ever revealing the
sensitive features in the clear. We demonstrate how it can be applied to
leave-one-out regression testing, and show using the adult income data set that
our method is practical to run.
",develop model regulate decision make sensitive feature like age race gender use must obscure model developer prevent bias however remain feature still need test correlation sensitive feature do knowledge feature resolve dilemma use fully homomorphic encryption scheme allow model developer train linear regression logistic regression model test possible bias without ever reveal sensitive feature clear demonstrate apply leave one out regression testing show use adult income datum set method practical run
"A framework for optimizing COVID-19 testing policy using a Multi Armed
  Bandit approach","  Testing is an important part of tackling the COVID-19 pandemic. Availability
of testing is a bottleneck due to constrained resources and effective
prioritization of individuals is necessary. Here, we discuss the impact of
different prioritization policies on COVID-19 patient discovery and the ability
of governments and health organizations to use the results for effective
decision making. We suggest a framework for testing that balances the maximal
discovery of positive individuals with the need for population-based
surveillance aimed at understanding disease spread and characteristics. This
framework draws from similar approaches to prioritization in the domain of
cyber-security based on ranking individuals using a risk score and then
reserving a portion of the capacity for random sampling. This approach is an
application of Multi-Armed-Bandits maximizing exploration/exploitation of the
underlying distribution. We find that individuals can be ranked for effective
testing using a few simple features, and that ranking them using such models we
can capture 65% (CI: 64.7%-68.3%) of the positive individuals using less than
20% of the testing capacity or 92.1% (CI: 91.1%-93.2%) of positives individuals
using 70% of the capacity, allowing reserving a significant portion of the
tests for population studies. Our approach allows experts and decision-makers
to tailor the resulting policies as needed allowing transparency into the
ranking policy and the ability to understand the disease spread in the
population and react quickly and in an informed manner.
",test important part tackle covid-19 pandemic availability testing bottleneck due constrain resource effective prioritization individual necessary discuss impact different prioritization policy covid-19 patient discovery ability government health organization use result effective decision making suggest framework testing balance maximal discovery positive individual need population base surveillance aim understand disease spread characteristic framework draw similar approach prioritization domain cyber security base rank individual use risk score reserving portion capacity random sampling approach application multi armed bandit maximize underlying distribution find individual rank effective testing use simple feature rank use model capture 65 ci positive individual use less 20 testing capacity ci positive individual use 70 capacity allowing reserve significant portion test population study approach allow expert decision maker tailor result policy need allow transparency rank policy ability understand disease spread population react quickly inform manner
HopSkipJumpAttack: A Query-Efficient Decision-Based Attack,"  The goal of a decision-based adversarial attack on a trained model is to
generate adversarial examples based solely on observing output labels returned
by the targeted model. We develop HopSkipJumpAttack, a family of algorithms
based on a novel estimate of the gradient direction using binary information at
the decision boundary. The proposed family includes both untargeted and
targeted attacks optimized for $\ell_2$ and $\ell_\infty$ similarity metrics
respectively. Theoretical analysis is provided for the proposed algorithms and
the gradient direction estimate. Experiments show HopSkipJumpAttack requires
significantly fewer model queries than Boundary Attack. It also achieves
competitive performance in attacking several widely-used defense mechanisms.
(HopSkipJumpAttack was named Boundary Attack++ in a previous version of the
preprint.)
",goal decision base adversarial attack train model generate adversarial example base solely observe output label return targeted model develop hopskipjumpattack family algorithm base novel estimate gradient direction use binary information decision boundary propose family include untargeted target attack optimize similarity metric respectively theoretical analysis provide propose algorithm gradient direction estimate experiment show hopskipjumpattack require significantly few model query boundary attack also achieve competitive performance attack several widely use defense mechanism hopskipjumpattack name boundary previous version preprint
Sparse Canonical Correlation Analysis via Concave Minimization,"  A new approach to the sparse Canonical Correlation Analysis (sCCA)is proposed
with the aim of discovering interpretable associations in very high-dimensional
multi-view, i.e.observations of multiple sets of variables on the same
subjects, problems. Inspired by the sparse PCA approach of Journee et al.
(2010), we also show that the sparse CCA formulation, while non-convex, is
equivalent to a maximization program of a convex objective over a compact set
for which we propose a first-order gradient method. This result helps us reduce
the search space drastically to the boundaries of the set. Consequently, we
propose a two-step algorithm, where we first infer the sparsity pattern of the
canonical directions using our fast algorithm, then we shrink each view, i.e.
observations of a set of covariates, to contain observations on the sets of
covariates selected in the previous step, and compute their canonical
directions via any CCA algorithm. We also introduceDirected Sparse CCA, which
is able to find associations which are aligned with a specified experiment
design, andMulti-View sCCA which is used to discover associations between
multiple sets of covariates. Our simulations establish the superior convergence
properties and computational efficiency of our algorithm as well as accuracy in
terms of the canonical correlation and its ability to recover the supports of
the canonical directions. We study the associations between metabolomics,
trasncriptomics and microbiomics in a multi-omic study usingMuLe, which is an
R-package that implements our approach, in order to form hypotheses on
mechanisms of adaptations of Drosophila Melanogaster to high doses of
environmental toxicants, specifically Atrazine, which is a commonly used
chemical fertilizer.
",new approach sparse canonical correlation analysis scca propose aim discover interpretable association high dimensional multi view multiple set variable subject problem inspire sparse pca approach journee et al 2010 also show sparse cca formulation non convex equivalent maximization program convex objective compact set propose first order gradient method result help we reduce search space drastically boundary set consequently propose two step algorithm first infer sparsity pattern canonical direction use fast algorithm shrink view observation set covariate contain observation set covariate select previous step compute canonical direction via cca algorithm also introducedirecte sparse cca able find association align specify experiment design andmulti view scca use discover association multiple set covariate simulation establish superior convergence property computational efficiency algorithm well accuracy term canonical correlation ability recover support canonical direction study association metabolomic trasncriptomic microbiomic multi omic study usingmule r package implement approach order form hypothesis mechanism adaptation drosophila melanogaster high dose environmental toxicant specifically atrazine commonly use chemical fertilizer
"Improving prostate whole gland segmentation in t2-weighted MRI with
  synthetically generated data","  Whole gland (WG) segmentation of the prostate plays a crucial role in
detection, staging and treatment planning of prostate cancer (PCa). Despite
promise shown by deep learning (DL) methods, they rely on the availability of a
considerable amount of annotated data. Augmentation techniques such as
translation and rotation of images present an alternative to increase data
availability. Nevertheless, the amount of information provided by the
transformed data is limited due to the correlation between the generated data
and the original. Based on the recent success of generative adversarial
networks (GAN) in producing synthetic images for other domains as well as in
the medical domain, we present a pipeline to generate WG segmentation masks and
synthesize T2-weighted MRI of the prostate based on a publicly available
multi-center dataset. Following, we use the generated data as a form of data
augmentation. Results show an improvement in the quality of the WG segmentation
when compared to standard augmentation techniques.
",whole gland wg segmentation prostate play crucial role detection stage treatment planning prostate cancer pca despite promise show deep learning dl method rely availability considerable amount annotate datum augmentation technique translation rotation image present alternative increase datum availability nevertheless amount information provide transform datum limited due correlation generate datum original base recent success generative adversarial network gin produce synthetic image domain well medical domain present pipeline generate wg segmentation mask synthesize t2 weighted mri prostate base publicly available multi center dataset follow use generate datum form datum augmentation result show improvement quality wg segmentation compare standard augmentation technique
"Explainable AI for Trees: From Local Explanations to Global
  Understanding","  Tree-based machine learning models such as random forests, decision trees,
and gradient boosted trees are the most popular non-linear predictive models
used in practice today, yet comparatively little attention has been paid to
explaining their predictions. Here we significantly improve the
interpretability of tree-based models through three main contributions: 1) The
first polynomial time algorithm to compute optimal explanations based on game
theory. 2) A new type of explanation that directly measures local feature
interaction effects. 3) A new set of tools for understanding global model
structure based on combining many local explanations of each prediction. We
apply these tools to three medical machine learning problems and show how
combining many high-quality local explanations allows us to represent global
structure while retaining local faithfulness to the original model. These tools
enable us to i) identify high magnitude but low frequency non-linear mortality
risk factors in the general US population, ii) highlight distinct population
sub-groups with shared risk characteristics, iii) identify non-linear
interaction effects among risk factors for chronic kidney disease, and iv)
monitor a machine learning model deployed in a hospital by identifying which
features are degrading the model's performance over time. Given the popularity
of tree-based machine learning models, these improvements to their
interpretability have implications across a broad set of domains.
",tree base machine learning model random forest decision tree gradient boost tree popular non linear predictive model use practice today yet comparatively little attention pay explain prediction significantly improve interpretability tree base model three main contribution 1 first polynomial time algorithm compute optimal explanation base game theory 2 new type explanation directly measure local feature interaction effect 3 new set tool understand global model structure base combine many local explanation prediction apply tool three medical machine learn problem show combine many high quality local explanation allow we represent global structure retain local faithfulness original model tool enable we identify high magnitude low frequency non linear mortality risk factor general us population ii highlight distinct population sub group share risk characteristic iii identify non linear interaction effect among risk factor chronic kidney disease iv monitor machine learning model deploy hospital identify feature degrading model performance time give popularity tree base machine learning model improvement interpretability implication across broad set domain
"Resource-Aware Asynchronous Online Federated Learning for Nonlinear
  Regression","  Many assumptions in the federated learning literature present a best-case
scenario that can not be satisfied in most real-world applications. An
asynchronous setting reflects the realistic environment in which federated
learning methods must be able to operate reliably. Besides varying amounts of
non-IID data at participants, the asynchronous setting models heterogeneous
client participation due to available computational power and battery
constraints and also accounts for delayed communications between clients and
the server. To reduce the communication overhead associated with asynchronous
online federated learning (ASO-Fed), we use the principles of
partial-sharing-based communication. In this manner, we reduce the
communication load of the participants and, therefore, render participation in
the learning task more accessible. We prove the convergence of the proposed
ASO-Fed and provide simulations to analyze its behavior further. The
simulations reveal that, in the asynchronous setting, it is possible to achieve
the same convergence as the federated stochastic gradient (Online-FedSGD) while
reducing the communication tenfold.
",many assumption federate learn literature present good case scenario satisfied real world application asynchronous setting reflect realistic environment federate learning method must able operate reliably besides vary amount non iid datum participant asynchronous setting model heterogeneous client participation due available computational power battery constraint also account delay communication client server reduce communication overhead associate asynchronous online federate learn aso feed use principle partial sharing base communication manner reduce communication load participant therefore render participation learn task accessible prove convergence propose aso fed provide simulation analyze behavior simulation reveal asynchronous setting possible achieve convergence federate stochastic gradient online fedsgd reduce communication tenfold
Optimal Subarchitecture Extraction For BERT,"  We extract an optimal subset of architectural parameters for the BERT
architecture from Devlin et al. (2018) by applying recent breakthroughs in
algorithms for neural architecture search. This optimal subset, which we refer
to as ""Bort"", is demonstrably smaller, having an effective (that is, not
counting the embedding layer) size of $5.5\%$ the original BERT-large
architecture, and $16\%$ of the net size. Bort is also able to be pretrained in
$288$ GPU hours, which is $1.2\%$ of the time required to pretrain the
highest-performing BERT parametric architectural variant, RoBERTa-large (Liu et
al., 2019), and about $33\%$ of that of the world-record, in GPU hours,
required to train BERT-large on the same hardware. It is also $7.9$x faster on
a CPU, as well as being better performing than other compressed variants of the
architecture, and some of the non-compressed variants: it obtains performance
improvements of between $0.3\%$ and $31\%$, absolute, with respect to
BERT-large, on multiple public natural language understanding (NLU) benchmarks.
",extract optimal subset architectural parameter bert architecture devlin et al 2018 apply recent breakthrough algorithm neural architecture search optimal subset refer bort demonstrably small effective counting embed layer size original bert large architecture net size bort also able pretraine 288 gpu hour time require pretrain highest perform bert parametric architectural variant roberta large liu et 2019 world record gpu hour require train bert large hardware also x fast cpu well well perform compress variant architecture non compressed variant obtain performance improvement absolute respect bert large multiple public natural language understand nlu benchmark
"Property Inference Attacks on Convolutional Neural Networks: Influence
  and Implications of Target Model's Complexity","  Machine learning models' goal is to make correct predictions for specific
tasks by learning important properties and patterns from data. By doing so,
there is a chance that the model learns properties that are unrelated to its
primary task. Property Inference Attacks exploit this and aim to infer from a
given model (\ie the target model) properties about the training dataset
seemingly unrelated to the model's primary goal. If the training data is
sensitive, such an attack could lead to privacy leakage. This paper
investigates the influence of the target model's complexity on the accuracy of
this type of attack, focusing on convolutional neural network classifiers. We
perform attacks on models that are trained on facial images to predict whether
someone's mouth is open. Our attacks' goal is to infer whether the training
dataset is balanced gender-wise. Our findings reveal that the risk of a privacy
breach is present independently of the target model's complexity: for all
studied architectures, the attack's accuracy is clearly over the baseline. We
discuss the implication of the property inference on personal data in the light
of Data Protection Regulations and Guidelines.
",machine learning model goal make correct prediction specific task learn important property pattern datum chance model learn property unrelated primary task property inference attack exploit aim infer give model target model property train dataset seemingly unrelated model primary goal training datum sensitive attack could lead privacy leakage paper investigate influence target model complexity accuracy type attack focus convolutional neural network classifier perform attack model train facial image predict whether someone mouth open attack goal infer whether training dataset balanced gender wise finding reveal risk privacy breach present independently target model complexity study architecture attack accuracy clearly baseline discuss implication property inference personal datum light datum protection regulation guideline
Online AUC Optimization for Sparse High-Dimensional Datasets,"  The Area Under the ROC Curve (AUC) is a widely used performance measure for
imbalanced classification arising from many application domains where
high-dimensional sparse data is abundant. In such cases, each $d$ dimensional
sample has only $k$ non-zero features with $k \ll d$, and data arrives
sequentially in a streaming form. Current online AUC optimization algorithms
have high per-iteration cost $\mathcal{O}(d)$ and usually produce non-sparse
solutions in general, and hence are not suitable for handling the data
challenge mentioned above.
  In this paper, we aim to directly optimize the AUC score for high-dimensional
sparse datasets under online learning setting and propose a new algorithm,
\textsc{FTRL-AUC}. Our proposed algorithm can process data in an online fashion
with a much cheaper per-iteration cost $\mathcal{O}(k)$, making it amenable for
high-dimensional sparse streaming data analysis. Our new algorithmic design
critically depends on a novel reformulation of the U-statistics AUC objective
function as the empirical saddle point reformulation, and the innovative
introduction of the ""lazy update"" rule so that the per-iteration complexity is
dramatically reduced from $\mathcal{O}(d)$ to $\mathcal{O}(k)$. Furthermore,
\textsc{FTRL-AUC} can inherently capture sparsity more effectively by applying
a generalized Follow-The-Regularized-Leader (FTRL) framework.
  Experiments on real-world datasets demonstrate that \textsc{FTRL-AUC}
significantly improves both run time and model sparsity while achieving
competitive AUC scores compared with the state-of-the-art methods. Comparison
with the online learning method for logistic loss demonstrates that
\textsc{FTRL-AUC} achieves higher AUC scores especially when datasets are
imbalanced.
",area roc curve auc widely use performance measure imbalance classification arise many application domain high dimensional sparse datum abundant case dimensional sample k non zero feature k datum arrive sequentially streaming form current online auc optimization algorithm high per iteration cost usually produce non sparse solution general hence suitable handle datum challenge mention paper aim directly optimize auc score high dimensional sparse dataset online learning set propose new algorithm ftrl auc propose algorithm process datum online fashion much cheap per iteration cost k make amenable high dimensional sparse stream datum analysis new algorithmic design critically depend novel reformulation u statistic auc objective function empirical saddle point reformulation innovative introduction lazy update rule per iteration complexity dramatically reduce k furthermore ftrl auc inherently capture sparsity effectively apply generalize follow the regularize leader ftrl framework experiment real world dataset demonstrate ftrl auc significantly improve run time model sparsity achieve competitive auc score compare state of the art method comparison online learning method logistic loss demonstrate ftrl auc achieve high auc score especially dataset imbalance
Model-based Reinforcement Learning: A Survey,"  Sequential decision making, commonly formalized as Markov Decision Process
(MDP) optimization, is a important challenge in artificial intelligence. Two
key approaches to this problem are reinforcement learning (RL) and planning.
This paper presents a survey of the integration of both fields, better known as
model-based reinforcement learning. Model-based RL has two main steps. First,
we systematically cover approaches to dynamics model learning, including
challenges like dealing with stochasticity, uncertainty, partial observability,
and temporal abstraction. Second, we present a systematic categorization of
planning-learning integration, including aspects like: where to start planning,
what budgets to allocate to planning and real data collection, how to plan, and
how to integrate planning in the learning and acting loop. After these two
sections, we also discuss implicit model-based RL as an end-to-end alternative
for model learning and planning, and we cover the potential benefits of
model-based RL. Along the way, the survey also draws connections to several
related RL fields, like hierarchical RL and transfer learning. Altogether, the
survey presents a broad conceptual overview of the combination of planning and
learning for MDP optimization.
",sequential decision make commonly formalize markov decision process mdp optimization important challenge artificial intelligence two key approach problem reinforcement learning rl plan paper present survey integration field well know model base reinforcement learn model base rl two main step first systematically cover approach dynamic model learning include challenge like deal stochasticity uncertainty partial observability temporal abstraction second present systematic categorization planning learn integration include aspect like start plan budget allocate plan real data collection plan integrate planning learn act loop two section also discuss implicit model base rl end to end alternative model learn planning cover potential benefit model base rl along way survey also draw connection several related rl field like hierarchical rl transfer learn altogether survey present broad conceptual overview combination planning learn mdp optimization
Variable Weights Neural Network For Diabetes Classification,"  As witnessed in the past year, where the world was brought to the ground by a
pandemic, fighting Life-threatening diseases have found greater focus than
ever. The first step in fighting a disease is to diagnose it at the right time.
Diabetes has been affecting people for a long time and is growing among people
faster than ever. The number of people who have Diabetes reached 422 million in
2018, as reported by WHO, and the global prevalence of diabetes among adults
above the age of 18 has risen to 8.5%. Now Diabetes is a disease that shows no
or very few symptoms among the people affected by it for a long time, and even
in some cases, people realize they have it when they have lost any chance of
controlling it. So getting Diabetes diagnosed at an early stage can make a huge
difference in how one can approach curing it. Moving in this direction in this
paper, we have designed a liquid machine learning approach to detect Diabetes
with no cost using deep learning. In this work, we have used a dataset of 520
instances. Our approach shows a significant improvement in the previous
state-of-the-art results. Its power to generalize well on small dataset deals
with the critical problem of lesser data in medical sciences.
",witness past year world bring ground pandemic fight life threaten disease find great focus ever first step fight disease diagnose right time diabetes affect people long time grow among people fast ever number people diabete reach 422 million 2018 report global prevalence diabete among adult age 18 risen diabetes disease show symptom among people affect long time even case people realize lose chance control getting diabete diagnose early stage make huge difference one approach cure move direction paper design liquid machine learn approach detect diabetes cost use deep learning work use dataset 520 instance approach show significant improvement previous state of the art result power generalize well small dataset deal critical problem less data medical science
"Combining resampling and reweighting for faithful stochastic
  optimization","  Many machine learning and data science tasks require solving non-convex
optimization problems. When the loss function is a sum of multiple terms, a
popular method is the stochastic gradient descent. Viewed as a process for
sampling the loss function landscape, the stochastic gradient descent is known
to prefer flat minima. Though this is desired for certain optimization problems
such as in deep learning, it causes issues when the goal is to find the global
minimum, especially if the global minimum resides in a sharp valley.
  Illustrated with a simple motivating example, we show that the fundamental
reason is that the difference in the Lipschitz constants of multiple terms in
the loss function causes stochastic gradient descent to experience different
variances at different minima. In order to mitigate this effect and perform
faithful optimization, we propose a combined resampling-reweighting scheme to
balance the variance at local minima and extend to general loss functions. We
explain from the numerical stability perspective how the proposed scheme is
more likely to select the true global minimum, and the local convergence
analysis perspective how it converges to a minimum faster when compared with
the vanilla stochastic gradient descent. Experiments from robust statistics and
computational chemistry are provided to demonstrate the theoretical findings.
",many machine learn datum science task require solve non convex optimization problem loss function sum multiple term popular method stochastic gradient descent view process sample loss function landscape stochastic gradient descent know prefer flat minima though desire certain optimization problem deep learning cause issue goal find global minimum especially global minimum reside sharp valley illustrate simple motivating example show fundamental reason difference lipschitz constant multiple term loss function cause stochastic gradient descent experience different variance different minima order mitigate effect perform faithful optimization propose combine resampling reweighte scheme balance variance local minima extend general loss function explain numerical stability perspective propose scheme likely select true global minimum local convergence analysis perspective converge minimum fast compare vanilla stochastic gradient descent experiment robust statistic computational chemistry provide demonstrate theoretical finding
"Blending Knowledge in Deep Recurrent Networks for Adverse Event
  Prediction at Hospital Discharge","  Deep learning architectures have an extremely high-capacity for modeling
complex data in a wide variety of domains. However, these architectures have
been limited in their ability to support complex prediction problems using
insurance claims data, such as readmission at 30 days, mainly due to data
sparsity issue. Consequently, classical machine learning methods, especially
those that embed domain knowledge in handcrafted features, are often on par
with, and sometimes outperform, deep learning approaches. In this paper, we
illustrate how the potential of deep learning can be achieved by blending
domain knowledge within deep learning architectures to predict adverse events
at hospital discharge, including readmissions. More specifically, we introduce
a learning architecture that fuses a representation of patient data computed by
a self-attention based recurrent neural network, with clinically relevant
features. We conduct extensive experiments on a large claims dataset and show
that the blended method outperforms the standard machine learning approaches.
",deep learning architecture extremely high capacity modeling complex datum wide variety domain however architecture limited ability support complex prediction problem use insurance claim datum readmission 30 day mainly due data sparsity issue consequently classical machine learn method especially embe domain knowledge handcraft feature often par sometimes outperform deep learning approach paper illustrate potential deep learning achieve blend domain knowledge within deep learning architecture predict adverse event hospital discharge include readmission specifically introduce learn architecture fuse representation patient datum compute self attention base recurrent neural network clinically relevant feature conduct extensive experiment large claim dataset show blend method outperform standard machine learning approach
"Learning Physics through Images: An Application to Wind-Driven Spatial
  Patterns","  For centuries, scientists have observed nature to understand the laws that
govern the physical world. The traditional process of turning observations into
physical understanding is slow. Imperfect models are constructed and tested to
explain relationships in data. Powerful new algorithms are available that can
enable computers to learn physics by observing images and videos. Inspired by
this idea, instead of training machine learning models using physical
quantities, we trained them using images, that is, pixel information. For this
work, and as a proof of concept, the physics of interest are wind-driven
spatial patterns. Examples of these phenomena include features in Aeolian dunes
and the deposition of volcanic ash, wildfire smoke, and air pollution plumes.
We assume that the spatial patterns were collected by an imaging device that
records the magnitude of the logarithm of deposition as a red, green, blue
(RGB) color image with channels containing values ranging from 0 to 255. In
this paper, we explore deep convolutional neural network-based autoencoders to
exploit relationships in wind-driven spatial patterns, which commonly occur in
geosciences, and reduce their dimensionality. Reducing the data dimension size
with an encoder allows us to train regression models linking geographic and
meteorological scalar input quantities to the encoded space. Once this is
achieved, full predictive spatial patterns are reconstructed using the decoder.
We demonstrate this approach on images of spatial deposition from a pollution
source, where the encoder compresses the dimensionality to 0.02% of the
original size and the full predictive model performance on test data achieves
an accuracy of 92%.
",century scientist observe nature understand law govern physical world traditional process turn observation physical understanding slow imperfect model construct test explain relationship datum powerful new algorithm available enable computer learn physics observe image video inspire idea instead train machine learning model use physical quantity train use image pixel information work proof concept physics interest wind drive spatial pattern example phenomenon include feature aeolian dune deposition volcanic ash wildfire smoke air pollution plume assume spatial pattern collect image device record magnitude logarithm deposition red green blue rgb color image channel contain value range 0 paper explore deep convolutional neural network base autoencoder exploit relationship wind drive spatial pattern commonly occur geoscience reduce dimensionality reduce datum dimension size encoder allow we train regression model link geographic meteorological scalar input quantity encode space achieve full predictive spatial pattern reconstruct use decoder demonstrate approach image spatial deposition pollution source encoder compress dimensionality original size full predictive model performance test datum achieve accuracy 92
Understanding Metrics for Paraphrasing,"Paraphrase generation is a difficult problem. This is not only because of the
limitations in text generation capabilities but also due that to the lack of a
proper definition of what qualifies as a paraphrase and corresponding metrics
to measure how good it is. Metrics for evaluation of paraphrasing quality is an
on going research problem. Most of the existing metrics in use having been
borrowed from other tasks do not capture the complete essence of a good
paraphrase, and often fail at borderline-cases. In this work, we propose a
novel metric $ROUGE_P$ to measure the quality of paraphrases along the
dimensions of adequacy, novelty and fluency. We also provide empirical evidence
to show that the current natural language generation metrics are insufficient
to measure these desired properties of a good paraphrase. We look at paraphrase
model fine-tuning and generation from the lens of metrics to gain a deeper
understanding of what it takes to generate and evaluate a good paraphrase.",paraphrase generation difficult problem limitation text generation capability also due lack proper definition qualifie paraphrase correspond metric measure good metric evaluation paraphrasing quality go research problem exist metric use borrow task capture complete essence good paraphrase often fail borderline case work propose novel metric rouge_p measure quality paraphrase along dimension adequacy novelty fluency also provide empirical evidence show current natural language generation metric insufficient measure desire property good paraphrase look paraphrase model fine tuning generation lens metric gain deep understanding take generate evaluate good paraphrase
Towards General Function Approximation in Zero-Sum Markov Games,"  This paper considers two-player zero-sum finite-horizon Markov games with
simultaneous moves. The study focuses on the challenging settings where the
value function or the model is parameterized by general function classes.
Provably efficient algorithms for both decoupled and {coordinated} settings are
developed. In the {decoupled} setting where the agent controls a single player
and plays against an arbitrary opponent, we propose a new model-free algorithm.
The sample complexity is governed by the Minimax Eluder dimension -- a new
dimension of the function class in Markov games. As a special case, this method
improves the state-of-the-art algorithm by a $\sqrt{d}$ factor in the regret
when the reward function and transition kernel are parameterized with
$d$-dimensional linear features. In the {coordinated} setting where both
players are controlled by the agent, we propose a model-based algorithm and a
model-free algorithm. In the model-based algorithm, we prove that sample
complexity can be bounded by a generalization of Witness rank to Markov games.
The model-free algorithm enjoys a $\sqrt{K}$-regret upper bound where $K$ is
the number of episodes.
",paper consider two player zero sum finite horizon markov games simultaneous move study focus challenge setting value function model parameterized general function class provably efficient algorithm decouple coordinated setting develop decouple set agent control single player play arbitrary opponent propose new model free algorithm sample complexity govern minimax eluder dimension new dimension function class markov games special case method improve state of the art algorithm factor regret reward function transition kernel parameterized -dimensional linear feature coordinate set player control agent propose model base algorithm model free algorithm model base algorithm prove sample complexity bound generalization witness rank markov games model free algorithm enjoy k -regret upper bind k number episode
Catch-A-Waveform: Learning to Generate Audio from a Single Short Example,"  Models for audio generation are typically trained on hours of recordings.
Here, we illustrate that capturing the essence of an audio source is typically
possible from as little as a few tens of seconds from a single training signal.
Specifically, we present a GAN-based generative model that can be trained on
one short audio signal from any domain (e.g. speech, music, etc.) and does not
require pre-training or any other form of external supervision. Once trained,
our model can generate random samples of arbitrary duration that maintain
semantic similarity to the training waveform, yet exhibit new compositions of
its audio primitives. This enables a long line of interesting applications,
including generating new jazz improvisations or new a-cappella rap variants
based on a single short example, producing coherent modifications to famous
songs (e.g. adding a new verse to a Beatles song based solely on the original
recording), filling-in of missing parts (inpainting), extending the bandwidth
of a speech signal (super-resolution), and enhancing old recordings without
access to any clean training example. We show that in all cases, no more than
20 seconds of training audio commonly suffice for our model to achieve
state-of-the-art results. This is despite its complete lack of prior knowledge
about the nature of audio signals in general.
",model audio generation typically train hour recording illustrate capture essence audio source typically possible little ten second single training signal specifically present gan base generative model train one short audio signal domain speech music etc require pre training form external supervision train model generate random sample arbitrary duration maintain semantic similarity training waveform yet exhibit new composition audio primitive enable long line interesting application include generate new jazz improvisation new a cappella rap variant base single short example produce coherent modification famous song add new verse beatle song base solely original recording filling in missing part inpainte extend bandwidth speech signal super resolution enhance old recording without access clean training example show case 20 second training audio commonly suffice model achieve state of the art result despite complete lack prior knowledge nature audio signal general
Linguistic Features for Readability Assessment,"  Readability assessment aims to automatically classify text by the level
appropriate for learning readers. Traditional approaches to this task utilize a
variety of linguistically motivated features paired with simple machine
learning models. More recent methods have improved performance by discarding
these features and utilizing deep learning models. However, it is unknown
whether augmenting deep learning models with linguistically motivated features
would improve performance further. This paper combines these two approaches
with the goal of improving overall model performance and addressing this
question. Evaluating on two large readability corpora, we find that, given
sufficient training data, augmenting deep learning models with linguistically
motivated features does not improve state-of-the-art performance. Our results
provide preliminary evidence for the hypothesis that the state-of-the-art deep
learning models represent linguistic features of the text related to
readability. Future research on the nature of representations formed in these
models can shed light on the learned features and their relations to
linguistically motivated ones hypothesized in traditional approaches.
",readability assessment aims automatically classify text level appropriate learning reader traditional approach task utilize variety linguistically motivated feature pair simple machine learning model recent method improve performance discard feature utilize deep learning model however unknown whether augment deep learning model linguistically motivated feature would improve performance paper combine two approach goal improve overall model performance address question evaluate two large readability corpora find give sufficient training datum augment deep learning model linguistically motivated feature improve state of the art performance result provide preliminary evidence hypothesis state of the art deep learning model represent linguistic feature text relate readability future research nature representation form model shed light learn feature relation linguistically motivated one hypothesize traditional approach
"Fast Optimization Algorithm on Riemannian Manifolds and Its Application
  in Low-Rank Representation","  The paper addresses the problem of optimizing a class of composite functions
on Riemannian manifolds and a new first order optimization algorithm (FOA) with
a fast convergence rate is proposed. Through the theoretical analysis for FOA,
it has been proved that the algorithm has quadratic convergence. The
experiments in the matrix completion task show that FOA has better performance
than other first order optimization methods on Riemannian manifolds. A fast
subspace pursuit method based on FOA is proposed to solve the low-rank
representation model based on augmented Lagrange method on the low rank matrix
variety. Experimental results on synthetic and real data sets are presented to
demonstrate that both FOA and SP-RPRG(ALM) can achieve superior performance in
terms of faster convergence and higher accuracy.
",paper address problem optimize class composite function riemannian manifold new first order optimization algorithm foa fast convergence rate propose theoretical analysis foa prove algorithm quadratic convergence experiment matrix completion task show foa well performance first order optimization method riemannian manifold fast subspace pursuit method base foa propose solve low rank representation model base augmented lagrange method low rank matrix variety experimental result synthetic real datum set present demonstrate foa sp rprg alm achieve superior performance term fast convergence high accuracy
"Constructing the Matrix Multilayer Perceptron and its Application to the
  VAE","  Like most learning algorithms, the multilayer perceptrons (MLP) is designed
to learn a vector of parameters from data. However, in certain scenarios we are
interested in learning structured parameters (predictions) in the form of
symmetric positive definite matrices. Here, we introduce a variant of the MLP,
referred to as the matrix MLP, that is specialized at learning symmetric
positive definite matrices. We also present an application of the model within
the context of the variational autoencoder (VAE). Our formulation of the VAE
extends the vanilla formulation to the cases where the recognition and the
generative networks can be from the parametric family of distributions with
dense covariance matrices. Two specific examples are discussed in more detail:
the dense covariance Gaussian and its generalization, the power exponential
distribution. Our new developments are illustrated using both synthetic and
real data.
",like learn algorithms multilayer perceptron mlp design learn vector parameter datum however certain scenario interested learn structure parameter prediction form symmetric positive definite matrix introduce variant mlp refer matrix mlp specialized learn symmetric positive definite matrix also present application model within context variational autoencoder vae formulation vae extend vanilla formulation case recognition generative network parametric family distribution dense covariance matrice two specific example discuss detail dense covariance gaussian generalization power exponential distribution new development illustrate use synthetic real datum
Consistent regression of biophysical parameters with kernel methods,"  This paper introduces a novel statistical regression framework that allows
the incorporation of consistency constraints. A linear and nonlinear
(kernel-based) formulation are introduced, and both imply closed-form
analytical solutions. The models exploit all the information from a set of
drivers while being maximally independent of a set of auxiliary, protected
variables. We successfully illustrate the performance in the estimation of
chlorophyll content.
",paper introduce novel statistical regression framework allow incorporation consistency constraint linear nonlinear kernel base formulation introduce imply close form analytical solution model exploit information set driver maximally independent set auxiliary protect variable successfully illustrate performance estimation chlorophyll content
"How much progress have we made in neural network training? A New
  Evaluation Protocol for Benchmarking Optimizers","  Many optimizers have been proposed for training deep neural networks, and
they often have multiple hyperparameters, which make it tricky to benchmark
their performance. In this work, we propose a new benchmarking protocol to
evaluate both end-to-end efficiency (training a model from scratch without
knowing the best hyperparameter) and data-addition training efficiency (the
previously selected hyperparameters are used for periodically re-training the
model with newly collected data). For end-to-end efficiency, unlike previous
work that assumes random hyperparameter tuning, which over-emphasizes the
tuning time, we propose to evaluate with a bandit hyperparameter tuning
strategy. A human study is conducted to show that our evaluation protocol
matches human tuning behavior better than the random search. For data-addition
training, we propose a new protocol for assessing the hyperparameter
sensitivity to data shift. We then apply the proposed benchmarking framework to
7 optimizers and various tasks, including computer vision, natural language
processing, reinforcement learning, and graph mining. Our results show that
there is no clear winner across all the tasks.
",many optimizer propose train deep neural network often multiple hyperparameter make tricky benchmark performance work propose new benchmarking protocol evaluate end to end efficiency training model scratch without know good hyperparameter data addition training efficiency previously select hyperparameter use periodically re training model newly collect datum end to end efficiency unlike previous work assume random hyperparameter tune over emphasize tuning time propose evaluate bandit hyperparameter tuning strategy human study conduct show evaluation protocol match human tuning behavior well random search data addition training propose new protocol assess hyperparameter sensitivity datum shift apply propose benchmarke framework 7 optimizer various task include computer vision natural language processing reinforcement learn graph mining result show clear winner across task
"Meta-Learning GNN Initializations for Low-Resource Molecular Property
  Prediction","  Building in silico models to predict chemical properties and activities is a
crucial step in drug discovery. However, limited labeled data often hinders the
application of deep learning in this setting. Meanwhile advances in
meta-learning have enabled state-of-the-art performances in few-shot learning
benchmarks, naturally prompting the question: Can meta-learning improve deep
learning performance in low-resource drug discovery projects? In this work, we
assess the transferability of graph neural networks initializations learned by
the Model-Agnostic Meta-Learning (MAML) algorithm - and its variants FO-MAML
and ANIL - for chemical properties and activities tasks. Using the ChEMBL20
dataset to emulate low-resource settings, our benchmark shows that
meta-initializations perform comparably to or outperform multi-task
pre-training baselines on 16 out of 20 in-distribution tasks and on all
out-of-distribution tasks, providing an average improvement in AUPRC of 11.2%
and 26.9% respectively. Finally, we observe that meta-initializations
consistently result in the best performing models across fine-tuning sets with
$k \in \{16, 32, 64, 128, 256\}$ instances.
",build silico model predict chemical property activitie crucial step drug discovery however limited label datum often hinder application deep learning set meanwhile advance meta learning enable state of the art performance few shot learning benchmark naturally prompt question meta learning improve deep learning performance low resource drug discovery project work assess transferability graph neural network initialization learn model agnostic meta learn maml algorithm variants fo maml anil chemical property activity task use chembl20 dataset emulate low resource setting benchmark show meta initialization perform comparably outperform multi task pre training baseline 16 20 in distribution task out of distribution task provide average improvement auprc respectively finally observe meta initializations consistently result good perform model across fine tuning set k 16 32 64 128 instance
CLAR: Contrastive Learning of Auditory Representations,"  Learning rich visual representations using contrastive self-supervised
learning has been extremely successful. However, it is still a major question
whether we could use a similar approach to learn superior auditory
representations. In this paper, we expand on prior work (SimCLR) to learn
better auditory representations. We (1) introduce various data augmentations
suitable for auditory data and evaluate their impact on predictive performance,
(2) show that training with time-frequency audio features substantially
improves the quality of the learned representations compared to raw signals,
and (3) demonstrate that training with both supervised and contrastive losses
simultaneously improves the learned representations compared to self-supervised
pre-training followed by supervised fine-tuning. We illustrate that by
combining all these methods and with substantially less labeled data, our
framework (CLAR) achieves significant improvement on prediction performance
compared to supervised approach. Moreover, compared to self-supervised
approach, our framework converges faster with significantly better
representations.
",learn rich visual representation use contrastive self supervise learning extremely successful however still major question whether could use similar approach learn superior auditory representation paper expand prior work simclr learn well auditory representation 1 introduce various data augmentation suitable auditory datum evaluate impact predictive performance 2 show training time frequency audio feature substantially improve quality learn representation compare raw signal 3 demonstrate training supervise contrastive loss simultaneously improves learn representation compare self supervise pre training follow supervise fine tuning illustrate combine method substantially less label datum framework clar achieve significant improvement prediction performance compare supervised approach moreover compare self supervise approach framework converge fast significantly well representation
Smooth Adversarial Training,"  It is commonly believed that networks cannot be both accurate and robust,
that gaining robustness means losing accuracy. It is also generally believed
that, unless making networks larger, network architectural elements would
otherwise matter little in improving adversarial robustness. Here we present
evidence to challenge these common beliefs by a careful study about adversarial
training. Our key observation is that the widely-used ReLU activation function
significantly weakens adversarial training due to its non-smooth nature. Hence
we propose smooth adversarial training (SAT), in which we replace ReLU with its
smooth approximations to strengthen adversarial training. The purpose of smooth
activation functions in SAT is to allow it to find harder adversarial examples
and compute better gradient updates during adversarial training.
  Compared to standard adversarial training, SAT improves adversarial
robustness for ""free"", i.e., no drop in accuracy and no increase in
computational cost. For example, without introducing additional computations,
SAT significantly enhances ResNet-50's robustness from 33.0% to 42.3%, while
also improving accuracy by 0.9% on ImageNet. SAT also works well with larger
networks: it helps EfficientNet-L1 to achieve 82.2% accuracy and 58.6%
robustness on ImageNet, outperforming the previous state-of-the-art defense by
9.5% for accuracy and 11.6% for robustness. Models are available at
https://github.com/cihangxie/SmoothAdversarialTraining.
",commonly believe network accurate robust gain robustness mean lose accuracy also generally believe unless make network large network architectural element would otherwise matter little improve adversarial robustness present evidence challenge common belief careful study adversarial training key observation widely use relu activation function significantly weaken adversarial training due non smooth nature hence propose smooth adversarial training sit replace relu smooth approximation strengthen adversarial training purpose smooth activation function sit allow find hard adversarial example compute well gradient update adversarial training compare standard adversarial training sit improve adversarial robustness free drop accuracy increase computational cost example without introduce additional computation sit significantly enhance resnet-50 robustness also improve accuracy imagenet sit also work well large network help efficientnet l1 achieve accuracy robustness imagenet outperform previous state of the art defense accuracy robustness model available https
"Explicit construction of the minimum error variance estimator for
  stochastic LTI state-space systems","  In this short article, we showcase the derivation of the optimal (minimum
error variance) estimator, when one part of the stochastic LTI system output is
not measured but is able to be predicted from the measured system outputs.
Similar derivations have been done before but not using state-space
representation.
",short article showcase derivation optimal minimum error variance estimator one part stochastic lti system output measure able predict measure system output similar derivation do use state space representation
Deep Co-Attention Network for Multi-View Subspace Learning,"  Many real-world applications involve data from multiple modalities and thus
exhibit the view heterogeneity. For example, user modeling on social media
might leverage both the topology of the underlying social network and the
content of the users' posts; in the medical domain, multiple views could be
X-ray images taken at different poses. To date, various techniques have been
proposed to achieve promising results, such as canonical correlation analysis
based methods, etc. In the meanwhile, it is critical for decision-makers to be
able to understand the prediction results from these methods. For example,
given the diagnostic result that a model provided based on the X-ray images of
a patient at different poses, the doctor needs to know why the model made such
a prediction. However, state-of-the-art techniques usually suffer from the
inability to utilize the complementary information of each view and to explain
the predictions in an interpretable manner.
  To address these issues, in this paper, we propose a deep co-attention
network for multi-view subspace learning, which aims to extract both the common
information and the complementary information in an adversarial setting and
provide robust interpretations behind the prediction to the end-users via the
co-attention mechanism. In particular, it uses a novel cross reconstruction
loss and leverages the label information to guide the construction of the
latent representation by incorporating the classifier into our model. This
improves the quality of latent representation and accelerates the convergence
speed. Finally, we develop an efficient iterative algorithm to find the optimal
encoders and discriminator, which are evaluated extensively on synthetic and
real-world data sets. We also conduct a case study to demonstrate how the
proposed method robustly interprets the predictions on an image data set.
",many real world application involve datum multiple modality thus exhibit view heterogeneity example user model social medium might leverage topology underlie social network content user post medical domain multiple view could x ray image take different pose date various technique propose achieve promising result canonical correlation analysis base method etc meanwhile critical decision maker able understand prediction result method example give diagnostic result model provide base x ray image patient different pose doctor need know model make prediction however state of the art technique usually suffer inability utilize complementary information view explain prediction interpretable manner address issue paper propose deep co attention network multi view subspace learning aim extract common information complementary information adversarial setting provide robust interpretation behind prediction end user via co attention mechanism particular use novel cross reconstruction loss leverage label information guide construction latent representation incorporate classifier model improve quality latent representation accelerate convergence speed finally develop efficient iterative algorithm find optimal encoder discriminator evaluate extensively synthetic real world datum set also conduct case study demonstrate propose method robustly interpret prediction image datum set
Pain Assessment based on fNIRS using Bidirectional LSTMs,"  Assessing pain in patients unable to speak (also called non-verbal patients)
is extremely complicated and often is done by clinical judgement. However, this
method is not reliable since patients vital signs can fluctuate significantly
due to other underlying medical conditions. No objective diagnosis test exists
to date that can assist medical practitioners in the diagnosis of pain. In this
study we propose the use of functional near-infrared spectroscopy (fNIRS) and
deep learning for the assessment of human pain. The aim of this study is to
explore the use deep learning to automatically learn features from fNIRS raw
data to reduce the level of subjectivity and domain knowledge required in the
design of hand-crafted features. Four deep learning models were evaluated,
multilayer perceptron (MLP), forward and backward long short-term memory
net-works (LSTM), and bidirectional LSTM. The results showed that the Bi-LSTM
model achieved the highest accuracy (90.6%)and faster than the other three
models. These results advance knowledge in pain assessment using neuroimaging
as a method of diagnosis and represent a step closer to developing a
physiologically based diagnosis of human pain that will benefit vulnerable
populations who cannot self-report pain.
",assess pain patient unable speak also call non verbal patient extremely complicated often do clinical judgement however method reliable since patient vital sign fluctuate significantly due underlie medical condition objective diagnosis test exist date assist medical practitioner diagnosis pain study propose use functional near infrared spectroscopy fnir deep learning assessment human pain aim study explore use deep learning automatically learn feature fnirs raw datum reduce level subjectivity domain knowledge require design hand craft feature four deep learning model evaluate multilayer perceptron mlp forward backward long short term memory net work lstm bidirectional lstm result show bi lstm model achieve high accuracy fast three model result advance knowledge pain assessment use neuroimage method diagnosis represent step close develop physiologically base diagnosis human pain benefit vulnerable population self report pain
(De)Randomized Smoothing for Certifiable Defense against Patch Attacks,"  Patch adversarial attacks on images, in which the attacker can distort pixels
within a region of bounded size, are an important threat model since they
provide a quantitative model for physical adversarial attacks. In this paper,
we introduce a certifiable defense against patch attacks that guarantees for a
given image and patch attack size, no patch adversarial examples exist. Our
method is related to the broad class of randomized smoothing robustness schemes
which provide high-confidence probabilistic robustness certificates. By
exploiting the fact that patch attacks are more constrained than general sparse
attacks, we derive meaningfully large robustness certificates against them.
Additionally, in contrast to smoothing-based defenses against L_p and sparse
attacks, our defense method against patch attacks is de-randomized, yielding
improved, deterministic certificates. Compared to the existing patch
certification method proposed by Chiang et al. (2020), which relies on interval
bound propagation, our method can be trained significantly faster, achieves
high clean and certified robust accuracy on CIFAR-10, and provides certificates
at ImageNet scale. For example, for a 5-by-5 patch attack on CIFAR-10, our
method achieves up to around 57.6% certified accuracy (with a classifier with
around 83.8% clean accuracy), compared to at most 30.3% certified accuracy for
the existing method (with a classifier with around 47.8% clean accuracy). Our
results effectively establish a new state-of-the-art of certifiable defense
against patch attacks on CIFAR-10 and ImageNet. Code is available at
https://github.com/alevine0/patchSmoothing.
",patch adversarial attack image attacker distort pixel within region bound size important threat model since provide quantitative model physical adversarial attack paper introduce certifiable defense patch attack guarantee give image patch attack size patch adversarial example exist method relate broad class randomize smooth robustness scheme provide high confidence probabilistic robustness certificate exploit fact patch attack constrain general sparse attack derive meaningfully large robustness certificate additionally contrast smoothing base defense l_p sparse attack defense method patch attack de randomized yielding improve deterministic certificate compare exist patch certification method propose chiang et al 2020 rely interval bind propagation method train significantly fast achieve high clean certify robust accuracy cifar-10 provide certificate imagenet scale example 5 by-5 patch attack cifar-10 method achieve around certify accuracy classifier around clean accuracy compare certify accuracy exist method classifier around clean accuracy result effectively establish new state of the art certifiable defense patch attack cifar-10 imagenet code available https
MOGPTK: The Multi-Output Gaussian Process Toolkit,"  We present MOGPTK, a Python package for multi-channel data modelling using
Gaussian processes (GP). The aim of this toolkit is to make multi-output GP
(MOGP) models accessible to researchers, data scientists, and practitioners
alike. MOGPTK uses a Python front-end, relies on the GPflow suite and is built
on a TensorFlow back-end, thus enabling GPU-accelerated training. The toolkit
facilitates implementing the entire pipeline of GP modelling, including data
loading, parameter initialization, model learning, parameter interpretation, up
to data imputation and extrapolation. MOGPTK implements the main multi-output
covariance kernels from literature, as well as spectral-based parameter
initialization strategies. The source code, tutorials and examples in the form
of Jupyter notebooks, together with the API documentation, can be found at
http://github.com/GAMES-UChile/mogptk
",present mogptk python package multi channel datum modelling use gaussian process gp aim toolkit make multi output gp mogp model accessible researcher datum scientist practitioner alike mogptk use python front end rely gpflow suite build tensorflow back end thus enable gpu accelerate training toolkit facilitate implement entire pipeline gp modelling include datum loading parameter initialization model learn parameter interpretation datum imputation extrapolation mogptk implement main multi output covariance kernels literature well spectral base parameter initialization strategy source code tutorial examples form jupyter notebook together api documentation find http
Barcodes as summary of objective function's topology,"  We apply the canonical forms (barcodes) of gradient Morse complexes to
explore topology of loss surfaces. We present a novel algorithm for
calculations of the objective function's barcodes of local minima. We have
conducted experiments for calculating barcodes of local minima for benchmark
functions and for loss surfaces of neural networks. Our experiments confirm two
principal observations for loss surfaces of neural networks. First, the
barcodes of local minima are located in a small lower part of the range of
values of loss function of neural networks. Second, increase of the neural
network's depth brings down the barcodes of local minima. This has natural
implications for the neural network learning and the generalization ability.
",apply canonical form barcode gradient morse complex explore topology loss surface present novel algorithm calculation objective function barcode local minima conduct experiment calculate barcode local minima benchmark function loss surface neural network experiment confirm two principal observation loss surface neural network first barcode local minima locate small low part range value loss function neural network second increase neural network depth bring barcode local minima natural implication neural network learn generalization ability
"Addressing Cold Start in Recommender Systems with Hierarchical Graph
  Neural Networks","  Recommender systems have become an essential instrument in a wide range of
industries to personalize the user experience. A significant issue that has
captured both researchers' and industry experts' attention is the cold start
problem for new items. In this work, we present a graph neural network
recommender system using item hierarchy graphs and a bespoke architecture to
handle the cold start case for items. The experimental study on multiple
datasets and millions of users and interactions indicates that our method
achieves better forecasting quality than the state-of-the-art with a comparable
computational time.
",recommender system become essential instrument wide range industry personalize user experience significant issue capture researcher industry expert attention cold start problem new item work present graph neural network recommender system use item hierarchy graph bespoke architecture handle cold start case item experimental study multiple dataset million user interaction indicate method achieve well forecasting quality state of the art comparable computational time
"Policy Teaching in Reinforcement Learning via Environment Poisoning
  Attacks","  We study a security threat to reinforcement learning where an attacker
poisons the learning environment to force the agent into executing a target
policy chosen by the attacker. As a victim, we consider RL agents whose
objective is to find a policy that maximizes reward in infinite-horizon problem
settings. The attacker can manipulate the rewards and the transition dynamics
in the learning environment at training-time, and is interested in doing so in
a stealthy manner. We propose an optimization framework for finding an optimal
stealthy attack for different measures of attack cost. We provide lower/upper
bounds on the attack cost, and instantiate our attacks in two settings: (i) an
offline setting where the agent is doing planning in the poisoned environment,
and (ii) an online setting where the agent is learning a policy with poisoned
feedback. Our results show that the attacker can easily succeed in teaching any
target policy to the victim under mild conditions and highlight a significant
security threat to reinforcement learning agents in practice.
",study security threat reinforcement learn attacker poison learn environment force agent execute target policy choose attacker victim consider rl agent whose objective find policy maximize reward infinite horizon problem setting attacker manipulate reward transition dynamic learn environment training time interested stealthy manner propose optimization framework find optimal stealthy attack different measure attack cost provide bound attack cost instantiate attack two setting offline set agent planning poison environment ii online set agent learn policy poison feedback result show attacker easily succeed teach target policy victim mild condition highlight significant security threat reinforcement learn agent practice
Neural Network Models for Stock Selection Based on Fundamental Analysis,"  Application of neural network architectures for financial prediction has been
actively studied in recent years. This paper presents a comparative study that
investigates and compares feed-forward neural network (FNN) and adaptive neural
fuzzy inference system (ANFIS) on stock prediction using fundamental financial
ratios. The study is designed to evaluate the performance of each architecture
based on the relative return of the selected portfolios with respect to the
benchmark stock index. The results show that both architectures possess the
ability to separate winners and losers from a sample universe of stocks, and
the selected portfolios outperform the benchmark. Our study argues that FNN
shows superior performance over ANFIS.
",application neural network architecture financial prediction actively study recent year paper present comparative study investigate compare feed forward neural network fnn adaptive neural fuzzy inference system anfis stock prediction use fundamental financial ratio study design evaluate performance architecture base relative return select portfolio respect benchmark stock index result show architecture possess ability separate winner loser sample universe stock select portfolio outperform benchmark study argue fnn show superior performance anfis
Deep Image Orientation Angle Detection,"  Estimating and rectifying the orientation angle of any image is a pretty
challenging task. Initial work used the hand engineering features for this
purpose, where after the invention of deep learning using convolution-based
neural network showed significant improvement in this problem. However, this
paper shows that the combination of CNN and a custom loss function specially
designed for angles lead to a state-of-the-art results. This includes the
estimation of the orientation angle of any image or document at any degree (0
to 360 degree),
",estimate rectifying orientation angle image pretty challenging task initial work use hand engineering feature purpose invention deep learning use convolution base neural network show significant improvement problem however paper show combination cnn custom loss function specially design angle lead state of the art result include estimation orientation angle image document degree 0 360 degree
"Learning Hybrid Representations for Automatic 3D Vessel Centerline
  Extraction","  Automatic blood vessel extraction from 3D medical images is crucial for
vascular disease diagnoses. Existing methods based on convolutional neural
networks (CNNs) may suffer from discontinuities of extracted vessels when
segmenting such thin tubular structures from 3D images. We argue that
preserving the continuity of extracted vessels requires to take into account
the global geometry. However, 3D convolutions are computationally inefficient,
which prohibits the 3D CNNs from sufficiently large receptive fields to capture
the global cues in the entire image. In this work, we propose a hybrid
representation learning approach to address this challenge. The main idea is to
use CNNs to learn local appearances of vessels in image crops while using
another point-cloud network to learn the global geometry of vessels in the
entire image. In inference, the proposed approach extracts local segments of
vessels using CNNs, classifies each segment based on global geometry using the
point-cloud network, and finally connects all the segments that belong to the
same vessel using the shortest-path algorithm. This combination results in an
efficient, fully-automatic and template-free approach to centerline extraction
from 3D images. We validate the proposed approach on CTA datasets and
demonstrate its superior performance compared to both traditional and CNN-based
baselines.
",automatic blood vessel extraction 3d medical image crucial vascular disease diagnose exist method base convolutional neural network cnn may suffer discontinuity extract vessel segment thin tubular structure 3d image argue preserve continuity extract vessel require take account global geometry however 3d convolution computationally inefficient prohibit 3d cnn sufficiently large receptive field capture global cue entire image work propose hybrid representation learn approach address challenge main idea use cnn learn local appearance vessel image crop use another point cloud network learn global geometry vessel entire image inference propose approach extract local segment vessel use cnns classifie segment base global geometry use point cloud network finally connect segment belong vessel use short path algorithm combination result efficient fully automatic template free approach centerline extraction 3d image validate propose approach cta dataset demonstrate superior performance compare traditional cnn base baseline
"How Auto-Encoders Could Provide Credit Assignment in Deep Networks via
  Target Propagation","  We propose to exploit {\em reconstruction} as a layer-local training signal
for deep learning. Reconstructions can be propagated in a form of target
propagation playing a role similar to back-propagation but helping to reduce
the reliance on derivatives in order to perform credit assignment across many
levels of possibly strong non-linearities (which is difficult for
back-propagation). A regularized auto-encoder tends produce a reconstruction
that is a more likely version of its input, i.e., a small move in the direction
of higher likelihood. By generalizing gradients, target propagation may also
allow to train deep networks with discrete hidden units. If the auto-encoder
takes both a representation of input and target (or of any side information) in
input, then its reconstruction of input representation provides a target
towards a representation that is more likely, conditioned on all the side
information. A deep auto-encoder decoding path generalizes gradient propagation
in a learned way that can could thus handle not just infinitesimal changes but
larger, discrete changes, hopefully allowing credit assignment through a long
chain of non-linear operations. In addition to each layer being a good
auto-encoder, the encoder also learns to please the upper layers by
transforming the data into a space where it is easier to model by them,
flattening manifolds and disentangling factors. The motivations and theoretical
justifications for this approach are laid down in this paper, along with
conjectures that will have to be verified either mathematically or
experimentally, including a hypothesis stating that such auto-encoder mediated
target propagation could play in brains the role of credit assignment through
many non-linear, noisy and discrete transformations.
",propose exploit reconstruction layer local training signal deep learning reconstruction propagate form target propagation playing role similar back propagation help reduce reliance derivative order perform credit assignment across many level possibly strong non linearity difficult back propagation regularized auto encoder tends produce reconstruction likely version input small move direction high likelihood generalize gradient target propagation may also allow train deep network discrete hide unit auto encoder take representation input target side information input reconstruction input representation provide target towards representation likely condition side information deep auto encoder decode path generalize gradient propagation learn way could thus handle infinitesimal change large discrete change hopefully allow credit assignment long chain non linear operation addition layer good auto encoder encoder also learn please upper layer transform data space easy model flattening manifold disentangle factor motivation theoretical justification approach lay paper along conjecture verify either mathematically experimentally include hypothesis state auto encoder mediate target propagation could play brain role credit assignment many non linear noisy discrete transformation
"Learning a faceted customer segmentation for discovering new business
  opportunities at Intel","  For sales and marketing organizations within large enterprises, identifying
and understanding new markets, customers and partners is a key challenge.
Intel's Sales and Marketing Group (SMG) faces similar challenges while growing
in new markets and domains and evolving its existing business. In today's
complex technological and commercial landscape, there is need for intelligent
automation supporting a fine-grained understanding of businesses in order to
help SMG sift through millions of companies across many geographies and
languages and identify relevant directions. We present a system developed in
our company that mines millions of public business web pages, and extracts a
faceted customer representation. We focus on two key customer aspects that are
essential for finding relevant opportunities: industry segments (ranging from
broad verticals such as healthcare, to more specific fields such as 'video
analytics') and functional roles (e.g., 'manufacturer' or 'retail'). To address
the challenge of labeled data collection, we enrich our data with external
information gleaned from Wikipedia, and develop a semi-supervised multi-label,
multi-lingual deep learning model that parses customer website texts and
classifies them into their respective facets. Our system scans and indexes
companies as part of a large-scale knowledge graph that currently holds tens of
millions of connected entities with thousands being fetched, enriched and
connected to the graph by the hour in real time, and also supports knowledge
and insight discovery. In experiments conducted in our company, we are able to
significantly boost the performance of sales personnel in the task of
discovering new customers and commercial partnership opportunities.
",sale marketing organization within large enterprise identify understand new market customer partner key challenge intel sale marketing group smg face similar challenge grow new market domain evolve exist business today complex technological commercial landscape need intelligent automation support fine grain understanding business order help smg sift million company across many geography language identify relevant direction present system develop company mine millions public business web page extract faceted customer representation focus two key customer aspect essential find relevant opportunity industry segment range broad vertical healthcare specific field analytic functional role address challenge label datum collection enrich datum external information glean wikipedia develop semi supervised multi label multi lingual deep learning model parse customer website text classify respective facet system scan index company part large scale knowledge graph currently hold ten million connect entity thousand fetch enrich connect graph hour real time also support knowledge insight discovery experiment conduct company able significantly boost performance sale personnel task discover new customer commercial partnership opportunity
On Symbolically Encoding the Behavior of Random Forests,"  Recent work has shown that the input-output behavior of some machine learning
systems can be captured symbolically using Boolean expressions or tractable
Boolean circuits, which facilitates reasoning about the behavior of these
systems. While most of the focus has been on systems with Boolean inputs and
outputs, we address systems with discrete inputs and outputs, including ones
with discretized continuous variables as in systems based on decision trees. We
also focus on the suitability of encodings for computing prime implicants,
which have recently played a central role in explaining the decisions of
machine learning systems. We show some key distinctions with encodings for
satisfiability, and propose an encoding that is sound and complete for the
given task.
",recent work show input output behavior machine learning system capture symbolically use boolean expression tractable boolean circuit facilitate reason behavior system focus system boolean input output address system discrete input output include one discretize continuous variable system base decision tree also focus suitability encoding compute prime implicant recently play central role explain decision machine learn system show key distinction encoding satisfiability propose encode sound complete give task
Discovery of non-gaussian linear causal models using ICA,"  In recent years, several methods have been proposed for the discovery of
causal structure from non-experimental data (Spirtes et al. 2000; Pearl 2000).
Such methods make various assumptions on the data generating process to
facilitate its identification from purely observational data. Continuing this
line of research, we show how to discover the complete causal structure of
continuous-valued data, under the assumptions that (a) the data generating
process is linear, (b) there are no unobserved confounders, and (c) disturbance
variables have non-gaussian distributions of non-zero variances. The solution
relies on the use of the statistical method known as independent component
analysis (ICA), and does not require any pre-specified time-ordering of the
variables. We provide a complete Matlab package for performing this LiNGAM
analysis (short for Linear Non-Gaussian Acyclic Model), and demonstrate the
effectiveness of the method using artificially generated data.
",recent year several method propose discovery causal structure non experimental data spirte et al 2000 pearl 2000 method make various assumption datum generating process facilitate identification purely observational datum continue line research show discover complete causal structure continuous value data assumption datum generating process linear b unobserved confounder c disturbance variable non gaussian distribution non zero variance solution relie use statistical method know independent component analysis ica require pre specified time order variable provide complete matlab package perform lingam analysis short linear non gaussian acyclic model demonstrate effectiveness method use artificially generate datum
Bi-GCN: Binary Graph Convolutional Network,"  Graph Neural Networks (GNNs) have achieved tremendous success in graph
representation learning. Unfortunately, current GNNs usually rely on loading
the entire attributed graph into network for processing. This implicit
assumption may not be satisfied with limited memory resources, especially when
the attributed graph is large. In this paper, we pioneer to propose a Binary
Graph Convolutional Network (Bi-GCN), which binarizes both the network
parameters and input node features. Besides, the original matrix
multiplications are revised to binary operations for accelerations. According
to the theoretical analysis, our Bi-GCN can reduce the memory consumption by an
average of ~30x for both the network parameters and input data, and accelerate
the inference speed by an average of ~47x, on the citation networks. Meanwhile,
we also design a new gradient approximation based back-propagation method to
train our Bi-GCN well. Extensive experiments have demonstrated that our Bi-GCN
can give a comparable performance compared to the full-precision baselines.
Besides, our binarization approach can be easily applied to other GNNs, which
has been verified in the experiments.
",graph neural network gnns achieve tremendous success graph representation learn unfortunately current gnn usually rely load entire attribute graph network processing implicit assumption may satisfied limited memory resource especially attribute graph large paper pioneer propose binary graph convolutional network bi gcn binarize network parameter input node feature besides original matrix multiplication revise binary operation acceleration accord theoretical analysis bi gcn reduce memory consumption average network parameter input datum accelerate inference speed average citation network meanwhile also design new gradient approximation base back propagation method train bi gcn well extensive experiment demonstrate bi gcn give comparable performance compare full precision baseline besides binarization approach easily apply gnn verify experiment
"Topic Discovery via Latent Space Clustering of Pretrained Language Model
  Representations","  Topic models have been the prominent tools for automatic topic discovery from
text corpora. Despite their effectiveness, topic models suffer from several
limitations including the inability of modeling word ordering information in
documents, the difficulty of incorporating external linguistic knowledge, and
the lack of both accurate and efficient inference methods for approximating the
intractable posterior. Recently, pretrained language models (PLMs) have brought
astonishing performance improvements to a wide variety of tasks due to their
superior representations of text. Interestingly, there have not been standard
approaches to deploy PLMs for topic discovery as better alternatives to topic
models. In this paper, we begin by analyzing the challenges of using PLM
representations for topic discovery, and then propose a joint latent space
learning and clustering framework built upon PLM embeddings. In the latent
space, topic-word and document-topic distributions are jointly modeled so that
the discovered topics can be interpreted by coherent and distinctive terms and
meanwhile serve as meaningful summaries of the documents. Our model effectively
leverages the strong representation power and superb linguistic features
brought by PLMs for topic discovery, and is conceptually simpler than topic
models. On two benchmark datasets in different domains, our model generates
significantly more coherent and diverse topics than strong topic models, and
offers better topic-wise document representations, based on both automatic and
human evaluations.
",topic model prominent tool automatic topic discovery text corpora despite effectiveness topic model suffer several limitation include inability modeling word order information document difficulty incorporate external linguistic knowledge lack accurate efficient inference method approximate intractable posterior recently pretraine language model plm bring astonishing performance improvement wide variety task due superior representation text interestingly standard approach deploy plm topic discovery well alternative topic model paper begin analyze challenge use plm representation topic discovery propose joint latent space learn cluster framework build upon plm embedding latent space topic word document topic distribution jointly model discover topic interpret coherent distinctive term meanwhile serve meaningful summary document model effectively leverage strong representation power superb linguistic feature bring plm topic discovery conceptually simple topic model two benchmark dataset different domain model generate significantly coherent diverse topic strong topic model offer well topic wise document representation base automatic human evaluation
Time Series Forecasting Using Fuzzy Cognitive Maps: A Survey,"  Among various soft computing approaches for time series forecasting, Fuzzy
Cognitive Maps (FCM) have shown remarkable results as a tool to model and
analyze the dynamics of complex systems. FCM have similarities to recurrent
neural networks and can be classified as a neuro-fuzzy method. In other words,
FCMs are a mixture of fuzzy logic, neural network, and expert system aspects,
which act as a powerful tool for simulating and studying the dynamic behavior
of complex systems. The most interesting features are knowledge
interpretability, dynamic characteristics and learning capability. The goal of
this survey paper is mainly to present an overview on the most relevant and
recent FCM-based time series forecasting models proposed in the literature. In
addition, this article considers an introduction on the fundamentals of FCM
model and learning methodologies. Also, this survey provides some ideas for
future research to enhance the capabilities of FCM in order to cover some
challenges in the real-world experiments such as handling non-stationary data
and scalability issues. Moreover, equipping FCMs with fast learning algorithms
is one of the major concerns in this area.
",among various soft computing approach time series forecast fuzzy cognitive map fcm show remarkable result tool model analyze dynamic complex system fcm similarity recurrent neural network classify neuro fuzzy method word fcms mixture fuzzy logic neural network expert system aspect act powerful tool simulate study dynamic behavior complex system interesting feature knowledge interpretability dynamic characteristic learn capability goal survey paper mainly present overview relevant recent fcm base time series forecasting model propose literature addition article consider introduction fundamental fcm model learn methodology also survey provide idea future research enhance capability fcm order cover challenge real world experiment handle non stationary datum scalability issue moreover equip fcms fast learning algorithm one major concern area
"Robust Online Learning for Resource Allocation -- Beyond Euclidean
  Projection and Dynamic Fit","  Online-learning literature has focused on designing algorithms that ensure
sub-linear growth of the cumulative long-term constraint violations. The
drawback of this guarantee is that strictly feasible actions may cancel out
constraint violations on other time slots. For this reason, we introduce a new
performance measure called $\hCFit$, whose particular instance is the
cumulative positive part of the constraint violations. We propose a class of
non-causal algorithms for online-decision making, which guarantees, in slowly
changing environments, sub-linear growth of this quantity despite noisy
first-order feedback. Furthermore, we demonstrate by numerical experiments the
performance gain of our method relative to the state of art.
",online learn literature focus designing algorithm ensure sub linear growth cumulative long term constraint violation drawback guarantee strictly feasible action may cancel constraint violation time slot reason introduce new performance measure call whose particular instance cumulative positive part constraint violation propose class non causal algorithm online decision making guarantee slowly change environment sub linear growth quantity despite noisy first order feedback furthermore demonstrate numerical experiment performance gain method relative state art
"Nonconvex Approach for Sparse and Low-Rank Constrained Models with Dual
  Momentum","  In this manuscript, we research on the behaviors of surrogates for the rank
function on different image processing problems and their optimization
algorithms. We first propose a novel nonconvex rank surrogate on the general
rank minimization problem and apply this to the corrupted image completion
problem. Then, we propose that nonconvex rank surrogates can be introduced into
two well-known sparse and low-rank models: Robust Principal Component Analysis
(RPCA) and Low-Rank Representation (LRR). For optimization, we use alternating
direction method of multipliers (ADMM) and propose a trick, which is called the
dual momentum. We add the difference of the dual variable between the current
and the last iteration with a weight. This trick can avoid the local minimum
problem and make the algorithm converge to a solution with smaller recovery
error in the nonconvex optimization problem. Also, it can boost the convergence
when the variable updates too slowly. We also give a severe proof and verify
that the proposed algorithms are convergent. Then, several experiments are
conducted, including image completion, denoising, and spectral clustering with
outlier detection. These experiments show that the proposed methods are
effective in image and signal processing applications, and have the best
performance compared with state-of-the-art methods.
",manuscript research behavior surrogate rank function different image processing problem optimization algorithm first propose novel nonconvex rank surrogate general rank minimization problem apply corrupt image completion problem propose nonconvex rank surrogate introduce two well know sparse low rank model robust principal component analysis rpca low rank representation lrr optimization use alternate direction method multiplier admm propose trick call dual momentum add difference dual variable current last iteration weight trick avoid local minimum problem make algorithm converge solution small recovery error nonconvex optimization problem also boost convergence variable update slowly also give severe proof verify proposed algorithm convergent several experiment conduct include image completion denoise spectral cluster outli detection experiment show propose method effective image signal processing application good performance compare state of the art method
"Beyond accuracy: quantifying trial-by-trial behaviour of CNNs and humans
  by measuring error consistency","  A central problem in cognitive science and behavioural neuroscience as well
as in machine learning and artificial intelligence research is to ascertain
whether two or more decision makers (be they brains or algorithms) use the same
strategy. Accuracy alone cannot distinguish between strategies: two systems may
achieve similar accuracy with very different strategies. The need to
differentiate beyond accuracy is particularly pressing if two systems are near
ceiling performance, like Convolutional Neural Networks (CNNs) and humans on
visual object recognition. Here we introduce trial-by-trial error consistency,
a quantitative analysis for measuring whether two decision making systems
systematically make errors on the same inputs. Making consistent errors on a
trial-by-trial basis is a necessary condition for similar processing strategies
between decision makers. Our analysis is applicable to compare algorithms with
algorithms, humans with humans, and algorithms with humans. When applying error
consistency to object recognition we obtain three main findings: (1.)
Irrespective of architecture, CNNs are remarkably consistent with one another.
(2.) The consistency between CNNs and human observers, however, is little above
what can be expected by chance alone -- indicating that humans and CNNs are
likely implementing very different strategies. (3.) CORnet-S, a recurrent model
termed the ""current best model of the primate ventral visual stream"", fails to
capture essential characteristics of human behavioural data and behaves
essentially like a standard purely feedforward ResNet-50 in our analysis. Taken
together, error consistency analysis suggests that the strategies used by human
and machine vision are still very different -- but we envision our
general-purpose error consistency analysis to serve as a fruitful tool for
quantifying future progress.
",central problem cognitive science behavioural neuroscience well machine learn artificial intelligence research ascertain whether two decision maker brain algorithm use strategy accuracy alone distinguish strategy two system may achieve similar accuracy different strategy need differentiate beyond accuracy particularly press two system near ceiling performance like convolutional neural network cnn human visual object recognition introduce trial by trial error consistency quantitative analysis measure whether two decision make system systematically make error input make consistent error trial by trial basis necessary condition similar processing strategy decision maker analysis applicable compare algorithm algorithm human human algorithm human apply error consistency object recognition obtain three main finding 1 irrespective architecture cnn remarkably consistent one another 2 consistency cnn human observer however little expect chance alone indicate human cnn likely implement different strategy 3 cornet s recurrent model term current good model primate ventral visual stream fail capture essential characteristic human behavioural datum behave essentially like standard purely feedforward resnet-50 analysis take together error consistency analysis suggest strategy use human machine vision still different envision general purpose error consistency analysis serve fruitful tool quantify future progress
A Review on Methods and Applications in Multimodal Deep Learning,"  Deep Learning has implemented a wide range of applications and has become
increasingly popular in recent years. The goal of multimodal deep learning
(MMDL) is to create models that can process and link information using various
modalities. Despite the extensive development made for unimodal learning, it
still cannot cover all the aspects of human learning. Multimodal learning helps
to understand and analyze better when various senses are engaged in the
processing of information. This paper focuses on multiple types of modalities,
i.e., image, video, text, audio, body gestures, facial expressions, and
physiological signals. Detailed analysis of the baseline approaches and an
in-depth study of recent advancements during the last five years (2017 to 2021)
in multimodal deep learning applications has been provided. A fine-grained
taxonomy of various multimodal deep learning methods is proposed, elaborating
on different applications in more depth. Lastly, main issues are highlighted
separately for each domain, along with their possible future research
directions.
",deep learning implement wide range application become increasingly popular recent year goal multimodal deep learning mmdl create model process link information use various modality despite extensive development make unimodal learning still cover aspect human learn multimodal learning help understand analyze well various sense engage processing information paper focus multiple type modalitie image video text audio body gesture facial expression physiological signal detail analysis baseline approach in depth study recent advancement last five year 2017 2021 multimodal deep learning application provide fine grain taxonomy various multimodal deep learning method propose elaborate different application depth lastly main issue highlight separately domain along possible future research direction
"Hyperspectral Unmixing Network Inspired by Unfolding an Optimization
  Problem","  The hyperspectral image (HSI) unmixing task is essentially an inverse
problem, which is commonly solved by optimization algorithms under a predefined
(non-)linear mixture model. Although these optimization algorithms show
impressive performance, they are very computational demanding as they often
rely on an iterative updating scheme. Recently, the rise of neural networks has
inspired lots of learning based algorithms in unmixing literature. However,
most of them lack of interpretability and require a large training dataset. One
natural question then arises: can one leverage the model based algorithm and
learning based algorithm to achieve interpretable and fast algorithm for HSI
unmixing problem? In this paper, we propose two novel network architectures,
named U-ADMM-AENet and U-ADMM-BUNet, for abundance estimation and blind
unmixing respectively, by combining the conventional optimization-model based
unmixing method and the rising learning based unmixing method. We first
consider a linear mixture model with sparsity constraint, then we unfold
Alternating Direction Method of Multipliers (ADMM) algorithm to construct the
unmixing network structures. We also show that the unfolded structures can find
corresponding interpretations in machine learning literature, which further
demonstrates the effectiveness of proposed methods. Benefit from the
interpretation, the proposed networks can be initialized by incorporating prior
information about the HSI data. Different from traditional unfolding networks,
we propose a new training strategy for proposed networks to better fit in the
HSI applications. Extensive experiments show that the proposed methods can
achieve much faster convergence and competitive performance even with very
small size of training data, when compared with state-of-art algorithms.
",hyperspectral image hsi unmixing task essentially inverse problem commonly solve optimization algorithm predefine non- linear mixture model although optimization algorithm show impressive performance computational demanding often rely iterative update scheme recently rise neural network inspire lot learn base algorithm unmixe literature however lack interpretability require large training dataset one natural question arise one leverage model base algorithm learning base algorithm achieve interpretable fast algorithm hsi unmixing problem paper propose two novel network architecture name u admm aenet u admm bunet abundance estimation blind unmixe respectively combine conventional optimization model base unmixing method rise learning base unmixing method first consider linear mixture model sparsity constraint unfold alternate direction method multiplier admm algorithm construct unmixe network structure also show unfold structure find corresponding interpretation machine learn literature demonstrates effectiveness propose method benefit interpretation propose network initialize incorporate prior information hsi datum different traditional unfolding network propose new training strategy propose network well fit hsi application extensive experiment show propose method achieve much fast convergence competitive performance even small size training datum compare state of art algorithm
Quantum Energy Regression using Scattering Transforms,"  We present a novel approach to the regression of quantum mechanical energies
based on a scattering transform of an intermediate electron density
representation. A scattering transform is a deep convolution network computed
with a cascade of multiscale wavelet transforms. It possesses appropriate
invariant and stability properties for quantum energy regression. This new
framework removes fundamental limitations of Coulomb matrix based energy
regressions, and numerical experiments give state-of-the-art accuracy over
planar molecules.
",present novel approach regression quantum mechanical energy base scatter transform intermediate electron density representation scatter transform deep convolution network compute cascade multiscale wavelet transform possess appropriate invariant stability property quantum energy regression new framework remove fundamental limitation coulomb matrix base energy regression numerical experiment give state of the art accuracy planar molecule
Variational Lossy Autoencoder,"  Representation learning seeks to expose certain aspects of observed data in a
learned representation that's amenable to downstream tasks like classification.
For instance, a good representation for 2D images might be one that describes
only global structure and discards information about detailed texture. In this
paper, we present a simple but principled method to learn such global
representations by combining Variational Autoencoder (VAE) with neural
autoregressive models such as RNN, MADE and PixelRNN/CNN. Our proposed VAE
model allows us to have control over what the global latent code can learn and
, by designing the architecture accordingly, we can force the global latent
code to discard irrelevant information such as texture in 2D images, and hence
the VAE only ""autoencodes"" data in a lossy fashion. In addition, by leveraging
autoregressive models as both prior distribution $p(z)$ and decoding
distribution $p(x|z)$, we can greatly improve generative modeling performance
of VAEs, achieving new state-of-the-art results on MNIST, OMNIGLOT and
Caltech-101 Silhouettes density estimation tasks.
",representation learning seek expose certain aspect observe datum learn representation amenable downstream task like classification instance good representation 2d image might one describe global structure discard information detail texture paper present simple principled method learn global representation combine variational autoencoder vae neural autoregressive model rnn make propose vae model allow we control global latent code learn design architecture accordingly force global latent code discard irrelevant information texture 2d image hence vae autoencode datum lossy fashion addition leverage autoregressive model prior distribution p z decode distribution p greatly improve generative modeling performance vae achieve new state of the art result mnist omniglot caltech-101 silhouette density estimation task
Certifiable Machine Unlearning for Linear Models,"  Machine unlearning is the task of updating machine learning (ML) models after
a subset of the training data they were trained on is deleted. Methods for the
task are desired to combine effectiveness and efficiency, i.e., they should
effectively ""unlearn"" deleted data, but in a way that does not require
excessive computation effort (e.g., a full retraining) for a small amount of
deletions. Such a combination is typically achieved by tolerating some amount
of approximation in the unlearning. In addition, laws and regulations in the
spirit of ""the right to be forgotten"" have given rise to requirements for
certifiability, i.e., the ability to demonstrate that the deleted data has
indeed been unlearned by the ML model.
  In this paper, we present an experimental study of the three state-of-the-art
approximate unlearning methods for linear models and demonstrate the trade-offs
between efficiency, effectiveness and certifiability offered by each method. In
implementing the study, we extend some of the existing works and describe a
common ML pipeline to compare and evaluate the unlearning methods on six
real-world datasets and a variety of settings. We provide insights into the
effect of the quantity and distribution of the deleted data on ML models and
the performance of each unlearning method in different settings. We also
propose a practical online strategy to determine when the accumulated error
from approximate unlearning is large enough to warrant a full retrain of the ML
model.
",machine unlearn task update machine learning ml model subset training datum train delete method task desire combine effectiveness efficiency effectively unlearn delete datum way require excessive computation effort full retrain small amount deletion combination typically achieve tolerating amount approximation unlearn addition law regulation spirit right forget give rise requirement certifiability ability demonstrate delete datum indeed unlearn ml model paper present experimental study three state of the art approximate unlearning method linear model demonstrate trade off efficiency effectiveness certifiability offer method implement study extend exist work describe common ml pipeline compare evaluate unlearning method six real world dataset variety setting provide insight effect quantity distribution delete datum ml model performance unlearn method different setting also propose practical online strategy determine accumulate error approximate unlearn large enough warrant full retrain ml model
The Space of Transferable Adversarial Examples,"  Adversarial examples are maliciously perturbed inputs designed to mislead
machine learning (ML) models at test-time. They often transfer: the same
adversarial example fools more than one model.
  In this work, we propose novel methods for estimating the previously unknown
dimensionality of the space of adversarial inputs. We find that adversarial
examples span a contiguous subspace of large (~25) dimensionality. Adversarial
subspaces with higher dimensionality are more likely to intersect. We find that
for two different models, a significant fraction of their subspaces is shared,
thus enabling transferability.
  In the first quantitative analysis of the similarity of different models'
decision boundaries, we show that these boundaries are actually close in
arbitrary directions, whether adversarial or benign. We conclude by formally
studying the limits of transferability. We derive (1) sufficient conditions on
the data distribution that imply transferability for simple model classes and
(2) examples of scenarios in which transfer does not occur. These findings
indicate that it may be possible to design defenses against transfer-based
attacks, even for models that are vulnerable to direct attacks.
",adversarial example maliciously perturb input design mislead machine learning ml model test time often transfer adversarial example fool one model work propose novel method estimate previously unknown dimensionality space adversarial input find adversarial example span contiguous subspace large dimensionality adversarial subspace high dimensionality likely intersect find two different model significant fraction subspace share thus enable transferability first quantitative analysis similarity different model decision boundary show boundary actually close arbitrary direction whether adversarial benign conclude formally study limit transferability derive 1 sufficient condition datum distribution imply transferability simple model class 2 example scenario transfer occur finding indicate may possible design defense transfer base attack even model vulnerable direct attack
Estimating Forces of Robotic Pouring Using a LSTM RNN,"  In machine learning, it is very important for a robot to be able to estimate
dynamics from sequences of input data. This problem can be solved using a
recurrent neural network. In this paper, we will discuss the preprocessing of
10 states of the dataset, then the use of a LSTM recurrent neural network to
estimate one output state (dynamics) from the other 9 input states. We will
discuss the architecture of the recurrent neural network, the data collection
and preprocessing, the loss function, the results of the test data, and the
discussion of changes that could improve the network. The results of this paper
will be used for artificial intelligence research and identify the capabilities
of a LSTM recurrent neural network architecture to estimate dynamics of a
system.
",machine learn important robot able estimate dynamic sequence input datum problem solve use recurrent neural network paper discuss preprocesse 10 state dataset use lstm recurrent neural network estimate one output state dynamic 9 input state discuss architecture recurrent neural network datum collection preprocessing loss function result test datum discussion change could improve network result paper use artificial intelligence research identify capability lstm recurrent neural network architecture estimate dynamic system
"Transfer Learning with Deep Convolutional Neural Network (CNN) for
  Pneumonia Detection using Chest X-ray","  Pneumonia is a life-threatening disease, which occurs in the lungs caused by
either bacterial or viral infection. It can be life-endangering if not acted
upon in the right time and thus an early diagnosis of pneumonia is vital. The
aim of this paper is to automatically detect bacterial and viral pneumonia
using digital x-ray images. It provides a detailed report on advances made in
making accurate detection of pneumonia and then presents the methodology
adopted by the authors. Four different pre-trained deep Convolutional Neural
Network (CNN)- AlexNet, ResNet18, DenseNet201, and SqueezeNet were used for
transfer learning. 5247 Bacterial, viral and normal chest x-rays images
underwent preprocessing techniques and the modified images were trained for the
transfer learning based classification task. In this work, the authors have
reported three schemes of classifications: normal vs pneumonia, bacterial vs
viral pneumonia and normal, bacterial and viral pneumonia. The classification
accuracy of normal and pneumonia images, bacterial and viral pneumonia images,
and normal, bacterial and viral pneumonia were 98%, 95%, and 93.3%
respectively. This is the highest accuracy in any scheme than the accuracies
reported in the literature. Therefore, the proposed study can be useful in
faster-diagnosing pneumonia by the radiologist and can help in the fast airport
screening of pneumonia patients.
",pneumonia life threaten disease occur lung cause either bacterial viral infection life endangering act upon right time thus early diagnosis pneumonia vital aim paper automatically detect bacterial viral pneumonia use digital x ray image provide detailed report advance make make accurate detection pneumonia present methodology adopt author four different pre trained deep convolutional neural network cnn alexnet resnet18 densenet201 squeezenet use transfer learn 5247 bacterial viral normal chest x ray image underwent preprocessing technique modify image train transfer learn base classification task work author report three scheme classification normal vs pneumonia bacterial vs viral pneumonia normal bacterial viral pneumonia classification accuracy normal pneumonia image bacterial viral pneumonia image normal bacterial viral pneumonia 98 95 respectively high accuracy scheme accuracy report literature therefore propose study useful fast diagnose pneumonia radiologist help fast airport screen pneumonia patient
"Generalizing electrocardiogram delineation -- Training convolutional
  neural networks with synthetic data augmentation","  Obtaining per-beat information is a key task in the analysis of cardiac
electrocardiograms (ECG), as many downstream diagnosis tasks are dependent on
ECG-based measurements. Those measurements, however, are costly to produce,
especially in recordings that change throughout long periods of time. However,
existing annotated databases for ECG delineation are small, being insufficient
in size and in the array of pathological conditions they represent. This
article delves has two main contributions. First, a pseudo-synthetic data
generation algorithm was developed, based in probabilistically composing ECG
traces given ""pools"" of fundamental segments, as cropped from the original
databases, and a set of rules for their arrangement into coherent synthetic
traces. The generation of conditions is controlled by imposing expert knowledge
on the generated trace, which increases the input variability for training the
model. Second, two novel segmentation-based loss functions have been developed,
which attempt at enforcing the prediction of an exact number of independent
structures and at producing closer segmentation boundaries by focusing on a
reduced number of samples. The best performing model obtained an $F_1$-score of
99.38\% and a delineation error of $2.19 \pm 17.73$ ms and $4.45 \pm 18.32$ ms
for all wave's fiducials (onsets and offsets, respectively), as averaged across
the P, QRS and T waves for three distinct freely available databases. The
excellent results were obtained despite the heterogeneous characteristics of
the tested databases, in terms of lead configurations (Holter, 12-lead),
sampling frequencies ($250$, $500$ and $2,000$ Hz) and represented
pathophysiologies (e.g., different types of arrhythmias, sinus rhythm with
structural heart disease), hinting at its generalization capabilities, while
outperforming current state-of-the-art delineation approaches.
",obtain per beat information key task analysis cardiac electrocardiogram ecg many downstream diagnosis task dependent ecg base measurement measurement however costly produce especially recording change throughout long period time however exist annotated database ecg delineation small insufficient size array pathological condition represent article delve two main contribution first pseudo synthetic datum generation algorithm develop base probabilistically compose ecg trace give pool fundamental segment crop original database set rule arrangement coherent synthetic trace generation condition control impose expert knowledge generate trace increase input variability training model second two novel segmentation base loss function develop attempt enforce prediction exact number independent structure produce close segmentation boundary focus reduce number sample well perform model obtain f_1 -score delineation error ms ms wave fiducial onset offset respectively average across p qrs wave three distinct freely available database excellent result obtain despite heterogeneous characteristic test database term lead configuration holter 12 lead sample frequency 250 500 hz represent pathophysiologie different type arrhythmias sinus rhythm structural heart disease hint generalization capability outperform current state of the art delineation approach
"Predicting Seriousness of Injury in a Traffic Accident: A New Imbalanced
  Dataset and Benchmark","  The paper introduces a new dataset to assess the performance of machine
learning algorithms in the prediction of the seriousness of injury in a traffic
accident. The dataset is created by aggregating publicly available datasets
from the UK Department for Transport, which are drastically imbalanced with
missing attributes sometimes approaching 50\% of the overall data
dimensionality. The paper presents the data analysis pipeline starting from the
publicly available data of road traffic accidents and ending with predictors of
possible injuries and their degree of severity. It addresses the huge
incompleteness of public data with a MissForest model. The paper also
introduces two baseline approaches to create injury predictors: a supervised
artificial neural network and a reinforcement learning model. The dataset can
potentially stimulate diverse aspects of machine learning research on
imbalanced datasets and the two approaches can be used as baseline references
when researchers test more advanced learning algorithms in this area.
",paper introduce new dataset assess performance machine learning algorithm prediction seriousness injury traffic accident dataset create aggregate publicly available dataset uk department transport drastically imbalance miss attribute sometimes approach overall datum dimensionality paper present datum analysis pipeline start publicly available datum road traffic accident end predictor possible injury degree severity address huge incompleteness public datum missfor model paper also introduce two baseline approach create injury predictor supervise artificial neural network reinforcement learning model dataset potentially stimulate diverse aspect machine learn research imbalance dataset two approach use baseline reference researcher test advanced learning algorithm area
SmartGD: A Self-Challenging Generative Adversarial Network for Graph Drawing,"A multitude of studies have been conducted on graph drawing, but many
existing methods only focus on optimizing particular aesthetic aspects of graph
layout. Given a graph, generating a good layout that satisfies certain human
aesthetic preference remains a challenging task, especially if such preference
can not be expressed as a differentiable objective function. In this paper, we
propose a student-teacher GAN-based graph drawing framework, SmartGD, which
learns to draw graphs just like how humans learn to perform tasks. The student
network in the SmartGD learns graph drawing by imitating good layout examples,
while the teacher network in SmartGD is responsible for providing ratings
regarding the goodness of the generated layouts. When there is a lack of
concrete aesthetic criteria to specify what constitutes a good layout, the
student network can learn from the good layout examples. On the other hand,
when the goodness of a layout can be assessed by quantitative criteria (even if
not differentiable), the student network can use it as a concrete goal to
optimize the target aesthetics. To accomplish the goal, we propose a novel
variant of GAN, self-challenging GAN, to learn the optimal layout distribution
with respect to any aesthetic criterion, whether the criterion is
differentiable or not. The proposed graph drawing framework can not only draw
graphs in a similar style as the good layout examples but also optimize the
graph layouts according to any given aesthetic criteria when available. Once
the model is trained, it can be used to visualize arbitrary graphs according to
the style of the example layouts or the chosen aesthetic criteria. The
comprehensive experimental studies show that SmartGD outperforms 12 benchmark
methods according to the commonly agreed metrics.",multitude study conduct graph draw many exist method focus optimize particular aesthetic aspect graph layout give graph generate good layout satisfie certain human aesthetic preference remain challenge task especially preference express differentiable objective function paper propose student teacher gan base graph draw framework smartgd learn draw graph like human learn perform task student network smartgd learn graph drawing imitate good layout example teacher network smartgd responsible provide rating regard goodness generate layout lack concrete aesthetic criterion specify constitute good layout student network learn good layout example hand goodness layout assess quantitative criterion even differentiable student network use concrete goal optimize target aesthetic accomplish goal propose novel variant gin self challenge gan learn optimal layout distribution respect aesthetic criterion whether criterion differentiable propose graph drawing framework draw graph similar style good layout example also optimize graph layout accord give aesthetic criterion available model train use visualize arbitrary graph accord style example layout choose aesthetic criterion comprehensive experimental study show smartgd outperform 12 benchmark method accord commonly agree metric
General Identification of Dynamic Treatment Regimes Under Interference,"  In many applied fields, researchers are often interested in tailoring
treatments to unit-level characteristics in order to optimize an outcome of
interest. Methods for identifying and estimating treatment policies are the
subject of the dynamic treatment regime literature. Separately, in many
settings the assumption that data are independent and identically distributed
does not hold due to inter-subject dependence. The phenomenon where a subject's
outcome is dependent on his neighbor's exposure is known as interference. These
areas intersect in myriad real-world settings. In this paper we consider the
problem of identifying optimal treatment policies in the presence of
interference. Using a general representation of interference, via
Lauritzen-Wermuth-Freydenburg chain graphs (Lauritzen and Richardson, 2002), we
formalize a variety of policy interventions under interference and extend
existing identification theory (Tian, 2008; Sherman and Shpitser, 2018).
Finally, we illustrate the efficacy of policy maximization under interference
in a simulation study.
",many applied field researcher often interested tailor treatment unit level characteristic order optimize outcome interest method identify estimate treatment policy subject dynamic treatment regime literature separately many setting assumption datum independent identically distribute hold due inter subject dependence phenomenon subject outcome dependent neighbor exposure know interference area intersect myriad real world setting paper consider problem identify optimal treatment policy presence interference use general representation interference via lauritzen wermuth freydenburg chain graph lauritzen richardson 2002 formalize variety policy intervention interference extend exist identification theory tian 2008 sherman shpitser 2018 finally illustrate efficacy policy maximization interference simulation study
"Towards Prior-Free Approximately Truthful One-Shot Auction Learning via
  Differential Privacy","  Designing truthful, revenue maximizing auctions is a core problem of auction
design. Multi-item settings have long been elusive. Recent work
(arXiv:1706.03459) introduces effective deep learning techniques to find such
auctions for the prior-dependent setting, in which distributions about bidder
preferences are known. One remaining problem is to obtain priors in a way that
excludes the possibility of manipulating the resulting auctions. Using
techniques from differential privacy for the construction of approximately
truthful mechanisms, we modify the RegretNet approach to be applicable to the
prior-free setting. In this more general setting, no distributional information
is assumed, but we trade this property for worse performance. We present
preliminary empirical results and qualitative analysis for this work in
progress.
",design truthful revenue maximize auction core problem auction design multi item setting long elusive recent work introduce effective deep learning technique find auction prior dependent setting distribution bidder preference know one remain problem obtain prior way exclude possibility manipulate result auction use technique differential privacy construction approximately truthful mechanism modify regretnet approach applicable prior free set general set distributional information assume trade property bad performance present preliminary empirical result qualitative analysis work progress
Inference for Heteroskedastic PCA with Missing Data,"  This paper studies how to construct confidence regions for principal
component analysis (PCA) in high dimension, a problem that has been vastly
under-explored. While computing measures of uncertainty for nonlinear/nonconvex
estimators is in general difficult in high dimension, the challenge is further
compounded by the prevalent presence of missing data and heteroskedastic noise.
We propose a suite of solutions to perform valid inference on the principal
subspace based on two estimators: a vanilla SVD-based approach, and a more
refined iterative scheme called $\textsf{HeteroPCA}$ (Zhang et al., 2018). We
develop non-asymptotic distributional guarantees for both estimators, and
demonstrate how these can be invoked to compute both confidence regions for the
principal subspace and entrywise confidence intervals for the spiked covariance
matrix. Particularly worth highlighting is the inference procedure built on top
of $\textsf{HeteroPCA}$, which is not only valid but also statistically
efficient for broader scenarios (e.g., it covers a wider range of missing rates
and signal-to-noise ratios). Our solutions are fully data-driven and adaptive
to heteroskedastic random noise, without requiring prior knowledge about the
noise levels and noise distributions.
",paper study construct confidence region principal component analysis pca high dimension problem vastly under explore computing measure uncertainty estimator general difficult high dimension challenge compound prevalent presence miss datum heteroskedastic noise propose suite solution perform valid inference principal subspace base two estimator vanilla svd base approach refine iterative scheme call heteropca zhang et 2018 develop non asymptotic distributional guarantee estimator demonstrate invoke compute confidence region principal subspace entrywise confidence interval spike covariance matrix particularly worth highlight inference procedure build top heteropca valid also statistically efficient broad scenario cover wide range miss rate signal to noise ratio solution fully data drive adaptive heteroskedastic random noise without require prior knowledge noise level noise distribution
Topology and Geometry of Half-Rectified Network Optimization,"  The loss surface of deep neural networks has recently attracted interest in
the optimization and machine learning communities as a prime example of
high-dimensional non-convex problem. Some insights were recently gained using
spin glass models and mean-field approximations, but at the expense of strongly
simplifying the nonlinear nature of the model.
  In this work, we do not make any such assumption and study conditions on the
data distribution and model architecture that prevent the existence of bad
local minima. Our theoretical work quantifies and formalizes two important
\emph{folklore} facts: (i) the landscape of deep linear networks has a
radically different topology from that of deep half-rectified ones, and (ii)
that the energy landscape in the non-linear case is fundamentally controlled by
the interplay between the smoothness of the data distribution and model
over-parametrization. Our main theoretical contribution is to prove that
half-rectified single layer networks are asymptotically connected, and we
provide explicit bounds that reveal the aforementioned interplay.
  The conditioning of gradient descent is the next challenge we address. We
study this question through the geometry of the level sets, and we introduce an
algorithm to efficiently estimate the regularity of such sets on large-scale
networks. Our empirical results show that these level sets remain connected
throughout all the learning phase, suggesting a near convex behavior, but they
become exponentially more curvy as the energy level decays, in accordance to
what is observed in practice with very low curvature attractors.
",loss surface deep neural network recently attract interest optimization machine learn community prime example high dimensional non convex problem insight recently gain use spin glass model mean field approximation expense strongly simplify nonlinear nature model work make assumption study condition datum distribution model architecture prevent existence bad local minima theoretical work quantifie formalize two important folklore fact landscape deep linear network radically different topology deep half rectify one ii energy landscape non linear case fundamentally control interplay smoothness datum distribution model over parametrization main theoretical contribution prove half rectify single layer network asymptotically connect provide explicit bound reveal aforementioned interplay conditioning gradient descent next challenge address study question geometry level set introduce algorithm efficiently estimate regularity set large scale network empirical result show level set remain connected throughout learn phase suggest near convex behavior become exponentially curvy energy level decay accordance observe practice low curvature attractor
"A General Framework for Multi-fidelity Bayesian Optimization with
  Gaussian Processes","  How can we efficiently gather information to optimize an unknown function,
when presented with multiple, mutually dependent information sources with
different costs? For example, when optimizing a robotic system, intelligently
trading off computer simulations and real robot testings can lead to
significant savings. Existing methods, such as multi-fidelity GP-UCB or Entropy
Search-based approaches, either make simplistic assumptions on the interaction
among different fidelities or use simple heuristics that lack theoretical
guarantees. In this paper, we study multi-fidelity Bayesian optimization with
complex structural dependencies among multiple outputs, and propose
MF-MI-Greedy, a principled algorithmic framework for addressing this problem.
In particular, we model different fidelities using additive Gaussian processes
based on shared latent structures with the target function. Then we use
cost-sensitive mutual information gain for efficient Bayesian global
optimization. We propose a simple notion of regret which incorporates the cost
of different fidelities, and prove that MF-MI-Greedy achieves low regret. We
demonstrate the strong empirical performance of our algorithm on both synthetic
and real-world datasets.
",efficiently gather information optimize unknown function present multiple mutually dependent information source different cost example optimize robotic system intelligently trade computer simulation real robot testing lead significant saving exist method multi fidelity gp ucb entropy search base approach either make simplistic assumption interaction among different fidelity use simple heuristic lack theoretical guarantee paper study multi fidelity bayesian optimization complex structural dependency among multiple output propose mf mi greedy principle algorithmic framework address problem particular model different fidelity use additive gaussian process base shared latent structure target function use cost sensitive mutual information gain efficient bayesian global optimization propose simple notion regret incorporate cost different fidelity prove mf mi greedy achieve low regret demonstrate strong empirical performance algorithm synthetic real world dataset
"Multi-Agent Neural Rewriter for Vehicle Routing with Limited Disclosure
  of Costs","  We interpret solving the multi-vehicle routing problem as a team Markov game
with partially observable costs. For a given set of customers to serve, the
playing agents (vehicles) have the common goal to determine the team-optimal
agent routes with minimal total cost. Each agent thereby observes only its own
cost. Our multi-agent reinforcement learning approach, the so-called
multi-agent Neural Rewriter, builds on the single-agent Neural Rewriter to
solve the problem by iteratively rewriting solutions. Parallel agent action
execution and partial observability require new rewriting rules for the game.
We propose the introduction of a so-called pool in the system which serves as a
collection point for unvisited nodes. It enables agents to act simultaneously
and exchange nodes in a conflict-free manner. We realize limited disclosure of
agent-specific costs by only sharing them during learning. During inference,
each agents acts decentrally, solely based on its own cost. First empirical
results on small problem sizes demonstrate that we reach a performance close to
the employed OR-Tools benchmark which operates in the perfect cost information
setting.
",interpret solve multi vehicle routing problem team markov game partially observable cost give set customer serve play agent vehicle common goal determine team optimal agent route minimal total cost agent thereby observe cost multi agent reinforcement learning approach so call multi agent neural rewriter build single agent neural rewriter solve problem iteratively rewrite solution parallel agent action execution partial observability require new rewrite rule game propose introduction so call pool system serve collection point unvisite node enable agent act simultaneously exchange node conflict free manner realize limited disclosure agent specific cost share learn inference agent act decentrally solely base cost first empirical result small problem size demonstrate reach performance close employ or tool benchmark operate perfect cost information set
RoFormer: Enhanced Transformer with Rotary Position Embedding,"  Position encoding in transformer architecture provides supervision for
dependency modeling between elements at different positions in the sequence. We
investigate various methods to encode positional information in
transformer-based language models and propose a novel implementation named
Rotary Position Embedding(RoPE). The proposed RoPE encodes absolute positional
information with rotation matrix and naturally incorporates explicit relative
position dependency in self-attention formulation. Notably, RoPE comes with
valuable properties such as flexibility of being expand to any sequence
lengths, decaying inter-token dependency with increasing relative distances,
and capability of equipping the linear self-attention with relative position
encoding. As a result, the enhanced transformer with rotary position embedding,
or RoFormer, achieves superior performance in tasks with long texts. We release
the theoretical analysis along with some preliminary experiment results on
Chinese data. The undergoing experiment for English benchmark will soon be
updated.
",position encode transformer architecture provide supervision dependency modeling element different position sequence investigate various method encode positional information transformer base language model propose novel implementation name rotary position embed rope propose rope encode absolute positional information rotation matrix naturally incorporate explicit relative position dependency self attention formulation notably rope come valuable property flexibility expand sequence length decay inter token dependency increase relative distance capability equip linear self attention relative position encoding result enhance transformer rotary position embed roformer achieve superior performance task long text release theoretical analysis along preliminary experiment result chinese datum undergo experiment english benchmark soon update
NEW: A Generic Learning Model for Tie Strength Prediction in Networks,"  Tie strength prediction, sometimes named weight prediction, is vital in
exploring the diversity of connectivity pattern emerged in networks. Due to the
fundamental significance, it has drawn much attention in the field of network
analysis and mining. Some related works appeared in recent years have
significantly advanced our understanding of how to predict the strong and weak
ties in the social networks. However, most of the proposed approaches are
scenario-aware methods heavily depending on some special contexts and even
exclusively used in social networks. As a result, they are less applicable to
various kinds of networks.
  In contrast to the prior studies, here we propose a new computational
framework called Neighborhood Estimating Weight (NEW) which is purely driven by
the basic structure information of the network and has the flexibility for
adapting to diverse types of networks. In NEW, we design a novel index, i.e.,
connection inclination, to generate the representative features of the network,
which is capable of capturing the actual distribution of the tie strength. In
order to obtain the optimized prediction results, we also propose a
parameterized regression model which approximately has a linear time complexity
and thus is readily extended to the implementation in large-scale networks. The
experimental results on six real-world networks demonstrate that our proposed
predictive model outperforms the state of the art methods, which is powerful
for predicting the missing tie strengths when only a part of the network's tie
strength information is available.
",tie strength prediction sometimes name weight prediction vital explore diversity connectivity pattern emerge network due fundamental significance draw much attention field network analysis mining relate work appear recent year significantly advanced understanding predict strong weak tie social network however propose approach scenario aware method heavily depend special contexts even exclusively use social network result less applicable various kind network contrast prior study propose new computational framework call neighborhood estimate weight new purely drive basic structure information network flexibility adapt diverse type network new design novel index connection inclination generate representative feature network capable capture actual distribution tie strength order obtain optimize prediction result also propose parameterized regression model approximately linear time complexity thus readily extend implementation large scale network experimental result six real world network demonstrate propose predictive model outperform state art method powerful predict miss tie strength part network tie strength information available
FOCUS: Dealing with Label Quality Disparity in Federated Learning,"  Ubiquitous systems with End-Edge-Cloud architecture are increasingly being
used in healthcare applications. Federated Learning (FL) is highly useful for
such applications, due to silo effect and privacy preserving. Existing FL
approaches generally do not account for disparities in the quality of local
data labels. However, the clients in ubiquitous systems tend to suffer from
label noise due to varying skill-levels, biases or malicious tampering of the
annotators. In this paper, we propose Federated Opportunistic Computing for
Ubiquitous Systems (FOCUS) to address this challenge. It maintains a small set
of benchmark samples on the FL server and quantifies the credibility of the
client local data without directly observing them by computing the mutual
cross-entropy between performance of the FL model on the local datasets and
that of the client local FL model on the benchmark dataset. Then, a credit
weighted orchestration is performed to adjust the weight assigned to clients in
the FL model based on their credibility values. FOCUS has been experimentally
evaluated on both synthetic data and real-world data. The results show that it
effectively identifies clients with noisy labels and reduces their impact on
the model performance, thereby significantly outperforming existing FL
approaches.
",ubiquitous system end edge cloud architecture increasingly use healthcare application federate learning fl highly useful application due silo effect privacy preserve exist fl approach generally account disparity quality local datum label however client ubiquitous system tend suffer label noise due vary skill level bias malicious tamper annotator paper propose federate opportunistic compute ubiquitous system focus address challenge maintain small set benchmark sample fl server quantifie credibility client local datum without directly observe compute mutual cross entropy performance fl model local dataset client local fl model benchmark dataset credit weight orchestration perform adjust weight assign client fl model base credibility value focus experimentally evaluate synthetic datum real world datum result show effectively identify client noisy label reduce impact model performance thereby significantly outperform exist fl approach
Discovering Valuable Items from Massive Data,"  Suppose there is a large collection of items, each with an associated cost
and an inherent utility that is revealed only once we commit to selecting it.
Given a budget on the cumulative cost of the selected items, how can we pick a
subset of maximal value? This task generalizes several important problems such
as multi-arm bandits, active search and the knapsack problem. We present an
algorithm, GP-Select, which utilizes prior knowledge about similarity be- tween
items, expressed as a kernel function. GP-Select uses Gaussian process
prediction to balance exploration (estimating the unknown value of items) and
exploitation (selecting items of high value). We extend GP-Select to be able to
discover sets that simultaneously have high utility and are diverse. Our
preference for diversity can be specified as an arbitrary monotone submodular
function that quantifies the diminishing returns obtained when selecting
similar items. Furthermore, we exploit the structure of the model updates to
achieve an order of magnitude (up to 40X) speedup in our experiments without
resorting to approximations. We provide strong guarantees on the performance of
GP-Select and apply it to three real-world case studies of industrial
relevance: (1) Refreshing a repository of prices in a Global Distribution
System for the travel industry, (2) Identifying diverse, binding-affine
peptides in a vaccine de- sign task and (3) Maximizing clicks in a web-scale
recommender system by recommending items to users.
",suppose large collection item associate cost inherent utility reveal commit selecting give budget cumulative cost select item pick subset maximal value task generalize several important problem multi arm bandit active search knapsack problem present algorithm gp select utilize prior knowledge similarity be- tween item express kernel function gp select use gaussian process prediction balance exploration estimate unknown value item exploitation select item high value extend gp select able discover set simultaneously high utility diverse preference diversity specify arbitrary monotone submodular function quantifie diminish return obtain select similar item furthermore exploit structure model update achieve order magnitude 40x speedup experiment without resort approximation provide strong guarantee performance gp select apply three real world case study industrial relevance 1 refresh repository price global distribution system travel industry 2 identify diverse binding affine peptide vaccine de- sign task 3 maximize click web scale recommender system recommend item user
"Robbing the Fed: Directly Obtaining Private Data in Federated Learning
  with Modified Models","  Federated learning has quickly gained popularity with its promises of
increased user privacy and efficiency. Previous works have shown that federated
gradient updates contain information that can be used to approximately recover
user data in some situations. These previous attacks on user privacy have been
limited in scope and do not scale to gradient updates aggregated over even a
handful of data points, leaving some to conclude that data privacy is still
intact for realistic training regimes. In this work, we introduce a new threat
model based on minimal but malicious modifications of the shared model
architecture which enable the server to directly obtain a verbatim copy of user
data from gradient updates without solving difficult inverse problems. Even
user data aggregated over large batches -- where previous methods fail to
extract meaningful content -- can be reconstructed by these minimally modified
models.
",federate learning quickly gain popularity promise increase user privacy efficiency previous work show federated gradient update contain information use approximately recover user data situation previous attack user privacy limited scope scale gradient update aggregate even handful datum point leave conclude data privacy still intact realistic training regime work introduce new threat model base minimal malicious modification share model architecture enable server directly obtain verbatim copy user data gradient update without solve difficult inverse problem even user datum aggregate large batch previous method fail extract meaningful content reconstruct minimally modify model
"Machine Learning Framework for Sensing and Modeling Interference in IoT
  Frequency Bands","  Spectrum scarcity has surfaced as a prominent concern in wireless radio
communications with the emergence of new technologies over the past few years.
As a result, there is growing need for better understanding of the spectrum
occupancy with newly emerging access technologies supporting the Internet of
Things. In this paper, we present a framework to capture and model the traffic
behavior of short-time spectrum occupancy for IoT applications in the shared
bands to determine the existing interference. The proposed capturing method
utilizes a software defined radio to monitor the short bursts of IoT
transmissions by capturing the time series data which is converted to power
spectral density to extract the observed occupancy. Furthermore, we propose the
use of an unsupervised machine learning technique to enhance conventionally
implemented energy detection methods. Our experimental results show that the
temporal and frequency behavior of the spectrum can be well-captured using the
combination of two models, namely, semi-Markov chains and a
Poisson-distribution arrival rate. We conduct an extensive measurement campaign
in different urban environments and incorporate the spatial effect on the IoT
shared spectrum.
",spectrum scarcity surface prominent concern wireless radio communication emergence new technology past year result grow need well understand spectrum occupancy newly emerge access technology support internet thing paper present framework capture model traffic behavior short time spectrum occupancy iot application share band determine exist interference propose capture method utilize software define radio monitor short burst iot transmission capture time series datum convert power spectral density extract observe occupancy furthermore propose use unsupervised machine learn technique enhance conventionally implement energy detection method experimental result show temporal frequency behavior spectrum well capture use combination two model namely semi markov chain poisson distribution arrival rate conduct extensive measurement campaign different urban environment incorporate spatial effect iot share spectrum
Discriminative Relational Topic Models,"  Many scientific and engineering fields involve analyzing network data. For
document networks, relational topic models (RTMs) provide a probabilistic
generative process to describe both the link structure and document contents,
and they have shown promise on predicting network structures and discovering
latent topic representations. However, existing RTMs have limitations in both
the restricted model expressiveness and incapability of dealing with imbalanced
network data. To expand the scope and improve the inference accuracy of RTMs,
this paper presents three extensions: 1) unlike the common link likelihood with
a diagonal weight matrix that allows the-same-topic interactions only, we
generalize it to use a full weight matrix that captures all pairwise topic
interactions and is applicable to asymmetric networks; 2) instead of doing
standard Bayesian inference, we perform regularized Bayesian inference
(RegBayes) with a regularization parameter to deal with the imbalanced link
structure issue in common real networks and improve the discriminative ability
of learned latent representations; and 3) instead of doing variational
approximation with strict mean-field assumptions, we present collapsed Gibbs
sampling algorithms for the generalized relational topic models by exploring
data augmentation without making restricting assumptions. Under the generic
RegBayes framework, we carefully investigate two popular discriminative loss
functions, namely, the logistic log-loss and the max-margin hinge loss.
Experimental results on several real network datasets demonstrate the
significance of these extensions on improving the prediction performance, and
the time efficiency can be dramatically improved with a simple fast
approximation method.
",many scientific engineering field involve analyze network datum document network relational topic model rtms provide probabilistic generative process describe link structure document content show promise predict network structure discover latent topic representation however exist rtms limitation restrict model expressiveness incapability deal imbalanced network datum expand scope improve inference accuracy rtms paper present three extension 1 unlike common link likelihood diagonal weight matrix allow the same topic interaction generalize use full weight matrix capture pairwise topic interaction applicable asymmetric network 2 instead standard bayesian inference perform regularized bayesian inference regbaye regularization parameter deal imbalance link structure issue common real network improve discriminative ability learn latent representation 3 instead variational approximation strict mean field assumption present collapse gibb sampling algorithm generalize relational topic model explore datum augmentation without make restrict assumption generic regbaye framework carefully investigate two popular discriminative loss function namely logistic log loss max margin hinge loss experimental result several real network dataset demonstrate significance extension improve prediction performance time efficiency dramatically improve simple fast approximation method
Potential-Function Proofs for First-Order Methods,"  This note discusses proofs for convergence of first-order methods based on
simple potential-function arguments. We cover methods like gradient descent
(for both smooth and non-smooth settings), mirror descent, and some accelerated
variants.
",note discuss proof convergence first order method base simple potential function argument cover method like gradient descent smooth non smooth setting mirror descent accelerate variant
"Predicting Localized Primordial Star Formation with Deep Convolutional
  Neural Networks","  We investigate applying 3D deep convolutional neural networks as fast
surrogate models of the formation and feedback effects of primordial stars in
hydrodynamic cosmological simulations of the first galaxies. Here, we present
the surrogate model to predict localized primordial star formation; the
feedback model will be presented in a subsequent paper. The star formation
prediction model consists of two sub-models: the first is a 3D volume
classifier that predicts which (10 comoving kpc)$^3$ volumes will host star
formation, followed by a 3D Inception-based U-net voxel segmentation model that
predicts which voxels will form primordial stars. We find that the combined
model predicts primordial star forming volumes with high skill, with $F_1
>0.995$ and true skill score $>0.994$. The star formation is localized within
the volume to $\lesssim5^3$~voxels ($\sim1.6$~comoving kpc$^3$) with
$F_1>0.399$ and true skill score $>0.857$. Applied to simulations with low
spatial resolution, the model predicts star forming regions in the same
locations and at similar redshifts as sites in resolved full-physics
simulations that explicitly model primordial star formation and feedback. When
applied to simulations with lower mass resolution, we find that the model
predicts star forming regions at later redshift due to delayed structure
formation resulting from lower mass resolution. Our model predicts primordial
star formation without halo finding, so will be useful in spatially
under-resolved simulations that cannot resolve primordial star forming halos.
To our knowledge, this is the first model that can predict primordial star
forming regions that match highly-resolved cosmological simulations.
",investigate apply 3d deep convolutional neural network fast surrogate model formation feedback effect primordial star hydrodynamic cosmological simulation first galaxy present surrogate model predict localized primordial star formation feedback model present subsequent paper star formation prediction model consist two sub model first 3d volume classifier predict 10 comoving kpc volume host star formation follow 3d inception base u net voxel segmentation model predict voxel form primordial star find combine model predict primordial star form volume high skill f_1 true skill score star formation localize within volume kpc f_1 true skill score apply simulation low spatial resolution model predicts star form region location similar redshift site resolve full physics simulation explicitly model primordial star formation feedback apply simulation low mass resolution find model predicts star form region later redshift due delay structure formation result low mass resolution model predict primordial star formation without halo find useful spatially under resolve simulation resolve primordial star form halos knowledge first model predict primordial star forming region match highly resolve cosmological simulation
Deep neural network for pier scour prediction,"  With the advancement in computing power over last decades, deep neural
networks (DNN), consisting of two or more hidden layers with large number of
nodes, are being suggested as an alternate to commonly used single-hidden-layer
neural networks (ANN). DNN are found to be flexible models with a very large
number of parameters, thus making them capable of modelling very complex and
highly nonlinear relationships existing between inputs and outputs. This paper
investigates the potential of a DNN consisting of 3 hidden layers (100, 80 and
50 nodes) to predict the local scour around bridge piers using field data. To
update the weights and bias of DNN, an adaptive learning rate optimization
algorithm was used. The dataset consists of 232 pier scour measurements, out of
which a total of 154 data were used to train whereas remaining 78 data to test
the created model. A correlation coefficient value of 0.957 (root mean square
error = 0.306m) was achieved by DNN in comparison to 0.938 (0.388m) by ANN,
indicating an improved performance by DNN for scour depth perdition.
Encouraging performance on the used dataset in the work suggests the need of
more studies on the use of DNN for various civil engineering applications.
",advancement computing power last decade deep neural network dnn consist two hide layer large number node suggest alternate commonly use single hide layer neural network ann dnn find flexible model large number parameter thus make capable modelling complex highly nonlinear relationship exist input output paper investigate potential dnn consist 3 hide layer 100 80 50 node predict local scour around bridge pier use field datum update weights bias dnn adaptive learning rate optimization algorithm use dataset consist 232 pier scour measurement total 154 datum use train whereas remain 78 datum test create model correlation coefficient value root mean square error achieve dnn comparison ann indicating improve performance dnn scour depth perdition encourage performance use dataset work suggest need study use dnn various civil engineering application
"FedCom: A Byzantine-Robust Local Model Aggregation Rule Using Data
  Commitment for Federated Learning","  Federated learning (FL) is a promising privacy-preserving distributed machine
learning methodology that allows multiple clients (i.e., workers) to
collaboratively train statistical models without disclosing private training
data. Due to the characteristics of data remaining localized and the
uninspected on-device training process, there may exist Byzantine workers
launching data poisoning and model poisoning attacks, which would seriously
deteriorate model performance or prevent the model from convergence. Most of
the existing Byzantine-robust FL schemes are either ineffective against several
advanced poisoning attacks or need to centralize a public validation dataset,
which is intractable in FL. Moreover, to the best of our knowledge, none of the
existing Byzantine-robust distributed learning methods could well exert its
power in Non-Independent and Identically distributed (Non-IID) data among
clients. To address these issues, we propose FedCom, a novel Byzantine-robust
federated learning framework by incorporating the idea of commitment from
cryptography, which could achieve both data poisoning and model poisoning
tolerant FL under practical Non-IID data partitions. Specifically, in FedCom,
each client is first required to make a commitment to its local training data
distribution. Then, we identify poisoned datasets by comparing the Wasserstein
distance among commitments submitted by different clients. Furthermore, we
distinguish abnormal local model updates from benign ones by testing each local
model's behavior on its corresponding data commitment. We conduct an extensive
performance evaluation of FedCom. The results demonstrate its effectiveness and
superior performance compared to the state-of-the-art Byzantine-robust schemes
in defending against typical data poisoning and model poisoning attacks under
practical Non-IID data distributions.
",federate learning fl promise privacy preserve distribute machine learn methodology allow multiple client worker collaboratively train statistical model without disclose private training datum due characteristic datum remain localize uninspected on device training process may exist byzantine worker launch datum poisoning model poisoning attack would seriously deteriorate model performance prevent model convergence exist byzantine robust fl scheme either ineffective several advanced poisoning attack need centralize public validation dataset intractable fl moreover good knowledge none exist byzantine robust distribute learning method could well exert power non independent identically distribute non iid datum among client address issue propose fedcom novel byzantine robust federate learning framework incorporate idea commitment cryptography could achieve datum poison model poison tolerant fl practical non iid datum partition specifically fedcom client first require make commitment local training datum distribution identify poison dataset compare wasserstein distance among commitment submit different client furthermore distinguish abnormal local model update benign one test local model behavior correspond datum commitment conduct extensive performance evaluation fedcom result demonstrate effectiveness superior performance compare state of the art byzantine robust scheme defend typical datum poison model poisoning attack practical non iid datum distribution
A Novel Neural Network Training Framework with Data Assimilation,"  In recent years, the prosperity of deep learning has revolutionized the
Artificial Neural Networks. However, the dependence of gradients and the
offline training mechanism in the learning algorithms prevents the ANN for
further improvement. In this study, a gradient-free training framework based on
data assimilation is proposed to avoid the calculation of gradients. In data
assimilation algorithms, the error covariance between the forecasts and
observations is used to optimize the parameters. Feedforward Neural Networks
(FNNs) are trained by gradient decent, data assimilation algorithms (Ensemble
Kalman Filter (EnKF) and Ensemble Smoother with Multiple Data Assimilation
(ESMDA)), respectively. ESMDA trains FNN with pre-defined iterations by
updating the parameters using all the available observations which can be
regard as offline learning. EnKF optimize FNN when new observation available by
updating parameters which can be regard as online learning. Two synthetic cases
with the regression of a Sine Function and a Mexican Hat function are assumed
to validate the effectiveness of the proposed framework. The Root Mean Square
Error (RMSE) and coefficient of determination (R2) are used as criteria to
assess the performance of different methods. The results show that the proposed
training framework performed better than the gradient decent method. The
proposed framework provides alternatives for online/offline training the
existing ANNs (e.g., Convolutional Neural Networks, Recurrent Neural Networks)
without the dependence of gradients.
",recent year prosperity deep learning revolutionize artificial neural network however dependence gradient offline training mechanism learn algorithm prevent ann improvement study gradient free training framework base datum assimilation propose avoid calculation gradient datum assimilation algorithm error covariance forecast observation use optimize parameter feedforward neural network fnns train gradient decent data assimilation algorithm ensemble kalman filter enkf ensemble smoother multiple datum assimilation esmda respectively esmda train fnn pre define iteration update parameter use available observation regard offline learn enkf optimize fnn new observation available update parameter regard online learn two synthetic case regression sine function mexican hat function assume validate effectiveness propose framework root mean square error rmse coefficient determination r2 use criterion assess performance different method result show propose training framework perform well gradient decent method propose framework provide alternative train exist ann convolutional neural network recurrent neural network without dependence gradient
Text-Free Prosody-Aware Generative Spoken Language Modeling,"  Speech pre-training has primarily demonstrated efficacy on classification
tasks, while its capability of generating novel speech, similar to how GPT-2
can generate coherent paragraphs, has barely been explored. Generative Spoken
Language Modeling (GSLM) \cite{Lakhotia2021} is the only prior work addressing
the generative aspects of speech pre-training, which replaces text with
discovered phone-like units for language modeling and shows the ability to
generate meaningful novel sentences. Unfortunately, despite eliminating the
need of text, the units used in GSLM discard most of the prosodic information.
Hence, GSLM fails to leverage prosody for better comprehension, and does not
generate expressive speech. In this work, we present a prosody-aware generative
spoken language model (pGSLM). It is composed of a multi-stream transformer
language model (MS-TLM) of speech, represented as discovered unit and prosodic
feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to
waveforms. We devise a series of metrics for prosody modeling and generation,
and re-use metrics from GSLM for content modeling. Experimental results show
that the pGSLM can utilize prosody to improve both prosody and content
modeling, and also generate natural, meaningful, and coherent speech given a
spoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm.
Codes and models are available at
https://github.com/pytorch/fairseq/tree/main/examples/textless_nlp/pgslm.
",speech pre training primarily demonstrate efficacy classification task capability generate novel speech similar gpt-2 generate coherent paragraph barely explore generative speak language modeling gslm lakhotia2021 prior work address generative aspect speech pre training replace text discover phone like unit language modeling show ability generate meaningful novel sentence unfortunately despite eliminate need text unit use gslm discard prosodic information hence gslm fail leverage prosody well comprehension generate expressive speech work present prosody aware generative speak language model pgslm compose multi stream transformer language model ms tlm speech represent discover unit prosodic feature stream adapt hifi gan model convert ms tlm output waveform devise series metric prosody modeling generation re use metric gslm content model experimental result show pgslm utilize prosody improve prosody content modeling also generate natural meaningful coherent speech give speak prompt audio sample find https code model available https
"Prediction of motor insurance claims occurrence as an imbalanced machine
  learning problem","  The insurance industry, with its large datasets, is a natural place to use
big data solutions. However it must be stressed, that significant number of
applications for machine learning in insurance industry, like fraud detection
or claim prediction, deals with the problem of machine learning on an
imbalanced data set. This is due to the fact that frauds or claims are rare
events when compared with the entire population of drivers. The problem of
imbalanced learning is often hard to overcome. Therefore, the main goal of this
work is to present and apply various methods of dealing with an imbalanced
dataset in the context of claim occurrence prediction in car insurance. In
addition, the above techniques are used to compare the results of machine
learning algorithms in the context of claim occurrence prediction in car
insurance. Our study covers the following techniques: logistic-regression,
decision tree, random forest, xgBoost, feed-forward network. The problem is the
classification one.
",insurance industry large dataset natural place use big datum solution however must stress significant number application machine learn insurance industry like fraud detection claim prediction deal problem machine learn imbalance datum set due fact fraud claim rare event compare entire population driver problem imbalance learning often hard overcome therefore main goal work present apply various method deal imbalanced dataset context claim occurrence prediction car insurance addition technique use compare result machine learn algorithm context claim occurrence prediction car insurance study cover follow technique logistic regression decision tree random forest xgboost feed forward network problem classification one
Premise Selection for Mathematics by Corpus Analysis and Kernel Methods,"  Smart premise selection is essential when using automated reasoning as a tool
for large-theory formal proof development. A good method for premise selection
in complex mathematical libraries is the application of machine learning to
large corpora of proofs. This work develops learning-based premise selection in
two ways. First, a newly available minimal dependency analysis of existing
high-level formal mathematical proofs is used to build a large knowledge base
of proof dependencies, providing precise data for ATP-based re-verification and
for training premise selection algorithms. Second, a new machine learning
algorithm for premise selection based on kernel methods is proposed and
implemented. To evaluate the impact of both techniques, a benchmark consisting
of 2078 large-theory mathematical problems is constructed,extending the older
MPTP Challenge benchmark. The combined effect of the techniques results in a
50% improvement on the benchmark over the Vampire/SInE state-of-the-art system
for automated reasoning in large theories.
",smart premise selection essential use automate reasoning tool large theory formal proof development good method premise selection complex mathematical library application machine learn large corpora proof work develop learning base premise selection two way first newly available minimal dependency analysis exist high level formal mathematical proof use build large knowledge base proof dependencie provide precise datum atp base re verification training premise selection algorithm second new machine learn algorithm premise selection base kernel method propose implement evaluate impact technique benchmark consist 2078 large theory mathematical problem construct extend old mptp challenge benchmark combine effect technique result 50 improvement benchmark state of the art system automate reasoning large theory
Learning Symmetries of Classical Integrable Systems,"  The solution of problems in physics is often facilitated by a change of
variables. In this work we present neural transformations to learn symmetries
of Hamiltonian mechanical systems. Maintaining the Hamiltonian structure
requires novel network architectures that parametrize symplectic
transformations. We demonstrate the utility of these architectures by learning
the structure of integrable models. Our work exemplifies the adaptation of
neural transformations to a family constrained by more than the condition of
invertibility, which we expect to be a common feature of applications of these
methods.
",solution problem physics often facilitate change variable work present neural transformation learn symmetry hamiltonian mechanical system maintain hamiltonian structure require novel network architecture parametrize symplectic transformation demonstrate utility architecture learn structure integrable model work exemplify adaptation neural transformation family constrain condition invertibility expect common feature application method
"Improving Voice Separation by Incorporating End-to-end Speech
  Recognition","  Despite recent advances in voice separation methods, many challenges remain
in realistic scenarios such as noisy recording and the limits of available
data. In this work, we propose to explicitly incorporate the phonetic and
linguistic nature of speech by taking a transfer learning approach using an
end-to-end automatic speech recognition (E2EASR) system. The voice separation
is conditioned on deep features extracted from E2EASR to cover the long-term
dependence of phonetic aspects. Experimental results on speech separation and
enhancement task on the AVSpeech dataset show that the proposed method
significantly improves the signal-to-distortion ratio over the baseline model
and even outperforms an audio visual model, that utilizes visual information of
lip movements.
",despite recent advance voice separation method many challenge remain realistic scenario noisy recording limit available datum work propose explicitly incorporate phonetic linguistic nature speech take transfer learning approach use end to end automatic speech recognition e2easr system voice separation condition deep feature extract e2easr cover long term dependence phonetic aspect experimental result speech separation enhancement task avspeech dataset show propose method significantly improve signal to distortion ratio baseline model even outperform audio visual model utilize visual information lip movement
"Hydroelectric Generation Forecasting with Long Short Term Memory (LSTM)
  Based Deep Learning Model for Turkey","  Hydroelectricity is one of the renewable energy source, has been used for
many years in Turkey. The production of hydraulic power plants based on water
reservoirs varies based on different parameters. For this reason, the
estimation of hydraulic production gains importance in terms of the planning of
electricity generation. In this article, the estimation of Turkey's monthly
hydroelectricity production has been made with the long-short-term memory
(LSTM) network-based deep learning model. The designed deep learning model is
based on hydraulic production time series and future production planning for
many years. By using real production data and different LSTM deep learning
models, their performance on the monthly forecast of hydraulic electricity
generation of the next year has been examined. The obtained results showed that
the use of time series based on real production data for many years and deep
learning model together is successful in long-term prediction. In the study, it
is seen that the 100-layer LSTM model, in which 120 months (10 years)
hydroelectric generation time data are used according to the RMSE and MAPE
values, are the highest model in terms of estimation accuracy, with a MAPE
value of 0.1311 (13.1%) in the annual total and 1.09% as the monthly average
distribution. In this model, the best results were obtained for the 100-layer
LSTM model, in which the time data of 144 months (12 years) hydroelectric
generation data are used, with a RMSE value of 29,689 annually and 2474.08 in
monthly distribution. According to the results of the study, time data covering
at least 120 months of production is recommended to create an acceptable
hydropower forecasting model with LSTM.
",hydroelectricity one renewable energy source use many year turkey production hydraulic power plant base water reservoir vary base different parameter reason estimation hydraulic production gain importance term plan electricity generation article estimation turkey monthly hydroelectricity production make long short term memory lstm network base deep learning model design deep learning model base hydraulic production time series future production plan many year use real production datum different lstm deep learning model performance monthly forecast hydraulic electricity generation next year examine obtain result show use time series base real production datum many year deep learning model together successful long term prediction study see 100 layer lstm model 120 month 10 year hydroelectric generation time datum use accord rmse mape value high model term estimation accuracy mape value annual total monthly average distribution model good result obtain 100 layer lstm model time datum 144 month 12 year hydroelectric generation datum use rmse value annually monthly distribution accord result study time datum cover least 120 month production recommend create acceptable hydropower forecasting model lstm
"Exploring the Distributed Knowledge Congruence in Proxy-data-free
  Federated Distillation","  Federated learning (FL) is a distributed machine learning paradigm in which
the server periodically aggregates local model parameters from clients without
assembling their private data.
  Constrained communication and personalization requirements pose severe
challenges to FL. Federated distillation (FD) is proposed to simultaneously
address the above two problems, which exchanges knowledge between the server
and clients, supporting heterogeneous local models while significantly reducing
communication overhead. However, most existing FD methods require a proxy
dataset, which is often unavailable in reality.
  A few recent proxy-data-free FD approaches can eliminate the need for
additional public data, but suffer from remarkable discrepancy among local
knowledge due to model heterogeneity, leading to ambiguous representation on
the server and inevitable accuracy degradation.
  To tackle this issue, we propose a proxy-data-free FD algorithm based on
distributed knowledge congruence (FedDKC). FedDKC leverages well-designed
refinement strategies to narrow local knowledge differences into an acceptable
upper bound, so as to mitigate the negative effects of knowledge incongruence.
  Specifically, from perspectives of peak probability and Shannon entropy of
local knowledge, we design kernel-based knowledge refinement (KKR) and
searching-based knowledge refinement (SKR) respectively, and theoretically
guarantee that the refined-local knowledge can satisfy an approximately-similar
distribution and be regarded as congruent.
  Extensive experiments conducted on three common datasets demonstrate that our
proposed FedDKC significantly outperforms the state-of-the-art (accuracy boosts
in 93.33% comparisons,
  Top-1 accuracy boosts by up to 4.38%, and Top-5 accuracy boosts by up to
10.31%) on various heterogeneous settings while evidently improving the
convergence speed.
",federate learning fl distribute machine learn paradigm server periodically aggregate local model parameter client without assemble private datum constrain communication personalization requirement pose severe challenge fl federate distillation fd propose simultaneously address two problem exchange knowledge server client support heterogeneous local model significantly reduce communication overhead however exist fd method require proxy dataset often unavailable reality recent proxy data free fd approach eliminate need additional public datum suffer remarkable discrepancy among local knowledge due model heterogeneity lead ambiguous representation server inevitable accuracy degradation tackle issue propose proxy data free fd algorithm base distribute knowledge congruence feddkc feddkc leverage well design refinement strategy narrow local knowledge difference acceptable upper bind mitigate negative effect knowledge incongruence specifically perspective peak probability shannon entropy local knowledge design kernel base knowledge refinement kkr searching base knowledge refinement skr respectively theoretically guarantee refine local knowledge satisfy approximately similar distribution regard congruent extensive experiment conduct three common dataset demonstrate propose feddkc significantly outperform state of the art accuracy boost comparison top-1 accuracy boost top-5 accuracy boost various heterogeneous setting evidently improve convergence speed
Exoplanet Characterization using Conditional Invertible Neural Networks,"  The characterization of an exoplanet's interior is an inverse problem, which
requires statistical methods such as Bayesian inference in order to be solved.
Current methods employ Markov Chain Monte Carlo (MCMC) sampling to infer the
posterior probability of planetary structure parameters for a given exoplanet.
These methods are time consuming since they require the calculation of a large
number of planetary structure models. To speed up the inference process when
characterizing an exoplanet, we propose to use conditional invertible neural
networks (cINNs) to calculate the posterior probability of the internal
structure parameters. cINNs are a special type of neural network which excel in
solving inverse problems. We constructed a cINN using FrEIA, which was then
trained on a database of $5.6\cdot 10^6$ internal structure models to recover
the inverse mapping between internal structure parameters and observable
features (i.e., planetary mass, planetary radius and composition of the host
star). The cINN method was compared to a Metropolis-Hastings MCMC. For that we
repeated the characterization of the exoplanet K2-111 b, using both the MCMC
method and the trained cINN. We show that the inferred posterior probability of
the internal structure parameters from both methods are very similar, with the
biggest differences seen in the exoplanet's water content. Thus cINNs are a
possible alternative to the standard time-consuming sampling methods. Indeed,
using cINNs allows for orders of magnitude faster inference of an exoplanet's
composition than what is possible using an MCMC method, however, it still
requires the computation of a large database of internal structures to train
the cINN. Since this database is only computed once, we found that using a cINN
is more efficient than an MCMC, when more than 10 exoplanets are characterized
using the same cINN.
",characterization exoplanet interior inverse problem require statistical method bayesian inference order solve current method employ markov chain monte carlo mcmc sampling infer posterior probability planetary structure parameter give exoplanet method time consume since require calculation large number planetary structure model speed inference process characterize exoplanet propose use conditional invertible neural network cinns calculate posterior probability internal structure parameter cinns special type neural network excel solve inverse problem construct cinn use freia train database internal structure model recover inverse mapping internal structure parameter observable feature planetary mass planetary radius composition host star cinn method compare metropolis hastings mcmc repeat characterization exoplanet k2 111 b use mcmc method train cinn show infer posterior probability internal structure parameter method similar big difference see exoplanet water content thus cinn possible alternative standard time consume sampling method indeed use cinns allow order magnitude fast inference exoplanet composition possible use mcmc method however still require computation large database internal structure train cinn since database compute find use cinn efficient mcmc 10 exoplanet characterize use cinn
"Level set image segmentation with velocity term learned from data with
  applications to lung nodule segmentation","  Purpose: Lung nodule segmentation, i.e., the algorithmic delineation of the
lung nodule surface, is a fundamental component of computational nodule
analysis pipelines. We propose a new method for segmentation that is a machine
learning based extension of current approaches, using labeled image examples to
improve its accuracy.
  Approach: We introduce an extension of the standard level set image
segmentation method where the velocity function is learned from data via
machine learning regression methods, rather than a priori designed. Instead,
the method employs a set of features to learn a velocity function that guides
the level set evolution from initialization.
  Results: We apply the method to image volumes of lung nodules from CT scans
in the publicly available LIDC dataset, obtaining an average intersection over
union score of 0.7185($\pm$0.1114), which is competitive with other methods. We
analyze segmentation performance by anatomical and appearance-based categories
of the nodules, finding that the method performs better for isolated nodules
with well-defined margins. We find that the segmentation performance for
nodules in more complex surroundings and having more complex CT appearance is
improved with the addition of combined global-local features.
  Conclusions: The level set machine learning segmentation approach proposed
herein is competitive with current methods. It provides accurate lung nodule
segmentation results in a variety of anatomical contexts.
",purpose lung nodule segmentation algorithmic delineation lung nodule surface fundamental component computational nodule analysis pipeline propose new method segmentation machine learn base extension current approach use label image example improve accuracy approach introduce extension standard level set image segmentation method velocity function learn datum via machine learn regression method rather priori design instead method employ set feature learn velocity function guide level set evolution initialization result apply method image volume lung nodule ct scan publicly available lidc dataset obtain average intersection union score competitive method analyze segmentation performance anatomical appearance base category nodule find method perform well isolate nodule well define margin find segmentation performance nodule complex surrounding complex ct appearance improve addition combine global local feature conclusion level set machine learn segmentation approach propose herein competitive current method provide accurate lung nodule segmentation result variety anatomical context
"Decoding the Protein-ligand Interactions Using Parallel Graph Neural
  Networks","  Protein-ligand interactions (PLIs) are fundamental to biochemical research
and their identification is crucial for estimating biophysical and biochemical
properties for rational therapeutic design. Currently, experimental
characterization of these properties is the most accurate method, however, this
is very time-consuming and labor-intensive. A number of computational methods
have been developed in this context but most of the existing PLI prediction
heavily depends on 2D protein sequence data. Here, we present a novel parallel
graph neural network (GNN) to integrate knowledge representation and reasoning
for PLI prediction to perform deep learning guided by expert knowledge and
informed by 3D structural data. We develop two distinct GNN architectures, GNNF
is the base implementation that employs distinct featurization to enhance
domain-awareness, while GNNP is a novel implementation that can predict with no
prior knowledge of the intermolecular interactions. The comprehensive
evaluation demonstrated that GNN can successfully capture the binary
interactions between ligand and proteins 3D structure with 0.979 test accuracy
for GNNF and 0.958 for GNNP for predicting activity of a protein-ligand
complex. These models are further adapted for regression tasks to predict
experimental binding affinities and pIC50 is crucial for drugs potency and
efficacy. We achieve a Pearson correlation coefficient of 0.66 and 0.65 on
experimental affinity and 0.50 and 0.51 on pIC50 with GNNF and GNNP,
respectively, outperforming similar 2D sequence-based models. Our method can
serve as an interpretable and explainable artificial intelligence (AI) tool for
predicted activity, potency, and biophysical properties of lead candidates. To
this end, we show the utility of GNNP on SARS-Cov-2 protein targets by
screening a large compound library and comparing our prediction with the
experimentally measured data.
",protein ligand interaction plis fundamental biochemical research identification crucial estimate biophysical biochemical property rational therapeutic design currently experimental characterization property accurate method however time consume labor intensive number computational method develop context exist pli prediction heavily depend 2d protein sequence data present novel parallel graph neural network gnn integrate knowledge representation reasoning pli prediction perform deep learning guide expert knowledge inform 3d structural datum develop two distinct gnn architecture gnnf base implementation employ distinct featurization enhance domain awareness gnnp novel implementation predict prior knowledge intermolecular interaction comprehensive evaluation demonstrate gnn successfully capture binary interaction ligand protein 3d structure test accuracy gnnf gnnp predict activity protein ligand complex model adapt regression task predict experimental bind affinity pic50 crucial drug potency efficacy achieve pearson correlation coefficient experimental affinity pic50 gnnf gnnp respectively outperform similar 2d sequence base model method serve interpretable explainable artificial intelligence ai tool predict activity potency biophysical property lead candidate end show utility gnnp sars cov-2 protein target screen large compound library compare prediction experimentally measure datum
"Towards the Unification and Data-Driven Synthesis of Autonomous Vehicle
  Safety Concepts","  As safety-critical autonomous vehicles (AVs) will soon become pervasive in
our society, a number of safety concepts for trusted AV deployment have been
recently proposed throughout industry and academia. Yet, agreeing upon an
""appropriate"" safety concept is still an elusive task. In this paper, we
advocate for the use of Hamilton Jacobi (HJ) reachability as a unifying
mathematical framework for comparing existing safety concepts, and propose ways
to expand its modeling premises in a data-driven fashion. Specifically, we show
that (i) existing predominant safety concepts can be embedded in the HJ
reachability framework, thereby enabling a common language for comparing and
contrasting modeling assumptions, and (ii) HJ reachability can serve as an
inductive bias to effectively reason, in a data-driven context, about two
critical, yet often overlooked aspects of safety: responsibility and
context-dependency.
",safety critical autonomous vehicle avs soon become pervasive society number safety concept trust av deployment recently propose throughout industry academia yet agree upon appropriate safety concept still elusive task paper advocate use hamilton jacobi hj reachability unify mathematical framework compare exist safety concept propose way expand modeling premise data drive fashion specifically show exist predominant safety concept embed hj reachability framework thereby enable common language compare contrast modeling assumption ii hj reachability serve inductive bias effectively reason data drive context two critical yet often overlook aspect safety responsibility context dependency
Local Graph Clustering Beyond Cheeger's Inequality,"  Motivated by applications of large-scale graph clustering, we study
random-walk-based LOCAL algorithms whose running times depend only on the size
of the output cluster, rather than the entire graph. All previously known such
algorithms guarantee an output conductance of $\tilde{O}(\sqrt{\phi(A)})$ when
the target set $A$ has conductance $\phi(A)\in[0,1]$. In this paper, we improve
it to $$\tilde{O}\bigg( \min\Big\{\sqrt{\phi(A)},
\frac{\phi(A)}{\sqrt{\mathsf{Conn}(A)}} \Big\} \bigg)\enspace, $$ where the
internal connectivity parameter $\mathsf{Conn}(A) \in [0,1]$ is defined as the
reciprocal of the mixing time of the random walk over the induced subgraph on
$A$.
  For instance, using $\mathsf{Conn}(A) = \Omega(\lambda(A) / \log n)$ where
$\lambda$ is the second eigenvalue of the Laplacian of the induced subgraph on
$A$, our conductance guarantee can be as good as
$\tilde{O}(\phi(A)/\sqrt{\lambda(A)})$. This builds an interesting connection
to the recent advance of the so-called improved Cheeger's Inequality [KKL+13],
which says that global spectral algorithms can provide a conductance guarantee
of $O(\phi_{\mathsf{opt}}/\sqrt{\lambda_3})$ instead of
$O(\sqrt{\phi_{\mathsf{opt}}})$.
  In addition, we provide theoretical guarantee on the clustering accuracy (in
terms of precision and recall) of the output set. We also prove that our
analysis is tight, and perform empirical evaluation to support our theory on
both synthetic and real data.
  It is worth noting that, our analysis outperforms prior work when the cluster
is well-connected. In fact, the better it is well-connected inside, the more
significant improvement (both in terms of conductance and accuracy) we can
obtain. Our results shed light on why in practice some random-walk-based
algorithms perform better than its previous theory, and help guide future
research about local clustering.
",motivated application large scale graph clustering study random walk base local algorithm whose running time depend size output cluster rather entire graph previously know algorithm guarantee output conductance target set conductance paper improve conn internal connectivity parameter conn define reciprocal mixing time random walk induce subgraph instance use conn n second eigenvalue laplacian induce subgraph conductance guarantee good build interesting connection recent advance so call improved cheeger inequality say global spectral algorithm provide conductance guarantee opt instead opt addition provide theoretical guarantee clustering accuracy term precision recall output set also prove analysis tight perform empirical evaluation support theory synthetic real datum worth note analysis outperform prior work cluster well connect fact well well connect inside significant improvement term conductance accuracy obtain result shed light practice random walk base algorithm perform well previous theory help guide future research local clustering
Reinforcement Learning in Reward-Mixing MDPs,"  Learning a near optimal policy in a partially observable system remains an
elusive challenge in contemporary reinforcement learning. In this work, we
consider episodic reinforcement learning in a reward-mixing Markov decision
process (MDP). There, a reward function is drawn from one of multiple possible
reward models at the beginning of every episode, but the identity of the chosen
reward model is not revealed to the agent. Hence, the latent state space, for
which the dynamics are Markovian, is not given to the agent. We study the
problem of learning a near optimal policy for two reward-mixing MDPs. Unlike
existing approaches that rely on strong assumptions on the dynamics, we make no
assumptions and study the problem in full generality. Indeed, with no further
assumptions, even for two switching reward-models, the problem requires several
new ideas beyond existing algorithmic and analysis techniques for efficient
exploration. We provide the first polynomial-time algorithm that finds an
$\epsilon$-optimal policy after exploring $\tilde{O}(poly(H,\epsilon^{-1})
\cdot S^2 A^2)$ episodes, where $H$ is time-horizon and $S, A$ are the number
of states and actions respectively. This is the first efficient algorithm that
does not require any assumptions in partially observed environments where the
observation space is smaller than the latent state space.
",learn near optimal policy partially observable system remain elusive challenge contemporary reinforcement learning work consider episodic reinforcement learn reward mix markov decision process mdp reward function draw one multiple possible reward model begin every episode identity choose reward model reveal agent hence latent state space dynamic markovian give agent study problem learn near optimal policy two reward mix mdps unlike exist approach rely strong assumption dynamic make assumption study problem full generality indeed assumption even two switch reward model problem require several new idea beyond exist algorithmic analysis technique efficient exploration provide first polynomial time algorithm find -optimal policy explore poly h -1 episode h time horizon number state action respectively first efficient algorithm require assumption partially observe environment observation space small latent state space
MS MARCO: Benchmarking Ranking Models in the Large-Data Regime,"  Evaluation efforts such as TREC, CLEF, NTCIR and FIRE, alongside public
leaderboard such as MS MARCO, are intended to encourage research and track our
progress, addressing big questions in our field. However, the goal is not
simply to identify which run is ""best"", achieving the top score. The goal is to
move the field forward by developing new robust techniques, that work in many
different settings, and are adopted in research and practice. This paper uses
the MS MARCO and TREC Deep Learning Track as our case study, comparing it to
the case of TREC ad hoc ranking in the 1990s. We show how the design of the
evaluation effort can encourage or discourage certain outcomes, and raising
questions about internal and external validity of results. We provide some
analysis of certain pitfalls, and a statement of best practices for avoiding
such pitfalls. We summarize the progress of the effort so far, and describe our
desired end state of ""robust usefulness"", along with steps that might be
required to get us there.
",evaluation effort trec clef ntcir fire alongside public leaderboard ms marco intend encourage research track progress address big question field however goal simply identify run well achieve top score goal move field forward develop new robust technique work many different setting adopt research practice paper use ms marco trec deep learn track case study compare case trec ad hoc rank 1990 show design evaluation effort encourage discourage certain outcome raise question internal external validity result provide analysis certain pitfall statement good practice avoid pitfall summarize progress effort far describe desire end state robust usefulness along step might require get we
Optimal Nonparametric Inference via Deep Neural Network,"  Deep neural network is a state-of-art method in modern science and
technology. Much statistical literature have been devoted to understanding its
performance in nonparametric estimation, whereas the results are suboptimal due
to a redundant logarithmic sacrifice. In this paper, we show that such
log-factors are not necessary. We derive upper bounds for the $L^2$ minimax
risk in nonparametric estimation. Sufficient conditions on network
architectures are provided such that the upper bounds become optimal (without
log-sacrifice). Our proof relies on an explicitly constructed network estimator
based on tensor product B-splines. We also derive asymptotic distributions for
the constructed network and a relating hypothesis testing procedure. The
testing procedure is further proven as minimax optimal under suitable network
architectures.
",deep neural network state of art method modern science technology much statistical literature devote understanding performance nonparametric estimation whereas result suboptimal due redundant logarithmic sacrifice paper show log factor necessary derive upper bound minimax risk nonparametric estimation sufficient condition network architecture provide upper bound become optimal without log sacrifice proof rely explicitly construct network estimator base tensor product b spline also derive asymptotic distribution construct network relate hypothesis testing procedure testing procedure prove minimax optimal suitable network architecture
"Improving Textual Network Embedding with Global Attention via Optimal
  Transport","  Constituting highly informative network embeddings is an important tool for
network analysis. It encodes network topology, along with other useful side
information, into low-dimensional node-based feature representations that can
be exploited by statistical modeling. This work focuses on learning
context-aware network embeddings augmented with text data. We reformulate the
network-embedding problem, and present two novel strategies to improve over
traditional attention mechanisms: ($i$) a content-aware sparse attention module
based on optimal transport, and ($ii$) a high-level attention parsing module.
Our approach yields naturally sparse and self-normalized relational inference.
It can capture long-term interactions between sequences, thus addressing the
challenges faced by existing textual network embedding schemes. Extensive
experiments are conducted to demonstrate our model can consistently outperform
alternative state-of-the-art methods.
",constitute highly informative network embedding important tool network analysis encode network topology along useful side information low dimensional node base feature representation exploit statistical modeling work focus learn context aware network embedding augment text datum reformulate network embed problem present two novel strategy improve traditional attention mechanism content aware sparse attention module base optimal transport ii high level attention parse module approach yield naturally sparse self normalize relational inference capture long term interaction sequence thus address challenge face exist textual network embed scheme extensive experiment conduct demonstrate model consistently outperform alternative state of the art method
"Foresee: Attentive Future Projections of Chaotic Road Environments with
  Online Training","  In this paper, we train a recurrent neural network to learn dynamics of a
chaotic road environment and to project the future of the environment on an
image. Future projection can be used to anticipate an unseen environment for
example, in autonomous driving. Road environment is highly dynamic and complex
due to the interaction among traffic participants such as vehicles and
pedestrians. Even in this complex environment, a human driver is efficacious to
safely drive on chaotic roads irrespective of the number of traffic
participants. The proliferation of deep learning research has shown the
efficacy of neural networks in learning this human behavior. In the same
direction, we investigate recurrent neural networks to understand the chaotic
road environment which is shared by pedestrians, vehicles (cars, trucks,
bicycles etc.), and sometimes animals as well. We propose \emph{Foresee}, a
unidirectional gated recurrent units (GRUs) network with attention to project
future of the environment in the form of images. We have collected several
videos on Delhi roads consisting of various traffic participants, background
and infrastructure differences (like 3D pedestrian crossing) at various times
on various days. We train \emph{Foresee} in an unsupervised way and we use
online training to project frames up to $0.5$ seconds in advance. We show that
our proposed model performs better than state of the art methods (prednet and
Enc. Dec. LSTM) and finally, we show that our trained model generalizes to a
public dataset for future projections.
",paper train recurrent neural network learn dynamic chaotic road environment project future environment image future projection use anticipate unseen environment example autonomous drive road environment highly dynamic complex due interaction among traffic participant vehicle pedestrian even complex environment human driver efficacious safely drive chaotic road irrespective number traffic participant proliferation deep learning research show efficacy neural network learn human behavior direction investigate recurrent neural network understand chaotic road environment share pedestrian vehicle car truck bicycle etc sometimes animal well propose foresee unidirectional gate recurrent unit grus network attention project future environment form image collect several video delhi road consist various traffic participant background infrastructure difference like 3d pedestrian crossing various time various day train foresee unsupervised way use online training project frame second advance show propose model perform well state art method prednet enc lstm finally show train model generalize public dataset future projection
"High-Resolution Peak Demand Estimation Using Generalized Additive Models
  and Deep Neural Networks","  This paper presents a method for estimating high-resolution electricity peak
demand given lower resolution data. The technique won a data competition
organized by the British distribution network operator Western Power
Distribution. The exercise was to estimate the minimum and maximum load values
in a single substation in a one-minute resolution as precisely as possible. In
contrast, the data was given in half-hourly and hourly resolutions. The winning
method combines generalized additive models (GAM) and deep artificial neural
networks (DNN) which are popular in load forecasting. We provide an extensive
analysis of the prediction models, including the importance of input parameters
with a focus on load, weather, and seasonal effects. In addition, we provide a
rigorous evaluation study that goes beyond the competition frame to analyze the
robustness. The results show that the proposed methods are superior, not only
in the single competition month but also in the meaningful evaluation study.
",paper present method estimate high resolution electricity peak demand give low resolution datum technique data competition organize british distribution network operator western power distribution exercise estimate minimum maximum load value single substation one minute resolution precisely possible contrast datum give half hourly hourly resolution win method combine generalize additive model gam deep artificial neural network dnn popular load forecasting provide extensive analysis prediction model include importance input parameter focus load weather seasonal effect addition provide rigorous evaluation study go beyond competition frame analyze robustness result show propose method superior single competition month also meaningful evaluation study
"Fully scalable online-preprocessing algorithm for short oligonucleotide
  microarray atlases","  Accumulation of standardized data collections is opening up novel
opportunities for holistic characterization of genome function. The limited
scalability of current preprocessing techniques has, however, formed a
bottleneck for full utilization of contemporary microarray collections. While
short oligonucleotide arrays constitute a major source of genome-wide profiling
data, scalable probe-level preprocessing algorithms have been available only
for few measurement platforms based on pre-calculated model parameters from
restricted reference training sets. To overcome these key limitations, we
introduce a fully scalable online-learning algorithm that provides tools to
process large microarray atlases including tens of thousands of arrays. Unlike
the alternatives, the proposed algorithm scales up in linear time with respect
to sample size and is readily applicable to all short oligonucleotide
platforms. This is the only available preprocessing algorithm that can learn
probe-level parameters based on sequential hyperparameter updates at small,
consecutive batches of data, thus circumventing the extensive memory
requirements of the standard approaches and opening up novel opportunities to
take full advantage of contemporary microarray data collections. Moreover,
using the most comprehensive data collections to estimate probe-level effects
can assist in pinpointing individual probes affected by various biases and
provide new tools to guide array design and quality control. The implementation
is freely available in R/Bioconductor at
http://www.bioconductor.org/packages/devel/bioc/html/RPA.html
",accumulation standardize data collection open novel opportunity holistic characterization genome function limited scalability current preprocessing technique however form bottleneck full utilization contemporary microarray collection short oligonucleotide arrays constitute major source genome wide profiling datum scalable probe level preprocessing algorithm available measurement platform base pre calculated model parameter restrict reference training set overcome key limitation introduce fully scalable online learn algorithm provide tool process large microarray atlas include ten thousand array unlike alternative propose algorithm scale linear time respect sample size readily applicable short oligonucleotide platform available preprocesse algorithm learn probe level parameter base sequential hyperparameter update small consecutive batch datum thus circumvent extensive memory requirement standard approach open novel opportunity take full advantage contemporary microarray datum collection moreover use comprehensive datum collection estimate probe level effect assist pinpoint individual probe affect various bias provide new tool guide array design quality control implementation freely available http
"Evaluating Generative Adversarial Networks on Explicitly Parameterized
  Distributions","  The true distribution parameterizations of commonly used image datasets are
inaccessible. Rather than designing metrics for feature spaces with unknown
characteristics, we propose to measure GAN performance by evaluating on
explicitly parameterized, synthetic data distributions. As a case study, we
examine the performance of 16 GAN variants on six multivariate distributions of
varying dimensionalities and training set sizes. In this learning environment,
we observe that: GANs exhibit similar performance trends across
dimensionalities; learning depends on the underlying distribution and its
complexity; the number of training samples can have a large impact on
performance; evaluation and relative comparisons are metric-dependent; diverse
sets of hyperparameters can produce a ""best"" result; and some GANs are more
robust to hyperparameter changes than others. These observations both
corroborate findings of previous GAN evaluation studies and make novel
contributions regarding the relationship between size, complexity, and GAN
performance.
",true distribution parameterization commonly use image dataset inaccessible rather design metric feature space unknown characteristic propose measure gin performance evaluate explicitly parameterize synthetic data distribution case study examine performance 16 gin variant six multivariate distribution vary dimensionality train set size learn environment observe gan exhibit similar performance trend across dimensionality learn depend underlie distribution complexity number training sample large impact performance evaluation relative comparison metric dependent diverse set hyperparameter produce good result gan robust hyperparameter change other observation corroborate finding previous gan evaluation study make novel contribution regard relationship size complexity gan performance
What's a good imputation to predict with missing values?,"  How to learn a good predictor on data with missing values? Most efforts focus
on first imputing as well as possible and second learning on the completed data
to predict the outcome. Yet, this widespread practice has no theoretical
grounding. Here we show that for almost all imputation functions, an
impute-then-regress procedure with a powerful learner is Bayes optimal. This
result holds for all missing-values mechanisms, in contrast with the classic
statistical results that require missing-at-random settings to use imputation
in probabilistic modeling. Moreover, it implies that perfect conditional
imputation is not needed for good prediction asymptotically. In fact, we show
that on perfectly imputed data the best regression function will generally be
discontinuous, which makes it hard to learn. Crafting instead the imputation so
as to leave the regression function unchanged simply shifts the problem to
learning discontinuous imputations. Rather, we suggest that it is easier to
learn imputation and regression jointly. We propose such a procedure, adapting
NeuMiss, a neural network capturing the conditional links across observed and
unobserved variables whatever the missing-value pattern. Experiments confirm
that joint imputation and regression through NeuMiss is better than various two
step procedures in our experiments with finite number of samples.
",learn good predictor datum miss value effort focus first impute well possible second learning complete datum predict outcome yet widespread practice theoretical grounding show almost imputation function impute then regress procedure powerful learner baye optimal result hold missing value mechanism contrast classic statistical result require miss at random setting use imputation probabilistic modeling moreover imply perfect conditional imputation need good prediction asymptotically fact show perfectly impute datum good regression function generally discontinuous make hard learn craft instead imputation leave regression function unchanged simply shift problem learn discontinuous imputation rather suggest easy learn imputation regression jointly propose procedure adapt neumiss neural network capture conditional link across observe unobserved variable whatever missing value pattern experiment confirm joint imputation regression neumiss well various two step procedure experiment finite number sample
"Text Line Segmentation for Challenging Handwritten Document Images Using
  Fully Convolutional Network","  This paper presents a method for text line segmentation of challenging
historical manuscript images. These manuscript images contain narrow interline
spaces with touching components, interpenetrating vowel signs and inconsistent
font types and sizes. In addition, they contain curved, multi-skewed and
multi-directed side note lines within a complex page layout. Therefore,
bounding polygon labeling would be very difficult and time consuming. Instead
we rely on line masks that connect the components on the same text line. Then
these line masks are predicted using a Fully Convolutional Network (FCN). In
the literature, FCN has been successfully used for text line segmentation of
regular handwritten document images. The present paper shows that FCN is useful
with challenging manuscript images as well. Using a new evaluation metric that
is sensitive to over segmentation as well as under segmentation, testing
results on a publicly available challenging handwritten dataset are comparable
with the results of a previous work on the same dataset.
",paper present method text line segmentation challenge historical manuscript image manuscript image contain narrow interline space touch component interpenetrate vowel sign inconsistent font type size addition contain curved multi skewed multi directed side note line within complex page layout therefore bound polygon labeling would difficult time consume instead rely line mask connect component text line line mask predict use fully convolutional network fcn literature fcn successfully use text line segmentation regular handwritten document image present paper show fcn useful challenging manuscript image well use new evaluation metric sensitive segmentation well segmentation testing result publicly available challenge handwritten dataset comparable result previous work dataset
"Electricity Load Forecasting -- An Evaluation of Simple 1D-CNN Network
  Structures","  This paper presents a convolutional neural network (CNN) which can be used
for forecasting electricity load profiles 36 hours into the future. In contrast
to well established CNN architectures, the input data is one-dimensional. A
parameter scanning of network parameters is conducted in order to gain
information about the influence of the kernel size, number of filters, and
dense size. The results show that a good forecast quality can already be
achieved with basic CNN architectures.The method works not only for smooth sum
loads of many hundred consumers, but also for the load of apartment buildings.
",paper present convolutional neural network cnn use forecast electricity load profile 36 hour future contrast well establish cnn architecture input datum one dimensional parameter scan network parameter conduct order gain information influence kernel size number filter dense size result show good forecast quality already achieve basic cnn method work smooth sum load many hundred consumer also load apartment building
"Sequential Density Estimation via NCWFAs Sequential Density Estimation
  via Nonlinear Continuous Weighted Finite Automata","  Weighted finite automata (WFAs) have been widely applied in many fields. One
of the classic problems for WFAs is probability distribution estimation over
sequences of discrete symbols. Although WFAs have been extended to deal with
continuous input data, namely continuous WFAs (CWFAs), it is still unclear how
to approximate density functions over sequences of continuous random variables
using WFA-based models, due to the limitation on the expressiveness of the
model as well as the tractability of approximating density functions via CWFAs.
In this paper, we propose a nonlinear extension to the CWFA model to first
improve its expressiveness, we refer to it as the nonlinear continuous WFAs
(NCWFAs). Then we leverage the so-called RNADE method, which is a well-known
density estimator based on neural networks, and propose the RNADE-NCWFA model.
The RNADE-NCWFA model computes a density function by design. We show that this
model is strictly more expressive than the Gaussian HMM model, which CWFA
cannot approximate. Empirically, we conduct a synthetic experiment using
Gaussian HMM generated data. We focus on evaluating the model's ability to
estimate densities for sequences of varying lengths (longer length than the
training data). We observe that our model performs the best among the compared
baseline methods.
",weight finite automata wfas widely apply many field one classic problem wfas probability distribution estimation sequence discrete symbol although wfas extend deal continuous input datum namely continuous wfas cwfas still unclear approximate density function sequence continuous random variable use wfa base model due limitation expressiveness model well tractability approximate density function via cwfas paper propose nonlinear extension cwfa model first improve expressiveness refer nonlinear continuous wfas ncwfas leverage so call rnade method well know density estimator base neural network propose rnade ncwfa model rnade ncwfa model compute density function design show model strictly expressive gaussian hmm model cwfa approximate empirically conduct synthetic experiment use gaussian hmm generate datum focus evaluate model ability estimate density sequence vary length long length training datum observe model perform good among compare baseline method
Variance Reduction with Sparse Gradients,"  Variance reduction methods such as SVRG and SpiderBoost use a mixture of
large and small batch gradients to reduce the variance of stochastic gradients.
Compared to SGD, these methods require at least double the number of operations
per update to model parameters. To reduce the computational cost of these
methods, we introduce a new sparsity operator: The random-top-k operator. Our
operator reduces computational complexity by estimating gradient sparsity
exhibited in a variety of applications by combining the top-k operator and the
randomized coordinate descent operator. With this operator, large batch
gradients offer an extra benefit beyond variance reduction: A reliable estimate
of gradient sparsity. Theoretically, our algorithm is at least as good as the
best algorithm (SpiderBoost), and further excels in performance whenever the
random-top-k operator captures gradient sparsity. Empirically, our algorithm
consistently outperforms SpiderBoost using various models on various tasks
including image classification, natural language processing, and sparse matrix
factorization. We also provide empirical evidence to support the intuition
behind our algorithm via a simple gradient entropy computation, which serves to
quantify gradient sparsity at every iteration.
",variance reduction method svrg spiderboost use mixture large small batch gradient reduce variance stochastic gradient compare sgd method require least double number operation per update model parameter reduce computational cost method introduce new sparsity operator random top k operator operator reduce computational complexity estimate gradient sparsity exhibit variety application combine top k operator randomize coordinate descent operator operator large batch gradient offer extra benefit beyond variance reduction reliable estimate gradient sparsity theoretically algorithm least good good algorithm spiderboost excel performance whenever random top k operator capture gradient sparsity empirically algorithm consistently outperform spiderboost use various model various task include image classification natural language processing sparse matrix factorization also provide empirical evidence support intuition behind algorithm via simple gradient entropy computation serve quantify gradient sparsity every iteration
"Adaptive template systems: Data-driven feature selection for learning
  with persistence diagrams","  Feature extraction from persistence diagrams, as a tool to enrich machine
learning techniques, has received increasing attention in recent years. In this
paper we explore an adaptive methodology to localize features in persistent
diagrams, which are then used in learning tasks. Specifically, we investigate
three algorithms, CDER, GMM and HDBSCAN, to obtain adaptive template
functions/features. Said features are evaluated in three classification
experiments with persistence diagrams. Namely, manifold, human shapes and
protein classification. The main conclusion of our analysis is that adaptive
template systems, as a feature extraction technique, yield competitive and
often superior results in the studied examples. Moreover, from the adaptive
algorithms here studied, CDER consistently provides the most reliable and
robust adaptive featurization.
",feature extraction persistence diagram tool enrich machine learn technique receive increase attention recent year paper explore adaptive methodology localize feature persistent diagram use learn task specifically investigate three algorithm cder gmm hdbscan obtain adaptive template say feature evaluate three classification experiment persistence diagram namely manifold human shape protein classification main conclusion analysis adaptive template system feature extraction technique yield competitive often superior result study example moreover adaptive algorithm study cder consistently provide reliable robust adaptive featurization
Exploring Generalization in Deep Learning,"  With a goal of understanding what drives generalization in deep networks, we
consider several recently suggested explanations, including norm-based control,
sharpness and robustness. We study how these measures can ensure
generalization, highlighting the importance of scale normalization, and making
a connection between sharpness and PAC-Bayes theory. We then investigate how
well the measures explain different observed phenomena.
",goal understand drive generalization deep network consider several recently suggest explanation include norm base control sharpness robustness study measure ensure generalization highlight importance scale normalization make connection sharpness pac bayes theory investigate well measure explain different observed phenomenon
"A Joint Two-Phase Time-Sensitive Regularized Collaborative Ranking Model
  for Point of Interest Recommendation","  The popularity of location-based social networks (LBSNs) has led to a
tremendous amount of user check-in data. Recommending points of interest (POIs)
plays a key role in satisfying users' needs in LBSNs. While recent work has
explored the idea of adopting collaborative ranking (CR) for recommendation,
there have been few attempts to incorporate temporal information for POI
recommendation using CR. In this article, we propose a two-phase CR algorithm
that incorporates the geographical influence of POIs and is regularized based
on the variance of POIs popularity and users' activities over time. The
time-sensitive regularizer penalizes user and POIs that have been more
time-sensitive in the past, helping the model to account for their long-term
behavioral patterns while learning from user-POI interactions. Moreover, in the
first phase, it attempts to rank visited POIs higher than the unvisited ones,
and at the same time, apply the geographical influence. In the second phase,
our algorithm tries to rank users' favorite POIs higher on the recommendation
list. Both phases employ a collaborative learning strategy that enables the
model to capture complex latent associations from two different perspectives.
Experiments on real-world datasets show that our proposed time-sensitive
collaborative ranking model beats state-of-the-art POI recommendation methods.
",popularity location base social network lbsns lead tremendous amount user check in datum recommend point interest pois play key role satisfy user need lbsns recent work explore idea adopt collaborative rank cr recommendation attempt incorporate temporal information poi recommendation use cr article propose two phase cr algorithm incorporate geographical influence pois regularize base variance pois popularity user activity time time sensitive regularizer penalize user pois time sensitive past help model account long term behavioral pattern learn user poi interaction moreover first phase attempt rank visit pois high unvisited one time apply geographical influence second phase algorithm try rank user favorite pois high recommendation list phase employ collaborative learning strategy enable model capture complex latent association two different perspective experiment real world dataset show propose time sensitive collaborative ranking model beat state of the art poi recommendation method
Shortcomings of Counterfactual Fairness and a Proposed Modification,"  In this paper, I argue that counterfactual fairness does not constitute a
necessary condition for an algorithm to be fair, and subsequently suggest how
the constraint can be modified in order to remedy this shortcoming. To this
end, I discuss a hypothetical scenario in which counterfactual fairness and an
intuitive judgment of fairness come apart. Then, I turn to the question how the
concept of discrimination can be explicated in order to examine the
shortcomings of counterfactual fairness as a necessary condition of algorithmic
fairness in more detail. I then incorporate the insights of this analysis into
a novel fairness constraint, causal relevance fairness, which is a modification
of the counterfactual fairness constraint that seems to circumvent its
shortcomings.
",paper argue counterfactual fairness constitute necessary condition algorithm fair subsequently suggest constraint modify order remedy shortcoming end discuss hypothetical scenario counterfactual fairness intuitive judgment fairness come apart turn question concept discrimination explicate order examine shortcoming counterfactual fairness necessary condition algorithmic fairness detail incorporate insight analysis novel fairness constraint causal relevance fairness modification counterfactual fairness constraint seem circumvent shortcoming
Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space,"  When trained effectively, the Variational Autoencoder (VAE) can be both a
powerful generative model and an effective representation learning framework
for natural language. In this paper, we propose the first large-scale language
VAE model, Optimus. A universal latent embedding space for sentences is first
pre-trained on large text corpus, and then fine-tuned for various language
generation and understanding tasks. Compared with GPT-2, Optimus enables guided
language generation from an abstract level using the latent vectors. Compared
with BERT, Optimus can generalize better on low-resource language understanding
tasks due to the smooth latent space structure. Extensive experimental results
on a wide range of language tasks demonstrate the effectiveness of Optimus. It
achieves new state-of-the-art on VAE language modeling benchmarks. We hope that
our first pre-trained big VAE language model itself and results can help the
NLP community renew the interests of deep generative models in the era of
large-scale pre-training, and make these principled methods more practical.
",train effectively variational autoencoder vae powerful generative model effective representation learn framework natural language paper propose first large scale language vae model optimus universal latent embed space sentence first pre train large text corpus fine tune various language generation understand task compare gpt-2 optimus enable guide language generation abstract level use latent vector compare bert optimus generalize well low resource language understand task due smooth latent space structure extensive experimental result wide range language task demonstrate effectiveness optimus achieve new state of the art vae language modeling benchmark hope first pre train big vae language model result help nlp community renew interest deep generative model era large scale pre training make principled method practical
"Can Machine Learning Model with Static Features be Fooled: an
  Adversarial Machine Learning Approach","  The widespread adoption of smartphones dramatically increases the risk of
attacks and the spread of mobile malware, especially on the Android platform.
Machine learning-based solutions have been already used as a tool to supersede
signature-based anti-malware systems. However, malware authors leverage
features from malicious and legitimate samples to estimate statistical
difference in-order to create adversarial examples. Hence, to evaluate the
vulnerability of machine learning algorithms in malware detection, we propose
five different attack scenarios to perturb malicious applications (apps). By
doing this, the classification algorithm inappropriately fits the discriminant
function on the set of data points, eventually yielding a higher
misclassification rate. Further, to distinguish the adversarial examples from
benign samples, we propose two defense mechanisms to counter attacks. To
validate our attacks and solutions, we test our model on three different
benchmark datasets. We also test our methods using various classifier
algorithms and compare them with the state-of-the-art data poisoning method
using the Jacobian matrix. Promising results show that generated adversarial
samples can evade detection with a very high probability. Additionally, evasive
variants generated by our attack models when used to harden the developed
anti-malware system improves the detection rate up to 50% when using the
Generative Adversarial Network (GAN) method.
",widespread adoption smartphone dramatically increase risk attack spread mobile malware especially android platform machine learning base solution already use tool supersede signature base anti malware system however malware author leverage feature malicious legitimate sample estimate statistical difference in order create adversarial example hence evaluate vulnerability machine learning algorithm malware detection propose five different attack scenario perturb malicious application app classification algorithm inappropriately fit discriminant function set datum point eventually yield high misclassification rate distinguish adversarial example benign sample propose two defense mechanism counter attack validate attack solution test model three different benchmark dataset also test method use various classifier algorithm compare state of the art datum poisoning method use jacobian matrix promising result show generate adversarial sample evade detection high probability additionally evasive variant generate attack model use harden develop anti malware system improve detection rate 50 use generative adversarial network gan method
Generation of Paths in a Maze using a Deep Network without Learning,"  Trajectory- or path-planning is a fundamental issue in a wide variety of
applications. Here we show that it is possible to solve path planning for
multiple start- and end-points highly efficiently with a network that consists
only of max pooling layers, for which no network training is needed. Different
from competing approaches, very large mazes containing more than half a billion
nodes with dense obstacle configuration and several thousand path end-points
can this way be solved in very short time on parallel hardware.
",trajectory- path plan fundamental issue wide variety application show possible solve path planning multiple start- end point highly efficiently network consist max pooling layer network training need different competing approach large maze contain half billion node dense obstacle configuration several thousand path end point way solve short time parallel hardware
"Statistical limits of dictionary learning: random matrix theory and the
  spectral replica method","  We consider increasingly complex models of matrix denoising and dictionary
learning in the Bayes-optimal setting, in the challenging regime where the
matrices to infer have a rank growing linearly with the system size. This is in
contrast with most existing literature concerned with the low-rank (i.e.,
constant-rank) regime. We first consider a class of rotationally invariant
matrix denoising problems whose mutual information and minimum mean-square
error are computable using techniques from random matrix theory. Next, we
analyze the more challenging models of dictionary learning. To do so we
introduce a novel combination of the replica method from statistical mechanics
together with random matrix theory, coined spectral replica method. This allows
us to derive variational formulas for the mutual information between hidden
representations and the noisy data of the dictionary learning problem, as well
as for the overlaps quantifying the optimal reconstruction error. The proposed
method reduces the number of degrees of freedom from $\Theta(N^2)$ matrix
entries to $\Theta(N)$ eigenvalues (or singular values), and yields Coulomb gas
representations of the mutual information which are reminiscent of matrix
models in physics. The main ingredients are a combination of large deviation
results for random matrices together with a new replica symmetric decoupling
ansatz at the level of the probability distributions of eigenvalues (or
singular values) of certain overlap matrices and the use of
HarishChandra-Itzykson-Zuber spherical integrals.
",consider increasingly complex model matrix denoise dictionary learn bayes optimal setting challenge regime matrix infer rank grow linearly system size contrast exist literature concern low rank constant rank regime first consider class rotationally invariant matrix denoising problem whose mutual information minimum mean square error computable use technique random matrix theory next analyze challenge model dictionary learning introduce novel combination replica method statistical mechanic together random matrix theory coin spectral replica method allow we derive variational formula mutual information hide representation noisy datum dictionary learning problem well overlap quantify optimal reconstruction error propose method reduce number degree freedom matrix entry n eigenvalue singular value yield coulomb gas representation mutual information reminiscent matrix model physics main ingredient combination large deviation result random matrix together new replica symmetric decouple ansatz level probability distribution eigenvalue singular value certain overlap matrix use harishchandra itzykson zuber spherical integral
"Video Affective Effects Prediction with Multi-modal Fusion and Shot-Long
  Temporal Context","  Predicting the emotional impact of videos using machine learning is a
challenging task considering the varieties of modalities, the complicated
temporal contex of the video as well as the time dependency of the emotional
states. Feature extraction, multi-modal fusion and temporal context fusion are
crucial stages for predicting valence and arousal values in the emotional
impact, but have not been successfully exploited. In this paper, we propose a
comprehensive framework with novel designs of modal structure and multi-modal
fusion strategy. We select the most suitable modalities for valence and arousal
tasks respectively and each modal feature is extracted using the
modality-specific pre-trained deep model on large generic dataset.
Two-time-scale structures, one for the intra-clip and the other for the
inter-clip, are proposed to capture the temporal dependency of video content
and emotion states. To combine the complementary information from multiple
modalities, an effective and efficient residual-based progressive training
strategy is proposed. Each modality is step-wisely combined into the
multi-modal model, responsible for completing the missing parts of features.
With all those improvements above, our proposed prediction framework achieves
better performance on the LIRIS-ACCEDE dataset with a large margin compared to
the state-of-the-art.
",predict emotional impact video use machine learn challenge task consider variety modality complicated temporal contex video well time dependency emotional state feature extraction multi modal fusion temporal context fusion crucial stage predict valence arousal value emotional impact successfully exploit paper propose comprehensive framework novel design modal structure multi modal fusion strategy select suitable modality valence arousal task respectively modal feature extract use modality specific pre trained deep model large generic dataset two time scale structure one intra clip inter clip propose capture temporal dependency video content emotion state combine complementary information multiple modality effective efficient residual base progressive training strategy propose modality step wisely combine multi modal model responsible completing miss part feature improvement propose prediction framework achieve well performance liris accede dataset large margin compare state of the art
"Multi-objective Model-based Policy Search for Data-efficient Learning
  with Sparse Rewards","  The most data-efficient algorithms for reinforcement learning in robotics are
model-based policy search algorithms, which alternate between learning a
dynamical model of the robot and optimizing a policy to maximize the expected
return given the model and its uncertainties. However, the current algorithms
lack an effective exploration strategy to deal with sparse or misleading reward
scenarios: if they do not experience any state with a positive reward during
the initial random exploration, it is very unlikely to solve the problem. Here,
we propose a novel model-based policy search algorithm, Multi-DEX, that
leverages a learned dynamical model to efficiently explore the task space and
solve tasks with sparse rewards in a few episodes. To achieve this, we frame
the policy search problem as a multi-objective, model-based policy optimization
problem with three objectives: (1) generate maximally novel state trajectories,
(2) maximize the expected return and (3) keep the system in state-space regions
for which the model is as accurate as possible. We then optimize these
objectives using a Pareto-based multi-objective optimization algorithm. The
experiments show that Multi-DEX is able to solve sparse reward scenarios (with
a simulated robotic arm) in much lower interaction time than VIME, TRPO,
GEP-PG, CMA-ES and Black-DROPS.
",data efficient algorithm reinforcement learning robotic model base policy search algorithm alternate learn dynamical model robot optimize policy maximize expect return give model uncertainty however current algorithm lack effective exploration strategy deal sparse mislead reward scenario experience state positive reward initial random exploration unlikely solve problem propose novel model base policy search algorithm multi dex leverage learn dynamical model efficiently explore task space solve task sparse reward episode achieve frame policy search problem multi objective model base policy optimization problem three objective 1 generate maximally novel state trajectory 2 maximize expect return 3 keep system state space region model accurate possible optimize objective use pareto base multi objective optimization algorithm experiment show multi dex able solve sparse reward scenario simulate robotic arm much low interaction time vime trpo gep pg cma es black drop
Integrating LSTMs and GNNs for COVID-19 Forecasting,"  The spread of COVID-19 has coincided with the rise of Graph Neural Networks
(GNNs), leading to several studies proposing their use to better forecast the
evolution of the pandemic. Many such models also include Long Short Term Memory
(LSTM) networks, a common tool for time series forecasting. In this work, we
further investigate the integration of these two methods by implementing GNNs
within the gates of an LSTM and exploiting spatial information. In addition, we
introduce a skip connection which proves critical to jointly capture the
spatial and temporal patterns in the data. We validate our daily COVID-19 new
cases forecast model on data of 37 European nations for the last 472 days and
show superior performance compared to state-of-the-art graph time series models
based on mean absolute scaled error (MASE). This area of research has important
applications to policy-making and we analyze its potential for pandemic
resource control.
",spread covid-19 coincide rise graph neural network gnn lead several study propose use well forecast evolution pandemic many model also include long short term memory lstm network common tool time series forecasting work investigate integration two method implement gnn within gate lstm exploit spatial information addition introduce skip connection prove critical jointly capture spatial temporal pattern datum validate daily covid-19 new case forecast model datum 37 european nation last 472 day show superior performance compare state of the art graph time series model base mean absolute scale error mase area research important application policy making analyze potential pandemic resource control
Estimating Gaussian Copulas with Missing Data,"  In this work we present a rigorous application of the Expectation
Maximization algorithm to determine the marginal distributions and the
dependence structure in a Gaussian copula model with missing data. We further
show how to circumvent a priori assumptions on the marginals with
semiparametric modelling. The joint distribution learned through this algorithm
is considerably closer to the underlying distribution than existing methods.
",work present rigorous application expectation maximization algorithm determine marginal distribution dependence structure gaussian copula model miss datum show circumvent priori assumption marginal semiparametric modelling joint distribution learn algorithm considerably close underlying distribution exist method
"Verification of Image-based Neural Network Controllers Using Generative
  Models","  Neural networks are often used to process information from image-based
sensors to produce control actions. While they are effective for this task, the
complex nature of neural networks makes their output difficult to verify and
predict, limiting their use in safety-critical systems. For this reason, recent
work has focused on combining techniques in formal methods and reachability
analysis to obtain guarantees on the closed-loop performance of neural network
controllers. However, these techniques do not scale to the high-dimensional and
complicated input space of image-based neural network controllers. In this
work, we propose a method to address these challenges by training a generative
adversarial network (GAN) to map states to plausible input images. By
concatenating the generator network with the control network, we obtain a
network with a low-dimensional input space. This insight allows us to use
existing closed-loop verification tools to obtain formal guarantees on the
performance of image-based controllers. We apply our approach to provide safety
guarantees for an image-based neural network controller for an autonomous
aircraft taxi problem. We guarantee that the controller will keep the aircraft
on the runway and guide the aircraft towards the center of the runway. The
guarantees we provide are with respect to the set of input images modeled by
our generator network, so we provide a recall metric to evaluate how well the
generator captures the space of plausible images.
",neural network often use process information image base sensor produce control action effective task complex nature neural network make output difficult verify predict limit use safety critical system reason recent work focus combine technique formal method reachability analysis obtain guarantee closed loop performance neural network controller however technique scale high dimensional complicated input space image base neural network controller work propose method address challenge train generative adversarial network gin map state plausible input image concatenate generator network control network obtain network low dimensional input space insight allow we use exist closed loop verification tool obtain formal guarantee performance image base controller apply approach provide safety guarantee image base neural network controller autonomous aircraft taxi problem guarantee controller keep aircraft runway guide aircraft towards center runway guarantee provide respect set input image model generator network provide recall metric evaluate well generator capture space plausible image
"An FPGA-Based On-Device Reinforcement Learning Approach using Online
  Sequential Learning","  DQN (Deep Q-Network) is a method to perform Q-learning for reinforcement
learning using deep neural networks. DQNs require a large buffer and batch
processing for an experience replay and rely on a backpropagation based
iterative optimization, making them difficult to be implemented on
resource-limited edge devices. In this paper, we propose a lightweight
on-device reinforcement learning approach for low-cost FPGA devices. It
exploits a recently proposed neural-network based on-device learning approach
that does not rely on the backpropagation method but uses OS-ELM (Online
Sequential Extreme Learning Machine) based training algorithm. In addition, we
propose a combination of L2 regularization and spectral normalization for the
on-device reinforcement learning so that output values of the neural network
can be fit into a certain range and the reinforcement learning becomes stable.
The proposed reinforcement learning approach is designed for PYNQ-Z1 board as a
low-cost FPGA platform. The evaluation results using OpenAI Gym demonstrate
that the proposed algorithm and its FPGA implementation complete a CartPole-v0
task 29.77x and 89.40x faster than a conventional DQN-based approach when the
number of hidden-layer nodes is 64.
",dqn deep q network method perform q learn reinforcement learning use deep neural network dqns require large buffer batch processing experience replay rely backpropagation base iterative optimization make difficult implement resource limit edge device paper propose lightweight on device reinforcement learn approach low cost fpga device exploit recently propose neural network base on device learn approach rely backpropagation method use os elm online sequential extreme learning machine base training algorithm addition propose combination l2 regularization spectral normalization on device reinforcement learn output value neural network fit certain range reinforcement learning become stable propose reinforcement learning approach design pynq z1 board low cost fpga platform evaluation result use openai gym demonstrate propose algorithm fpga implementation complete cartpole v0 task fast conventional dqn base approach number hide layer node 64
Novel Uncertainty Framework for Deep Learning Ensembles,"  Deep neural networks have become the default choice for many of the machine
learning tasks such as classification and regression. Dropout, a method
commonly used to improve the convergence of deep neural networks, generates an
ensemble of thinned networks with extensive weight sharing. Recent studies that
dropout can be viewed as an approximate variational inference in Gaussian
processes, and used as a practical tool to obtain uncertainty estimates of the
network. We propose a novel statistical mechanics based framework to dropout
and use this framework to propose a new generic algorithm that focuses on
estimates of the variance of the loss as measured by the ensemble of thinned
networks. Our approach can be applied to a wide range of deep neural network
architectures and machine learning tasks. In classification, this algorithm
allows the generation of a don't-know answer to be generated, which can
increase the reliability of the classifier. Empirically we demonstrate
state-of-the-art AUC results on publicly available benchmarks.
",deep neural network become default choice many machine learn task classification regression dropout method commonly use improve convergence deep neural network generate ensemble thin network extensive weight share recent study dropout view approximate variational inference gaussian process use practical tool obtain uncertainty estimate network propose novel statistical mechanic base framework dropout use framework propose new generic algorithm focus estimate variance loss measure ensemble thin network approach apply wide range deep neural network architecture machine learn task classification algorithm allow generation answer generate increase reliability classifier empirically demonstrate state of the art auc result publicly available benchmark
"PPSGCN: A Privacy-Preserving Subgraph Sampling Based Distributed GCN
  Training Method","  Graph convolutional networks (GCNs) have been widely adopted for graph
representation learning and achieved impressive performance. For larger graphs
stored separately on different clients, distributed GCN training algorithms
were proposed to improve efficiency and scalability. However, existing methods
directly exchange node features between different clients, which results in
data privacy leakage. Federated learning was incorporated in graph learning to
tackle data privacy, while they suffer from severe performance drop due to
non-iid data distribution. Besides, these approaches generally involve heavy
communication and memory overhead during the training process. In light of
these problems, we propose a Privacy-Preserving Subgraph sampling based
distributed GCN training method (PPSGCN), which preserves data privacy and
significantly cuts back on communication and memory overhead. Specifically,
PPSGCN employs a star-topology client-server system. We firstly sample a local
node subset in each client to form a global subgraph, which greatly reduces
communication and memory costs. We then conduct local computation on each
client with features or gradients of the sampled nodes. Finally, all clients
securely communicate with the central server with homomorphic encryption to
combine local results while preserving data privacy. Compared with federated
graph learning methods, our PPSGCN model is trained on a global graph to avoid
the negative impact of local data distribution. We prove that our PPSGCN
algorithm would converge to a local optimum with probability 1. Experiment
results on three prevalent benchmarks demonstrate that our algorithm
significantly reduces communication and memory overhead while maintaining
desirable performance. Further studies not only demonstrate the fast
convergence of PPSGCN, but discuss the trade-off between communication and
local computation cost as well.
",graph convolutional network gcns widely adopt graph representation learning achieve impressive performance large graph store separately different client distribute gcn training algorithm propose improve efficiency scalability however exist method directly exchange node feature different client result datum privacy leakage federate learning incorporate graph learning tackle datum privacy suffer severe performance drop due non iid datum distribution besides approach generally involve heavy communication memory overhead training process light problem propose privacy preserve subgraph sampling base distribute gcn training method ppsgcn preserve datum privacy significantly cut back communication memory overhead specifically ppsgcn employs star topology client server system firstly sample local node subset client form global subgraph greatly reduce communication memory cost conduct local computation client feature gradient sample node finally client securely communicate central server homomorphic encryption combine local result preserve datum privacy compare federate graph learn method ppsgcn model train global graph avoid negative impact local datum distribution prove ppsgcn algorithm would converge local optimum probability experiment result three prevalent benchmark demonstrate algorithm significantly reduce communication memory overhead maintain desirable performance study demonstrate fast convergence ppsgcn discuss trade off communication local computation cost well
Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching,"  Data Poisoning attacks modify training data to maliciously control a model
trained on such data. In this work, we focus on targeted poisoning attacks
which cause a reclassification of an unmodified test image and as such breach
model integrity. We consider a particularly malicious poisoning attack that is
both ""from scratch"" and ""clean label"", meaning we analyze an attack that
successfully works against new, randomly initialized models, and is nearly
imperceptible to humans, all while perturbing only a small fraction of the
training data. Previous poisoning attacks against deep neural networks in this
setting have been limited in scope and success, working only in simplified
settings or being prohibitively expensive for large datasets. The central
mechanism of the new attack is matching the gradient direction of malicious
examples. We analyze why this works, supplement with practical considerations.
and show its threat to real-world practitioners, finding that it is the first
poisoning method to cause targeted misclassification in modern deep networks
trained from scratch on a full-sized, poisoned ImageNet dataset. Finally we
demonstrate the limitations of existing defensive strategies against such an
attack, concluding that data poisoning is a credible threat, even for
large-scale deep learning systems.
",data poisoning attack modify training datum maliciously control model train datum work focus target poisoning attack cause reclassification unmodified test image breach model integrity consider particularly malicious poison attack scratch clean label meaning analyze attack successfully work new randomly initialize model nearly imperceptible human perturb small fraction training datum previous poisoning attack deep neural network set limited scope success work simplified setting prohibitively expensive large dataset central mechanism new attack matching gradient direction malicious example analyze work supplement practical consideration show threat real world practitioner find first poison method cause target misclassification modern deep network train scratch full sized poison imagenet dataset finally demonstrate limitation exist defensive strategy attack conclude datum poison credible threat even large scale deep learning system
"A comparative study between vision transformers and CNNs in digital
  pathology","  Recently, vision transformers were shown to be capable of outperforming
convolutional neural networks when pretrained on sufficient amounts of data. In
comparison to convolutional neural networks, vision transformers have a weaker
inductive bias and therefore allow a more flexible feature detection. Due to
their promising feature detection, this work explores vision transformers for
tumor detection in digital pathology whole slide images in four tissue types,
and for tissue type identification. We compared the patch-wise classification
performance of the vision transformer DeiT-Tiny to the state-of-the-art
convolutional neural network ResNet18. Due to the sparse availability of
annotated whole slide images, we further compared both models pretrained on
large amounts of unlabeled whole-slide images using state-of-the-art
self-supervised approaches. The results show that the vision transformer
performed slightly better than the ResNet18 for three of four tissue types for
tumor detection while the ResNet18 performed slightly better for the remaining
tasks. The aggregated predictions of both models on slide level were
correlated, indicating that the models captured similar imaging features. All
together, the vision transformer models performed on par with the ResNet18
while requiring more effort to train. In order to surpass the performance of
convolutional neural networks, vision transformers might require more
challenging tasks to benefit from their weak inductive bias.
",recently vision transformer show capable outperform convolutional neural network pretraine sufficient amount datum comparison convolutional neural network vision transformer weak inductive bias therefore allow flexible feature detection due promising feature detection work explore vision transformer tumor detection digital pathology whole slide image four tissue type tissue type identification compare patch wise classification performance vision transformer deit tiny state of the art convolutional neural network resnet18 due sparse availability annotate whole slide image compare model pretraine large amount unlabeled whole slide image use state of the art self supervise approach result show vision transformer perform slightly well resnet18 three four tissue type tumor detection resnet18 perform slightly well remaining task aggregate prediction model slide level correlate indicating model capture similar imaging feature together vision transformer model perform par resnet18 require effort train order surpass performance convolutional neural network vision transformer might require challenging task benefit weak inductive bias
"Revisiting Adversarial Autoencoder for Unsupervised Word Translation
  with Cycle Consistency and Improved Training","  Adversarial training has shown impressive success in learning bilingual
dictionary without any parallel data by mapping monolingual embeddings to a
shared space. However, recent work has shown superior performance for
non-adversarial methods in more challenging language pairs. In this work, we
revisit adversarial autoencoder for unsupervised word translation and propose
two novel extensions to it that yield more stable training and improved
results. Our method includes regularization terms to enforce cycle consistency
and input reconstruction, and puts the target encoders as an adversary against
the corresponding discriminator. Extensive experimentations with European,
non-European and low-resource languages show that our method is more robust and
achieves better performance than recently proposed adversarial and
non-adversarial approaches.
",adversarial training show impressive success learn bilingual dictionary without parallel datum mapping monolingual embedding share space however recent work show superior performance non adversarial method challenge language pair work revisit adversarial autoencoder unsupervised word translation propose two novel extension yield stable training improve result method include regularization term enforce cycle consistency input reconstruction put target encoder adversary correspond discriminator extensive experimentation european non european low resource language show method robust achieve well performance recently propose adversarial non adversarial approach
Generalised Random Forest Space Overview,"  Assuming a view of the Random Forest as a special case of a nested ensemble
of interchangeable modules, we construct a generalisation space allowing one to
easily develop novel methods based on this algorithm. We discuss the role and
required properties of modules at each level, especially in context of some
already proposed RF generalisations.
",assume view random forest special case nest ensemble interchangeable module construct generalisation space allow one easily develop novel method base algorithm discuss role require property module level especially context already propose rf generalisation
"Superpixel-based Knowledge Infusion in Deep Neural Networks for Image
  Classification","  Superpixels are higher-order perceptual groups of pixels in an image, often
carrying much more information than the raw pixels. There is an inherent
relational structure to the relationship among different superpixels of an
image such as adjacent superpixels are neighbours of each other. Our interest
here is to treat these relative positions of various superpixels as relational
information of an image. This relational information can convey higher-order
spatial information about the image, such as the relationship between
superpixels representing two eyes in an image of a cat. That is, two eyes are
placed adjacent to each other in a straight line or the mouth is below the
nose. Our motive in this paper is to assist computer vision models,
specifically those based on Deep Neural Networks (DNNs), by incorporating this
higher-order information from superpixels. We construct a hybrid model that
leverages (a) Convolutional Neural Network (CNN) to deal with spatial
information in an image and (b) Graph Neural Network (GNN) to deal with
relational superpixel information in the image. The proposed model is learned
using a generic hybrid loss function. Our experiments are extensive, and we
evaluate the predictive performance of our proposed hybrid vision model on
seven different image classification datasets from a variety of domains such as
digit and object recognition, biometrics, medical imaging. The results
demonstrate that the relational superpixel information processed by a GNN can
improve the performance of a standard CNN-based vision system.
",superpixels high order perceptual group pixel image often carry much information raw pixel inherent relational structure relationship among different superpixel image adjacent superpixel neighbour interest treat relative position various superpixel relational information image relational information convey high order spatial information image relationship superpixel represent two eye image cat two eye place adjacent straight line mouth nose motive paper assist computer vision model specifically base deep neural network dnn incorporate high order information superpixel construct hybrid model leverage convolutional neural network cnn deal spatial information image b graph neural network gnn deal relational superpixel information image propose model learn use generic hybrid loss function experiment extensive evaluate predictive performance propose hybrid vision model seven different image classification dataset variety domain digit object recognition biometric medical imaging result demonstrate relational superpixel information process gnn improve performance standard cnn base vision system
SWIFT: Scalable Wasserstein Factorization for Sparse Nonnegative Tensors,"  Existing tensor factorization methods assume that the input tensor follows
some specific distribution (i.e. Poisson, Bernoulli, and Gaussian), and solve
the factorization by minimizing some empirical loss functions defined based on
the corresponding distribution. However, it suffers from several drawbacks: 1)
In reality, the underlying distributions are complicated and unknown, making it
infeasible to be approximated by a simple distribution. 2) The correlation
across dimensions of the input tensor is not well utilized, leading to
sub-optimal performance. Although heuristics were proposed to incorporate such
correlation as side information under Gaussian distribution, they can not
easily be generalized to other distributions. Thus, a more principled way of
utilizing the correlation in tensor factorization models is still an open
challenge. Without assuming any explicit distribution, we formulate the tensor
factorization as an optimal transport problem with Wasserstein distance, which
can handle non-negative inputs.
  We introduce SWIFT, which minimizes the Wasserstein distance that measures
the distance between the input tensor and that of the reconstruction. In
particular, we define the N-th order tensor Wasserstein loss for the widely
used tensor CP factorization and derive the optimization algorithm that
minimizes it. By leveraging sparsity structure and different equivalent
formulations for optimizing computational efficiency, SWIFT is as scalable as
other well-known CP algorithms. Using the factor matrices as features, SWIFT
achieves up to 9.65% and 11.31% relative improvement over baselines for
downstream prediction tasks. Under the noisy conditions, SWIFT achieves up to
15% and 17% relative improvements over the best competitors for the prediction
tasks.
",exist tensor factorization method assume input tensor follow specific distribution poisson bernoulli gaussian solve factorization minimize empirical loss function define base corresponding distribution however suffer several drawback 1 reality underlie distribution complicate unknown making infeasible approximate simple distribution 2 correlation across dimension input tensor well utilize lead sub optimal performance although heuristic propose incorporate correlation side information gaussian distribution easily generalize distribution thus principle way utilize correlation tensor factorization model still open challenge without assume explicit distribution formulate tensor factorization optimal transport problem wasserstein distance handle non negative input introduce swift minimize wasserstein distance measure distance input tensor reconstruction particular define n th order tensor wasserstein loss widely use tensor cp factorization derive optimization algorithm minimize leverage sparsity structure different equivalent formulation optimize computational efficiency swift scalable well know cp algorithm use factor matrix feature swift achieve relative improvement baseline downstream prediction task noisy condition swift achieve 15 17 relative improvement good competitor prediction task
WeNet: Weighted Networks for Recurrent Network Architecture Search,"  In recent years, there has been increasing demand for automatic architecture
search in deep learning. Numerous approaches have been proposed and led to
state-of-the-art results in various applications, including image
classification and language modeling. In this paper, we propose a novel way of
architecture search by means of weighted networks (WeNet), which consist of a
number of networks, with each assigned a weight. These weights are updated with
back-propagation to reflect the importance of different networks. Such weighted
networks bear similarity to mixture of experts. We conduct experiments on Penn
Treebank and WikiText-2. We show that the proposed WeNet can find recurrent
architectures which result in state-of-the-art performance.
",recent year increase demand automatic architecture search deep learn numerous approach propose lead state of the art result various application include image classification language model paper propose novel way architecture search means weight network wenet consist number network assign weight weight update back propagation reflect importance different network weight network bear similarity mixture expert conduct experiment penn treebank wikitext-2 show propose wenet find recurrent architecture result state of the art performance
Predicting Sleeping Quality using Convolutional Neural Networks,"  Identifying sleep stages and patterns is an essential part of diagnosing and
treating sleep disorders. With the advancement of smart technologies, sensor
data related to sleeping patterns can be captured easily. In this paper, we
propose a Convolution Neural Network (CNN) architecture that improves the
classification performance. In particular, we benchmark the classification
performance from different methods, including traditional machine learning
methods such as Logistic Regression (LR), Decision Trees (DT), k-Nearest
Neighbour (k-NN), Naive Bayes (NB) and Support Vector Machine (SVM), on 3
publicly available sleep datasets. The accuracy, sensitivity, specificity,
precision, recall, and F-score are reported and will serve as a baseline to
simulate the research in this direction in the future.
",identify sleep stage pattern essential part diagnose treat sleep disorder advancement smart technology sensor datum relate sleep pattern capture easily paper propose convolution neural network cnn architecture improve classification performance particular benchmark classification performance different method include traditional machine learning method logistic regression lr decision tree dt k near neighbour k nn naive baye nb support vector machine svm 3 publicly available sleep dataset accuracy sensitivity specificity precision recall f score report serve baseline simulate research direction future
"Improving Semantic Embedding Consistency by Metric Learning for
  Zero-Shot Classification","  This paper addresses the task of zero-shot image classification. The key
contribution of the proposed approach is to control the semantic embedding of
images -- one of the main ingredients of zero-shot learning -- by formulating
it as a metric learning problem. The optimized empirical criterion associates
two types of sub-task constraints: metric discriminating capacity and accurate
attribute prediction. This results in a novel expression of zero-shot learning
not requiring the notion of class in the training phase: only pairs of
image/attributes, augmented with a consistency indicator, are given as ground
truth. At test time, the learned model can predict the consistency of a test
image with a given set of attributes , allowing flexible ways to produce
recognition inferences. Despite its simplicity, the proposed approach gives
state-of-the-art results on four challenging datasets used for zero-shot
recognition evaluation.
",paper address task zero shot image classification key contribution propose approach control semantic embed image one main ingredient zero shot learning formulate metric learning problem optimize empirical criterion associate two type sub task constraint metric discriminating capacity accurate attribute prediction result novel expression zero shot learning require notion class training phase pair augmented consistency indicator give ground truth test time learn model predict consistency test image give set attribute allow flexible way produce recognition inference despite simplicity propose approach give state of the art result four challenging dataset use zero shot recognition evaluation
Use Of Vapnik-Chervonenkis Dimension in Model Selection,"  In this dissertation, I derive a new method to estimate the
Vapnik-Chervonenkis Dimension (VCD) for the class of linear functions. This
method is inspired by the technique developed by Vapnik et al. Vapnik et al.
(1994). My contribution rests on the approximation of the expected maximum
difference between two empirical Losses (EMDBTEL). In fact, I use a
cross-validated form of the error to compute the EMDBTEL, and I make the bound
on the EMDBTEL tighter by minimizing a constant in of its right upper bound. I
also derive two bounds for the true unknown risk using the additive (ERM1) and
the multiplicative (ERM2) Chernoff bounds. These bounds depend on the estimated
VCD and the empirical risk. These bounds can be used to perform model selection
and to declare with high probability, the chosen model will perform better
without making strong assumptions about the data generating process (DG).
  I measure the accuracy of my technique on simulated datasets and also on
three real datasets. The model selection provided by VCD was always as good as
if not better than the other methods under reasonable conditions.
",dissertation derive new method estimate vapnik chervonenki dimension vcd class linear function method inspire technique develop vapnik et al vapnik et al 1994 contribution rest approximation expect maximum difference two empirical loss emdbtel fact use cross validated form error compute emdbtel make bind emdbtel tight minimizing constant right upper bind also derive two bound true unknown risk use additive erm1 multiplicative erm2 chernoff bound bound depend estimate vcd empirical risk bound use perform model selection declare high probability choose model perform well without make strong assumption datum generating process dg measure accuracy technique simulated dataset also three real dataset model selection provide vcd always good well method reasonable condition
"Identifying Ventricular Arrhythmias and Their Predictors by Applying
  Machine Learning Methods to Electronic Health Records in Patients With
  Hypertrophic Cardiomyopathy(HCM-VAr-Risk Model)","  Clinical risk stratification for sudden cardiac death (SCD) in hypertrophic
cardiomyopathy (HC) employs rules derived from American College of Cardiology
Foundation/American Heart Association (ACCF/AHA) guidelines or the HCM Risk-SCD
model (C-index of 0.69), which utilize a few clinical variables. We assessed
whether data-driven machine learning methods that consider a wider range of
variables can effectively identify HC patients with ventricular arrhythmias
(VAr) that lead to SCD. We scanned the electronic health records of 711 HC
patients for sustained ventricular tachycardia or ventricular fibrillation.
Patients with ventricular tachycardia or ventricular fibrillation (n = 61) were
tagged as VAr cases and the remaining (n = 650) as non-VAr. The 2-sample t test
and information gain criterion were used to identify the most informative
clinical variables that distinguish VAr from non-VAr; patient records were
reduced to include only these variables. Data imbalance stemming from low
number of VAr cases was addressed by applying a combination of over- and
under-sampling strategies.We trained and tested multiple classifiers under this
sampling approach, showing effective classification. We evaluated 93 clinical
variables, of which 22 proved predictive of VAr. The ensemble of logistic
regression and naive Bayes classifiers, trained based on these 22 variables and
corrected for data imbalance, was most effective in separating VAr from non-VAr
cases (sensitivity = 0.73, specificity = 0.76, C-index = 0.83). Our method
(HCM-VAr-Risk Model) identified 12 new predictors of VAr, in addition to 10
established SCD predictors. In conclusion, this is the first application of
machine learning for identifying HC patients with VAr, using clinical
attributes.
",clinical risk stratification sudden cardiac death scd hypertrophic cardiomyopathy hc employ rule derive american college cardiology heart association guideline hcm risk scd model c index utilize clinical variable assess whether data drive machine learning method consider wide range variable effectively identify hc patient ventricular arrhythmias var lead scd scan electronic health record 711 hc patient sustain ventricular tachycardia ventricular fibrillation patient ventricular tachycardia ventricular fibrillation n 61 tagged var case remain n 650 non var 2 sample test information gain criterion use identify informative clinical variable distinguish var non var patient record reduce include variable data imbalance stem low number var case address apply combination over- under sampling train test multiple classifier sampling approach show effective classification evaluate 93 clinical variable 22 prove predictive var ensemble logistic regression naive baye classifier train base 22 variable correct data imbalance effective separate var non var case sensitivity specificity c index method hcm var risk model identify 12 new predictor var addition 10 establish scd predictor conclusion first application machine learning identify hc patient var use clinical attribute
"Probabilistic Knowledge Graph Construction: Compositional and
  Incremental Approaches","  Knowledge graph construction consists of two tasks: extracting information
from external resources (knowledge population) and inferring missing
information through a statistical analysis on the extracted information
(knowledge completion). In many cases, insufficient external resources in the
knowledge population hinder the subsequent statistical inference. The gap
between these two processes can be reduced by an incremental population
approach. We propose a new probabilistic knowledge graph factorisation method
that benefits from the path structure of existing knowledge (e.g. syllogism)
and enables a common modelling approach to be used for both incremental
population and knowledge completion tasks. More specifically, the probabilistic
formulation allows us to develop an incremental population algorithm that
trades off exploitation-exploration. Experiments on three benchmark datasets
show that the balanced exploitation-exploration helps the incremental
population, and the additional path structure helps to predict missing
information in knowledge completion.
",knowledge graph construction consist two task extract information external resource knowledge population infer miss information statistical analysis extract information knowledge completion many case insufficient external resource knowledge population hinder subsequent statistical inference gap two process reduce incremental population approach propose new probabilistic knowledge graph factorisation method benefit path structure exist knowledge syllogism enable common modelling approach use incremental population knowledge completion task specifically probabilistic formulation allow we develop incremental population algorithm trade exploitation exploration experiment three benchmark dataset show balanced exploitation exploration help incremental population additional path structure help predict miss information knowledge completion
"Design Target Achievement Index: A Differentiable Metric to Enhance Deep
  Generative Models in Multi-Objective Inverse Design","  Deep Generative Machine Learning Models have been growing in popularity
across the design community thanks to their ability to learn and mimic complex
data distributions. While early works are promising, further advancement will
depend on addressing several critical considerations such as design quality,
feasibility, novelty, and targeted inverse design. We propose the Design Target
Achievement Index (DTAI), a differentiable, tunable metric that scores a
design's ability to achieve designer-specified minimum performance targets. We
demonstrate that DTAI can drastically improve the performance of generated
designs when directly used as a training loss in Deep Generative Models. We
apply the DTAI loss to a Performance-Augmented Diverse GAN (PaDGAN) and
demonstrate superior generative performance compared to a set of baseline Deep
Generative Models including a Multi-Objective PaDGAN and specialized tabular
generation algorithms like the Conditional Tabular GAN (CTGAN). We further
enhance PaDGAN with an auxiliary feasibility classifier to encourage feasible
designs. To evaluate methods, we propose a comprehensive set of evaluation
metrics for generative methods that focus on feasibility, diversity, and
satisfaction of design performance targets. Methods are tested on a challenging
benchmarking problem: the FRAMED bicycle frame design dataset featuring
mixed-datatype parametric data, heavily skewed and multimodal distributions,
and ten competing performance objectives.
",deep generative machine learning model grow popularity across design community thank ability learn mimic complex data distribution early work promise advancement depend address several critical consideration design quality feasibility novelty target inverse design propose design target achievement index dtai differentiable tunable metric score design ability achieve designer specify minimum performance target demonstrate dtai drastically improve performance generate design directly use training loss deep generative model apply dtai loss performance augment diverse gan padgan demonstrate superior generative performance compare set baseline deep generative model include multi objective padgan specialized tabular generation algorithm like conditional tabular gan ctgan enhance padgan auxiliary feasibility classifier encourage feasible design evaluate method propose comprehensive set evaluation metric generative method focus feasibility diversity satisfaction design performance target method test challenge benchmarking problem frame bicycle frame design dataset feature mixed datatype parametric datum heavily skewed multimodal distribution ten compete performance objective
"Text as Environment: A Deep Reinforcement Learning Text Readability
  Assessment Model","  Evaluating the readability of a text can significantly facilitate the precise
expression of information in a written form. The formulation of text
readability assessment demands the identification of meaningful properties of
the text and correct conversion of features to the right readability level.
Sophisticated features and models are being used to evaluate the
comprehensibility of texts accurately. Still, these models are challenging to
implement, heavily language-dependent, and do not perform well on short texts.
Deep reinforcement learning models are demonstrated to be helpful in further
improvement of state-of-the-art text readability assessment models. The main
contributions of the proposed approach are the automation of feature
extraction, loosening the tight language dependency of text readability
assessment task, and efficient use of text by finding the minimum portion of a
text required to assess its readability. The experiments on Weebit, Cambridge
Exams, and Persian readability datasets display the model's state-of-the-art
precision, efficiency, and the capability to be applied to other languages.
",evaluate readability text significantly facilitate precise expression information write form formulation text readability assessment demand identification meaningful property text correct conversion feature right readability level sophisticated feature model use evaluate comprehensibility text accurately still model challenge implement heavily language dependent perform well short text deep reinforcement learning model demonstrate helpful improvement state of the art text readability assessment model main contribution propose approach automation feature extraction loosen tight language dependency text readability assessment task efficient use text find minimum portion text require assess readability experiment weebit cambridge exam persian readability dataset display model state of the art precision efficiency capability apply language
Investigating and Explaining the Frequency Bias in Image Classification,"  CNNs exhibit many behaviors different from humans, one of which is the
capability of employing high-frequency components. This paper discusses the
frequency bias phenomenon in image classification tasks: the high-frequency
components are actually much less exploited than the low- and mid-frequency
components. We first investigate the frequency bias phenomenon by presenting
two observations on feature discrimination and learning priority. Furthermore,
we hypothesize that (i) the spectral density, (ii) class consistency directly
affect the frequency bias. Specifically, our investigations verify that the
spectral density of datasets mainly affects the learning priority, while the
class consistency mainly affects the feature discrimination.
",cnns exhibit many behavior different human one capability employ high frequency component paper discuss frequency bias phenomenon image classification task high frequency component actually much less exploit low- mid frequency component first investigate frequency bias phenomenon present two observation feature discrimination learn priority furthermore hypothesize spectral density ii class consistency directly affect frequency bias specifically investigation verify spectral density dataset mainly affect learn priority class consistency mainly affect feature discrimination
"Performance Dynamics and Termination Errors in Reinforcement Learning: A
  Unifying Perspective","  In reinforcement learning, a decision needs to be made at some point as to
whether it is worthwhile to carry on with the learning process or to terminate
it. In many such situations, stochastic elements are often present which govern
the occurrence of rewards, with the sequential occurrences of positive rewards
randomly interleaved with negative rewards. For most practical learners, the
learning is considered useful if the number of positive rewards always exceeds
the negative ones. A situation that often calls for learning termination is
when the number of negative rewards exceeds the number of positive rewards.
However, while this seems reasonable, the error of premature termination,
whereby termination is enacted along with the conclusion of learning failure
despite the positive rewards eventually far outnumber the negative ones, can be
significant. In this paper, using combinatorial analysis we study the error
probability in wrongly terminating a reinforcement learning activity which
undermines the effectiveness of an optimal policy, and we show that the
resultant error can be quite high. Whilst we demonstrate mathematically that
such errors can never be eliminated, we propose some practical mechanisms that
can effectively reduce such errors. Simulation experiments have been carried
out, the results of which are in close agreement with our theoretical findings.
",reinforcement learning decision need make point whether worthwhile carry learning process terminate many situation stochastic element often present govern occurrence reward sequential occurrence positive reward randomly interleave negative reward practical learner learn consider useful number positive reward always exceed negative one situation often call learn termination number negative reward exceed number positive reward however seem reasonable error premature termination whereby termination enact along conclusion learn failure despite positive reward eventually far outnumber negative one significant paper use combinatorial analysis study error probability wrongly terminate reinforcement learning activity undermine effectiveness optimal policy show resultant error quite high whilst demonstrate mathematically error never eliminate propose practical mechanism effectively reduce error simulation experiment carry result close agreement theoretical finding
"Adversarial Feature Training for Generalizable Robotic Visuomotor
  Control","  Deep reinforcement learning (RL) has enabled training action-selection
policies, end-to-end, by learning a function which maps image pixels to action
outputs. However, it's application to visuomotor robotic policy training has
been limited because of the challenge of large-scale data collection when
working with physical hardware. A suitable visuomotor policy should perform
well not just for the task-setup it has been trained for, but also for all
varieties of the task, including novel objects at different viewpoints
surrounded by task-irrelevant objects. However, it is impractical for a robotic
setup to sufficiently collect interactive samples in a RL framework to
generalize well to novel aspects of a task. In this work, we demonstrate that
by using adversarial training for domain transfer, it is possible to train
visuomotor policies based on RL frameworks, and then transfer the acquired
policy to other novel task domains. We propose to leverage the deep RL
capabilities to learn complex visuomotor skills for uncomplicated task setups,
and then exploit transfer learning to generalize to new task domains provided
only still images of the task in the target domain. We evaluate our method on
two real robotic tasks, picking and pouring, and compare it to a number of
prior works, demonstrating its superiority.
",deep reinforcement learning rl enable training action selection policy end to end learning function map image pixel action output however application visuomotor robotic policy training limited challenge large scale data collection work physical hardware suitable visuomotor policy perform well task setup train also variety task include novel object different viewpoint surround task irrelevant object however impractical robotic setup sufficiently collect interactive sample rl framework generalize well novel aspect task work demonstrate use adversarial training domain transfer possible train visuomotor policy base rl framework transfer acquire policy novel task domain propose leverage deep rl capability learn complex visuomotor skill uncomplicate task setup exploit transfer learning generalize new task domain provide still image task target domain evaluate method two real robotic task picking pour compare number prior work demonstrate superiority
"Computational Model of Music Sight Reading: A Reinforcement Learning
  Approach","  Although the Music Sight Reading process has been studied from the cognitive
psychology view points, but the computational learning methods like the
Reinforcement Learning have not yet been used to modeling of such processes. In
this paper, with regards to essential properties of our specific problem, we
consider the value function concept and will indicate that the optimum policy
can be obtained by the method we offer without to be getting involved with
computing of the complex value functions. Also, we will offer a normative
behavioral model for the interaction of the agent with the musical pitch
environment and by using a slightly different version of Partially observable
Markov decision processes we will show that our method helps for faster
learning of state-action pairs in our implemented agents.
",although music sight reading process study cognitive psychology view point computational learning method like reinforcement learning yet use modeling process paper regard essential property specific problem consider value function concept indicate optimum policy obtain method offer without get involve compute complex value function also offer normative behavioral model interaction agent musical pitch environment use slightly different version partially observable markov decision process show method help fast learn state action pair implement agent
"BooST: Boosting Smooth Trees for Partial Effect Estimation in Nonlinear
  Regressions","  In this paper, we introduce a new machine learning (ML) model for nonlinear
regression called the Boosted Smooth Transition Regression Trees (BooST), which
is a combination of boosting algorithms with smooth transition regression
trees. The main advantage of the BooST model is the estimation of the
derivatives (partial effects) of very general nonlinear models. Therefore, the
model can provide more interpretation about the mapping between the covariates
and the dependent variable than other tree-based models, such as Random
Forests. We present several examples with both simulated and real data.
",paper introduce new machine learning ml model nonlinear regression call boost smooth transition regression tree boost combination boost algorithm smooth transition regression tree main advantage boost model estimation derivative partial effect general nonlinear model therefore model provide interpretation mapping covariate dependent variable tree base model random forest present several example simulate real datum
Robust Value Iteration for Continuous Control Tasks,"  When transferring a control policy from simulation to a physical system, the
policy needs to be robust to variations in the dynamics to perform well.
Commonly, the optimal policy overfits to the approximate model and the
corresponding state-distribution, often resulting in failure to trasnfer
underlying distributional shifts. In this paper, we present Robust Fitted Value
Iteration, which uses dynamic programming to compute the optimal value function
on the compact state domain and incorporates adversarial perturbations of the
system dynamics. The adversarial perturbations encourage a optimal policy that
is robust to changes in the dynamics. Utilizing the continuous-time perspective
of reinforcement learning, we derive the optimal perturbations for the states,
actions, observations and model parameters in closed-form. Notably, the
resulting algorithm does not require discretization of states or actions.
Therefore, the optimal adversarial perturbations can be efficiently
incorporated in the min-max value function update. We apply the resulting
algorithm to the physical Furuta pendulum and cartpole. By changing the masses
of the systems we evaluate the quantitative and qualitative performance across
different model parameters. We show that robust value iteration is more robust
compared to deep reinforcement learning algorithm and the non-robust version of
the algorithm. Videos of the experiments are shown at
https://sites.google.com/view/rfvi
",transfer control policy simulation physical system policy need robust variation dynamic perform well commonly optimal policy overfit approximate model correspond state distribution often result failure trasnfer underlie distributional shift paper present robust fit value iteration use dynamic programming compute optimal value function compact state domain incorporate adversarial perturbation system dynamic adversarial perturbation encourage optimal policy robust change dynamic utilize continuous time perspective reinforcement learning derive optimal perturbation state action observation model parameter close form notably result algorithm require discretization state action therefore optimal adversarial perturbation efficiently incorporate min max value function update apply result algorithm physical furuta pendulum cartpole change masse system evaluate quantitative qualitative performance across different model parameter show robust value iteration robust compare deep reinforcement learn algorithm non robust version algorithm video experiment show https
"Interleaved Text/Image Deep Mining on a Large-Scale Radiology Database
  for Automated Image Interpretation","  Despite tremendous progress in computer vision, there has not been an attempt
for machine learning on very large-scale medical image databases. We present an
interleaved text/image deep learning system to extract and mine the semantic
interactions of radiology images and reports from a national research
hospital's Picture Archiving and Communication System. With natural language
processing, we mine a collection of representative ~216K two-dimensional key
images selected by clinicians for diagnostic reference, and match the images
with their descriptions in an automated manner. Our system interleaves between
unsupervised learning and supervised learning on document- and sentence-level
text collections, to generate semantic labels and to predict them given an
image. Given an image of a patient scan, semantic topics in radiology levels
are predicted, and associated key-words are generated. Also, a number of
frequent disease types are detected as present or absent, to provide more
specific interpretation of a patient scan. This shows the potential of
large-scale learning and prediction in electronic patient records available in
most modern clinical institutions.
",despite tremendous progress computer vision attempt machine learn large scale medical image database present interleaved deep learning system extract mine semantic interaction radiology image report national research hospital picture archive communication system natural language processing mine collection representative two dimensional key image select clinician diagnostic reference match image description automate manner system interleave unsupervised learn supervise learn document- sentence level text collection generate semantic label predict give image give image patient scan semantic topic radiology level predict associate key word generate also number frequent disease type detect present absent provide specific interpretation patient scan show potential large scale learn prediction electronic patient record available modern clinical institution
"Extracting stochastic dynamical systems with $\alpha$-stable L\'evy
  noise from data","  With the rapid increase of valuable observational, experimental and simulated
data for complex systems, much efforts have been devoted to identifying
governing laws underlying the evolution of these systems. Despite the wide
applications of non-Gaussian fluctuations in numerous physical phenomena, the
data-driven approaches to extract stochastic dynamical systems with
(non-Gaussian) L\'evy noise are relatively few so far. In this work, we propose
a data-driven method to extract stochastic dynamical systems with
$\alpha$-stable L\'evy noise from short burst data based on the properties of
$\alpha$-stable distributions. More specifically, we first estimate the L\'evy
jump measure and noise intensity via computing mean and variance of the
amplitude of the increment of the sample paths. Then we approximate the drift
coefficient by combining nonlocal Kramers-Moyal formulas with normalizing
flows. Numerical experiments on one- and two-dimensional prototypical examples
illustrate the accuracy and effectiveness of our method. This approach will
become an effective scientific tool in discovering stochastic governing laws of
complex phenomena and understanding dynamical behaviors under non-Gaussian
fluctuations.
",rapid increase valuable observational experimental simulate datum complex system much effort devote identify govern law underlie evolution system despite wide application non gaussian fluctuation numerous physical phenomena data drive approach extract stochastic dynamical system non gaussian noise relatively far work propose data drive method extract stochastic dynamical system -stable noise short burst datum base property -stable distribution specifically first estimate jump measure noise intensity via computing mean variance amplitude increment sample path approximate drift coefficient combine nonlocal kramer moyal formula normalize flow numerical experiment one- two dimensional prototypical example illustrate accuracy effectiveness method approach become effective scientific tool discover stochastic govern law complex phenomenon understand dynamical behavior non gaussian fluctuation
"State and Topology Estimation for Unobservable Distribution Systems
  using Deep Neural Networks","  Time-synchronized state estimation for reconfigurable distribution networks
is challenging because of limited real-time observability. This paper addresses
this challenge by formulating a deep learning (DL)-based approach for topology
identification (TI) and unbalanced three-phase distribution system state
estimation (DSSE). Two deep neural networks (DNNs) are trained for
time-synchronized DNN-based TI and DSSE, respectively, for systems that are
incompletely observed by synchrophasor measurement devices (SMDs) in real-time.
A data-driven approach for judicious SMD placement to facilitate reliable TI
and DSSE is also provided. Robustness of the proposed methodology is
demonstrated by considering non-Gaussian noise in the SMD measurements. A
comparison of the DNN-based DSSE with more conventional approaches indicates
that the DL-based approach gives better accuracy with smaller number of SMDs.
",time synchronize state estimation reconfigurable distribution network challenge limited real time observability paper address challenge formulate deep learning dl -based approach topology identification ti unbalanced three phase distribution system state estimation dsse two deep neural network dnn train time synchronize dnn base ti dsse respectively system incompletely observe synchrophasor measurement device smds real time data drive approach judicious smd placement facilitate reliable ti dsse also provide robustness propose methodology demonstrate consider non gaussian noise smd measurement comparison dnn base dsse conventional approach indicate dl base approach give well accuracy small number smds
Be Causal: De-biasing Social Network Confounding in Recommendation,"  In recommendation systems, the existence of the missing-not-at-random (MNAR)
problem results in the selection bias issue, degrading the recommendation
performance ultimately. A common practice to address MNAR is to treat missing
entries from the so-called ""exposure"" perspective, i.e., modeling how an item
is exposed (provided) to a user. Most of the existing approaches use heuristic
models or re-weighting strategy on observed ratings to mimic the
missing-at-random setting. However, little research has been done to reveal how
the ratings are missing from a causal perspective. To bridge the gap, we
propose an unbiased and robust method called DENC (De-bias Network Confounding
in Recommendation) inspired by confounder analysis in causal inference. In
general, DENC provides a causal analysis on MNAR from both the inherent factors
(e.g., latent user or item factors) and auxiliary network's perspective.
Particularly, the proposed exposure model in DENC can control the social
network confounder meanwhile preserves the observed exposure information. We
also develop a deconfounding model through the balanced representation learning
to retain the primary user and item features, which enables DENC generalize
well on the rating prediction. Extensive experiments on three datasets validate
that our proposed model outperforms the state-of-the-art baselines.
",recommendation system existence miss not at random mnar problem result selection bias issue degrade recommendation performance ultimately common practice address mnar treat miss entry so call exposure perspective modeling item expose provide user exist approach use heuristic model re weighting strategy observe rating mimic miss at random setting however little research do reveal rating miss causal perspective bridge gap propose unbiased robust method call denc de bias network confound recommendation inspire confounder analysis causal inference general denc provide causal analysis mnar inherent factor latent user item factor auxiliary network perspective particularly propose exposure model denc control social network confounder meanwhile preserve observe exposure information also develop deconfounding model balanced representation learning retain primary user item feature enable denc generalize well rating prediction extensive experiment three dataset validate propose model outperform state of the art baseline
"Contextual User Browsing Bandits for Large-Scale Online Mobile
  Recommendation","  Online recommendation services recommend multiple commodities to users.
Nowadays, a considerable proportion of users visit e-commerce platforms by
mobile devices. Due to the limited screen size of mobile devices, positions of
items have a significant influence on clicks: 1) Higher positions lead to more
clicks for one commodity. 2) The 'pseudo-exposure' issue: Only a few
recommended items are shown at first glance and users need to slide the screen
to browse other items. Therefore, some recommended items ranked behind are not
viewed by users and it is not proper to treat this kind of items as negative
samples. While many works model the online recommendation as contextual bandit
problems, they rarely take the influence of positions into consideration and
thus the estimation of the reward function may be biased. In this paper, we aim
at addressing these two issues to improve the performance of online mobile
recommendation. Our contributions are four-fold. First, since we concern the
reward of a set of recommended items, we model the online recommendation as a
contextual combinatorial bandit problem and define the reward of a recommended
set. Second, we propose a novel contextual combinatorial bandit method called
UBM-LinUCB to address two issues related to positions by adopting the User
Browsing Model (UBM), a click model for web search. Third, we provide a formal
regret analysis and prove that our algorithm achieves sublinear regret
independent of the number of items. Finally, we evaluate our algorithm on two
real-world datasets by a novel unbiased estimator. An online experiment is also
implemented in Taobao, one of the most popular e-commerce platforms in the
world. Results on two CTR metrics show that our algorithm outperforms the other
contextual bandit algorithms.
",online recommendation service recommend multiple commodity user nowadays considerable proportion user visit e commerce platform mobile device due limited screen size mobile device position item significant influence click 1 high position lead click one commodity 2 issue recommend item show first glance user need slide screen browse item therefore recommend item rank behind view user proper treat kind item negative sample many work model online recommendation contextual bandit problem rarely take influence position consideration thus estimation reward function may biased paper aim address two issue improve performance online mobile recommendation contribution four fold first since concern reward set recommend item model online recommendation contextual combinatorial bandit problem define reward recommend set second propose novel contextual combinatorial bandit method call ubm linucb address two issue relate position adopt user browsing model ubm click model web search third provide formal regret analysis prove algorithm achieve sublinear regret independent number item finally evaluate algorithm two real world dataset novel unbiased estimator online experiment also implement taobao one popular e commerce platform world result two ctr metric show algorithm outperform contextual bandit algorithm
Statistical Consequences of Dueling Bandits,"  Multi-Armed-Bandit frameworks have often been used by researchers to assess
educational interventions, however, recent work has shown that it is more
beneficial for a student to provide qualitative feedback through preference
elicitation between different alternatives, making a dueling bandits framework
more appropriate. In this paper, we explore the statistical quality of data
under this framework by comparing traditional uniform sampling to a dueling
bandit algorithm and find that dueling bandit algorithms perform well at
cumulative regret minimisation, but lead to inflated Type-I error rates and
reduced power under certain circumstances. Through these results we provide
insight into the challenges and opportunities in using dueling bandit
algorithms to run adaptive experiments.
",multi armed bandit framework often use researcher assess educational intervention however recent work show beneficial student provide qualitative feedback preference elicitation different alternative make duel bandit framework appropriate paper explore statistical quality datum framework compare traditional uniform sample duel bandit algorithm find duel bandit algorithm perform well cumulative regret minimisation lead inflate type i error rate reduce power certain circumstance result provide insight challenge opportunity use duel bandit algorithm run adaptive experiment
Transformaly -- Two (Feature Spaces) Are Better Than One,"  Anomaly detection is a well-established research area that seeks to identify
samples outside of a predetermined distribution. An anomaly detection pipeline
is comprised of two main stages: (1) feature extraction and (2) normality score
assignment. Recent papers used pre-trained networks for feature extraction
achieving state-of-the-art results. However, the use of pre-trained networks
does not fully-utilize the normal samples that are available at train time.
This paper suggests taking advantage of this information by using
teacher-student training. In our setting, a pretrained teacher network is used
to train a student network on the normal training samples. Since the student
network is trained only on normal samples, it is expected to deviate from the
teacher network in abnormal cases. This difference can serve as a complementary
representation to the pre-trained feature vector. Our method -- Transformaly --
exploits a pre-trained Vision Transformer (ViT) to extract both feature
vectors: the pre-trained (agnostic) features and the teacher-student
(fine-tuned) features. We report state-of-the-art AUROC results in both the
common unimodal setting, where one class is considered normal and the rest are
considered abnormal, and the multimodal setting, where all classes but one are
considered normal, and just one class is considered abnormal. The code is
available at https://github.com/MatanCohen1/Transformaly.
",anomaly detection well establish research area seeks identify sample outside predetermine distribution anomaly detection pipeline comprise two main stage 1 feature extraction 2 normality score assignment recent paper use pre train network feature extraction achieve state of the art result however use pre train network fully utilize normal sample available train time paper suggest take advantage information use teacher student training setting pretraine teacher network use train student network normal training sample since student network train normal sample expect deviate teacher network abnormal case difference serve complementary representation pre train feature vector method transformaly exploit pre train vision transformer vit extract feature vector pre train agnostic feature teacher student fine tune feature report state of the art auroc result common unimodal set one class consider normal rest consider abnormal multimodal setting class one consider normal one class consider abnormal code available https
"Latent Dirichlet Allocation Uncovers Spectral Characteristics of Drought
  Stressed Plants","  Understanding the adaptation process of plants to drought stress is essential
in improving management practices, breeding strategies as well as engineering
viable crops for a sustainable agriculture in the coming decades.
Hyper-spectral imaging provides a particularly promising approach to gain such
understanding since it allows to discover non-destructively spectral
characteristics of plants governed primarily by scattering and absorption
characteristics of the leaf internal structure and biochemical constituents.
Several drought stress indices have been derived using hyper-spectral imaging.
However, they are typically based on few hyper-spectral images only, rely on
interpretations of experts, and consider few wavelengths only. In this study,
we present the first data-driven approach to discovering spectral drought
stress indices, treating it as an unsupervised labeling problem at massive
scale. To make use of short range dependencies of spectral wavelengths, we
develop an online variational Bayes algorithm for latent Dirichlet allocation
with convolved Dirichlet regularizer. This approach scales to massive datasets
and, hence, provides a more objective complement to plant physiological
practices. The spectral topics found conform to plant physiological knowledge
and can be computed in a fraction of the time compared to existing LDA
approaches.
",understand adaptation process plant drought stress essential improve management practice breed strategy well engineer viable crop sustainable agriculture come decade hyper spectral imaging provide particularly promising approach gain understanding since allow discover non destructively spectral characteristic plant govern primarily scatter absorption characteristic leaf internal structure biochemical constituent several drought stress index derive use hyper spectral imaging however typically base hyper spectral image rely interpretation expert consider wavelength study present first data drive approach discover spectral drought stress indice treat unsupervised labeling problem massive scale make use short range dependencie spectral wavelength develop online variational baye algorithm latent dirichlet allocation convolve dirichlet regularizer approach scale massive dataset hence provide objective complement plant physiological practice spectral topic find conform plant physiological knowledge compute fraction time compare exist lda approach
GottBERT: a pure German Language Model,"  Lately, pre-trained language models advanced the field of natural language
processing (NLP). The introduction of Bidirectional Encoders for Transformers
(BERT) and its optimized version RoBERTa have had significant impact and
increased the relevance of pre-trained models. First, research in this field
mainly started on English data followed by models trained with multilingual
text corpora. However, current research shows that multilingual models are
inferior to monolingual models. Currently, no German single language RoBERTa
model is yet published, which we introduce in this work (GottBERT). The German
portion of the OSCAR data set was used as text corpus. In an evaluation we
compare its performance on the two Named Entity Recognition (NER) tasks Conll
2003 and GermEval 2014 as well as on the text classification tasks GermEval
2018 (fine and coarse) and GNAD with existing German single language BERT
models and two multilingual ones. GottBERT was pre-trained related to the
original RoBERTa model using fairseq. All downstream tasks were trained using
hyperparameter presets taken from the benchmark of German BERT. The experiments
were setup utilizing FARM. Performance was measured by the $F_{1}$ score.
GottBERT was successfully pre-trained on a 256 core TPU pod using the RoBERTa
BASE architecture. Even without extensive hyper-parameter optimization, in all
NER and one text classification task, GottBERT already outperformed all other
tested German and multilingual models. In order to support the German NLP
field, we publish GottBERT under the AGPLv3 license.
",lately pre train language model advance field natural language processing nlp introduction bidirectional encoder transformer bert optimize version roberta significant impact increase relevance pre train model first research field mainly start english datum follow model train multilingual text corpora however current research show multilingual model inferior monolingual model currently german single language roberta model yet publish introduce work gottbert german portion oscar datum set use text corpus evaluation compare performance two name entity recognition ner task conll 2003 germeval 2014 well text classification task germeval 2018 fine coarse gnad exist german single language bert model two multilingual one gottbert pre train related original roberta model use fairseq downstream task train use hyperparameter preset take benchmark german bert experiment setup utilize farm performance measure f 1 score gottbert successfully pre trained 256 core tpu pod use roberta base architecture even without extensive hyper parameter optimization ner one text classification task gottbert already outperform tested german multilingual model order support german nlp field publish gottbert agplv3 license
"A Mathematical Analysis of Learning Loss for Active Learning in
  Regression","  Active learning continues to remain significant in the industry since it is
data efficient. Not only is it cost effective on a constrained budget,
continuous refinement of the model allows for early detection and resolution of
failure scenarios during the model development stage. Identifying and fixing
failures with the model is crucial as industrial applications demand that the
underlying model performs accurately in all foreseeable use cases. One popular
state-of-the-art technique that specializes in continuously refining the model
via failure identification is Learning Loss. Although simple and elegant, this
approach is empirically motivated. Our paper develops a foundation for Learning
Loss which enables us to propose a novel modification we call LearningLoss++.
We show that gradients are crucial in interpreting how Learning Loss works,
with rigorous analysis and comparison of the gradients between Learning Loss
and LearningLoss++. We also propose a convolutional architecture that combines
features at different scales to predict the loss. We validate LearningLoss++
for regression on the task of human pose estimation (using MPII and LSP
datasets), as done in Learning Loss. We show that LearningLoss++ outperforms in
identifying scenarios where the model is likely to perform poorly, which on
model refinement translates into reliable performance in the open world.
",active learning continue remain significant industry since datum efficient cost effective constrain budget continuous refinement model allow early detection resolution failure scenario model development stage identify fix failure model crucial industrial application demand underlying model perform accurately foreseeable use case one popular state of the art technique specialize continuously refining model via failure identification learn loss although simple elegant approach empirically motivated paper develop foundation learn loss enable we propose novel modification call show gradient crucial interpreting learn loss work rigorous analysis comparison gradient learn loss also propose convolutional architecture combines feature different scale predict loss validate regression task human pose estimation use mpii lsp dataset do learn loss show outperform identify scenario model likely perform poorly model refinement translate reliable performance open world
Quantifying Perceptual Distortion of Adversarial Examples,"  Recent work has shown that additive threat models, which only permit the
addition of bounded noise to the pixels of an image, are insufficient for fully
capturing the space of imperceivable adversarial examples. For example, small
rotations and spatial transformations can fool classifiers, remain
imperceivable to humans, but have large additive distance from the original
images. In this work, we leverage quantitative perceptual metrics like LPIPS
and SSIM to define a novel threat model for adversarial attacks.
  To demonstrate the value of quantifying the perceptual distortion of
adversarial examples, we present and employ a unifying framework fusing
different attack styles. We first prove that our framework results in images
that are unattainable by attack styles in isolation. We then perform
adversarial training using attacks generated by our framework to demonstrate
that networks are only robust to classes of adversarial perturbations they have
been trained against, and combination attacks are stronger than any of their
individual components. Finally, we experimentally demonstrate that our combined
attacks retain the same perceptual distortion but induce far higher
misclassification rates when compared against individual attacks.
",recent work show additive threat model permit addition bound noise pixel image insufficient fully capture space imperceivable adversarial example example small rotation spatial transformation fool classifier remain imperceivable human large additive distance original image work leverage quantitative perceptual metric like lpip ssim define novel threat model adversarial attack demonstrate value quantify perceptual distortion adversarial example present employ unify framework fuse different attack style first prove framework result image unattainable attack style isolation perform adversarial training use attack generate framework demonstrate network robust class adversarial perturbation train combination attack strong individual component finally experimentally demonstrate combined attack retain perceptual distortion induce far high misclassification rate compare individual attack
"A Connection between Feed-Forward Neural Networks and Probabilistic
  Graphical Models","  Two of the most popular modelling paradigms in computer vision are
feed-forward neural networks (FFNs) and probabilistic graphical models (GMs).
Various connections between the two have been studied in recent works, such as
e.g. expressing mean-field based inference in a GM as an FFN. This paper
establishes a new connection between FFNs and GMs. Our key observation is that
any FFN implements a certain approximation of a corresponding Bayesian network
(BN). We characterize various benefits of having this connection. In
particular, it results in a new learning algorithm for BNs. We validate the
proposed methods for a classification problem on CIFAR-10 dataset and for
binary image segmentation on Weizmann Horse dataset. We show that statistically
learned BNs improve performance, having at the same time essentially better
generalization capability, than their FFN counterparts.
",two popular modelling paradigm computer vision feed forward neural network ffns probabilistic graphical model gm various connection two study recent work express mean field base inference gm ffn paper establish new connection ffns gms key observation ffn implement certain approximation correspond bayesian network bn characterize various benefit connection particular result new learning algorithm bns validate propose method classification problem cifar-10 dataset binary image segmentation weizmann horse dataset show statistically learn bns improve performance time essentially well generalization capability ffn counterpart
Approximate Bayesian Computation with Domain Expert in the Loop,"  Approximate Bayesian computation (ABC) is a popular likelihood-free inference
method for models with intractable likelihood functions. As ABC methods usually
rely on comparing summary statistics of observed and simulated data, the choice
of the statistics is crucial. This choice involves a trade-off between loss of
information and dimensionality reduction, and is often determined based on
domain knowledge. However, handcrafting and selecting suitable statistics is a
laborious task involving multiple trial-and-error steps. In this work, we
introduce an active learning method for ABC statistics selection which reduces
the domain expert's work considerably. By involving the experts, we are able to
handle misspecified models, unlike the existing dimension reduction methods.
Moreover, empirical results show better posterior estimates than with existing
methods, when the simulation budget is limited.
",approximate bayesian computation abc popular likelihood free inference method model intractable likelihood function abc method usually rely compare summary statistic observe simulated datum choice statistic crucial choice involve trade off loss information dimensionality reduction often determine base domain knowledge however handcraft select suitable statistic laborious task involve multiple trial and error step work introduce active learning method abc statistic selection reduce domain expert work considerably involve expert able handle misspecifie model unlike exist dimension reduction method moreover empirical result show well posterior estimate exist method simulation budget limit
Understanding Synthetic Gradients and Decoupled Neural Interfaces,"  When training neural networks, the use of Synthetic Gradients (SG) allows
layers or modules to be trained without update locking - without waiting for a
true error gradient to be backpropagated - resulting in Decoupled Neural
Interfaces (DNIs). This unlocked ability of being able to update parts of a
neural network asynchronously and with only local information was demonstrated
to work empirically in Jaderberg et al (2016). However, there has been very
little demonstration of what changes DNIs and SGs impose from a functional,
representational, and learning dynamics point of view. In this paper, we study
DNIs through the use of synthetic gradients on feed-forward networks to better
understand their behaviour and elucidate their effect on optimisation. We show
that the incorporation of SGs does not affect the representational strength of
the learning system for a neural network, and prove the convergence of the
learning system for linear and deep linear models. On practical problems we
investigate the mechanism by which synthetic gradient estimators approximate
the true loss, and, surprisingly, how that leads to drastically different
layer-wise representations. Finally, we also expose the relationship of using
synthetic gradients to other error approximation techniques and find a unifying
language for discussion and comparison.
",train neural network use synthetic gradient sg allow layer module train without update locking without wait true error gradient backpropagate result decouple neural interface dni unlocked ability able update part neural network asynchronously local information demonstrate work empirically jaderberg et al 2016 however little demonstration change dnis sgs impose functional representational learning dynamic point view paper study dnis use synthetic gradient feed forward network well understand behaviour elucidate effect optimisation show incorporation sgs affect representational strength learn system neural network prove convergence learn system linear deep linear model practical problem investigate mechanism synthetic gradient estimator approximate true loss surprisingly lead drastically different layer wise representation finally also expose relationship use synthetic gradient error approximation technique find unify language discussion comparison
"Layer Adaptive Node Selection in Bayesian Neural Networks: Statistical
  Guarantees and Implementation Details","  Sparse deep neural networks have proven to be efficient for predictive model
building in large-scale studies. Although several works have studied
theoretical and numerical properties of sparse neural architectures, they have
primarily focused on the edge selection. Sparsity through edge selection might
be intuitively appealing; however, it does not necessarily reduce the
structural complexity of a network. Instead pruning excessive nodes in each
layer leads to a structurally sparse network which would have lower
computational complexity and memory footprint. We propose a Bayesian sparse
solution using spike-and-slab Gaussian priors to allow for node selection
during training. The use of spike-and-slab prior alleviates the need of an
ad-hoc thresholding rule for pruning redundant nodes from a network. In
addition, we adopt a variational Bayes approach to circumvent the computational
challenges of traditional Markov Chain Monte Carlo (MCMC) implementation. In
the context of node selection, we establish the fundamental result of
variational posterior consistency together with the characterization of prior
parameters. In contrast to the previous works, our theoretical development
relaxes the assumptions of the equal number of nodes and uniform bounds on all
network weights, thereby accommodating sparse networks with layer-dependent
node structures or coefficient bounds. With a layer-wise characterization of
prior inclusion probabilities, we also discuss optimal contraction rates of the
variational posterior. Finally, we provide empirical evidence to substantiate
that our theoretical work facilitates layer-wise optimal node recovery together
with competitive predictive performance.
",sparse deep neural network prove efficient predictive model build large scale study although several work study theoretical numerical property sparse neural architecture primarily focus edge selection sparsity edge selection might intuitively appeal however necessarily reduce structural complexity network instead prune excessive node layer lead structurally sparse network would lower computational complexity memory footprint propose bayesian sparse solution use spike and slab gaussian prior allow node selection training use spike and slab prior alleviate need ad hoc thresholde rule prune redundant node network addition adopt variational baye approach circumvent computational challenge traditional markov chain monte carlo mcmc implementation context node selection establish fundamental result variational posterior consistency together characterization prior parameter contrast previous work theoretical development relax assumption equal number node uniform bound network weight thereby accommodate sparse network layer dependent node structure coefficient bound layer wise characterization prior inclusion probability also discuss optimal contraction rate variational posterior finally provide empirical evidence substantiate theoretical work facilitate layer wise optimal node recovery together competitive predictive performance
Learning Fast-Mixing Models for Structured Prediction,"  Markov Chain Monte Carlo (MCMC) algorithms are often used for approximate
inference inside learning, but their slow mixing can be difficult to diagnose
and the approximations can seriously degrade learning. To alleviate these
issues, we define a new model family using strong Doeblin Markov chains, whose
mixing times can be precisely controlled by a parameter. We also develop an
algorithm to learn such models, which involves maximizing the data likelihood
under the induced stationary distribution of these chains. We show empirical
improvements on two challenging inference tasks.
",markov chain monte carlo mcmc algorithm often use approximate inference inside learn slow mix difficult diagnose approximation seriously degrade learn alleviate issue define new model family use strong doeblin markov chain whose mix time precisely control parameter also develop algorithm learn model involve maximize datum likelihood induce stationary distribution chain show empirical improvement two challenging inference task
Text Summarization as Tree Transduction by Top-Down TreeLSTM,"  Extractive compression is a challenging natural language processing problem.
This work contributes by formulating neural extractive compression as a parse
tree transduction problem, rather than a sequence transduction task. Motivated
by this, we introduce a deep neural model for learning
structure-to-substructure tree transductions by extending the standard Long
Short-Term Memory, considering the parent-child relationships in the structural
recursion. The proposed model can achieve state of the art performance on
sentence compression benchmarks, both in terms of accuracy and compression
rate.
",extractive compression challenge natural language processing problem work contribute formulate neural extractive compression parse tree transduction problem rather sequence transduction task motivate introduce deep neural model learn structure to substructure tree transduction extend standard long short term memory consider parent child relationship structural recursion propose model achieve state art performance sentence compression benchmark term accuracy compression rate
EventGraD: Event-Triggered Communication in Parallel Machine Learning,"  Communication in parallel systems imposes significant overhead which often
turns out to be a bottleneck in parallel machine learning. To relieve some of
this overhead, in this paper, we present EventGraD - an algorithm with
event-triggered communication for stochastic gradient descent in parallel
machine learning. The main idea of this algorithm is to modify the requirement
of communication at every iteration in standard implementations of stochastic
gradient descent in parallel machine learning to communicating only when
necessary at certain iterations. We provide theoretical analysis of convergence
of our proposed algorithm. We also implement the proposed algorithm for
data-parallel training of a popular residual neural network used for training
the CIFAR-10 dataset and show that EventGraD can reduce the communication load
by up to 60% while retaining the same level of accuracy. In addition, EventGraD
can be combined with other approaches such as Top-K sparsification to decrease
communication further while maintaining accuracy.
",communication parallel system impose significant overhead often turn bottleneck parallel machine learning relieve overhead paper present eventgrad algorithm event trigger communication stochastic gradient descent parallel machine learn main idea algorithm modify requirement communication every iteration standard implementation stochastic gradient descent parallel machine learning communicate necessary certain iteration provide theoretical analysis convergence propose algorithm also implement propose algorithm data parallel training popular residual neural network use training cifar-10 dataset show eventgrad reduce communication load 60 retain level accuracy addition eventgrad combine approach top k sparsification decrease communication maintain accuracy
Dataset Distillation using Neural Feature Regression,"  Dataset distillation aims to learn a small synthetic dataset that preserves
most of the information from the original dataset. Dataset distillation can be
formulated as a bi-level meta-learning problem where the outer loop optimizes
the meta-dataset and the inner loop trains a model on the distilled data.
Meta-gradient computation is one of the key challenges in this formulation, as
differentiating through the inner loop learning procedure introduces
significant computation and memory costs. In this paper, we address these
challenges using neural Feature Regression with Pooling (FRePo), achieving the
state-of-the-art performance with an order of magnitude less memory requirement
and two orders of magnitude faster training than previous methods. The proposed
algorithm is analogous to truncated backpropagation through time with a pool of
models to alleviate various types of overfitting in dataset distillation. FRePo
significantly outperforms the previous methods on CIFAR100, Tiny ImageNet, and
ImageNet-1K. Furthermore, we show that high-quality distilled data can greatly
improve various downstream applications, such as continual learning and
membership inference defense.
",dataset distillation aim learn small synthetic dataset preserve information original dataset dataset distillation formulate bi level meta learn problem outer loop optimize meta dataset inner loop train model distil datum meta gradient computation one key challenge formulation differentiate inner loop learn procedure introduce significant computation memory cost paper address challenge use neural feature regression pool frepo achieve state of the art performance order magnitude less memory requirement two order magnitude fast train previous method propose algorithm analogous truncated backpropagation time pool model alleviate various type overfitte dataset distillation frepo significantly outperform previous method cifar100 tiny imagenet imagenet-1k furthermore show high quality distil datum greatly improve various downstream application continual learning membership inference defense
"Machine learning models and facial regions videos for estimating heart
  rate: a review on Patents, Datasets and Literature","  Estimating heart rate is important for monitoring users in various
situations. Estimates based on facial videos are increasingly being researched
because it makes it possible to monitor cardiac information in a non-invasive
way and because the devices are simpler, requiring only cameras that capture
the user's face. From these videos of the user's face, machine learning is able
to estimate heart rate. This study investigates the benefits and challenges of
using machine learning models to estimate heart rate from facial videos,
through patents, datasets, and articles review. We searched Derwent Innovation,
IEEE Xplore, Scopus, and Web of Science knowledge bases and identified 7 patent
filings, 11 datasets, and 20 articles on heart rate, photoplethysmography, or
electrocardiogram data. In terms of patents, we note the advantages of
inventions related to heart rate estimation, as described by the authors. In
terms of datasets, we discovered that most of them are for academic purposes
and with different signs and annotations that allow coverage for subjects other
than heartbeat estimation. In terms of articles, we discovered techniques, such
as extracting regions of interest for heart rate reading and using Video
Magnification for small motion extraction, and models such as EVM-CNN and
VGG-16, that extract the observed individual's heart rate, the best regions of
interest for signal extraction and ways to process them.
",estimate heart rate important monitoring user various situation estimate base facial video increasingly research make possible monitor cardiac information non invasive way device simple require camera capture user face video user face machine learn able estimate heart rate study investigate benefit challenge use machine learning model estimate heart rate facial video patent dataset article review search derwent innovation ieee xplore scopus web science knowledge basis identify 7 patent filing 11 dataset 20 article heart rate photoplethysmography electrocardiogram datum term patent note advantage invention relate heart rate estimation describe author term dataset discover academic purpose different sign annotation allow coverage subject heartbeat estimation term article discover technique extract region interest heart rate reading use video magnification small motion extraction model evm cnn vgg-16 extract observe individual heart rate good region interest signal extraction way process
Learning with little mixing,"We study square loss in a realizable time-series framework with martingale
difference noise. Our main result is a fast rate excess risk bound which shows
that whenever a trajectory hypercontractivity condition holds, the risk of the
least-squares estimator on dependent data matches the iid rate order-wise after
a burn-in time. In comparison, many existing results in learning from dependent
data have rates where the effective sample size is deflated by a factor of the
mixing-time of the underlying process, even after the burn-in time.
Furthermore, our results allow the covariate process to exhibit long range
correlations which are substantially weaker than geometric ergodicity. We call
this phenomenon learning with little mixing, and present several examples for
when it occurs: bounded function classes for which the $L^2$ and
$L^{2+\epsilon}$ norms are equivalent, ergodic finite state Markov chains,
various parametric models, and a broad family of infinite dimensional
$\ell^2(\mathbb{N})$ ellipsoids. By instantiating our main result to system
identification of nonlinear dynamics with generalized linear model transitions,
we obtain a nearly minimax optimal excess risk bound after only a polynomial
burn-in time.",study square loss realizable time series framework martingale difference noise main result fast rate excess risk bind show whenever trajectory hypercontractivity condition hold risk least square estimator dependent data match iid rate order wise burn in time comparison many exist result learn dependent data rate effective sample size deflate factor mixing time underlying process even burn in time furthermore result allow covariate process exhibit long range correlation substantially weak geometric ergodicity call phenomenon learn little mix present several example occur bound function class norm equivalent ergodic finite state markov chain various parametric model broad family infinite dimensional n ellipsoid instantiate main result system identification nonlinear dynamic generalize linear model transition obtain nearly minimax optimal excess risk bind polynomial burn in time
Effective Graph Learning with Adaptive Knowledge Exchange,"  Graph Neural Networks (GNNs), due to their capability to learn complex
relations (edges) among attributed objects (nodes) within graph datasets, have
already been widely used in various graph mining tasks. Considerable efforts
have been devoted to improving GNN learning through designing new architectures
and/or loss objectives. In this paper, we introduce a novel GNN learning
framework, called AKE-GNN (Adaptive-Knowledge-Exchange GNN), which adaptively
exchanges diverse knowledge learned from multiple graph views generated by
graph augmentations. Specifically, AKE-GNN iteratively exchanges redundant
channels in the weight matrix of one GNN by informative channels of another GNN
in a layer-wise manner. Furthermore, existing GNN models can be seamlessly
incorporated into our framework. Extensive experiments on node classification,
graph classification, and edge prediction demonstrate the effectiveness of
AKE-GNN. In particular, we conduct a series of experiments on 15 public
benchmarks, 8 popular GNN models, and 3 graph tasks -- node classification,
graph classification, and edge prediction -- and show that AKE-GNN consistently
outperforms existing popular GNN models and even their ensembles. On the Cora
semi-supervised node classification dataset, our framework achieves new
state-of-the-art results. Extensive ablation studies and analyses on knowledge
exchange methods also verify the effectiveness of AKE-GNN.
",graph neural network gnn due capability learn complex relation edge among attribute object node within graph dataset already widely use various graph mining task considerable effort devote improve gnn learning design new architecture loss objective paper introduce novel gnn learning framework call ake gnn adaptive knowledge exchange gnn adaptively exchange diverse knowledge learn multiple graph view generate graph augmentation specifically ake gnn iteratively exchange redundant channel weight matrix one gnn informative channel another gnn layer wise manner furthermore exist gnn model seamlessly incorporate framework extensive experiment node classification graph classification edge prediction demonstrate effectiveness ake gnn particular conduct series experiment 15 public benchmark 8 popular gnn model 3 graph task node classification graph classification edge prediction show ake gnn consistently outperform exist popular gnn model even ensemble cora semi supervised node classification dataset framework achieve new state of the art result extensive ablation study analyse knowledge exchange method also verify effectiveness ake gnn
Exploration--Exploitation in MDPs with Options,"  While a large body of empirical results show that temporally-extended actions
and options may significantly affect the learning performance of an agent, the
theoretical understanding of how and when options can be beneficial in online
reinforcement learning is relatively limited. In this paper, we derive an upper
and lower bound on the regret of a variant of UCRL using options. While we
first analyze the algorithm in the general case of semi-Markov decision
processes (SMDPs), we show how these results can be translated to the specific
case of MDPs with options and we illustrate simple scenarios in which the
regret of learning with options can be \textit{provably} much smaller than the
regret suffered when learning with primitive actions.
",large body empirical result show temporally extend action option may significantly affect learn performance agent theoretical understanding option beneficial online reinforcement learn relatively limited paper derive upper lower bind regret variant ucrl use option first analyze algorithm general case semi markov decision process smdps show result translate specific case mdps option illustrate simple scenario regret learn option provably much small regret suffer learn primitive action
Individual Fairness for $k$-Clustering,"  We give a local search based algorithm for $k$-median and $k$-means (and more
generally for any $k$-clustering with $\ell_p$ norm cost function) from the
perspective of individual fairness. More precisely, for a point $x$ in a point
set $P$ of size $n$, let $r(x)$ be the minimum radius such that the ball of
radius $r(x)$ centered at $x$ has at least $n/k$ points from $P$. Intuitively,
if a set of $k$ random points are chosen from $P$ as centers, every point $x\in
P$ expects to have a center within radius $r(x)$. An individually fair
clustering provides such a guarantee for every point $x\in P$. This notion of
fairness was introduced in [Jung et al., 2019] where they showed how to get an
approximately feasible $k$-clustering with respect to this fairness condition.
  In this work, we show how to get a bicriteria approximation for fair
$k$-clustering: The $k$-median ($k$-means) cost of our solution is within a
constant factor of the cost of an optimal fair $k$-clustering, and our solution
approximately satisfies the fairness condition (also within a constant factor).
Further, we complement our theoretical bounds with empirical evaluation.
",give local search base algorithm k -median k -mean generally k -clustering norm cost function perspective individual fairness precisely point x point set p size n let r x minimum radius ball radius r x center x least point p intuitively set k random point choose p center every point p expect center within radius r x individually fair clustering provide guarantee every point p notion fairness introduce jung et 2019 show get approximately feasible k -clustering respect fairness condition work show get bicriteria approximation fair k -clustering k -median k -mean cost solution within constant factor cost optimal fair k -clustering solution approximately satisfy fairness condition also within constant factor complement theoretical bound empirical evaluation
Deep Variational Implicit Processes,"Implicit processes (IPs) are a generalization of Gaussian processes (GPs).
IPs may lack a closed-form expression but are easy to sample from. Examples
include, among others, Bayesian neural networks or neural samplers. IPs can be
used as priors over functions, resulting in flexible models with
well-calibrated prediction uncertainty estimates. Methods based on IPs usually
carry out function-space approximate inference, which overcomes some of the
difficulties of parameter-space approximate inference. Nevertheless, the
approximations employed often limit the expressiveness of the final model,
resulting, \emph{e.g.}, in a Gaussian predictive distribution, which can be
restrictive. We propose here a multi-layer generalization of IPs called the
Deep Variational Implicit process (DVIP). This generalization is similar to
that of deep GPs over GPs, but it is more flexible due to the use of IPs as the
prior distribution over the latent functions. We describe a scalable
variational inference algorithm for training DVIP and show that it outperforms
previous IP-based methods and also deep GPs. We support these claims via
extensive regression and classification experiments. We also evaluate DVIP on
large datasets with up to several million data instances to illustrate its good
scalability and performance.",implicit process ips generalization gaussian process gps ips may lack closed form expression easy sample example include among other bayesian neural network neural sampler ips use prior function result flexible model well calibrate prediction uncertainty estimate method base ips usually carry function space approximate inference overcome difficulty parameter space approximate inference nevertheless approximation employ often limit expressiveness final model result gaussian predictive distribution restrictive propose multi layer generalization ips call deep variational implicit process dvip generalization similar deep gps gps flexible due use ips prior distribution latent function describe scalable variational inference algorithm training dvip show outperform previous ip base method also deep gps support claim via extensive regression classification experiment also evaluate dvip large dataset several million datum instance illustrate good scalability performance
Log-DenseNet: How to Sparsify a DenseNet,"  Skip connections are increasingly utilized by deep neural networks to improve
accuracy and cost-efficiency. In particular, the recent DenseNet is efficient
in computation and parameters, and achieves state-of-the-art predictions by
directly connecting each feature layer to all previous ones. However,
DenseNet's extreme connectivity pattern may hinder its scalability to high
depths, and in applications like fully convolutional networks, full DenseNet
connections are prohibitively expensive. This work first experimentally shows
that one key advantage of skip connections is to have short distances among
feature layers during backpropagation. Specifically, using a fixed number of
skip connections, the connection patterns with shorter backpropagation distance
among layers have more accurate predictions. Following this insight, we propose
a connection template, Log-DenseNet, which, in comparison to DenseNet, only
slightly increases the backpropagation distances among layers from 1 to ($1 +
\log_2 L$), but uses only $L\log_2 L$ total connections instead of $O(L^2)$.
Hence, Log-DenseNets are easier than DenseNets to implement and to scale. We
demonstrate the effectiveness of our design principle by showing better
performance than DenseNets on tabula rasa semantic segmentation, and
competitive results on visual recognition.
",skip connection increasingly utilize deep neural network improve accuracy cost efficiency particular recent densenet efficient computation parameter achieve state of the art prediction directly connect feature layer previous one however densenet extreme connectivity pattern may hinder scalability high depth application like fully convolutional network full densenet connection prohibitively expensive work first experimentally show one key advantage skip connection short distance among feature layer backpropagation specifically use fix number skip connection connection pattern short backpropagation distance among layer accurate prediction follow insight propose connection template log densenet comparison densenet slightly increase backpropagation distance among layer 1 1 l use l total connection instead hence log densenet easy densenet implement scale demonstrate effectiveness design principle show well performance densenet tabula rasa semantic segmentation competitive result visual recognition
"Data Driven Control with Learned Dynamics: Model-Based versus Model-Free
  Approach","  This paper compares two different types of data-driven control methods,
representing model-based and model-free approaches. One is a recently proposed
method - Deep Koopman Representation for Control (DKRC), which utilizes a deep
neural network to map an unknown nonlinear dynamical system to a
high-dimensional linear system, which allows for employing state-of-the-art
control strategy. The other one is a classic model-free control method based on
an actor-critic architecture - Deep Deterministic Policy Gradient (DDPG), which
has been proved to be effective in various dynamical systems. The comparison is
carried out in OpenAI Gym, which provides multiple control environments for
benchmark purposes. Two examples are provided for comparison, i.e., classic
Inverted Pendulum and Lunar Lander Continuous Control. From the results of the
experiments, we compare these two methods in terms of control strategies and
the effectiveness under various initialization conditions. We also examine the
learned dynamic model from DKRC with the analytical model derived from the
Euler-Lagrange Linearization method, which demonstrates the accuracy in the
learned model for unknown dynamics from a data-driven sample-efficient
approach.
",paper compare two different type data drive control method represent model base model free approach one recently propose method deep koopman representation control dkrc utilize deep neural network map unknown nonlinear dynamical system high dimensional linear system allow employ state of the art control strategy one classic model free control method base actor critic architecture deep deterministic policy gradient ddpg prove effective various dynamical system comparison carry openai gym provide multiple control environment benchmark purpose two example provide comparison classic invert pendulum lunar lander continuous control result experiment compare two method term control strategy effectiveness various initialization condition also examine learn dynamic model dkrc analytical model derive euler lagrange linearization method demonstrate accuracy learn model unknown dynamic data drive sample efficient approach
Learning Quantum Entanglement Distillation with Noisy Classical Communications,"Quantum networking relies on the management and exploitation of entanglement.
Practical sources of entangled qubits are imperfect, producing mixed quantum
state with reduced fidelity with respect to ideal Bell pairs. Therefore, an
important primitive for quantum networking is entanglement distillation, whose
goal is to enhance the fidelity of entangled qubits through local operations
and classical communication (LOCC). Existing distillation protocols assume the
availability of ideal, noiseless, communication channels. In this paper, we
study the case in which communication takes place over noisy binary symmetric
channels. We propose to implement local processing through parameterized
quantum circuits (PQCs) that are optimized to maximize the average fidelity,
while accounting for communication errors. The introduced approach, Noise
Aware-LOCCNet (NA-LOCCNet), is shown to have significant advantages over
existing protocols designed for noiseless communications.",quantum networking rely management exploitation entanglement practical source entangle qubit imperfect produce mixed quantum state reduce fidelity respect ideal bell pair therefore important primitive quantum network entanglement distillation whose goal enhance fidelity entangle qubit local operation classical communication locc exist distillation protocol assume availability ideal noiseless communication channel paper study case communication take place noisy binary symmetric channel propose implement local processing parameterized quantum circuit pqcs optimize maximize average fidelity accounting communication error introduce approach noise aware loccnet na loccnet show significant advantage exist protocol design noiseless communication
"Enhancing Speech Intelligibility in Text-To-Speech Synthesis using
  Speaking Style Conversion","  The increased adoption of digital assistants makes text-to-speech (TTS)
synthesis systems an indispensable feature of modern mobile devices. It is
hence desirable to build a system capable of generating highly intelligible
speech in the presence of noise. Past studies have investigated style
conversion in TTS synthesis, yet degraded synthesized quality often leads to
worse intelligibility. To overcome such limitations, we proposed a novel
transfer learning approach using Tacotron and WaveRNN based TTS synthesis. The
proposed speech system exploits two modification strategies: (a) Lombard
speaking style data and (b) Spectral Shaping and Dynamic Range Compression
(SSDRC) which has been shown to provide high intelligibility gains by
redistributing the signal energy on the time-frequency domain. We refer to this
extension as Lombard-SSDRC TTS system. Intelligibility enhancement as
quantified by the Intelligibility in Bits (SIIB-Gauss) measure shows that the
proposed Lombard-SSDRC TTS system shows significant relative improvement
between 110% and 130% in speech-shaped noise (SSN), and 47% to 140% in
competing-speaker noise (CSN) against the state-of-the-art TTS approach.
Additional subjective evaluation shows that Lombard-SSDRC TTS successfully
increases the speech intelligibility with relative improvement of 455% for SSN
and 104% for CSN in median keyword correction rate compared to the baseline TTS
method.
",increase adoption digital assistant make text to speech tts synthesis system indispensable feature modern mobile device hence desirable build system capable generate highly intelligible speech presence noise past study investigate style conversion tts synthesis yet degrade synthesized quality often lead bad intelligibility overcome limitation propose novel transfer learning approach use tacotron wavernn base tts synthesis propose speech system exploit two modification strategy lombard speak style datum b spectral shaping dynamic range compression ssdrc show provide high intelligibility gain redistributing signal energy time frequency domain refer extension lombard ssdrc tts system intelligibility enhancement quantify intelligibility bit siib gauss measure show propose lombard ssdrc tts system show significant relative improvement 110 130 speech shape noise ssn 47 140 compete speaker noise csn state of the art tt approach additional subjective evaluation show lombard ssdrc tts successfully increase speech intelligibility relative improvement 455 ssn 104 csn median keyword correction rate compare baseline tts method
End-to-end Networks for Supervised Single-channel Speech Separation,"  The performance of single channel source separation algorithms has improved
greatly in recent times with the development and deployment of neural networks.
However, many such networks continue to operate on the magnitude spectrogram of
a mixture, and produce an estimate of source magnitude spectrograms, to perform
source separation. In this paper, we interpret these steps as additional neural
network layers and propose an end-to-end source separation network that allows
us to estimate the separated speech waveform by operating directly on the raw
waveform of the mixture. Furthermore, we also propose the use of masking based
end-to-end separation networks that jointly optimize the mask and the latent
representations of the mixture waveforms. These networks show a significant
improvement in separation performance compared to existing architectures in our
experiments. To train these end-to-end models, we investigate the use of
composite cost functions that are derived from objective evaluation metrics as
measured on waveforms. We present subjective listening test results that
demonstrate the improvement attained by using masking based end-to-end networks
and also reveal insights into the performance of these cost functions for
end-to-end source separation.
",performance single channel source separation algorithm improve greatly recent time development deployment neural network however many network continue operate magnitude spectrogram mixture produce estimate source magnitude spectrogram perform source separation paper interpret step additional neural network layer propose end to end source separation network allow we estimate separate speech waveform operate directly raw waveform mixture furthermore also propose use mask base end to end separation network jointly optimize mask latent representation mixture waveform network show significant improvement separation performance compare exist architecture experiment train end to end model investigate use composite cost function derive objective evaluation metric measure waveform present subjective listening test result demonstrate improvement attain use mask base end to end network also reveal insight performance cost function end to end source separation
Federated Learning with GAN-based Data Synthesis for Non-IID Clients,"Federated learning (FL) has recently emerged as a popular privacy-preserving
collaborative learning paradigm. However, it suffers from the non-independent
and identically distributed (non-IID) data among clients. In this paper, we
propose a novel framework, named Synthetic Data Aided Federated Learning
(SDA-FL), to resolve this non-IID challenge by sharing synthetic data.
Specifically, each client pretrains a local generative adversarial network
(GAN) to generate differentially private synthetic data, which are uploaded to
the parameter server (PS) to construct a global shared synthetic dataset. To
generate confident pseudo labels for the synthetic dataset, we also propose an
iterative pseudo labeling mechanism performed by the PS. A combination of the
local private dataset and synthetic dataset with confident pseudo labels leads
to nearly identical data distributions among clients, which improves the
consistency among local models and benefits the global aggregation. Extensive
experiments evidence that the proposed framework outperforms the baseline
methods by a large margin in several benchmark datasets under both the
supervised and semi-supervised settings.",federate learning fl recently emerge popular privacy preserve collaborative learning paradigm however suffer non independent identically distribute non iid datum among client paper propose novel framework name synthetic datum aid federate learn sda fl resolve non iid challenge share synthetic datum specifically client pretrain local generative adversarial network gin generate differentially private synthetic datum upload parameter server ps construct global share synthetic dataset generate confident pseudo label synthetic dataset also propose iterative pseudo labeling mechanism perform ps combination local private dataset synthetic dataset confident pseudo label lead nearly identical data distribution among client improve consistency among local model benefit global aggregation extensive experiment evidence propose framework outperform baseline method large margin several benchmark dataset supervise semi supervised setting
Distributionally Robust Federated Averaging,"  In this paper, we study communication efficient distributed algorithms for
distributionally robust federated learning via periodic averaging with adaptive
sampling. In contrast to standard empirical risk minimization, due to the
minimax structure of the underlying optimization problem, a key difficulty
arises from the fact that the global parameter that controls the mixture of
local losses can only be updated infrequently on the global stage. To
compensate for this, we propose a Distributionally Robust Federated Averaging
(DRFA) algorithm that employs a novel snapshotting scheme to approximate the
accumulation of history gradients of the mixing parameter. We analyze the
convergence rate of DRFA in both convex-linear and nonconvex-linear settings.
We also generalize the proposed idea to objectives with regularization on the
mixture parameter and propose a proximal variant, dubbed as DRFA-Prox, with
provable convergence rates. We also analyze an alternative optimization method
for regularized cases in strongly-convex-strongly-concave and non-convex (under
PL condition)-strongly-concave settings. To the best of our knowledge, this
paper is the first to solve distributionally robust federated learning with
reduced communication, and to analyze the efficiency of local descent methods
on distributed minimax problems. We give corroborating experimental evidence
for our theoretical results in federated learning settings.
",paper study communication efficient distribute algorithm distributionally robust federate learning via periodic average adaptive sampling contrast standard empirical risk minimization due minimax structure underlie optimization problem key difficulty arise fact global parameter control mixture local loss update infrequently global stage compensate propose distributionally robust federate average drfa algorithm employ novel snapshotte scheme approximate accumulation history gradient mix parameter analyze convergence rate drfa convex linear nonconvex linear setting also generalize propose idea objective regularization mixture parameter propose proximal variant dub drfa prox provable convergence rate also analyze alternative optimization method regularize case strongly convex strongly concave non convex pl condition -strongly concave setting good knowledge paper first solve distributionally robust federate learn reduce communication analyze efficiency local descent method distribute minimax problem give corroborate experimental evidence theoretical result federate learning setting
Fast Multi-label Learning,"  Embedding approaches have become one of the most pervasive techniques for
multi-label classification. However, the training process of embedding methods
usually involves a complex quadratic or semidefinite programming problem, or
the model may even involve an NP-hard problem. Thus, such methods are
prohibitive on large-scale applications. More importantly, much of the
literature has already shown that the binary relevance (BR) method is usually
good enough for some applications. Unfortunately, BR runs slowly due to its
linear dependence on the size of the input data. The goal of this paper is to
provide a simple method, yet with provable guarantees, which can achieve
competitive performance without a complex training process. To achieve our
goal, we provide a simple stochastic sketch strategy for multi-label
classification and present theoretical results from both algorithmic and
statistical learning perspectives. Our comprehensive empirical studies
corroborate our theoretical findings and demonstrate the superiority of the
proposed methods.
",embed approach become one pervasive technique multi label classification however training process embed method usually involve complex quadratic semidefinite programming problem model may even involve np hard problem thus method prohibitive large scale application importantly much literature already show binary relevance br method usually good enough application unfortunately br run slowly due linear dependence size input datum goal paper provide simple method yet provable guarantee achieve competitive performance without complex training process achieve goal provide simple stochastic sketch strategy multi label classification present theoretical result algorithmic statistical learning perspective comprehensive empirical study corroborate theoretical finding demonstrate superiority propose method
Stochastic geometry to generalize the Mondrian Process,"  The stable under iterated tessellation (STIT) process is a stochastic process
that produces a recursive partition of space with cut directions drawn
independently from a distribution over the sphere. The case of random
axis-aligned cuts is known as the Mondrian process. Random forests and Laplace
kernel approximations built from the Mondrian process have led to efficient
online learning methods and Bayesian optimization. In this work, we utilize
tools from stochastic geometry to resolve some fundamental questions concerning
STIT processes in machine learning. First, we show that a STIT process with cut
directions drawn from a discrete distribution can be efficiently simulated by
lifting to a higher dimensional axis-aligned Mondrian process. Second, we
characterize all possible kernels that stationary STIT processes and their
mixtures can approximate. We also give a uniform convergence rate for the
approximation error of the STIT kernels to the targeted kernels, generalizing
the work of [3] for the Mondrian case. Third, we obtain consistency results for
STIT forests in density estimation and regression. Finally, we give a formula
for the density estimator arising from an infinite STIT random forest. This
allows for precise comparisons between the Mondrian forest, the Mondrian kernel
and the Laplace kernel in density estimation. Our paper calls for further
developments at the novel intersection of stochastic geometry and machine
learning.
",stable iterated tessellation stit process stochastic process produce recursive partition space cut direction draw independently distribution sphere case random axis align cut know mondrian process random forest laplace kernel approximation build mondrian process lead efficient online learning method bayesian optimization work utilize tool stochastic geometry resolve fundamental question concern stit process machine learn first show stit process cut direction draw discrete distribution efficiently simulate lift high dimensional axis align mondrian process second characterize possible kernel stationary stit process mixture approximate also give uniform convergence rate approximation error stit kernel target kernel generalize work 3 mondrian case third obtain consistency result stit forest density estimation regression finally give formula density estimator arise infinite stit random forest allow precise comparison mondrian forest mondrian kernel laplace kernel density estimation paper call development novel intersection stochastic geometry machine learning
Weakly-Supervised Disentanglement Without Compromises,"  Intelligent agents should be able to learn useful representations by
observing changes in their environment. We model such observations as pairs of
non-i.i.d. images sharing at least one of the underlying factors of variation.
First, we theoretically show that only knowing how many factors have changed,
but not which ones, is sufficient to learn disentangled representations.
Second, we provide practical algorithms that learn disentangled representations
from pairs of images without requiring annotation of groups, individual
factors, or the number of factors that have changed. Third, we perform a
large-scale empirical study and show that such pairs of observations are
sufficient to reliably learn disentangled representations on several benchmark
data sets. Finally, we evaluate our learned representations and find that they
are simultaneously useful on a diverse suite of tasks, including generalization
under covariate shifts, fairness, and abstract reasoning. Overall, our results
demonstrate that weak supervision enables learning of useful disentangled
representations in realistic scenarios.
",intelligent agent able learn useful representation observe change environment model observation pair image share least one underlie factor variation first theoretically show know many factor change one sufficient learn disentangle representation second provide practical algorithm learn disentangle representation pair image without require annotation group individual factor number factor change third perform large scale empirical study show pair observation sufficient reliably learn disentangle representation several benchmark data set finally evaluate learn representation find simultaneously useful diverse suite task include generalization covariate shift fairness abstract reasoning overall result demonstrate weak supervision enable learn useful disentangle representation realistic scenario
"Learning and Dynamical Models for Sub-seasonal Climate Forecasting:
  Comparison and Collaboration","  Sub-seasonal climate forecasting (SSF) is the prediction of key climate
variables such as temperature and precipitation on the 2-week to 2-month time
horizon. Skillful SSF would have substantial societal value in areas such as
agricultural productivity, hydrology and water resource management, and
emergency planning for extreme events such as droughts and wildfires. Despite
its societal importance, SSF has stayed a challenging problem compared to both
short-term weather forecasting and long-term seasonal forecasting. Recent
studies have shown the potential of machine learning (ML) models to advance
SSF. In this paper, for the first time, we perform a fine-grained comparison of
a suite of modern ML models with start-of-the-art physics-based dynamical
models from the Subseasonal Experiment (SubX) project for SSF in the western
contiguous United States. Additionally, we explore mechanisms to enhance the ML
models by using forecasts from dynamical models. Empirical results illustrate
that, on average, ML models outperform dynamical models while the ML models
tend to be conservatives in their forecasts compared to the SubX models.
Further, we illustrate that ML models make forecasting errors under extreme
weather conditions, e.g., cold waves due to the polar vortex, highlighting the
need for separate models for extreme events. Finally, we show that suitably
incorporating dynamical model forecasts as inputs to ML models can
substantially improve the forecasting performance of the ML models. The SSF
dataset constructed for the work, dynamical model predictions, and code for the
ML models are released along with the paper for the benefit of the broader
machine learning community.
",sub seasonal climate forecasting ssf prediction key climate variable temperature precipitation 2 week 2 month time horizon skillful ssf would substantial societal value area agricultural productivity hydrology water resource management emergency planning extreme event drought wildfire despite societal importance ssf stay challenging problem compare short term weather forecasting long term seasonal forecasting recent study show potential machine learning ml model advance ssf paper first time perform fine grain comparison suite modern ml model start of the art physics base dynamical model subseasonal experiment subx project ssf western contiguous united states additionally explore mechanism enhance ml model use forecast dynamical model empirical result illustrate average ml model outperform dynamical model ml model tend conservative forecast compare subx model illustrate ml model make forecasting error extreme weather condition cold wave due polar vortex highlighting need separate model extreme event finally show suitably incorporate dynamical model forecast input ml model substantially improve forecasting performance ml model ssf dataset construct work dynamical model prediction code ml model release along paper benefit broad machine learn community
Replay For Safety,"  Experience replay \citep{lin1993reinforcement, mnih2015human} is a widely
used technique to achieve efficient use of data and improved performance in RL
algorithms. In experience replay, past transitions are stored in a memory
buffer and re-used during learning. Various suggestions for sampling schemes
from the replay buffer have been suggested in previous works, attempting to
optimally choose those experiences which will most contribute to the
convergence to an optimal policy. Here, we give some conditions on the replay
sampling scheme that will ensure convergence, focusing on the well-known
Q-learning algorithm in the tabular setting. After establishing sufficient
conditions for convergence, we turn to suggest a slightly different usage for
experience replay - replaying memories in a biased manner as a means to change
the properties of the resulting policy. We initiate a rigorous study of
experience replay as a tool to control and modify the properties of the
resulting policy. In particular, we show that using an appropriate biased
sampling scheme can allow us to achieve a \emph{safe} policy. We believe that
using experience replay as a biasing mechanism that allows controlling the
resulting policy in desirable ways is an idea with promising potential for many
applications.
",experience replay lin1993reinforcement mnih2015human widely use technique achieve efficient use datum improve performance rl algorithm experience replay past transition store memory buffer re used learn various suggestion sample scheme replay buffer suggest previous work attempt optimally choose experience contribute convergence optimal policy give condition replay sampling scheme ensure convergence focus well know q learn algorithm tabular set establish sufficient condition convergence turn suggest slightly different usage experience replay replay memory bias manner mean change property result policy initiate rigorous study experience replay tool control modify property result policy particular show use appropriate biased sampling scheme allow we achieve safe policy believe use experience replay biasing mechanism allow control result policy desirable way idea promise potential many application
SpecSinGAN: Sound Effect Variation Synthesis Using Single-Image GANs,"  Single-image generative adversarial networks learn from the internal
distribution of a single training example to generate variations of it,
removing the need of a large dataset. In this paper we introduce SpecSinGAN, an
unconditional generative architecture that takes a single one-shot sound effect
(e.g., a footstep; a character jump) and produces novel variations of it, as if
they were different takes from the same recording session. We explore the use
of multi-channel spectrograms to train the model on the various layers that
comprise a single sound effect. A listening study comparing our model to real
recordings and to digital signal processing procedural audio models in terms of
sound plausibility and variation revealed that SpecSinGAN is more plausible and
varied than the procedural audio models considered, when using multi-channel
spectrograms. Sound examples can be found at the project website:
https://www.adrianbarahonarios.com/specsingan/
",single image generative adversarial network learn internal distribution single training example generate variation remove need large dataset paper introduce specsingan unconditional generative architecture take single one shot sound effect footstep character jump produce novel variation different take recording session explore use multi channel spectrogram train model various layer comprise single sound effect listening study compare model real recording digital signal process procedural audio model term sound plausibility variation reveal specsingan plausible varied procedural audio model consider use multi channel spectrogram sound example find project website https
Convolutional Neural Networks for Speech Controlled Prosthetic Hands,"  Speech recognition is one of the key topics in artificial intelligence, as it
is one of the most common forms of communication in humans. Researchers have
developed many speech-controlled prosthetic hands in the past decades,
utilizing conventional speech recognition systems that use a combination of
neural network and hidden Markov model. Recent advancements in general-purpose
graphics processing units (GPGPUs) enable intelligent devices to run deep
neural networks in real-time. Thus, state-of-the-art speech recognition systems
have rapidly shifted from the paradigm of composite subsystems optimization to
the paradigm of end-to-end optimization. However, a low-power embedded GPGPU
cannot run these speech recognition systems in real-time. In this paper, we
show the development of deep convolutional neural networks (CNN) for speech
control of prosthetic hands that run in real-time on a NVIDIA Jetson TX2
developer kit. First, the device captures and converts speech into 2D features
(like spectrogram). The CNN receives the 2D features and classifies the hand
gestures. Finally, the hand gesture classes are sent to the prosthetic hand
motion control system. The whole system is written in Python with Keras, a deep
learning library that has a TensorFlow backend. Our experiments on the CNN
demonstrate the 91% accuracy and 2ms running time of hand gestures (text
output) from speech commands, which can be used to control the prosthetic hands
in real-time.
",speech recognition one key topic artificial intelligence one common form communication human researcher develop many speech control prosthetic hand past decade utilize conventional speech recognition system use combination neural network hidden markov model recent advancement general purpose graphic processing unit gpgpu enable intelligent device run deep neural network real time thus state of the art speech recognition system rapidly shift paradigm composite subsystem optimization paradigm end to end optimization however low power embed gpgpu run speech recognition system real time paper show development deep convolutional neural networks cnn speech control prosthetic hand run real time nvidia jetson tx2 developer kit first device capture convert speech 2d feature like spectrogram cnn receive 2d feature classifie hand gesture finally hand gesture class send prosthetic hand motion control system whole system write python keras deep learning library tensorflow backend experiment cnn demonstrate 91 accuracy 2ms running time hand gesture text output speech command use control prosthetic hand real time
Statistically Robust Neural Network Classification,"  Despite their numerous successes, there are many scenarios where adversarial
risk metrics do not provide an appropriate measure of robustness. For example,
test-time perturbations may occur in a probabilistic manner rather than being
generated by an explicit adversary, while the poor train--test generalization
of adversarial metrics can limit their usage to simple problems. Motivated by
this, we develop a probabilistic robust risk framework, the statistically
robust risk (SRR), which considers pointwise corruption distributions, as
opposed to worst-case adversaries. The SRR provides a distinct and
complementary measure of robust performance, compared to natural and
adversarial risk. We show that the SRR admits estimation and training schemes
which are as simple and efficient as for the natural risk: these simply require
noising the inputs, but with a principled derivation for exactly how and why
this should be done. Furthermore, we demonstrate both theoretically and
experimentally that it can provide superior generalization performance compared
with adversarial risks, enabling application to high-dimensional datasets.
",despite numerous success many scenario adversarial risk metric provide appropriate measure robustness example test time perturbation may occur probabilistic manner rather generate explicit adversary poor train test generalization adversarial metric limit usage simple problem motivate develop probabilistic robust risk framework statistically robust risk srr consider pointwise corruption distribution oppose bad case adversary srr provide distinct complementary measure robust performance compare natural adversarial risk show srr admit estimation training scheme simple efficient natural risk simply require noise input principle derivation exactly do furthermore demonstrate theoretically experimentally provide superior generalization performance compare adversarial risk enable application high dimensional dataset
Fixed points of nonnegative neural networks,"  We consider the existence of fixed points of nonnegative neural networks,
i.e., neural networks that take as an input nonnegative vectors and process
them using nonnegative parameters. We first show that nonnegative neural
networks can be recognized as monotonic and (weakly) scalable functions within
the framework of nonlinear Perron-Frobenius theory. This fact enables us to
provide conditions for the existence of fixed points of nonnegative neural
networks, and these conditions are weaker than those obtained recently using
arguments in convex analysis. Furthermore, we prove that the shape of the fixed
point set of nonnegative neural networks is often an interval, which
degenerates to a point for the case of scalable networks. The results of this
paper contribute to the understanding of the behavior of autoencoders, because
the fixed point set of an autoencoder is precisely the set of points that can
be perfectly reconstructed. Moreover, they provide insight into neural networks
designed using the loop-unrolling technique, which can be seen as a fixed point
searching algorithm. The chief theoretical results of this paper are verified
in numerical simulations, where we consider an autoencoder that first
compresses angular power spectra in massive MIMO systems, and, second,
reconstruct the input spectra from the compressed signals.
",consider existence fix point nonnegative neural network neural network take input nonnegative vector process use nonnegative parameter first show nonnegative neural network recognize monotonic weakly scalable function within framework nonlinear perron frobenius theory fact enable we provide condition existence fix point nonnegative neural network condition weaker obtain recently use argument convex analysis furthermore prove shape fix point set nonnegative neural network often interval degenerate point case scalable network result paper contribute understand behavior autoencoder fix point set autoencoder precisely set point perfectly reconstruct moreover provide insight neural network design use loop unroll technique see fix point search algorithm chief theoretical result paper verify numerical simulation consider autoencoder first compress angular power spectra massive mimo system second reconstruct input spectra compress signal
An Improved Training Procedure for Neural Autoregressive Data Completion,"  Neural autoregressive models are explicit density estimators that achieve
state-of-the-art likelihoods for generative modeling. The D-dimensional data
distribution is factorized into an autoregressive product of one-dimensional
conditional distributions according to the chain rule. Data completion is a
more involved task than data generation: the model must infer missing variables
for any partially observed input vector. Previous work introduced an
order-agnostic training procedure for data completion with autoregressive
models. Missing variables in any partially observed input vector can be imputed
efficiently by choosing an ordering where observed dimensions precede
unobserved ones and by computing the autoregressive product in this order. In
this paper, we provide evidence that the order-agnostic (OA) training procedure
is suboptimal for data completion. We propose an alternative procedure (OA++)
that reaches better performance in fewer computations. It can handle all data
completion queries while training fewer one-dimensional conditional
distributions than the OA procedure. In addition, these one-dimensional
conditional distributions are trained proportionally to their expected usage at
inference time, reducing overfitting. Finally, our OA++ procedure can exploit
prior knowledge about the distribution of inference completion queries, as
opposed to OA. We support these claims with quantitative experiments on
standard datasets used to evaluate autoregressive generative models.
",neural autoregressive model explicit density estimator achieve state of the art likelihood generative modeling d dimensional data distribution factorize autoregressive product one dimensional conditional distribution accord chain rule datum completion involve task datum generation model must infer missing variable partially observe input vector previous work introduce order agnostic training procedure datum completion autoregressive model miss variable partially observe input vector impute efficiently choose order observe dimension precede unobserved one compute autoregressive product order paper provide evidence order agnostic oa training procedure suboptimal datum completion propose alternative procedure reach well performance few computation handle datum completion query train few one dimensional conditional distribution oa procedure addition one dimensional conditional distribution train proportionally expect usage inference time reduce overfitte finally procedure exploit prior knowledge distribution inference completion query oppose oa support claim quantitative experiment standard dataset use evaluate autoregressive generative model
Variational Inference In Pachinko Allocation Machines,"  The Pachinko Allocation Machine (PAM) is a deep topic model that allows
representing rich correlation structures among topics by a directed acyclic
graph over topics. Because of the flexibility of the model, however,
approximate inference is very difficult. Perhaps for this reason, only a small
number of potential PAM architectures have been explored in the literature. In
this paper we present an efficient and flexible amortized variational inference
method for PAM, using a deep inference network to parameterize the approximate
posterior distribution in a manner similar to the variational autoencoder. Our
inference method produces more coherent topics than state-of-art inference
methods for PAM while being an order of magnitude faster, which allows
exploration of a wider range of PAM architectures than have previously been
studied.
",pachinko allocation machine pam deep topic model allow represent rich correlation structure among topic direct acyclic graph topic flexibility model however approximate inference difficult perhaps reason small number potential pam architecture explore literature paper present efficient flexible amortize variational inference method pam use deep inference network parameterize approximate posterior distribution manner similar variational autoencoder inference method produce coherent topic state of art inference method pam order magnitude fast allow exploration wide range pam architecture previously study
Reconstructing Actions To Explain Deep Reinforcement Learning,"  Feature attribution has been a foundational building block for explaining the
input feature importance in supervised learning with Deep Neural Network
(DNNs), but face new challenges when applied to deep Reinforcement Learning
(RL).We propose a new approach to explaining deep RL actions by defining a
class of \emph{action reconstruction} functions that mimic the behavior of a
network in deep RL. This approach allows us to answer more complex
explainability questions than direct application of DNN attribution methods,
which we adapt to \emph{behavior-level attributions} in building our action
reconstructions. It also allows us to define \emph{agreement}, a metric for
quantitatively evaluating the explainability of our methods. Our experiments on
a variety of Atari games suggest that perturbation-based attribution methods
are significantly more suitable in reconstructing actions to explain the deep
RL agent than alternative attribution methods, and show greater
\emph{agreement} than existing explainability work utilizing attention. We
further show that action reconstruction allows us to demonstrate how a deep
agent learns to play Pac-Man game.
",feature attribution foundational building block explain input feature importance supervise learn deep neural network dnn face new challenge apply deep reinforcement learning rl propose new approach explain deep rl action define class action reconstruction function mimic behavior network deep rl approach allow we answer complex explainability question direct application dnn attribution method adapt behavior level attribution building action reconstruction also allow we define agreement metric quantitatively evaluate explainability method experiment variety atari game suggest perturbation base attribution method significantly suitable reconstructing action explain deep rl agent alternative attribution method show great agreement exist explainability work utilize attention show action reconstruction allow we demonstrate deep agent learn play pac man game
"Fully Decentralized Multi-Agent Reinforcement Learning with Networked
  Agents","  We consider the problem of \emph{fully decentralized} multi-agent
reinforcement learning (MARL), where the agents are located at the nodes of a
time-varying communication network. Specifically, we assume that the reward
functions of the agents might correspond to different tasks, and are only known
to the corresponding agent. Moreover, each agent makes individual decisions
based on both the information observed locally and the messages received from
its neighbors over the network. Within this setting, the collective goal of the
agents is to maximize the globally averaged return over the network through
exchanging information with their neighbors. To this end, we propose two
decentralized actor-critic algorithms with function approximation, which are
applicable to large-scale MARL problems where both the number of states and the
number of agents are massively large. Under the decentralized structure, the
actor step is performed individually by each agent with no need to infer the
policies of others. For the critic step, we propose a consensus update via
communication over the network. Our algorithms are fully incremental and can be
implemented in an online fashion. Convergence analyses of the algorithms are
provided when the value functions are approximated within the class of linear
functions. Extensive simulation results with both linear and nonlinear function
approximations are presented to validate the proposed algorithms. Our work
appears to be the first study of fully decentralized MARL algorithms for
networked agents with function approximation, with provable convergence
guarantees.
",consider problem fully decentralize multi agent reinforcement learn marl agent locate node time vary communication network specifically assume reward function agent might correspond different task know correspond agent moreover agent make individual decision base information observe locally message receive neighbor network within set collective goal agent maximize globally average return network exchange information neighbor end propose two decentralized actor critic algorithm function approximation applicable large scale marl problem number state number agent massively large decentralized structure actor step perform individually agent need infer policy other critic step propose consensus update via communication network algorithm fully incremental implement online fashion convergence analyse algorithms provide value function approximate within class linear function extensive simulation result linear nonlinear function approximation present validate propose algorithm work appear first study fully decentralize marl algorithm networked agent function approximation provable convergence guarantee
BoXHED2.0: Scalable boosting of dynamic survival analysis,"  Modern applications of survival analysis increasingly involve time-dependent
covariates. In healthcare settings, such covariates provide dynamic patient
histories that can be used to assess health risks in realtime by tracking the
hazard function. Hazard learning is thus particularly useful in healthcare
analytics, and the open-source package BoXHED 1.0 provides the first
implementation of a gradient boosted hazard estimator that is fully
nonparametric. This paper introduces BoXHED 2.0, a quantum leap over BoXHED 1.0
in several ways. Crucially, BoXHED 2.0 can deal with survival data that goes
far beyond right-censoring and it also supports recurring events. To our
knowledge, this is the only nonparametric machine learning implementation that
is able to do so. Another major improvement is that BoXHED 2.0 is orders of
magnitude more scalable, due in part to a novel data preprocessing step that
sidesteps the need for explicit quadrature when dealing with time-dependent
covariates. BoXHED 2.0 supports the use of GPUs and multicore CPUs, and is
available from GitHub: www.github.com/BoXHED.
",modern application survival analysis increasingly involve time dependent covariate healthcare setting covariate provide dynamic patient history use assess health risk realtime track hazard function hazard learning thus particularly useful healthcare analytic open source package boxhe provide first implementation gradient boost hazard estimator fully nonparametric paper introduce boxhe quantum leap boxhe several way crucially boxhe deal survival datum go far beyond right censor also support recur event knowledge nonparametric machine learn implementation able another major improvement boxhe order magnitude scalable due part novel datum preprocesse step sidestep need explicit quadrature deal time dependent covariate boxhe support use gpus multicore cpus available github
"Scaling up Ranking under Constraints for Live Recommendations by
  Replacing Optimization with Prediction","  Many important multiple-objective decision problems can be cast within the
framework of ranking under constraints and solved via a weighted bipartite
matching linear program. Some of these optimization problems, such as
personalized content recommendations, may need to be solved in real time and
thus must comply with strict time requirements to prevent the perception of
latency by consumers. Classical linear programming is too computationally
inefficient for such settings. We propose a novel approach to scale up ranking
under constraints by replacing the weighted bipartite matching optimization
with a prediction problem in the algorithm deployment stage. We show
empirically that the proposed approximate solution to the ranking problem leads
to a major reduction in required computing resources without much sacrifice in
constraint compliance and achieved utility, allowing us to solve larger
constrained ranking problems real-time, within the required 50 milliseconds,
than previously reported.
",many important multiple objective decision problem cast within framework rank constraint solve via weight bipartite matching linear program optimization problem personalize content recommendation may need solve real time thus must comply strict time requirement prevent perception latency consumer classical linear programming computationally inefficient setting propose novel approach scale ranking constraint replace weight bipartite matching optimization prediction problem algorithm deployment stage show empirically propose approximate solution rank problem lead major reduction require compute resource without much sacrifice constraint compliance achieve utility allow we solve large constrain rank problem real time within require 50 millisecond previously report
Scrutinizing and De-Biasing Intuitive Physics with Neural Stethoscopes,"  Visually predicting the stability of block towers is a popular task in the
domain of intuitive physics. While previous work focusses on prediction
accuracy, a one-dimensional performance measure, we provide a broader analysis
of the learned physical understanding of the final model and how the learning
process can be guided. To this end, we introduce neural stethoscopes as a
general purpose framework for quantifying the degree of importance of specific
factors of influence in deep neural networks as well as for actively promoting
and suppressing information as appropriate. In doing so, we unify concepts from
multitask learning as well as training with auxiliary and adversarial losses.
We apply neural stethoscopes to analyse the state-of-the-art neural network for
stability prediction. We show that the baseline model is susceptible to being
misled by incorrect visual cues. This leads to a performance breakdown to the
level of random guessing when training on scenarios where visual cues are
inversely correlated with stability. Using stethoscopes to promote meaningful
feature extraction increases performance from 51% to 90% prediction accuracy.
Conversely, training on an easy dataset where visual cues are positively
correlated with stability, the baseline model learns a bias leading to poor
performance on a harder dataset. Using an adversarial stethoscope, the network
is successfully de-biased, leading to a performance increase from 66% to 88%.
",visually predict stability block tower popular task domain intuitive physics previous work focusse prediction accuracy one dimensional performance measure provide broad analysis learn physical understanding final model learning process guide end introduce neural stethoscope general purpose framework quantify degree importance specific factor influence deep neural network well actively promote suppress information appropriate unify concept multitask learn well train auxiliary adversarial loss apply neural stethoscope analyse state of the art neural network stability prediction show baseline model susceptible mislead incorrect visual cue lead performance breakdown level random guess train scenario visual cue inversely correlate stability use stethoscope promote meaningful feature extraction increase performance 51 90 prediction accuracy conversely train easy dataset visual cue positively correlate stability baseline model learn bias lead poor performance hard dataset use adversarial stethoscope network successfully de biased lead performance increase 66 88
"One Model to Serve All: Star Topology Adaptive Recommender for
  Multi-Domain CTR Prediction","  Traditional industrial recommenders are usually trained on a single business
domain and then serve for this domain. However, in large commercial platforms,
it is often the case that the recommenders need to make click-through rate
(CTR) predictions for multiple business domains. Different domains have
overlapping user groups and items. Thus, there exist commonalities. Since the
specific user groups have disparity and the user behaviors may change in
various business domains, there also have distinctions. The distinctions result
in domain-specific data distributions, making it hard for a single shared model
to work well on all domains. To learn an effective and efficient CTR model to
handle multiple domains simultaneously, we present Star Topology Adaptive
Recommender (STAR). Concretely, STAR has the star topology, which consists of
the shared centered parameters and domain-specific parameters. The shared
parameters are applied to learn commonalities of all domains, and the
domain-specific parameters capture domain distinction for more refined
prediction. Given requests from different business domains, STAR can adapt its
parameters conditioned on the domain characteristics. The experimental result
from production data validates the superiority of the proposed STAR model.
Since 2020, STAR has been deployed in the display advertising system of
Alibaba, obtaining averaging 8.0% improvement on CTR and 6.0% on RPM (Revenue
Per Mille).
",traditional industrial recommender usually train single business domain serve domain however large commercial platform often case recommender need make click through rate ctr prediction multiple business domain different domain overlap user group item thus exist commonality since specific user group disparity user behavior may change various business domain also distinction distinction result domain specific data distribution make hard single share model work well domain learn effective efficient ctr model handle multiple domain simultaneously present star topology adaptive recommender star concretely star star topology consist share center parameter domain specific parameter share parameter apply learn commonality domain domain specific parameter capture domain distinction refined prediction give request different business domain star adapt parameter condition domain characteristic experimental result production data validate superiority propose star model since 2020 star deploy display advertising system alibaba obtain average improvement ctr rpm revenue per mille
"Enhancing Adversarial Example Transferability with an Intermediate Level
  Attack","  Neural networks are vulnerable to adversarial examples, malicious inputs
crafted to fool trained models. Adversarial examples often exhibit black-box
transfer, meaning that adversarial examples for one model can fool another
model. However, adversarial examples are typically overfit to exploit the
particular architecture and feature representation of a source model, resulting
in sub-optimal black-box transfer attacks to other target models. We introduce
the Intermediate Level Attack (ILA), which attempts to fine-tune an existing
adversarial example for greater black-box transferability by increasing its
perturbation on a pre-specified layer of the source model, improving upon
state-of-the-art methods. We show that we can select a layer of the source
model to perturb without any knowledge of the target models while achieving
high transferability. Additionally, we provide some explanatory insights
regarding our method and the effect of optimizing for adversarial examples
using intermediate feature maps. Our code is available at
https://github.com/CUVL/Intermediate-Level-Attack.
",neural network vulnerable adversarial example malicious input craft fool train model adversarial example often exhibit black box transfer mean adversarial example one model fool another model however adversarial example typically overfit exploit particular architecture feature representation source model result sub optimal black box transfer attack target model introduce intermediate level attack ila attempt fine tune exist adversarial example great black box transferability increase perturbation pre specified layer source model improve upon state of the art method show select layer source model perturb without knowledge target model achieve high transferability additionally provide explanatory insight regard method effect optimize adversarial example use intermediate feature map code available https
Modeling Lost Information in Lossy Image Compression,"  Lossy image compression is one of the most commonly used operators for
digital images. Most recently proposed deep-learning-based image compression
methods leverage the auto-encoder structure, and reach a series of promising
results in this field. The images are encoded into low dimensional latent
features first, and entropy coded subsequently by exploiting the statistical
redundancy. However, the information lost during encoding is unfortunately
inevitable, which poses a significant challenge to the decoder to reconstruct
the original images. In this work, we propose a novel invertible framework
called Invertible Lossy Compression (ILC) to largely mitigate the information
loss problem. Specifically, ILC introduces an invertible encoding module to
replace the encoder-decoder structure to produce the low dimensional
informative latent representation, meanwhile, transform the lost information
into an auxiliary latent variable that won't be further coded or stored. The
latent representation is quantized and encoded into bit-stream, and the latent
variable is forced to follow a specified distribution, i.e. isotropic Gaussian
distribution. In this way, recovering the original image is made tractable by
easily drawing a surrogate latent variable and applying the inverse pass of the
module with the sampled variable and decoded latent features. Experimental
results demonstrate that with a new component replacing the auto-encoder in
image compression methods, ILC can significantly outperform the baseline method
on extensive benchmark datasets by combining with the existing compression
algorithms.
",lossy image compression one commonly use operator digital image recently propose deep learn base image compression method leverage auto encoder structure reach series promise result field image encode low dimensional latent feature first entropy code subsequently exploit statistical redundancy however information lose encode unfortunately inevitable pose significant challenge decoder reconstruct original image work propose novel invertible framework call invertible lossy compression ilc largely mitigate information loss problem specifically ilc introduce invertible encoding module replace encoder decoder structure produce low dimensional informative latent representation meanwhile transform lose information auxiliary latent variable will code store latent representation quantize encode bit stream latent variable force follow specify distribution isotropic gaussian distribution way recover original image make tractable easily draw surrogate latent variable apply inverse pass module sample variable decode latent feature experimental result demonstrate new component replace auto encoder image compression method ilc significantly outperform baseline method extensive benchmark dataset combine exist compression algorithm
"RED-Attack: Resource Efficient Decision based Attack for Machine
  Learning","  Due to data dependency and model leakage properties, Deep Neural Networks
(DNNs) exhibit several security vulnerabilities. Several security attacks
exploited them but most of them require the output probability vector. These
attacks can be mitigated by concealing the output probability vector. To
address this limitation, decision-based attacks have been proposed which can
estimate the model but they require several thousand queries to generate a
single untargeted attack image. However, in real-time attacks, resources and
attack time are very crucial parameters. Therefore, in resource-constrained
systems, e.g., autonomous vehicles where an untargeted attack can have a
catastrophic effect, these attacks may not work efficiently. To address this
limitation, we propose a resource efficient decision-based methodology which
generates the imperceptible attack, i.e., the RED-Attack, for a given black-box
model. The proposed methodology follows two main steps to generate the
imperceptible attack, i.e., classification boundary estimation and adversarial
noise optimization. Firstly, we propose a half-interval search-based algorithm
for estimating a sample on the classification boundary using a target image and
a randomly selected image from another class. Secondly, we propose an
optimization algorithm which first, introduces a small perturbation in some
randomly selected pixels of the estimated sample. Then to ensure
imperceptibility, it optimizes the distance between the perturbed and target
samples. For illustration, we evaluate it for CFAR-10 and German Traffic Sign
Recognition (GTSR) using state-of-the-art networks.
",due data dependency model leakage property deep neural network dnn exhibit several security vulnerability several security attack exploit require output probability vector attack mitigate conceal output probability vector address limitation decision base attack propose estimate model require several thousand query generate single untargeted attack image however real time attack resource attack time crucial parameter therefore resource constrain system autonomous vehicle untargete attack catastrophic effect attack may work efficiently address limitation propose resource efficient decision base methodology generate imperceptible attack red attack give black box model propose methodology follow two main step generate imperceptible attack classification boundary estimation adversarial noise optimization firstly propose half interval search base algorithm estimate sample classification boundary use target image randomly select image another class secondly propose optimization algorithm first introduce small perturbation randomly select pixel estimate sample ensure imperceptibility optimize distance perturb target sample illustration evaluate cfar-10 german traffic sign recognition gtsr use state of the art network
"Hybrid Zero Dynamics Inspired Feedback Control Policy Design for 3D
  Bipedal Locomotion using Reinforcement Learning","  This paper presents a novel model-free reinforcement learning (RL) framework
to design feedback control policies for 3D bipedal walking. Existing RL
algorithms are often trained in an end-to-end manner or rely on prior knowledge
of some reference joint trajectories. Different from these studies, we propose
a novel policy structure that appropriately incorporates physical insights
gained from the hybrid nature of the walking dynamics and the well-established
hybrid zero dynamics approach for 3D bipedal walking. As a result, the overall
RL framework has several key advantages, including lightweight network
structure, short training time, and less dependence on prior knowledge. We
demonstrate the effectiveness of the proposed method on Cassie, a challenging
3D bipedal robot. The proposed solution produces stable limit walking cycles
that can track various walking speed in different directions. Surprisingly,
without specifically trained with disturbances to achieve robustness, it also
performs robustly against various adversarial forces applied to the torso
towards both the forward and the backward directions.
",paper present novel model free reinforcement learning rl framework design feedback control policy 3d bipedal walk exist rl algorithm often train end to end manner rely prior knowledge reference joint trajectory different study propose novel policy structure appropriately incorporate physical insight gain hybrid nature walk dynamic well establish hybrid zero dynamic approach 3d bipedal walking result overall rl framework several key advantage include lightweight network structure short training time less dependence prior knowledge demonstrate effectiveness propose method cassie challenge 3d bipedal robot propose solution produce stable limit walking cycle track various walking speed different direction surprisingly without specifically train disturbance achieve robustness also perform robustly various adversarial force apply torso towards forward backward direction
Machine learning for plant microRNA prediction: A systematic review,"  MicroRNAs (miRNAs) are endogenous small non-coding RNAs that play an
important role in post-transcriptional gene regulation. However, the
experimental determination of miRNA sequence and structure is both expensive
and time-consuming. Therefore, computational and machine learning-based
approaches have been adopted to predict novel microRNAs. With the involvement
of data science and machine learning in biology, multiple research studies have
been conducted to find microRNAs with different computational methods and
different miRNA features. Multiple approaches are discussed in detail
considering the learning algorithm/s used, features considered, dataset/s used
and the criteria used in evaluations. This systematic review focuses on the
machine learning methods developed for miRNA identification in plants. This
will help researchers to gain a detailed idea about past studies and identify
novel paths that solve drawbacks occurred in past studies. Our findings
highlight the need for plant-specific computational methods for miRNA
identification.
",microrna mirna endogenous small non coding rna play important role post transcriptional gene regulation however experimental determination mirna sequence structure expensive time consume therefore computational machine learning base approach adopt predict novel microrna involvement datum science machine learn biology multiple research study conduct find microrna different computational method different mirna feature multiple approach discuss detail consider learn use feature consider used criterion use evaluation systematic review focus machine learn method develop mirna identification plant help researcher gain detailed idea past study identify novel path solve drawback occur past study finding highlight need plant specific computational method mirna identification
Self-Enhanced GNN: Improving Graph Neural Networks Using Model Outputs,"  Graph neural networks (GNNs) have received much attention recently because of
their excellent performance on graph-based tasks. However, existing research on
GNNs focuses on designing more effective models without considering much about
the quality of the input data. In this paper, we propose self-enhanced GNN
(SEG), which improves the quality of the input data using the outputs of
existing GNN models for better performance on semi-supervised node
classification. As graph data consist of both topology and node labels, we
improve input data quality from both perspectives. For topology, we observe
that higher classification accuracy can be achieved when the ratio of
inter-class edges (connecting nodes from different classes) is low and propose
topology update to remove inter-class edges and add intra-class edges. For node
labels, we propose training node augmentation, which enlarges the training set
using the labels predicted by existing GNN models. SEG is a general framework
that can be easily combined with existing GNN models. Experimental results
validate that SEG consistently improves the performance of well-known GNN
models such as GCN, GAT and SGC across different datasets.
",graph neural network gnn receive much attention recently excellent performance graph base task however exist research gnns focus design effective model without consider much quality input datum paper propose self enhance gnn seg improve quality input datum use output exist gnn model well performance semi supervised node classification graph datum consist topology node label improve input datum quality perspective topology observe high classification accuracy achieve ratio inter class edge connect node different class low propose topology update remove inter class edge add intra class edge node label propose train node augmentation enlarge training set use label predict exist gnn model seg general framework easily combine exist gnn model experimental result validate seg consistently improve performance well know gnn model gcn gat sgc across different dataset
Achieving Counterfactual Fairness for Causal Bandit,"  In online recommendation, customers arrive in a sequential and stochastic
manner from an underlying distribution and the online decision model recommends
a chosen item for each arriving individual based on some strategy. We study how
to recommend an item at each step to maximize the expected reward while
achieving user-side fairness for customers, i.e., customers who share similar
profiles will receive a similar reward regardless of their sensitive attributes
and items being recommended. By incorporating causal inference into bandits and
adopting soft intervention to model the arm selection strategy, we first
propose the d-separation based UCB algorithm (D-UCB) to explore the utilization
of the d-separation set in reducing the amount of exploration needed to achieve
low cumulative regret. Based on that, we then propose the fair causal bandit
(F-UCB) for achieving the counterfactual individual fairness. Both theoretical
analysis and empirical evaluation demonstrate effectiveness of our algorithms.
",online recommendation customer arrive sequential stochastic manner underlie distribution online decision model recommend choose item arrive individual base strategy study recommend item step maximize expect reward achieve user side fairness customer customer share similar profile receive similar reward regardless sensitive attribute item recommend incorporate causal inference bandit adopt soft intervention model arm selection strategy first propose d separation base ucb algorithm d ucb explore utilization d separation set reduce amount exploration need achieve low cumulative regret base propose fair causal bandit f ucb achieve counterfactual individual fairness theoretical analysis empirical evaluation demonstrate effectiveness algorithm
Additive Bayesian Network Modelling with the R Package abn,"  The R package abn is designed to fit additive Bayesian models to
observational datasets. It contains routines to score Bayesian networks based
on Bayesian or information theoretic formulations of generalized linear models.
It is equipped with exact search and greedy search algorithms to select the
best network. It supports a possible blend of continuous, discrete and count
data and input of prior knowledge at a structural level. The Bayesian
implementation supports random effects to control for one-layer clustering. In
this paper, we give an overview of the methodology and illustrate the package's
functionalities using a veterinary dataset about respiratory diseases in
commercial swine production.
",r package abn design fit additive bayesian model observational dataset contain routine score bayesian network base bayesian information theoretic formulation generalize linear model equip exact search greedy search algorithm select good network support possible blend continuous discrete count datum input prior knowledge structural level bayesian implementation support random effect control one layer cluster paper give overview methodology illustrate package functionality use veterinary dataset respiratory disease commercial swine production
Reinforcement Control with Hierarchical Backpropagated Adaptive Critics,"  Present incremental learning methods are limited in the ability to achieve
reliable credit assignment over a large number time steps (or events). However,
this situation is typical for cases where the dynamical system to be controlled
requires relatively frequent control updates in order to maintain stability or
robustness yet has some action-consequences which must be established over
relatively long periods of time. To address this problem, the learning
capabilities of a control architecture comprised of two Backpropagated Adaptive
Critics (BACs) in a two-level hierarchy with continuous actions are explored.
The high-level BAC updates less frequently than the low-level BAC and controls
the latter to some degree. The response of the low-level to high-level signals
can either be determined a priori or it can emerge during learning. A general
approach called Response Induction Learning is introduced to address the latter
case.
",present incremental learning method limited ability achieve reliable credit assignment large number time step event however situation typical case dynamical system control require relatively frequent control update order maintain stability robustness yet action consequence must establish relatively long period time address problem learn capability control architecture comprise two backpropagate adaptive critic bacs two level hierarchy continuous action explore high level bac update less frequently low level bac control latter degree response low level high level signal either determine priori emerge learn general approach call response induction learning introduce address latter case
Practical Attacks Against Graph-based Clustering,"  Graph modeling allows numerous security problems to be tackled in a general
way, however, little work has been done to understand their ability to
withstand adversarial attacks. We design and evaluate two novel graph attacks
against a state-of-the-art network-level, graph-based detection system. Our
work highlights areas in adversarial machine learning that have not yet been
addressed, specifically: graph-based clustering techniques, and a global
feature space where realistic attackers without perfect knowledge must be
accounted for (by the defenders) in order to be practical. Even though less
informed attackers can evade graph clustering with low cost, we show that some
practical defenses are possible.
",graph modeling allow numerous security problem tackle general way however little work do understand ability withstand adversarial attack design evaluate two novel graph attack state of the art network level graph base detection system work highlight area adversarial machine learning yet address specifically graph base clustering technique global feature space realistic attacker without perfect knowledge must account defender order practical even though less inform attacker evade graph cluster low cost show practical defense possible
Stealing Neural Networks via Timing Side Channels,"  Deep learning is gaining importance in many applications. However, Neural
Networks face several security and privacy threats. This is particularly
significant in the scenario where Cloud infrastructures deploy a service with
Neural Network model at the back end. Here, an adversary can extract the Neural
Network parameters, infer the regularization hyperparameter, identify if a data
point was part of the training data, and generate effective transferable
adversarial examples to evade classifiers. This paper shows how a Neural
Network model is susceptible to timing side channel attack. In this paper, a
black box Neural Network extraction attack is proposed by exploiting the timing
side channels to infer the depth of the network. Although, constructing an
equivalent architecture is a complex search problem, it is shown how
Reinforcement Learning with knowledge distillation can effectively reduce the
search space to infer a target model. The proposed approach has been tested
with VGG architectures on CIFAR10 data set. It is observed that it is possible
to reconstruct substitute models with test accuracy close to the target models
and the proposed approach is scalable and independent of type of Neural Network
architectures.
",deep learning gain importance many application however neural network face several security privacy threat particularly significant scenario cloud infrastructure deploy service neural network model back end adversary extract neural network parameter infer regularization hyperparameter identify datum point part training datum generate effective transferable adversarial example evade classifier paper show neural network model susceptible timing side channel attack paper black box neural network extraction attack propose exploit time side channel infer depth network although construct equivalent architecture complex search problem show reinforcement learning knowledge distillation effectively reduce search space infer target model propose approach test vgg architectures cifar10 datum set observed possible reconstruct substitute model test accuracy close target model propose approach scalable independent type neural network architecture
Synthesizing Tasks for Block-based Programming,"  Block-based visual programming environments play a critical role in
introducing computing concepts to K-12 students. One of the key pedagogical
challenges in these environments is in designing new practice tasks for a
student that match a desired level of difficulty and exercise specific
programming concepts. In this paper, we formalize the problem of synthesizing
visual programming tasks. In particular, given a reference visual task $\rm
T^{in}$ and its solution code $\rm C^{in}$, we propose a novel methodology to
automatically generate a set $\{(\rm T^{out}, \rm C^{out})\}$ of new tasks
along with solution codes such that tasks $\rm T^{in}$ and $\rm T^{out}$ are
conceptually similar but visually dissimilar. Our methodology is based on the
realization that the mapping from the space of visual tasks to their solution
codes is highly discontinuous; hence, directly mutating reference task $\rm
T^{in}$ to generate new tasks is futile. Our task synthesis algorithm operates
by first mutating code $\rm C^{in}$ to obtain a set of codes $\{\rm C^{out}\}$.
Then, the algorithm performs symbolic execution over a code $\rm C^{out}$ to
obtain a visual task $\rm T^{out}$; this step uses the Monte Carlo Tree Search
(MCTS) procedure to guide the search in the symbolic tree. We demonstrate the
effectiveness of our algorithm through an extensive empirical evaluation and
user study on reference tasks taken from the \emph{Hour of Code: Classic Maze}
challenge by \emph{Code.org} and the \emph{Intro to Programming with Karel}
course by \emph{CodeHS.com}.
",block base visual programming environment play critical role introduce computing concept k-12 student one key pedagogical challenge environment design new practice task student match desire level difficulty exercise specific programming concept paper formalize problem synthesize visual programming task particular give reference visual task solution code propose novel methodology automatically generate set new task along solution code task conceptually similar visually dissimilar methodology base realization mapping space visual task solution code highly discontinuous hence directly mutate reference task generate new task futile task synthesis algorithm operate first mutate code obtain set code algorithm perform symbolic execution code obtain visual task step use monte carlo tree search mct procedure guide search symbolic tree demonstrate effectiveness algorithm extensive empirical evaluation user study reference task take hour code classic maze challenge intro programming karel course
On the Cryptographic Hardness of Learning Single Periodic Neurons,"  We show a simple reduction which demonstrates the cryptographic hardness of
learning a single periodic neuron over isotropic Gaussian distributions in the
presence of noise. More precisely, our reduction shows that any polynomial-time
algorithm (not necessarily gradient-based) for learning such functions under
small noise implies a polynomial-time quantum algorithm for solving worst-case
lattice problems, whose hardness form the foundation of lattice-based
cryptography. Our core hard family of functions, which are well-approximated by
one-layer neural networks, take the general form of a univariate periodic
function applied to an affine projection of the data. These functions have
appeared in previous seminal works which demonstrate their hardness against
gradient-based (Shamir'18), and Statistical Query (SQ) algorithms (Song et
al.'17). We show that if (polynomially) small noise is added to the labels, the
intractability of learning these functions applies to all polynomial-time
algorithms, beyond gradient-based and SQ algorithms, under the aforementioned
cryptographic assumptions. Moreover, we demonstrate the necessity of noise in
the hardness result by designing a polynomial-time algorithm for learning
certain families of such functions under exponentially small adversarial noise.
Our proposed algorithm is not a gradient-based or an SQ algorithm, but is
rather based on the celebrated Lenstra-Lenstra-Lov\'asz (LLL) lattice basis
reduction algorithm. Furthermore, in the absence of noise, this algorithm can
be directly applied to solve CLWE detection (Bruna et al.'21) and phase
retrieval with an optimal sample complexity of $d+1$ samples. In the former
case, this improves upon the quadratic-in-$d$ sample complexity required in
(Bruna et al.'21).
",show simple reduction demonstrate cryptographic hardness learn single periodic neuron isotropic gaussian distribution presence noise precisely reduction show polynomial time algorithm necessarily gradient base learning function small noise imply polynomial time quantum algorithm solve bad case lattice problem whose hardness form foundation lattice base cryptography core hard family function well approximate one layer neural network take general form univariate periodic function apply affine projection data function appear previous seminal work demonstrate hardness gradient base statistical query sq algorithm song et show polynomially small noise add label intractability learn function apply polynomial time algorithm beyond gradient base sq algorithm aforementione cryptographic assumption moreover demonstrate necessity noise hardness result design polynomial time algorithm learn certain family function exponentially small adversarial noise propose algorithm gradient base sq algorithm rather base celebrated lll lattice basis reduction algorithm furthermore absence noise algorithm directly apply solve clwe detection bruna et al phase retrieval optimal sample complexity sample former case improve upon quadratic in- sample complexity require bruna et al
Distilling BERT into Simple Neural Networks with Unlabeled Transfer Data,"  Recent advances in pre-training huge models on large amounts of text through
self supervision have obtained state-of-the-art results in various natural
language processing tasks. However, these huge and expensive models are
difficult to use in practise for downstream tasks. Some recent efforts use
knowledge distillation to compress these models. However, we see a gap between
the performance of the smaller student models as compared to that of the large
teacher. In this work, we leverage large amounts of in-domain unlabeled
transfer data in addition to a limited amount of labeled training instances to
bridge this gap for distilling BERT. We show that simple RNN based student
models even with hard distillation can perform at par with the huge teachers
given the transfer set. The student performance can be further improved with
soft distillation and leveraging teacher intermediate representations. We show
that our student models can compress the huge teacher by up to 26x while still
matching or even marginally exceeding the teacher performance in low-resource
settings with small amount of labeled data. Additionally, for the multilingual
extension of this work with XtremeDistil (Mukherjee and Hassan Awadallah,
2020), we demonstrate massive distillation of multilingual BERT-like teacher
models by upto 35x in terms of parameter compression and 51x in terms of
latency speedup for batch inference while retaining 95% of its F1-score for NER
over 41 languages.
",recent advance pre training huge model large amount text self supervision obtain state of the art result various natural language processing task however huge expensive model difficult use practise downstream task recent effort use knowledge distillation compress model however see gap performance small student model compare large teacher work leverage large amount in domain unlabeled transfer datum addition limited amount label training instance bridge gap distil bert show simple rnn base student model even hard distillation perform par huge teacher give transfer set student performance improve soft distillation leverage teacher intermediate representation show student model compress huge teacher 26x still match even marginally exceed teacher performance low resource setting small amount label datum additionally multilingual extension work xtremedistil mukherjee hassan awadallah 2020 demonstrate massive distillation multilingual bert like teacher model upto 35x term parameter compression 51x term latency speedup batch inference retain 95 f1 score ner 41 language
"Proximal Policy Optimization with Adaptive Threshold for Symmetric
  Relative Density Ratio","  Deep reinforcement learning (DRL) is one of the promising approaches for
introducing robots into complicated environments. The recent remarkable
progress of DRL stands on regularization of policy, which allows the policy to
improve stably and efficiently. A popular method, so-called proximal policy
optimization (PPO), and its variants constrain density ratio of the latest and
baseline policies when the density ratio exceeds a given threshold. This
threshold can be designed relatively intuitively, and in fact its recommended
value range has been suggested. However, the density ratio is asymmetric for
its center, and the possible error scale from its center, which should be close
to the threshold, would depend on how the baseline policy is given. In order to
maximize the values of regularization of policy, this paper proposes a new PPO
derived using relative Pearson (RPE) divergence, therefore so-called PPO-RPE,
to design the threshold adaptively. In PPO-RPE, the relative density ratio,
which can be formed with symmetry, replaces the raw density ratio. Thanks to
this symmetry, its error scale from center can easily be estimated, hence, the
threshold can be adapted for the estimated error scale. From three simple
benchmark simulations, the importance of algorithm-dependent threshold design
is revealed. By simulating additional four locomotion tasks, it is verified
that the proposed method statistically contributes to task accomplishment by
appropriately restricting the policy updates.
",deep reinforcement learn drl one promise approach introduce robot complicated environment recent remarkable progress drl stand regularization policy allow policy improve stably efficiently popular method so call proximal policy optimization ppo variant constrain density ratio late baseline policy density ratio exceed give threshold threshold design relatively intuitively fact recommend value range suggest however density ratio asymmetric center possible error scale center close threshold would depend baseline policy give order maximize value regularization policy paper propose new ppo derive use relative pearson rpe divergence therefore so call ppo rpe design threshold adaptively ppo rpe relative density ratio form symmetry replace raw density ratio thank symmetry error scale center easily estimate hence threshold adapt estimate error scale three simple benchmark simulation importance algorithm dependent threshold design reveal simulate additional four locomotion task verify propose method statistically contribute task accomplishment appropriately restrict policy update
Does Momentum Help? A Sample Complexity Analysis,"  Heavy ball momentum is a popular acceleration idea in stochastic
optimization. There have been several attempts to understand its perceived
benefits, but the complete picture is still unclear. Specifically, the error
expression in the presence of noise has two separate terms: the bias and the
variance, but most existing works only focus on bias and show that momentum
accelerates its decay. Such analyses overlook the interplay between bias and
variance and, therefore, miss important implications. In this work, we analyze
a sample complexity bound of stochastic approximation algorithms with
heavy-ball momentum that accounts for both bias and variance. We find that for
the same step size, which is small enough, the iterates with momentum have
improved sample complexity compared to the ones without. However, by using a
different step-size sequence, the non-momentum version can nullify this
benefit. Subsequently, we show that our sample complexity bounds are indeed
tight for a small enough neighborhood around the solution and large enough
noise variance. Our analysis also sheds some light on the finite-time behavior
of these algorithms. This explains the perceived benefit in the initial phase
of momentum-based schemes.
",heavy ball momentum popular acceleration idea stochastic optimization several attempt understand perceive benefit complete picture still unclear specifically error expression presence noise two separate term bias variance exist work focus bias show momentum accelerate decay analysis overlook interplay bias variance therefore miss important implication work analyze sample complexity bind stochastic approximation algorithm heavy ball momentum account bias variance find step size small enough iterate momentum improve sample complexity compare one without however use different step size sequence non momentum version nullify benefit subsequently show sample complexity bound indeed tight small enough neighborhood around solution large enough noise variance analysis also shed light finite time behavior algorithms explain perceive benefit initial phase momentum base scheme
"BERTology Meets Biology: Interpreting Attention in Protein Language
  Models","  Transformer architectures have proven to learn useful representations for
protein classification and generation tasks. However, these representations
present challenges in interpretability. In this work, we demonstrate a set of
methods for analyzing protein Transformer models through the lens of attention.
We show that attention: (1) captures the folding structure of proteins,
connecting amino acids that are far apart in the underlying sequence, but
spatially close in the three-dimensional structure, (2) targets binding sites,
a key functional component of proteins, and (3) focuses on progressively more
complex biophysical properties with increasing layer depth. We find this
behavior to be consistent across three Transformer architectures (BERT, ALBERT,
XLNet) and two distinct protein datasets. We also present a three-dimensional
visualization of the interaction between attention and protein structure. Code
for visualization and analysis is available at
https://github.com/salesforce/provis.
",transformer architecture prove learn useful representation protein classification generation task however representation present challenge interpretability work demonstrate set method analyze protein transformer model lens attention show attention 1 capture fold structure protein connect amino acid far apart underlying sequence spatially close three dimensional structure 2 target bind site key functional component protein 3 focus progressively complex biophysical property increase layer depth find behavior consistent across three transformer architecture bert albert xlnet two distinct protein dataset also present three dimensional visualization interaction attention protein structure code visualization analysis available https
Using Machine Learning to Emulate Agent-Based Simulations,"  In this proof-of-concept work, we evaluate the performance of multiple
machine-learning methods as statistical emulators for use in the analysis of
agent-based models (ABMs). Analysing ABM outputs can be challenging, as the
relationships between input parameters can be non-linear or even chaotic even
in relatively simple models, and each model run can require significant CPU
time. Statistical emulation, in which a statistical model of the ABM is
constructed to facilitate detailed model analyses, has been proposed as an
alternative to computationally costly Monte Carlo methods. Here we compare
multiple machine-learning methods for ABM emulation in order to determine the
approaches best suited to emulating the complex behaviour of ABMs. Our results
suggest that, in most scenarios, artificial neural networks (ANNs) and
gradient-boosted trees outperform Gaussian process emulators, currently the
most commonly used method for the emulation of complex computational models.
ANNs produced the most accurate model replications in scenarios with high
numbers of model runs, although training times were longer than the other
methods. We propose that agent-based modelling would benefit from using
machine-learning methods for emulation, as this can facilitate more robust
sensitivity analyses for the models while also reducing CPU time consumption
when calibrating and analysing the simulation.
",proof of concept work evaluate performance multiple machine learn method statistical emulator use analysis agent base model abms analyse abm output challenge relationship input parameter non linear even chaotic even relatively simple model model run require significant cpu time statistical emulation statistical model abm construct facilitate detailed model analysis propose alternative computationally costly monte carlo method compare multiple machine learn method abm emulation order determine approach well suited emulate complex behaviour abms result suggest scenario artificial neural network ann gradient boost tree outperform gaussian process emulator currently commonly use method emulation complex computational model ann produce accurate model replication scenario high number model run although training time long method propose agent base modelling would benefit use machine learn method emulation facilitate robust sensitivity analysis model also reduce cpu time consumption calibrate analyse simulation
"Model Selection for High-Dimensional Regression under the Generalized
  Irrepresentability Condition","  In the high-dimensional regression model a response variable is linearly
related to $p$ covariates, but the sample size $n$ is smaller than $p$. We
assume that only a small subset of covariates is `active' (i.e., the
corresponding coefficients are non-zero), and consider the model-selection
problem of identifying the active covariates. A popular approach is to estimate
the regression coefficients through the Lasso ($\ell_1$-regularized least
squares). This is known to correctly identify the active set only if the
irrelevant covariates are roughly orthogonal to the relevant ones, as
quantified through the so called `irrepresentability' condition. In this paper
we study the `Gauss-Lasso' selector, a simple two-stage method that first
solves the Lasso, and then performs ordinary least squares restricted to the
Lasso active set. We formulate `generalized irrepresentability condition'
(GIC), an assumption that is substantially weaker than irrepresentability. We
prove that, under GIC, the Gauss-Lasso correctly recovers the active set.
",high dimensional regression model response variable linearly relate p covariate sample size n small p assume small subset covariate active corresponding coefficient non zero consider model selection problem identify active covariate popular approach estimate regression coefficient lasso -regularized least square know correctly identify active set irrelevant covariate roughly orthogonal relevant one quantify call irrepresentability condition paper study gauss lasso selector simple two stage method first solve lasso perform ordinary least square restrict lasso active set formulate generalize irrepresentability condition gic assumption substantially weak irrepresentability prove gic gauss lasso correctly recover active set
Sentiment Classification in Bangla Textual Content: A Comparative Study,"  Sentiment analysis has been widely used to understand our views on social and
political agendas or user experiences over a product. It is one of the cores
and well-researched areas in NLP. However, for low-resource languages, like
Bangla, one of the prominent challenge is the lack of resources. Another
important limitation, in the current literature for Bangla, is the absence of
comparable results due to the lack of a well-defined train/test split. In this
study, we explore several publicly available sentiment labeled datasets and
designed classifiers using both classical and deep learning algorithms. In our
study, the classical algorithms include SVM and Random Forest, and deep
learning algorithms include CNN, FastText, and transformer-based models. We
compare these models in terms of model performance and time-resource
complexity. Our finding suggests transformer-based models, which have not been
explored earlier for Bangla, outperform all other models. Furthermore, we
created a weighted list of lexicon content based on the valence score per
class. We then analyzed the content for high significance entries per class, in
the datasets. For reproducibility, we make publicly available data splits and
the ranked lexicon list. The presented results can be used for future studies
as a benchmark.
",sentiment analysis widely use understand view social political agenda user experience product one core well research area nlp however low resource language like bangla one prominent challenge lack resource another important limitation current literature bangla absence comparable result due lack well define split study explore several publicly available sentiment label dataset design classifier use classical deep learning algorithm study classical algorithm include svm random forest deep learning algorithm include cnn fasttext transformer base model compare model term model performance time resource complexity finding suggest transformer base model explore early bangla outperform model furthermore create weighted list lexicon content base valence score per class analyze content high significance entry per class dataset reproducibility make publicly available datum split rank lexicon list present result use future study benchmark
VulBERTa: Simplified Source Code Pre-Training for Vulnerability Detection,"This paper presents VulBERTa, a deep learning approach to detect security
vulnerabilities in source code. Our approach pre-trains a RoBERTa model with a
custom tokenisation pipeline on real-world code from open-source C/C++
projects. The model learns a deep knowledge representation of the code syntax
and semantics, which we leverage to train vulnerability detection classifiers.
We evaluate our approach on binary and multi-class vulnerability detection
tasks across several datasets (Vuldeepecker, Draper, REVEAL and muVuldeepecker)
and benchmarks (CodeXGLUE and D2A). The evaluation results show that VulBERTa
achieves state-of-the-art performance and outperforms existing approaches
across different datasets, despite its conceptual simplicity, and limited cost
in terms of size of training data and number of model parameters.",paper present vulberta deep learn approach detect security vulnerability source code approach pre train roberta model custom tokenisation pipeline real world code open source project model learn deep knowledge representation code syntax semantic leverage train vulnerability detection classifier evaluate approach binary multi class vulnerability detection task across several dataset vuldeepecker draper reveal muvuldeepecker benchmark codexglue d2a evaluation result show vulberta achieve state of the art performance outperform exist approach across different dataset despite conceptual simplicity limited cost term size training datum number model parameter
Better Sign Language Translation with STMC-Transformer,"  Sign Language Translation (SLT) first uses a Sign Language Recognition (SLR)
system to extract sign language glosses from videos. Then, a translation system
generates spoken language translations from the sign language glosses. This
paper focuses on the translation system and introduces the STMC-Transformer
which improves on the current state-of-the-art by over 5 and 7 BLEU
respectively on gloss-to-text and video-to-text translation of the
PHOENIX-Weather 2014T dataset. On the ASLG-PC12 corpus, we report an increase
of over 16 BLEU.
  We also demonstrate the problem in current methods that rely on gloss
supervision. The video-to-text translation of our STMC-Transformer outperforms
translation of GT glosses. This contradicts previous claims that GT gloss
translation acts as an upper bound for SLT performance and reveals that glosses
are an inefficient representation of sign language. For future SLT research, we
therefore suggest an end-to-end training of the recognition and translation
models, or using a different sign language annotation scheme.
",sign language translation slt first use sign language recognition slr system extract sign language gloss video translation system generate speak language translation sign language gloss paper focus translation system introduce stmc transformer improve current state of the art 5 7 bleu respectively gloss to text video to text translation phoenix weather 2014 t dataset aslg pc12 corpus report increase 16 bleu also demonstrate problem current method rely glos supervision video to text translation stmc transformer outperform translation gt gloss contradict previous claim gt gloss translation act upper bind slt performance reveal gloss inefficient representation sign language future slt research therefore suggest end to end training recognition translation model use different sign language annotation scheme
Mycorrhiza: Genotype Assignment usingPhylogenetic Networks,"  Motivation The genotype assignment problem consists of predicting, from the
genotype of an individual, which of a known set of populations it originated
from. The problem arises in a variety of contexts, including wildlife
forensics, invasive species detection and biodiversity monitoring. Existing
approaches perform well under ideal conditions but are sensitive to a variety
of common violations of the assumptions they rely on. Results In this article,
we introduce Mycorrhiza, a machine learning approach for the genotype
assignment problem. Our algorithm makes use of phylogenetic networks to
engineer features that encode the evolutionary relationships among samples.
Those features are then used as input to a Random Forests classifier. The
classification accuracy was assessed on multiple published empirical SNP,
microsatellite or consensus sequence datasets with wide ranges of size,
geographical distribution and population structure and on simulated datasets.
It compared favorably against widely used assessment tests or mixture analysis
methods such as STRUCTURE and Admixture, and against another machine-learning
based approach using principal component analysis for dimensionality reduction.
Mycorrhiza yields particularly significant gains on datasets with a large
average fixation index (FST) or deviation from the Hardy-Weinberg equilibrium.
Moreover, the phylogenetic network approach estimates mixture proportions with
good accuracy.
",motivation genotype assignment problem consist predict genotype individual know set population originate problem arise variety context include wildlife forensic invasive specie detection biodiversity monitor exist approach perform well ideal condition sensitive variety common violation assumption rely result article introduce mycorrhiza machine learn approach genotype assignment problem algorithm make use phylogenetic network engineer feature encode evolutionary relationship among sample feature use input random forest classifier classification accuracy assess multiple publish empirical snp microsatellite consensus sequence dataset wide range size geographical distribution population structure simulate dataset compare favorably widely use assessment test mixture analysis method structure admixture another machine learn base approach use principal component analysis dimensionality reduction mycorrhiza yield particularly significant gain dataset large average fixation index fst deviation hardy weinberg equilibrium moreover phylogenetic network approach estimate mixture proportion good accuracy
Proximal Policy Optimization Algorithms,"  We propose a new family of policy gradient methods for reinforcement
learning, which alternate between sampling data through interaction with the
environment, and optimizing a ""surrogate"" objective function using stochastic
gradient ascent. Whereas standard policy gradient methods perform one gradient
update per data sample, we propose a novel objective function that enables
multiple epochs of minibatch updates. The new methods, which we call proximal
policy optimization (PPO), have some of the benefits of trust region policy
optimization (TRPO), but they are much simpler to implement, more general, and
have better sample complexity (empirically). Our experiments test PPO on a
collection of benchmark tasks, including simulated robotic locomotion and Atari
game playing, and we show that PPO outperforms other online policy gradient
methods, and overall strikes a favorable balance between sample complexity,
simplicity, and wall-time.
",propose new family policy gradient method reinforcement learn alternate sampling datum interaction environment optimize surrogate objective function use stochastic gradient ascent whereas standard policy gradient method perform one gradient update per datum sample propose novel objective function enable multiple epoch minibatch update new method call proximal policy optimization ppo benefits trust region policy optimization trpo much simple implement general well sample complexity empirically experiment test ppo collection benchmark task include simulate robotic locomotion atari game playing show ppo outperform online policy gradient method overall strike favorable balance sample complexity simplicity wall time
Describing a Knowledge Base,"  We aim to automatically generate natural language descriptions about an input
structured knowledge base (KB). We build our generation framework based on a
pointer network which can copy facts from the input KB, and add two attention
mechanisms: (i) slot-aware attention to capture the association between a slot
type and its corresponding slot value; and (ii) a new \emph{table position
self-attention} to capture the inter-dependencies among related slots. For
evaluation, besides standard metrics including BLEU, METEOR, and ROUGE, we
propose a KB reconstruction based metric by extracting a KB from the generation
output and comparing it with the input KB. We also create a new data set which
includes 106,216 pairs of structured KBs and their corresponding natural
language descriptions for two distinct entity types. Experiments show that our
approach significantly outperforms state-of-the-art methods. The reconstructed
KB achieves 68.8% - 72.6% F-score.
",aim automatically generate natural language description input structured knowledge base kb build generation framework base pointer network copy fact input kb add two attention mechanism slot aware attention capture association slot type correspond slot value ii new table position self attention capture inter dependency among relate slot evaluation besides standard metric include bleu meteor rouge propose kb reconstruction base metric extract kb generation output compare input kb also create new datum set include pair structured kb correspond natural language description two distinct entity type experiment show approach significantly outperform state of the art method reconstruct kb achieve f score
"Minimum Description Length Induction, Bayesianism, and Kolmogorov
  Complexity","  The relationship between the Bayesian approach and the minimum description
length approach is established. We sharpen and clarify the general modeling
principles MDL and MML, abstracted as the ideal MDL principle and defined from
Bayes's rule by means of Kolmogorov complexity. The basic condition under which
the ideal principle should be applied is encapsulated as the Fundamental
Inequality, which in broad terms states that the principle is valid when the
data are random, relative to every contemplated hypothesis and also these
hypotheses are random relative to the (universal) prior. Basically, the ideal
principle states that the prior probability associated with the hypothesis
should be given by the algorithmic universal probability, and the sum of the
log universal probability of the model plus the log of the probability of the
data given the model should be minimized. If we restrict the model class to the
finite sets then application of the ideal principle turns into Kolmogorov's
minimal sufficient statistic. In general we show that data compression is
almost always the best strategy, both in hypothesis identification and
prediction.
",relationship bayesian approach minimum description length approach establish sharpen clarify general modeling principle mdl mml abstract ideal mdl principle define baye rule mean kolmogorov complexity basic condition ideal principle apply encapsulate fundamental inequality broad term state principle valid datum random relative every contemplated hypothesis also hypothese random relative universal prior basically ideal principle state prior probability associate hypothesis give algorithmic universal probability sum log universal probability model plus log probability datum give model minimized restrict model class finite set application ideal principle turn kolmogorov minimal sufficient statistic general show datum compression almost always good strategy hypothesis identification prediction
"Multiple functional regression with both discrete and continuous
  covariates","  In this paper we present a nonparametric method for extending functional
regression methodology to the situation where more than one functional
covariate is used to predict a functional response. Borrowing the idea from
Kadri et al. (2010a), the method, which support mixed discrete and continuous
explanatory variables, is based on estimating a function-valued function in
reproducing kernel Hilbert spaces by virtue of positive operator-valued
kernels.
",paper present nonparametric method extend functional regression methodology situation one functional covariate use predict functional response borrowing idea kadri et al 2010a method support mixed discrete continuous explanatory variable base estimate function value function reproduce kernel hilbert space virtue positive operator value kernel
"Stochastic Primal-Dual Method for Empirical Risk Minimization with
  $\mathcal{O}(1)$ Per-Iteration Complexity","  Regularized empirical risk minimization problem with linear predictor appears
frequently in machine learning. In this paper, we propose a new stochastic
primal-dual method to solve this class of problems. Different from existing
methods, our proposed methods only require O(1) operations in each iteration.
We also develop a variance-reduction variant of the algorithm that converges
linearly. Numerical experiments suggest that our methods are faster than
existing ones such as proximal SGD, SVRG and SAGA on high-dimensional problems.
",regularize empirical risk minimization problem linear predictor appear frequently machine learn paper propose new stochastic primal dual method solve class problem different exist method propose method require 1 operation iteration also develop variance reduction variant algorithm converge linearly numerical experiment suggest method fast exist one proximal sgd svrg saga high dimensional problem
Variational Graph Auto-Encoders,"  We introduce the variational graph auto-encoder (VGAE), a framework for
unsupervised learning on graph-structured data based on the variational
auto-encoder (VAE). This model makes use of latent variables and is capable of
learning interpretable latent representations for undirected graphs. We
demonstrate this model using a graph convolutional network (GCN) encoder and a
simple inner product decoder. Our model achieves competitive results on a link
prediction task in citation networks. In contrast to most existing models for
unsupervised learning on graph-structured data and link prediction, our model
can naturally incorporate node features, which significantly improves
predictive performance on a number of benchmark datasets.
",introduce variational graph auto encoder vgae framework unsupervised learn graph structure datum base variational auto encoder vae model make use latent variable capable learn interpretable latent representation undirected graph demonstrate model use graph convolutional network gcn encoder simple inner product decoder model achieve competitive result link prediction task citation network contrast exist model unsupervise learn graph structure datum link prediction model naturally incorporate node feature significantly improve predictive performance number benchmark dataset
"CNN-Cap: Effective Convolutional Neural Network Based Capacitance Models
  for Full-Chip Parasitic Extraction","  Accurate capacitance extraction is becoming more important for designing
integrated circuits under advanced process technology. The pattern matching
based full-chip extraction methodology delivers fast computational speed, but
suffers from large error, and tedious efforts on building capacitance models of
the increasing structure patterns. In this work, we propose an effective method
for building convolutional neural network (CNN) based capacitance models
(called CNN-Cap) for two-dimensional (2-D) structures in full-chip capacitance
extraction. With a novel grid-based data representation, the proposed method is
able to model the pattern with a variable number of conductors, so that largely
reduce the number of patterns. Based on the ability of ResNet architecture on
capturing spatial information and the proposed training skills, the obtained
CNN-Cap exhibits much better performance over the multilayer perception neural
network based capacitance model while being more versatile. Extensive
experiments on a 55nm and a 15nm process technologies have demonstrated that
the error of total capacitance produced with CNN-Cap is always within 1.3% and
the error of produced coupling capacitance is less than 10% in over 99.5%
probability. CNN-Cap runs more than 4000X faster than 2-D field solver on a GPU
server, while it consumes negligible memory compared to the look-up table based
capacitance model.
",accurate capacitance extraction become important designing integrate circuit advanced process technology pattern matching base full chip extraction methodology deliver fast computational speed suffer large error tedious effort build capacitance model increase structure pattern work propose effective method build convolutional neural network cnn base capacitance model call cnn cap two dimensional 2 d structure full chip capacitance extraction novel grid base datum representation propose method able model pattern variable number conductor largely reduce number pattern base ability resnet architecture capture spatial information propose training skill obtain cnn cap exhibit much well performance multilayer perception neural network base capacitance model versatile extensive experiment 55 nm 15 nm process technology demonstrate error total capacitance produce cnn cap always within error produce coupling capacitance less 10 probability cnn cap run 4000x fast 2 d field solver gpu server consumes negligible memory compare look up table base capacitance model
"Heterogeneous Treatment Effect Estimation using machine learning for
  Healthcare application: tutorial and benchmark","  Developing new drugs for target diseases is a time-consuming and expensive
task, drug repurposing has become a popular topic in the drug development
field. As much health claim data become available, many studies have been
conducted on the data. The real-world data is noisy, sparse, and has many
confounding factors. In addition, many studies have shown that drugs effects
are heterogeneous among the population. Lots of advanced machine learning
models about estimating heterogeneous treatment effects (HTE) have emerged in
recent years, and have been applied to in econometrics and machine learning
communities. These studies acknowledge medicine and drug development as the
main application area, but there has been limited translational research from
the HTE methodology to drug development. We aim to introduce the HTE
methodology to the healthcare area and provide feasibility consideration when
translating the methodology with benchmark experiments on healthcare
administrative claim data. Also, we want to use benchmark experiments to show
how to interpret and evaluate the model when it is applied to healthcare
research. By introducing the recent HTE techniques to a broad readership in
biomedical informatics communities, we expect to promote the wide adoption of
causal inference using machine learning. We also expect to provide the
feasibility of HTE for personalized drug effectiveness.
",develop new drug target disease time consume expensive task drug repurposing become popular topic drug development field much health claim datum become available many study conduct datum real world datum noisy sparse many confound factor addition many study show drug effect heterogeneous among population lot advanced machine learning model estimate heterogeneous treatment effect hte emerge recent year apply econometric machine learn community study acknowledge medicine drug development main application area limited translational research hte methodology drug development aim introduce hte methodology healthcare area provide feasibility consideration translate methodology benchmark experiment healthcare administrative claim datum also want use benchmark experiment show interpret evaluate model applied healthcare research introduce recent hte technique broad readership biomedical informatic community expect promote wide adoption causal inference use machine learning also expect provide feasibility hte personalize drug effectiveness
"Cost-sensitive Learning for Utility Optimization in Online Advertising
  Auctions","  One of the most challenging problems in computational advertising is the
prediction of click-through and conversion rates for bidding in online
advertising auctions. An unaddressed problem in previous approaches is the
existence of highly non-uniform misprediction costs. While for model evaluation
these costs have been taken into account through recently proposed
business-aware offline metrics -- such as the Utility metric which measures the
impact on advertiser profit -- this is not the case when training the models
themselves. In this paper, to bridge the gap, we formally analyze the
relationship between optimizing the Utility metric and the log loss, which is
considered as one of the state-of-the-art approaches in conversion modeling.
Our analysis motivates the idea of weighting the log loss with the business
value of the predicted outcome. We present and analyze a new cost weighting
scheme and show that significant gains in offline and online performance can be
achieved.
",one challenging problem computational advertising prediction click through conversion rate bid online advertising auction unaddresse problem previous approach existence highly non uniform misprediction cost model evaluation cost take account recently propose business aware offline metric utility metric measure impact advertiser profit case training model paper bridge gap formally analyze relationship optimize utility metric log loss consider one state of the art approach conversion modeling analysis motivate idea weight log loss business value predict outcome present analyze new cost weighting scheme show significant gain offline online performance achieve
"Empirical Evaluation of Pretraining Strategies for Supervised Entity
  Linking","  In this work, we present an entity linking model which combines a Transformer
architecture with large scale pretraining from Wikipedia links. Our model
achieves the state-of-the-art on two commonly used entity linking datasets:
96.7% on CoNLL and 94.9% on TAC-KBP. We present detailed analyses to understand
what design choices are important for entity linking, including choices of
negative entity candidates, Transformer architecture, and input perturbations.
Lastly, we present promising results on more challenging settings such as
end-to-end entity linking and entity linking without in-domain training data.
",work present entity link model combine transformer architecture large scale pretraine wikipedia link model achieve state of the art two commonly use entity link dataset conll tac kbp present detailed analysis understand design choice important entity link include choice negative entity candidate transformer architecture input perturbation lastly present promising result challenge setting end to end entity link entity link without in domain training datum
Time Series Clustering for Human Behavior Pattern Mining,"  Human behavior modeling deals with learning and understanding behavior
patterns inherent in humans' daily routines. Existing pattern mining techniques
either assume human dynamics is strictly periodic, or require the number of
modes as input, or do not consider uncertainty in the sensor data. To handle
these issues, in this paper, we propose a novel clustering approach for
modeling human behavior (named, MTpattern) from time-series data. For mining
frequent human behavior patterns effectively, we utilize a three-stage
pipeline: (1) represent time series data into a sequence of regularly sampled
equal-sized unit time intervals for better analysis, (2) a new distance measure
scheme is proposed to cluster similar sequences which can handle temporal
variation and uncertainty in the data, and (3) exploit an exemplar-based
clustering mechanism and fine-tune its parameters to output minimum number of
clusters with given permissible distance constraints and without knowing the
number of modes present in the data. Then, the average of all sequences in a
cluster is considered as a human behavior pattern. Empirical studies on two
real-world datasets and a simulated dataset demonstrate the effectiveness of
MTpattern with respect to internal and external measures of clustering.
",human behavior modeling deal learn understand behavior pattern inherent human daily routine exist pattern mining technique either assume human dynamic strictly periodic require number mode input consider uncertainty sensor datum handle issue paper propose novel clustering approach model human behavior name mtpattern time series datum mining frequent human behavior pattern effectively utilize three stage pipeline 1 represent time series datum sequence regularly sample equal sized unit time interval well analysis 2 new distance measure scheme propose cluster similar sequence handle temporal variation uncertainty datum 3 exploit exemplar base clustering mechanism fine tune parameter output minimum number cluster give permissible distance constraint without know number mode present datum average sequence cluster consider human behavior pattern empirical study two real world dataset simulate dataset demonstrate effectiveness mtpattern respect internal external measure cluster
"Deep Within-Class Covariance Analysis for Robust Audio Representation
  Learning","  Convolutional Neural Networks (CNNs) can learn effective features, though
have been shown to suffer from a performance drop when the distribution of the
data changes from training to test data. In this paper we analyze the internal
representations of CNNs and observe that the representations of unseen data in
each class, spread more (with higher variance) in the embedding space of the
CNN compared to representations of the training data. More importantly, this
difference is more extreme if the unseen data comes from a shifted
distribution. Based on this observation, we objectively evaluate the degree of
representation's variance in each class via eigenvalue decomposition on the
within-class covariance of the internal representations of CNNs and observe the
same behaviour. This can be problematic as larger variances might lead to
mis-classification if the sample crosses the decision boundary of its class. We
apply nearest neighbor classification on the representations and empirically
show that the embeddings with the high variance actually have significantly
worse KNN classification performances, although this could not be foreseen from
their end-to-end classification results. To tackle this problem, we propose
Deep Within-Class Covariance Analysis (DWCCA), a deep neural network layer that
significantly reduces the within-class covariance of a DNN's representation,
improving performance on unseen test data from a shifted distribution. We
empirically evaluate DWCCA on two datasets for Acoustic Scene Classification
(DCASE2016 and DCASE2017). We demonstrate that not only does DWCCA
significantly improve the network's internal representation, it also increases
the end-to-end classification accuracy, especially when the test set exhibits a
distribution shift. By adding DWCCA to a VGG network, we achieve around 6
percentage points improvement in the case of a distribution mismatch.
",convolutional neural network cnn learn effective feature though show suffer performance drop distribution data change train test datum paper analyze internal representations cnn observe representation unseen datum class spread high variance embed space cnn compare representation training datum importantly difference extreme unseen datum comes shift distribution base observation objectively evaluate degree representation variance class via eigenvalue decomposition within class covariance internal representations cnns observe behaviour problematic large variance might lead mis classification sample crosse decision boundary class apply near neighbor classification representation empirically show embedding high variance actually significantly bad knn classification performance although could foresee end to end classification result tackle problem propose deep within class covariance analysis dwcca deep neural network layer significantly reduce within class covariance dnn representation improve performance unseen test datum shift distribution empirically evaluate dwcca two dataset acoustic scene classification dcase2016 dcase2017 demonstrate dwcca significantly improve network internal representation also increase end to end classification accuracy especially test set exhibit distribution shift add dwcca vgg network achieve around 6 percentage point improvement case distribution mismatch
"Automate Obstructive Sleep Apnea Diagnosis Using Convolutional Neural
  Networks","  Identifying sleep problem severity from overnight polysomnography (PSG)
recordings plays an important role in diagnosing and treating sleep disorders
such as the Obstructive Sleep Apnea (OSA). This analysis traditionally is done
by specialists manually through visual inspections, which can be tedious,
time-consuming, and is prone to subjective errors. One of the solutions is to
use Convolutional Neural Networks (CNN) where the convolutional and pooling
layers behave as feature extractors and some fully-connected (FCN) layers are
used for making final predictions for the OSA severity. In this paper, a CNN
architecture with 1D convolutional and FCN layers for classification is
presented. The PSG data for this project are from the Cleveland Children's
Sleep and Health Study database and classification results confirm the
effectiveness of the proposed CNN method. The proposed 1D CNN model achieves
excellent classification results without manually preprocesssing PSG signals
such as feature extraction and feature reduction.
",identify sleep problem severity overnight polysomnography psg recording play important role diagnose treat sleep disorder obstructive sleep apnea osa analysis traditionally do specialist manually visual inspection tedious time consume prone subjective error one solution use convolutional neural network cnn convolutional pooling layer behave feature extractor fully connect fcn layer use make final prediction osa severity paper cnn architecture 1d convolutional fcn layer classification present psg data project cleveland child sleep health study database classification result confirm effectiveness propose cnn method propose 1d cnn model achieve excellent classification result without manually preprocessse psg signal feature extraction feature reduction
Automating Turbulence Modeling by Multi-Agent Reinforcement Learning,"  The modeling of turbulent flows is critical to scientific and engineering
problems ranging from aircraft design to weather forecasting and climate
prediction. Over the last sixty years numerous turbulence models have been
proposed, largely based on physical insight and engineering intuition. Recent
advances in machine learning and data science have incited new efforts to
complement these approaches. To date, all such efforts have focused on
supervised learning which, despite demonstrated promise, encounters
difficulties in generalizing beyond the distributions of the training data. In
this work we introduce multi-agent reinforcement learning (MARL) as an
automated discovery tool of turbulence models. We demonstrate the potential of
this approach on Large Eddy Simulations of homogeneous and isotropic turbulence
using as reward the recovery of the statistical properties of Direct Numerical
Simulations. Here, the closure model is formulated as a control policy enacted
by cooperating agents, which detect critical spatio-temporal patterns in the
flow field to estimate the unresolved sub-grid scale (SGS) physics. The present
results are obtained with state-of-the-art algorithms based on experience
replay and compare favorably with established dynamic SGS modeling approaches.
Moreover, we show that the present turbulence models generalize across grid
sizes and flow conditions as expressed by the Reynolds numbers.
",model turbulent flow critical scientific engineering problem range aircraft design weather forecasting climate prediction last sixty year numerous turbulence model propose largely base physical insight engineering intuition recent advance machine learn datum science incite new effort complement approach date effort focus supervised learning despite demonstrate promise encounter difficulty generalize beyond distribution train datum work introduce multi agent reinforcement learn marl automate discovery tool turbulence model demonstrate potential approach large eddy simulation homogeneous isotropic turbulence use reward recovery statistical property direct numerical simulation closure model formulate control policy enact cooperate agent detect critical spatio temporal pattern flow field estimate unresolved sub grid scale sgs physics present result obtain state of the art algorithm base experience replay compare favorably establish dynamic sgs modeling approach moreover show present turbulence model generalize across grid size flow condition express reynold number
Riemannian Metric Learning via Optimal Transport,"We introduce an optimal transport-based model for learning a metric tensor
from cross-sectional samples of evolving probability measures on a common
Riemannian manifold. We neurally parametrize the metric as a spatially-varying
matrix field and efficiently optimize our model's objective using
backpropagation. Using this learned metric, we can nonlinearly interpolate
between probability measures and compute geodesics on the manifold. We show
that metrics learned using our method improve the quality of trajectory
inference on scRNA and bird migration data at the cost of little additional
cross-sectional data.",introduce optimal transport base model learn metric tensor cross sectional sample evolve probability measure common riemannian manifold neurally parametrize metric spatially vary matrix field efficiently optimize model objective use backpropagation use learn metric nonlinearly interpolate probability measure compute geodesic manifold show metric learn use method improve quality trajectory inference scrna bird migration datum cost little additional cross sectional datum
Robust Attack Graph Generation,"  We present a method to learn automaton models that are more robust to input
modifications. It iteratively aligns sequences to a learned model, modifies the
sequences to their aligned versions, and re-learns the model. Automaton
learning algorithms are typically very good at modeling the frequent behavior
of a software system. Our solution can be used to also learn the behavior
present in infrequent sequences, as these will be aligned to the frequent ones
represented by the model. We apply our method to the SAGE tool for modeling
attacker behavior from intrusion alerts. In experiments, we demonstrate that
our algorithm learns models that can handle noise such as added and removed
symbols from sequences. Furthermore, it learns more concise models that fit
better to the training data.
",present method learn automaton model robust input modification iteratively align sequence learn model modifie sequence align version re learn model automaton learning algorithm typically good modeling frequent behavior software system solution use also learn behavior present infrequent sequence align frequent one represent model apply method sage tool modeling attacker behavior intrusion alert experiment demonstrate algorithm learn model handle noise add remove symbol sequence furthermore learn concise model fit well training datum
Minority Class Oriented Active Learning for Imbalanced Datasets,"  Active learning aims to optimize the dataset annotation process when
resources are constrained. Most existing methods are designed for balanced
datasets. Their practical applicability is limited by the fact that a majority
of real-life datasets are actually imbalanced. Here, we introduce a new active
learning method which is designed for imbalanced datasets. It favors samples
likely to be in minority classes so as to reduce the imbalance of the labeled
subset and create a better representation for these classes. We also compare
two training schemes for active learning: (1) the one commonly deployed in deep
active learning using model fine tuning for each iteration and (2) a scheme
which is inspired by transfer learning and exploits generic pre-trained models
and train shallow classifiers for each iteration. Evaluation is run with three
imbalanced datasets. Results show that the proposed active learning method
outperforms competitive baselines. Equally interesting, they also indicate that
the transfer learning training scheme outperforms model fine tuning if features
are transferable from the generic dataset to the unlabeled one. This last
result is surprising and should encourage the community to explore the design
of deep active learning methods.
",active learning aim optimize dataset annotation process resource constrain exist method design balanced dataset practical applicability limited fact majority real life dataset actually imbalance introduce new active learning method design imbalance dataset favor sample likely minority class reduce imbalance label subset create well representation class also compare two training scheme active learn 1 one commonly deploy deep active learning use model fine tuning iteration 2 scheme inspire transfer learning exploit generic pre train model train shallow classifier iteration evaluation run three imbalance dataset result show propose active learning method outperform competitive baseline equally interesting also indicate transfer learn training scheme outperform model fine tuning feature transferable generic dataset unlabele one last result surprising encourage community explore design deep active learning method
DP-REC: Private & Communication-Efficient Federated Learning,"  Privacy and communication efficiency are important challenges in federated
training of neural networks, and combining them is still an open problem. In
this work, we develop a method that unifies highly compressed communication and
differential privacy (DP). We introduce a compression technique based on
Relative Entropy Coding (REC) to the federated setting. With a minor
modification to REC, we obtain a provably differentially private learning
algorithm, DP-REC, and show how to compute its privacy guarantees. Our
experiments demonstrate that DP-REC drastically reduces communication costs
while providing privacy guarantees comparable to the state-of-the-art.
",privacy communication efficiency important challenge federate training neural network combine still open problem work develop method unify highly compressed communication differential privacy dp introduce compression technique base relative entropy code rec federate set minor modification rec obtain provably differentially private learn algorithm dp rec show compute privacy guarantee experiment demonstrate dp rec drastically reduce communication cost provide privacy guarantee comparable state of the art
Information Maximizing Exploration with a Latent Dynamics Model,"  All reinforcement learning algorithms must handle the trade-off between
exploration and exploitation. Many state-of-the-art deep reinforcement learning
methods use noise in the action selection, such as Gaussian noise in policy
gradient methods or $\epsilon$-greedy in Q-learning. While these methods are
appealing due to their simplicity, they do not explore the state space in a
methodical manner. We present an approach that uses a model to derive reward
bonuses as a means of intrinsic motivation to improve model-free reinforcement
learning. A key insight of our approach is that this dynamics model can be
learned in the latent feature space of a value function, representing the
dynamics of the agent and the environment. This method is both theoretically
grounded and computationally advantageous, permitting the efficient use of
Bayesian information-theoretic methods in high-dimensional state spaces. We
evaluate our method on several continuous control tasks, focusing on improving
exploration.
",reinforcement learning algorithm must handle trade off exploration exploitation many state of the art deep reinforcement learning method use noise action selection gaussian noise policy gradient method -greedy q learn method appeal due simplicity explore state space methodical manner present approach use model derive reward bonus mean intrinsic motivation improve model free reinforcement learning key insight approach dynamic model learn latent feature space value function represent dynamic agent environment method theoretically ground computationally advantageous permit efficient use bayesian information theoretic method high dimensional state space evaluate method several continuous control task focus improve exploration
MISeval: a Metric Library for Medical Image Segmentation Evaluation,"  Correct performance assessment is crucial for evaluating modern artificial
intelligence algorithms in medicine like deep-learning based medical image
segmentation models. However, there is no universal metric library in Python
for standardized and reproducible evaluation. Thus, we propose our open-source
publicly available Python package MISeval: a metric library for Medical Image
Segmentation Evaluation. The implemented metrics can be intuitively used and
easily integrated into any performance assessment pipeline. The package
utilizes modern CI/CD strategies to ensure functionality and stability. MISeval
is available from PyPI (miseval) and GitHub:
https://github.com/frankkramer-lab/miseval.
",correct performance assessment crucial evaluate modern artificial intelligence algorithm medicine like deep learn base medical image segmentation model however universal metric library python standardize reproducible evaluation thus propose open source publicly available python package miseval metric library medical image segmentation evaluation implement metric intuitively use easily integrate performance assessment pipeline package utilize modern strategy ensure functionality stability miseval available pypi miseval github https
ChemicalX: A Deep Learning Library for Drug Pair Scoring,"  In this paper, we introduce ChemicalX, a PyTorch-based deep learning library
designed for providing a range of state of the art models to solve the drug
pair scoring task. The primary objective of the library is to make deep drug
pair scoring models accessible to machine learning researchers and
practitioners in a streamlined framework.The design of ChemicalX reuses
existing high level model training utilities, geometric deep learning, and deep
chemistry layers from the PyTorch ecosystem. Our system provides neural network
layers, custom pair scoring architectures, data loaders, and batch iterators
for end users. We showcase these features with example code snippets and case
studies to highlight the characteristics of ChemicalX. A range of experiments
on real world drug-drug interaction, polypharmacy side effect, and combination
synergy prediction tasks demonstrate that the models available in ChemicalX are
effective at solving the pair scoring task. Finally, we show that ChemicalX
could be used to train and score machine learning models on large drug pair
datasets with hundreds of thousands of compounds on commodity hardware.
",paper introduce chemicalx pytorch base deep learning library design provide range state art model solve drug pair scoring task primary objective library make deep drug pair scoring model accessible machine learn researcher practitioner streamline design chemicalx reuse exist high level model training utility geometric deep learn deep chemistry layer pytorch ecosystem system provide neural network layer custom pair scoring architecture datum loader batch iterator end user showcase feature example code snippet case study highlight characteristic chemicalx range experiment real world drug drug interaction polypharmacy side effect combination synergy prediction task demonstrate model available chemicalx effective solve pair scoring task finally show chemicalx could use train score machine learning model large drug pair dataset hundred thousand compound commodity hardware
Goldilocks Neural Networks,"  We introduce the new ""Goldilocks"" class of activation functions, which
non-linearly deform the input signal only locally when the input signal is in
the appropriate range. The small local deformation of the signal enables better
understanding of how and why the signal is transformed through the layers.
Numerical results on CIFAR-10 and CIFAR-100 data sets show that Goldilocks
networks perform better than, or comparably to SELU and RELU, while introducing
tractability of data deformation through the layers.
",introduce new goldilock class activation function non linearly deform input signal locally input signal appropriate range small local deformation signal enable well understanding signal transform layer numerical result cifar-10 cifar-100 data set show goldilock network perform well comparably selu relu introduce tractability datum deformation layer
Clustering Algorithms: A Comparative Approach,"  Many real-world systems can be studied in terms of pattern recognition tasks,
so that proper use (and understanding) of machine learning methods in practical
applications becomes essential. While a myriad of classification methods have
been proposed, there is no consensus on which methods are more suitable for a
given dataset. As a consequence, it is important to comprehensively compare
methods in many possible scenarios. In this context, we performed a systematic
comparison of 7 well-known clustering methods available in the R language. In
order to account for the many possible variations of data, we considered
artificial datasets with several tunable properties (number of classes,
separation between classes, etc). In addition, we also evaluated the
sensitivity of the clustering methods with regard to their parameters
configuration. The results revealed that, when considering the default
configurations of the adopted methods, the spectral approach usually
outperformed the other clustering algorithms. We also found that the default
configuration of the adopted implementations was not accurate. In these cases,
a simple approach based on random selection of parameters values proved to be a
good alternative to improve the performance. All in all, the reported approach
provides subsidies guiding the choice of clustering algorithms.
",many real world system study term pattern recognition task proper use understand machine learn method practical application become essential myriad classification method propose consensus method suitable give dataset consequence important comprehensively compare method many possible scenario context perform systematic comparison 7 well know clustering method available r language order account many possible variation datum consider artificial dataset several tunable property number class separation class etc addition also evaluate sensitivity cluster method regard parameter configuration result reveal consider default configuration adopt method spectral approach usually outperform cluster algorithm also find default configuration adopt implementation accurate case simple approach base random selection parameter value prove good alternative improve performance report approach provide subsidy guide choice cluster algorithm
High-Performance Support Vector Machines and Its Applications,"  The support vector machines (SVM) algorithm is a popular classification
technique in data mining and machine learning. In this paper, we propose a
distributed SVM algorithm and demonstrate its use in a number of applications.
The algorithm is named high-performance support vector machines (HPSVM). The
major contribution of HPSVM is two-fold. First, HPSVM provides a new way to
distribute computations to the machines in the cloud without shuffling the
data. Second, HPSVM minimizes the inter-machine communications in order to
maximize the performance. We apply HPSVM to some real-world classification
problems and compare it with the state-of-the-art SVM technique implemented in
R on several public data sets. HPSVM achieves similar or better results.
",support vector machine svm algorithm popular classification technique data mining machine learn paper propose distribute svm algorithm demonstrate use number application algorithm name high performance support vector machine hpsvm major contribution hpsvm two fold first hpsvm provide new way distribute computation machine cloud without shuffle datum second hpsvm minimize inter machine communication order maximize performance apply hpsvm real world classification problem compare state of the art svm technique implement r several public datum set hpsvm achieve similar well result
CLUES: Few-Shot Learning Evaluation in Natural Language Understanding,"  Most recent progress in natural language understanding (NLU) has been driven,
in part, by benchmarks such as GLUE, SuperGLUE, SQuAD, etc. In fact, many NLU
models have now matched or exceeded ""human-level"" performance on many tasks in
these benchmarks. Most of these benchmarks, however, give models access to
relatively large amounts of labeled data for training. As such, the models are
provided far more data than required by humans to achieve strong performance.
That has motivated a line of work that focuses on improving few-shot learning
performance of NLU models. However, there is a lack of standardized evaluation
benchmarks for few-shot NLU resulting in different experimental settings in
different papers. To help accelerate this line of work, we introduce CLUES
(Constrained Language Understanding Evaluation Standard), a benchmark for
evaluating the few-shot learning capabilities of NLU models. We demonstrate
that while recent models reach human performance when they have access to large
amounts of labeled data, there is a huge gap in performance in the few-shot
setting for most tasks. We also demonstrate differences between alternative
model families and adaptation techniques in the few shot setting. Finally, we
discuss several principles and choices in designing the experimental settings
for evaluating the true few-shot learning performance and suggest a unified
standardized approach to few-shot learning evaluation. We aim to encourage
research on NLU models that can generalize to new tasks with a small number of
examples. Code and data for CLUES are available at
https://github.com/microsoft/CLUES.
",recent progress natural language understand nlu drive part benchmark glue superglue squad etc fact many nlu model match exceed human level performance many task benchmark benchmark however give model access relatively large amount label datum training model provide far datum require human achieve strong performance motivated line work focus improve few shot learning performance nlu model however lack standardized evaluation benchmark few shot nlu result different experimental setting different paper help accelerate line work introduce clue constrain language understand evaluation standard benchmark evaluate few shot learning capability nlu model demonstrate recent model reach human performance access large amount label datum huge gap performance few shot set task also demonstrate difference alternative model family adaptation technique shot set finally discuss several principle choice design experimental setting evaluate true few shot learning performance suggest unified standardized approach few shot learn evaluation aim encourage research nlu model generalize new task small number example code datum clue available https
GCN for HIN via Implicit Utilization of Attention and Meta-paths,"  Heterogeneous information network (HIN) embedding, aiming to map the
structure and semantic information in a HIN to distributed representations, has
drawn considerable research attention. Graph neural networks for HIN embeddings
typically adopt a hierarchical attention (including node-level and
meta-path-level attentions) to capture the information from meta-path-based
neighbors. However, this complicated attention structure often cannot achieve
the function of selecting meta-paths due to severe overfitting. Moreover, when
propagating information, these methods do not distinguish direct (one-hop)
meta-paths from indirect (multi-hop) ones. But from the perspective of network
science, direct relationships are often believed to be more essential, which
can only be used to model direct information propagation. To address these
limitations, we propose a novel neural network method via implicitly utilizing
attention and meta-paths, which can relieve the severe overfitting brought by
the current over-parameterized attention mechanisms on HIN. We first use the
multi-layer graph convolutional network (GCN) framework, which performs a
discriminative aggregation at each layer, along with stacking the information
propagation of direct linked meta-paths layer-by-layer, realizing the function
of attentions for selecting meta-paths in an indirect way. We then give an
effective relaxation and improvement via introducing a new propagation
operation which can be separated from aggregation. That is, we first model the
whole propagation process with well-defined probabilistic diffusion dynamics,
and then introduce a random graph-based constraint which allows it to reduce
noise with the increase of layers. Extensive experiments demonstrate the
superiority of the new approach over state-of-the-art methods.
",heterogeneous information network hin embed aiming map structure semantic information hin distribute representation draw considerable research attention graph neural network hin embedding typically adopt hierarchical attention include node level meta path level attention capture information meta path base neighbor however complicated attention structure often achieve function select meta path due severe overfitting moreover propagate information method distinguish direct one hop meta path indirect multi hop one perspective network science direct relationship often believe essential use model direct information propagation address limitation propose novel neural network method via implicitly utilize attention meta path relieve severe overfitting bring current over parameterize attention mechanism hin first use multi layer graph convolutional network gcn framework perform discriminative aggregation layer along stack information propagation direct link meta path layer by layer realize function attention select meta path indirect way give effective relaxation improvement via introduce new propagation operation separate aggregation first model whole propagation process well define probabilistic diffusion dynamic introduce random graph base constraint allow reduce noise increase layer extensive experiment demonstrate superiority new approach state of the art method
"Guiding attention in Sequence-to-sequence models for Dialogue Act
  prediction","  The task of predicting dialog acts (DA) based on conversational dialog is a
key component in the development of conversational agents. Accurately
predicting DAs requires a precise modeling of both the conversation and the
global tag dependencies. We leverage seq2seq approaches widely adopted in
Neural Machine Translation (NMT) to improve the modelling of tag sequentiality.
Seq2seq models are known to learn complex global dependencies while currently
proposed approaches using linear conditional random fields (CRF) only model
local tag dependencies. In this work, we introduce a seq2seq model tailored for
DA classification using: a hierarchical encoder, a novel guided attention
mechanism and beam search applied to both training and inference. Compared to
the state of the art our model does not require handcrafted features and is
trained end-to-end. Furthermore, the proposed approach achieves an unmatched
accuracy score of 85% on SwDA, and state-of-the-art accuracy score of 91.6% on
MRDA.
",task predict dialog act da base conversational dialog key component development conversational agent accurately predict das require precise modeling conversation global tag dependencie leverage seq2seq approach widely adopt neural machine translation nmt improve model tag sequentiality seq2seq model know learn complex global dependency currently propose approach use linear conditional random field crf model local tag dependencie work introduce seq2seq model tailor da classification use hierarchical encoder novel guide attention mechanism beam search apply training inference compare state art model require handcraft feature train end to end furthermore propose approach achieve unmatched accuracy score 85 swda state of the art accuracy score mrda
On the Inclusion Relation of Reproducing Kernel Hilbert Spaces,"  To help understand various reproducing kernels used in applied sciences, we
investigate the inclusion relation of two reproducing kernel Hilbert spaces.
Characterizations in terms of feature maps of the corresponding reproducing
kernels are established. A full table of inclusion relations among widely-used
translation invariant kernels is given. Concrete examples for Hilbert-Schmidt
kernels are presented as well. We also discuss the preservation of such a
relation under various operations of reproducing kernels. Finally, we briefly
discuss the special inclusion with a norm equivalence.
",help understand various reproducing kernel use apply sciences investigate inclusion relation two reproduce kernel hilbert space characterization term feature map correspond reproduce kernel establish full table inclusion relation among widely use translation invariant kernel give concrete example hilbert schmidt kernel present well also discuss preservation relation various operation reproduce kernel finally briefly discuss special inclusion norm equivalence
Machine Learning for Malware Evolution Detection,"  Malware evolves over time and antivirus must adapt to such evolution. Hence,
it is critical to detect those points in time where malware has evolved so that
appropriate countermeasures can be undertaken. In this research, we perform a
variety of experiments on a significant number of malware families to determine
when malware evolution is likely to have occurred. All of the evolution
detection techniques that we consider are based on machine learning and can be
fully automated -- in particular, no reverse engineering or other
labor-intensive manual analysis is required. Specifically, we consider analysis
based on hidden Markov models (HMM) and the word embedding techniques HMM2Vec
and Word2Vec.
",malware evolve time antivirus must adapt evolution hence critical detect point time malware evolve appropriate countermeasure undertake research perform variety experiment significant number malware family determine malware evolution likely occur evolution detection technique consider base machine learn fully automate particular reverse engineering labor intensive manual analysis require specifically consider analysis base hidden markov model hmm word embed technique hmm2vec word2vec
"Improving Federated Relational Data Modeling via Basis Alignment and
  Weight Penalty","  Federated learning (FL) has attracted increasing attention in recent years.
As a privacy-preserving collaborative learning paradigm, it enables a broader
range of applications, especially for computer vision and natural language
processing tasks. However, to date, there is limited research of federated
learning on relational data, namely Knowledge Graph (KG). In this work, we
present a modified version of the graph neural network algorithm that performs
federated modeling over KGs across different participants. Specifically, to
tackle the inherent data heterogeneity issue and inefficiency in algorithm
convergence, we propose a novel optimization algorithm, named FedAlign, with 1)
optimal transportation (OT) for on-client personalization and 2) weight
constraint to speed up the convergence. Extensive experiments have been
conducted on several widely used datasets. Empirical results show that our
proposed method outperforms the state-of-the-art FL methods, such as FedAVG and
FedProx, with better convergence.
",federate learning fl attract increase attention recent year privacy preserve collaborative learning paradigm enable broad range application especially computer vision natural language processing task however date limit research federate learn relational datum namely knowledge graph kg work present modify version graph neural network algorithm perform federate modeling kg across different participant specifically tackle inherent datum heterogeneity issue inefficiency algorithm convergence propose novel optimization algorithm name fedalign 1 optimal transportation ot on client personalization 2 weight constraint speed convergence extensive experiment conduct several widely use dataset empirical result show propose method outperform state of the art fl method fedavg fedprox well convergence
"End-to-End Environmental Sound Classification using a 1D Convolutional
  Neural Network","  In this paper, we present an end-to-end approach for environmental sound
classification based on a 1D Convolution Neural Network (CNN) that learns a
representation directly from the audio signal. Several convolutional layers are
used to capture the signal's fine time structure and learn diverse filters that
are relevant to the classification task. The proposed approach can deal with
audio signals of any length as it splits the signal into overlapped frames
using a sliding window. Different architectures considering several input sizes
are evaluated, including the initialization of the first convolutional layer
with a Gammatone filterbank that models the human auditory filter response in
the cochlea. The performance of the proposed end-to-end approach in classifying
environmental sounds was assessed on the UrbanSound8k dataset and the
experimental results have shown that it achieves 89% of mean accuracy.
Therefore, the propose approach outperforms most of the state-of-the-art
approaches that use handcrafted features or 2D representations as input.
Furthermore, the proposed approach has a small number of parameters compared to
other architectures found in the literature, which reduces the amount of data
required for training.
",paper present end to end approach environmental sound classification base 1d convolution neural network cnn learn representation directly audio signal several convolutional layer use capture signal fine time structure learn diverse filter relevant classification task propose approach deal audio signal length split signal overlapped frame use slide window different architecture consider several input size evaluate include initialization first convolutional layer gammatone filterbank model human auditory filter response cochlea performance propose end to end approach classify environmental sound assess urbansound8k dataset experimental result show achieve 89 mean accuracy therefore propose approach outperform state of the art approach use handcraft feature 2d representation input furthermore propose approach small number parameter compare architecture find literature reduce amount datum require training
Fast Botnet Detection From Streaming Logs Using Online Lanczos Method,"  Botnet, a group of coordinated bots, is becoming the main platform of
malicious Internet activities like DDOS, click fraud, web scraping, spam/rumor
distribution, etc. This paper focuses on design and experiment of a new
approach for botnet detection from streaming web server logs, motivated by its
wide applicability, real-time protection capability, ease of use and better
security of sensitive data. Our algorithm is inspired by a Principal Component
Analysis (PCA) to capture correlation in data, and we are first to recognize
and adapt Lanczos method to improve the time complexity of PCA-based botnet
detection from cubic to sub-cubic, which enables us to more accurately and
sensitively detect botnets with sliding time windows rather than fixed time
windows. We contribute a generalized online correlation matrix update formula,
and a new termination condition for Lanczos iteration for our purpose based on
error bound and non-decreasing eigenvalues of symmetric matrices. On our
dataset of an ecommerce website logs, experiments show the time cost of Lanczos
method with different time windows are consistently only 20% to 25% of PCA.
",botnet group coordinate bot become main platform malicious internet activity like ddo click fraud web scrape distribution etc paper focus design experiment new approach botnet detection stream web server log motivate wide applicability real time protection capability ease use well security sensitive data algorithm inspire principal component analysis pca capture correlation datum first recognize adapt lanczo method improve time complexity pca base botnet detection cubic sub cubic enable we accurately sensitively detect botnet slide time window rather fix time window contribute generalize online correlation matrix update formula new termination condition lanczos iteration purpose base error bind non decreasing eigenvalue symmetric matrix dataset ecommerce website log experiment show time cost lanczo method different time window consistently 20 25 pca
"GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy
  Efficient Inference","  Attention-based models have demonstrated remarkable success in various
natural language understanding tasks. However, efficient execution remains a
challenge for these models which are memory-bound due to their massive number
of parameters. We present GOBO, a model quantization technique that compresses
the vast majority (typically 99.9%) of the 32-bit floating-point parameters of
state-of-the-art BERT models and their variants to 3 bits while maintaining
their accuracy. Unlike other quantization methods, GOBO does not require
fine-tuning nor retraining to compensate for the quantization error. We present
two practical hardware applications of GOBO. In the first GOBO reduces memory
storage and traffic and as a result inference latency and energy consumption.
This GOBO memory compression mechanism is plug-in compatible with many
architectures; we demonstrate it with the TPU, Eyeriss, and an architecture
using Tensor Cores-like units. Second, we present a co-designed hardware
architecture that also reduces computation. Uniquely, the GOBO architecture
maintains most of the weights in 3b even during computation, a property that:
(1) makes the processing elements area efficient, allowing us to pack more
compute power per unit area, (2) replaces most multiply-accumulations with
additions, and (3) reduces the off-chip traffic by amplifying on-chip memory
capacity.
",attention base model demonstrate remarkable success various natural language understanding task however efficient execution remain challenge model memory bind due massive number parameter present gobo model quantization technique compress vast majority typically 32 bit float point parameter state of the art bert model variants 3 bit maintain accuracy unlike quantization method gobo require fine tune retrain compensate quantization error present two practical hardware application gobo first gobo reduce memory storage traffic result inference latency energy consumption gobo memory compression mechanism plug in compatible many architecture demonstrate tpu eyeriss architecture use tensor core like unit second present co design hardware architecture also reduce computation uniquely gobo architecture maintain weight 3b even computation property 1 make processing element area efficient allow we pack compute power per unit area 2 replace multiply accumulation addition 3 reduce off chip traffic amplifying on chip memory capacity
"A Convolutional Neural Network based Live Object Recognition System as
  Blind Aid","  This paper introduces a live object recognition system that serves as a blind
aid. Visually impaired people heavily rely on their other senses such as touch
and auditory signals for understanding the environment around them. The act of
knowing what object is in front of the blind person without touching it (by
hand or some other tool) is very difficult. In some cases, the physical contact
between the person and object can be dangerous, and even lethal.
  This project employs a Convolutional Neural Network for recognition of
pre-trained objects on the ImageNet dataset. A camera, aligned with the
system's predetermined orientation serves as input to the computer system,
which has the object recognition Neural Network deployed to carry out real-time
object detection. Output from the network can then be parsed to present to the
visually impaired person either in the form of audio or Braille text.
",paper introduce live object recognition system serve blind aid visually impair people heavily rely sense touch auditory signal understand environment around act know object front blind person without touch hand tool difficult case physical contact person object dangerous even lethal project employ convolutional neural network recognition pre train object imagenet dataset camera align system predetermine orientation serve input computer system object recognition neural network deploy carry real time object detection output network parse present visually impair person either form audio braille text
Analogies Explained: Towards Understanding Word Embeddings,"  Word embeddings generated by neural network methods such as word2vec (W2V)
are well known to exhibit seemingly linear behaviour, e.g. the embeddings of
analogy ""woman is to queen as man is to king"" approximately describe a
parallelogram. This property is particularly intriguing since the embeddings
are not trained to achieve it. Several explanations have been proposed, but
each introduces assumptions that do not hold in practice. We derive a
probabilistically grounded definition of paraphrasing that we re-interpret as
word transformation, a mathematical description of ""$w_x$ is to $w_y$"". From
these concepts we prove existence of linear relationships between W2V-type
embeddings that underlie the analogical phenomenon, identifying explicit error
terms.
",word embedding generate neural network method word2vec w2v well know exhibit seemingly linear behaviour embedding analogy woman queen man king approximately describe parallelogram property particularly intriguing since embedding train achieve several explanation propose introduce assumption hold practice derive probabilistically ground definition paraphrase re interpret word transformation mathematical description w_x w_y concept prove existence linear relationship w2v type embedding underlie analogical phenomenon identify explicit error term
Logistic-ELM: A Novel Fault Diagnosis Method for Rolling Bearings,"  The fault diagnosis of rolling bearings is a critical technique to realize
predictive maintenance for mechanical condition monitoring. In real industrial
systems, the main challenges for the fault diagnosis of rolling bearings
pertain to the accuracy and real-time requirements. Most existing methods focus
on ensuring the accuracy, and the real-time requirement is often neglected. In
this paper, considering both requirements, we propose a novel fast fault
diagnosis method for rolling bearings, based on extreme learning machine (ELM)
and logistic mapping, named logistic-ELM. First, we identify 14 kinds of
time-domain features from the original vibration signals according to
mechanical vibration principles and adopt the sequential forward selection
(SFS) strategy to select optimal features from them to ensure the basic
predictive accuracy and efficiency. Next, we propose the logistic-ELM for fast
fault classification, where the biases in ELM are omitted and the random input
weights are replaced by the chaotic logistic mapping sequence which involves a
higher uncorrelation to obtain more accurate results with fewer hidden neurons.
We conduct extensive experiments on the rolling bearing vibration signal
dataset of the Case Western Reserve University (CWRU) Bearing Data Centre. The
experimental results show that the proposed approach outperforms existing SOTA
comparison methods in terms of the predictive accuracy, and the highest
accuracy is 100% in seven separate sub data environments. The relevant code is
publicly available at https://github.com/TAN-OpenLab/logistic-ELM.
",fault diagnosis rolling bearing critical technique realize predictive maintenance mechanical condition monitor real industrial system main challenge fault diagnosis roll bearing pertain accuracy real time requirement exist method focus ensure accuracy real time requirement often neglect paper consider requirement propose novel fast fault diagnosis method rolling bearing base extreme learning machine elm logistic mapping name logistic elm first identify 14 kind time domain feature original vibration signal accord mechanical vibration principle adopt sequential forward selection sfs strategy select optimal feature ensure basic predictive accuracy efficiency next propose logistic elm fast fault classification bias elm omit random input weight replace chaotic logistic mapping sequence involve high uncorrelation obtain accurate result few hide neuron conduct extensive experiment roll bear vibration signal dataset case western reserve university cwru bear datum centre experimental result show propose approach outperform exist sota comparison method term predictive accuracy high accuracy 100 seven separate sub data environment relevant code publicly available https
"Bayesian Learning of Consumer Preferences for Residential Demand
  Response","  In coming years residential consumers will face real-time electricity tariffs
with energy prices varying day to day, and effective energy saving will require
automation - a recommender system, which learns consumer's preferences from her
actions. A consumer chooses a scenario of home appliance use to balance her
comfort level and the energy bill. We propose a Bayesian learning algorithm to
estimate the comfort level function from the history of appliance use. In
numeric experiments with datasets generated from a simulation model of a
consumer interacting with small home appliances the algorithm outperforms
popular regression analysis tools. Our approach can be extended to control an
air heating and conditioning system, which is responsible for up to half of a
household's energy bill.
",come year residential consumer face real time electricity tariff energy price vary day day effective energy saving require automation recommender system learn consumer preference action consumer choose scenario home appliance use balance comfort level energy bill propose bayesian learning algorithm estimate comfort level function history appliance use numeric experiment dataset generate simulation model consumer interact small home appliance algorithm outperform popular regression analysis tool approach extend control air heating conditioning system responsible half household energy bill
Rates of Convergence of Spectral Methods for Graphon Estimation,"  This paper studies the problem of estimating the grahpon model - the
underlying generating mechanism of a network. Graphon estimation arises in many
applications such as predicting missing links in networks and learning user
preferences in recommender systems. The graphon model deals with a random graph
of $n$ vertices such that each pair of two vertices $i$ and $j$ are connected
independently with probability $\rho \times f(x_i,x_j)$, where $x_i$ is the
unknown $d$-dimensional label of vertex $i$, $f$ is an unknown symmetric
function, and $\rho$ is a scaling parameter characterizing the graph sparsity.
Recent studies have identified the minimax error rate of estimating the graphon
from a single realization of the random graph. However, there exists a wide gap
between the known error rates of computationally efficient estimation
procedures and the minimax optimal error rate.
  Here we analyze a spectral method, namely universal singular value
thresholding (USVT) algorithm, in the relatively sparse regime with the average
vertex degree $n\rho=\Omega(\log n)$. When $f$ belongs to H\""{o}lder or Sobolev
space with smoothness index $\alpha$, we show the error rate of USVT is at most
$(n\rho)^{ -2 \alpha / (2\alpha+d)}$, approaching the minimax optimal error
rate $\log (n\rho)/(n\rho)$ for $d=1$ as $\alpha$ increases. Furthermore, when
$f$ is analytic, we show the error rate of USVT is at most $\log^d
(n\rho)/(n\rho)$. In the special case of stochastic block model with $k$
blocks, the error rate of USVT is at most $k/(n\rho)$, which is larger than the
minimax optimal error rate by at most a multiplicative factor $k/\log k$. This
coincides with the computational gap observed for community detection. A key
step of our analysis is to derive the eigenvalue decaying rate of the edge
probability matrix using piecewise polynomial approximations of the graphon
function $f$.
",paper study problem estimate grahpon model underlie generating mechanism network graphon estimation arise many application predict miss link network learn user preference recommender system graphon model deal random graph n vertice pair two vertex j connect independently probability f x_i x_j x_i unknown -dimensional label vertex f unknown symmetric function scale parameter characterize graph sparsity recent study identify minimax error rate estimate graphon single realization random graph however exist wide gap know error rate computationally efficient estimation procedure minimax optimal error rate analyze spectral method namely universal singular value thresholding usvt algorithm relatively sparse regime average vertex degree n f belong lder sobolev space smoothness index show error rate usvt -2 approach minimax optimal error rate increase furthermore f analytic show error rate usvt special case stochastic block model k block error rate usvt large minimax optimal error rate multiplicative factor k coincide computational gap observe community detection key step analysis derive eigenvalue decaying rate edge probability matrix use piecewise polynomial approximation graphon function f
C-MinHash: Practically Reducing Two Permutations to Just One,"  Traditional minwise hashing (MinHash) requires applying $K$ independent
permutations to estimate the Jaccard similarity in massive binary (0/1) data,
where $K$ can be (e.g.,) 1024 or even larger, depending on applications. The
recent work on C-MinHash (Li and Li, 2021) has shown, with rigorous proofs,
that only two permutations are needed. An initial permutation is applied to
break whatever structures which might exist in the data, and a second
permutation is re-used $K$ times to produce $K$ hashes, via a circulant
shifting fashion. (Li and Li, 2021) has proved that, perhaps surprisingly, even
though the $K$ hashes are correlated, the estimation variance is strictly
smaller than the variance of the traditional MinHash.
  It has been demonstrated in (Li and Li, 2021) that the initial permutation in
C-MinHash is indeed necessary. For the ease of theoretical analysis, they have
used two independent permutations. In this paper, we show that one can actually
simply use one permutation. That is, one single permutation is used for both
the initial pre-processing step to break the structures in the data and the
circulant hashing step to generate $K$ hashes. Although the theoretical
analysis becomes very complicated, we are able to explicitly write down the
expression for the expectation of the estimator. The new estimator is no longer
unbiased but the bias is extremely small and has essentially no impact on the
estimation accuracy (mean square errors). An extensive set of experiments are
provided to verify our claim for using just one permutation.
",traditional minwise hash minhash require apply k independent permutation estimate jaccard similarity massive binary datum k 1024 even large depend application recent work c minhash li li 2021 show rigorous proof two permutation need initial permutation apply break whatever structure might exist datum second permutation re use k time produce k hash via circulant shift fashion li li 2021 prove perhaps surprisingly even though k hash correlate estimation variance strictly small variance traditional minhash demonstrate li li 2021 initial permutation c minhash indeed necessary ease theoretical analysis use two independent permutation paper show one actually simply use one permutation one single permutation use initial pre processing step break structure datum circulant hash step generate k hash although theoretical analysis become complicated able explicitly write expression expectation estimator new estimator long unbiased bias extremely small essentially impact estimation accuracy mean square error extensive set experiment provide verify claim use one permutation
Dueling Network Architectures for Deep Reinforcement Learning,"  In recent years there have been many successes of using deep representations
in reinforcement learning. Still, many of these applications use conventional
architectures, such as convolutional networks, LSTMs, or auto-encoders. In this
paper, we present a new neural network architecture for model-free
reinforcement learning. Our dueling network represents two separate estimators:
one for the state value function and one for the state-dependent action
advantage function. The main benefit of this factoring is to generalize
learning across actions without imposing any change to the underlying
reinforcement learning algorithm. Our results show that this architecture leads
to better policy evaluation in the presence of many similar-valued actions.
Moreover, the dueling architecture enables our RL agent to outperform the
state-of-the-art on the Atari 2600 domain.
",recent year many success use deep representation reinforcement learning still many application use conventional architecture convolutional network lstms auto encoder paper present new neural network architecture model free reinforcement learning duel network represent two separate estimator one state value function one state dependent action advantage function main benefit factoring generalize learn across action without impose change underlie reinforcement learn algorithm result show architecture lead well policy evaluation presence many similar value action moreover duel architecture enable rl agent outperform state of the art atari 2600 domain
Manipulation-Proof Machine Learning,"  An increasing number of decisions are guided by machine learning algorithms.
In many settings, from consumer credit to criminal justice, those decisions are
made by applying an estimator to data on an individual's observed behavior. But
when consequential decisions are encoded in rules, individuals may
strategically alter their behavior to achieve desired outcomes. This paper
develops a new class of estimator that is stable under manipulation, even when
the decision rule is fully transparent. We explicitly model the costs of
manipulating different behaviors, and identify decision rules that are stable
in equilibrium. Through a large field experiment in Kenya, we show that
decision rules estimated with our strategy-robust method outperform those based
on standard supervised learning approaches.
",increase number decision guide machine learning algorithm many setting consumer credit criminal justice decision make apply estimator datum individual observe behavior consequential decision encode rule individual may strategically alter behavior achieve desire outcome paper develop new class estimator stable manipulation even decision rule fully transparent explicitly model cost manipulate different behavior identify decision rule stable equilibrium large field experiment kenya show decision rule estimate strategy robust method outperform base standard supervised learning approach
Satellite Image Time Series Analysis for Big Earth Observation Data,"  The development of analytical software for big Earth observation data faces
several challenges. Designers need to balance between conflicting factors.
Solutions that are efficient for specific hardware architectures can not be
used in other environments. Packages that work on generic hardware and open
standards will not have the same performance as dedicated solutions. Software
that assumes that its users are computer programmers are flexible but may be
difficult to learn for a wide audience. This paper describes sits, an
open-source R package for satellite image time series analysis using machine
learning. To allow experts to use satellite imagery to the fullest extent, sits
adopts a time-first, space-later approach. It supports the complete cycle of
data analysis for land classification. Its API provides a simple but powerful
set of functions. The software works in different cloud computing environments.
Satellite image time series are input to machine learning classifiers, and the
results are post-processed using spatial smoothing. Since machine learning
methods need accurate training data, sits includes methods for quality
assessment of training samples. The software also provides methods for
validation and accuracy measurement. The package thus comprises a production
environment for big EO data analysis. We show that this approach produces high
accuracy for land use and land cover maps through a case study in the Cerrado
biome, one of the world's fast moving agricultural frontiers for the year 2018.
",development analytical software big earth observation datum face several challenge designer need balance conflict factor solution efficient specific hardware architecture use environment package work generic hardware open standard performance dedicated solution software assume user computer programmer flexible may difficult learn wide audience paper describe sit open source r package satellite image time series analysis use machine learning allow expert use satellite imagery full extent sit adopt time first space later approach support complete cycle datum analysis land classification api provide simple powerful set function software work different cloud computing environment satellite image time series input machine learn classifier result post processed use spatial smoothing since machine learning method need accurate training datum sit include method quality assessment training sample software also provide method validation accuracy measurement package thus comprise production environment big eo datum analysis show approach produce high accuracy land use land cover map case study cerrado biome one world fast move agricultural frontier year 2018
PRISMA: PRoximal Iterative SMoothing Algorithm,"  Motivated by learning problems including max-norm regularized matrix
completion and clustering, robust PCA and sparse inverse covariance selection,
we propose a novel optimization algorithm for minimizing a convex objective
which decomposes into three parts: a smooth part, a simple non-smooth Lipschitz
part, and a simple non-smooth non-Lipschitz part. We use a time variant
smoothing strategy that allows us to obtain a guarantee that does not depend on
knowing in advance the total number of iterations nor a bound on the domain.
",motivated learning problem include max norm regularize matrix completion cluster robust pca sparse inverse covariance selection propose novel optimization algorithm minimize convex objective decompose three part smooth part simple non smooth lipschitz part simple non smooth non lipschitz part use time variant smoothing strategy allow we obtain guarantee depend know advance total number iteration bind domain
Social Learning in Multi Agent Multi Armed Bandits,"  In this paper, we introduce a distributed version of the classical stochastic
Multi-Arm Bandit (MAB) problem. Our setting consists of a large number of
agents $n$ that collaboratively and simultaneously solve the same instance of
$K$ armed MAB to minimize the average cumulative regret over all agents. The
agents can communicate and collaborate among each other \emph{only} through a
pairwise asynchronous gossip based protocol that exchange a limited number of
bits. In our model, agents at each point decide on (i) which arm to play, (ii)
whether to, and if so (iii) what and whom to communicate with. Agents in our
model are decentralized, namely their actions only depend on their observed
history in the past.
  We develop a novel algorithm in which agents, whenever they choose,
communicate only arm-ids and not samples, with another agent chosen uniformly
and independently at random. The per-agent regret scaling achieved by our
algorithm is $O \left( \frac{\lceil\frac{K}{n}\rceil+\log(n)}{\Delta}
  \log(T) + \frac{\log^3(n) \log \log(n)}{\Delta^2}
  \right)$. Furthermore, any agent in our algorithm communicates only a total
of $\Theta(\log(T))$ times over a time interval of $T$.
  We compare our results to two benchmarks - one where there is no
communication among agents and one corresponding to complete interaction. We
show both theoretically and empirically, that our algorithm experiences a
significant reduction both in per-agent regret when compared to the case when
agents do not collaborate and in communication complexity when compared to the
full interaction setting which requires $T$ communication attempts by an agent
over $T$ arm pulls.
",paper introduce distribute version classical stochastic multi arm bandit mab problem setting consist large number agent n collaboratively simultaneously solve instance k armed mab minimize average cumulative regret agent agent communicate collaborate among pairwise asynchronous gossip base protocol exchange limit number bit model agent point decide arm play ii whether iii communicate agent model decentralize namely action depend observed history past develop novel algorithm agent whenever choose communicate arm id sample another agent choose uniformly independently random per agent regret scaling achieve algorithm k n n n n furthermore agent algorithm communicate total time time interval compare result two benchmark one communication among agent one correspond complete interaction show theoretically empirically algorithm experience significant reduction per agent regret compare case agent collaborate communication complexity compare full interaction setting require communication attempt agent arm pull
Deep Learning-Based Semantic Segmentation of Microscale Objects,"  Accurate estimation of the positions and shapes of microscale objects is
crucial for automated imaging-guided manipulation using a non-contact technique
such as optical tweezers. Perception methods that use traditional computer
vision algorithms tend to fail when the manipulation environments are crowded.
In this paper, we present a deep learning model for semantic segmentation of
the images representing such environments. Our model successfully performs
segmentation with a high mean Intersection Over Union score of 0.91.
",accurate estimation position shape microscale object crucial automate imaging guide manipulation use non contact technique optical tweezer perception method use traditional computer vision algorithm tend fail manipulation environment crowd paper present deep learning model semantic segmentation image represent environment model successfully perform segmentation high mean intersection union score
Dual GNNs: Graph Neural Network Learning with Limited Supervision,"  Graph Neural Networks (GNNs) require a relatively large number of labeled
nodes and a reliable/uncorrupted graph connectivity structure in order to
obtain good performance on the semi-supervised node classification task. The
performance of GNNs can degrade significantly as the number of labeled nodes
decreases or the graph connectivity structure is corrupted by adversarial
attacks or due to noises in data measurement /collection. Therefore, it is
important to develop GNN models that are able to achieve good performance when
there is limited supervision knowledge -- a few labeled nodes and noisy graph
structures. In this paper, we propose a novel Dual GNN learning framework to
address this challenge task. The proposed framework has two GNN based node
prediction modules. The primary module uses the input graph structure to induce
regular node embeddings and predictions with a regular GNN baseline, while the
auxiliary module constructs a new graph structure through fine-grained spectral
clusterings and learns new node embeddings and predictions. By integrating the
two modules in a dual GNN learning framework, we perform joint learning in an
end-to-end fashion. This general framework can be applied on many GNN baseline
models. The experimental results validate that the proposed dual GNN framework
can greatly outperform the GNN baseline methods when the labeled nodes are
scarce and the graph connectivity structure is noisy.
",graph neural network gnns require relatively large number label node graph connectivity structure order obtain good performance semi supervised node classification task performance gnn degrade significantly number label node decrease graph connectivity structure corrupt adversarial attack due noise datum measurement therefore important develop gnn model able achieve good performance limited supervision knowledge label node noisy graph structure paper propose novel dual gnn learn framework address challenge task propose framework two gnn base node prediction module primary module use input graph structure induce regular node embedding prediction regular gnn baseline auxiliary module construct new graph structure fine grain spectral clustering learn new node embedding prediction integrate two module dual gnn learn framework perform joint learning end to end fashion general framework apply many gnn baseline model experimental result validate propose dual gnn framework greatly outperform gnn baseline method label node scarce graph connectivity structure noisy
Magnetic Manifold Hamiltonian Monte Carlo,"  Markov chain Monte Carlo (MCMC) algorithms offer various strategies for
sampling; the Hamiltonian Monte Carlo (HMC) family of samplers are MCMC
algorithms which often exhibit improved mixing properties. The recently
introduced magnetic HMC, a generalization of HMC motivated by the physics of
particles influenced by magnetic field forces, has been demonstrated to improve
the performance of HMC. In many applications, one wishes to sample from a
distribution restricted to a constrained set, often manifested as an embedded
manifold (for example, the surface of a sphere). We introduce magnetic manifold
HMC, an HMC algorithm on embedded manifolds motivated by the physics of
particles constrained to a manifold and moving under magnetic field forces. We
discuss the theoretical properties of magnetic Hamiltonian dynamics on
manifolds, and introduce a reversible and symplectic integrator for the HMC
updates. We demonstrate that magnetic manifold HMC produces favorable sampling
behaviors relative to the canonical variant of manifold-constrained HMC.
",markov chain monte carlo mcmc algorithm offer various strategy sample hamiltonian monte carlo hmc family sampler mcmc algorithm often exhibit improve mixing property recently introduce magnetic hmc generalization hmc motivated physics particle influence magnetic field force demonstrate improve performance hmc many application one wish sample distribution restrict constrain set often manifest embed manifold example surface sphere introduce magnetic manifold hmc hmc algorithm embed manifold motivated physics particle constrain manifold move magnetic field force discuss theoretical property magnetic hamiltonian dynamic manifold introduce reversible symplectic integrator hmc update demonstrate magnetic manifold hmc produce favorable sample behavior relative canonical variant manifold constrain hmc
Explaining Anomalies in Groups with Characterizing Subspace Rules,"  Anomaly detection has numerous applications and has been studied vastly. We
consider a complementary problem that has a much sparser literature: anomaly
description. Interpretation of anomalies is crucial for practitioners for
sense-making, troubleshooting, and planning actions. To this end, we present a
new approach called x-PACS (for eXplaining Patterns of Anomalies with
Characterizing Subspaces), which ""reverse-engineers"" the known anomalies by
identifying (1) the groups (or patterns) that they form, and (2) the
characterizing subspace and feature rules that separate each anomalous pattern
from normal instances. Explaining anomalies in groups not only saves analyst
time and gives insight into various types of anomalies, but also draws
attention to potentially critical, repeating anomalies.
  In developing x-PACS, we first construct a desiderata for the anomaly
description problem. From a descriptive data mining perspective, our method
exhibits five desired properties in our desiderata. Namely, it can unearth
anomalous patterns (i) of multiple different types, (ii) hidden in arbitrary
subspaces of a high dimensional space, (iii) interpretable by the analysts,
(iv) different from normal patterns of the data, and finally (v) succinct,
providing the shortest data description. Furthermore, x-PACS is highly
parallelizable and scales linearly in terms of data size.
  No existing work on anomaly description satisfies all of these properties
simultaneously. While not our primary goal, the anomalous patterns we find
serve as interpretable ""signatures"" and can be used for detection. We show the
effectiveness of x-PACS in explanation as well as detection on real-world
datasets as compared to state-of-the-art.
",anomaly detection numerous application study vastly consider complementary problem much sparser literature anomaly description interpretation anomalie crucial practitioner sense make troubleshooting planning action end present new approach call x pac explain pattern anomaly characterize subspace reverse engineer know anomaly identify 1 group pattern form 2 characterize subspace feature rule separate anomalous pattern normal instance explain anomaly group save analyst time give insight various type anomaly also draw attention potentially critical repeating anomaly develop x pac first construct desiderata anomaly description problem descriptive datum mining perspective method exhibit five desire property desiderata namely unearth anomalous pattern multiple different type ii hide arbitrary subspace high dimensional space iii interpretable analyst iv different normal pattern datum finally v succinct provide short datum description furthermore x pacs highly parallelizable scale linearly term datum size exist work anomaly description satisfy property simultaneously primary goal anomalous pattern find serve interpretable signature use detection show effectiveness x pacs explanation well detection real world dataset compare state of the art
Adversarial Robustness as a Prior for Learned Representations,"  An important goal in deep learning is to learn versatile, high-level feature
representations of input data. However, standard networks' representations seem
to possess shortcomings that, as we illustrate, prevent them from fully
realizing this goal. In this work, we show that robust optimization can be
re-cast as a tool for enforcing priors on the features learned by deep neural
networks. It turns out that representations learned by robust models address
the aforementioned shortcomings and make significant progress towards learning
a high-level encoding of inputs. In particular, these representations are
approximately invertible, while allowing for direct visualization and
manipulation of salient input features. More broadly, our results indicate
adversarial robustness as a promising avenue for improving learned
representations. Our code and models for reproducing these results is available
at https://git.io/robust-reps .
",important goal deep learning learn versatile high level feature representation input datum however standard network representation seem possess shortcoming illustrate prevent fully realize goal work show robust optimization re cast tool enforce prior feature learn deep neural network turn representation learn robust model address aforementioned shortcoming make significant progress towards learn high level encoding input particular representation approximately invertible allow direct visualization manipulation salient input feature broadly result indicate adversarial robustness promising avenue improve learn representation code model reproduce result available https
Faithful Explanations for Deep Graph Models,"  This paper studies faithful explanations for Graph Neural Networks (GNNs).
First, we provide a new and general method for formally characterizing the
faithfulness of explanations for GNNs. It applies to existing explanation
methods, including feature attributions and subgraph explanations. Second, our
analytical and empirical results demonstrate that feature attribution methods
cannot capture the nonlinear effect of edge features, while existing subgraph
explanation methods are not faithful. Third, we introduce \emph{k-hop
Explanation with a Convolutional Core} (KEC), a new explanation method that
provably maximizes faithfulness to the original GNN by leveraging information
about the graph structure in its adjacency matrix and its \emph{k-th} power.
Lastly, our empirical results over both synthetic and real-world datasets for
classification and anomaly detection tasks with GNNs demonstrate the
effectiveness of our approach.
",paper study faithful explanation graph neural network gnn first provide new general method formally characterize faithfulness explanation gnns apply exist explanation method include feature attribution subgraph explanation second analytical empirical result demonstrate feature attribution method capture nonlinear effect edge feature exist subgraph explanation method faithful third introduce k hop explanation convolutional core kec new explanation method provably maximize faithfulness original gnn leverage information graph structure adjacency matrix k th power lastly empirical result synthetic real world dataset classification anomaly detection task gnns demonstrate effectiveness approach
"G-Elo: Generalization of the Elo algorithm by modelling the discretized
  margin of victory","  In this work we develop a new algorithm for rating of teams (or players) in
one-on-one games by exploiting the observed difference of the game-points (such
as goals), also known as a margin of victory (MOV). Our objective is to obtain
the Elo-style algorithm whose operation is simple to implement and to
understand intuitively. This is done in three steps: first, we define the
probabilistic model between the teams' skills and the discretized MOV variable:
this generalizes the model underpinning the Elo algorithm, where the MOV
variable is discretized into three categories (win/loss/draw). Second, with the
formal probabilistic model at hand, the optimization required by the maximum
likelihood rule is implemented via stochastic gradient; this yields simple
on-line equations for the rating updates which are identical in their general
form to those characteristic of the Elo algorithm: the main difference lies in
the way the scores and the expected scores are defined. Third, we propose a
simple method to estimate the coefficients of the model, and thus define the
operation of the algorithm; it is done in a closed form using the historical
data so the algorithm is tailored to the sport of interest and the coefficients
defining its operation are determined in entirely transparent manner. The
alternative, optimization-based strategy to find the coefficients is also
presented. We show numerical examples based on the results of the association
football of the English Premier League and the American football of the
National Football League.
",work develop new algorithm rating team player one on one game exploit observe difference game point goal also know margin victory mov objective obtain elo style algorithm whose operation simple implement understand intuitively do three step first define probabilistic model team skill discretized mov variable generalize model underpin elo algorithm mov variable discretize three category second formal probabilistic model hand optimization require maximum likelihood rule implement via stochastic gradient yield simple on line equation rating update identical general form characteristic elo algorithm main difference lie way score expect score define third propose simple method estimate coefficient model thus define operation algorithm do close form use historical datum algorithm tailor sport interest coefficient define operation determine entirely transparent manner alternative optimization base strategy find coefficient also present show numerical example base result association football english premier league american football national football league
"Ten years of image analysis and machine learning competitions in
  dementia","  Machine learning methods exploiting multi-parametric biomarkers, especially
based on neuroimaging, have huge potential to improve early diagnosis of
dementia and to predict which individuals are at-risk of developing dementia.
To benchmark algorithms in the field of machine learning and neuroimaging in
dementia and assess their potential for use in clinical practice and clinical
trials, seven grand challenges have been organized in the last decade.
  The seven grand challenges addressed questions related to screening, clinical
status estimation, prediction and monitoring in (pre-clinical) dementia. There
was little overlap in clinical questions, tasks and performance metrics.
Whereas this aids providing insight on a broad range of questions, it also
limits the validation of results across challenges. The validation process
itself was mostly comparable between challenges, using similar methods for
ensuring objective comparison, uncertainty estimation and statistical testing.
In general, winning algorithms performed rigorous data preprocessing and
combined a wide range of input features.
  Despite high state-of-the-art performances, most of the methods evaluated by
the challenges are not clinically used. To increase impact, future challenges
could pay more attention to statistical analysis of which factors relate to
higher performance, to clinical questions beyond Alzheimer's disease, and to
using testing data beyond the Alzheimer's Disease Neuroimaging Initiative.
Grand challenges would be an ideal venue for assessing the generalizability of
algorithm performance to unseen data of other cohorts. Key for increasing
impact in this way are larger testing data sizes, which could be reached by
sharing algorithms rather than data to exploit data that cannot be shared.
",machine learning method exploit multi parametric biomarker especially base neuroimage huge potential improve early diagnosis dementia predict individual at risk develop dementia benchmark algorithms field machine learn neuroimage dementia assess potential use clinical practice clinical trial seven grand challenge organize last decade seven grand challenge address question relate screening clinical status estimation prediction monitor pre clinical dementia little overlap clinical question task performance metric whereas aid provide insight broad range question also limit validation result across challenge validation process mostly comparable challenge use similar method ensure objective comparison uncertainty estimation statistical testing general win algorithm perform rigorous datum preprocesse combine wide range input feature despite high state of the art performance method evaluate challenge clinically use increase impact future challenge could pay attention statistical analysis factor relate high performance clinical question beyond alzheimer disease use testing datum beyond alzheimer disease neuroimage initiative grand challenge would ideal venue assess generalizability algorithm performance unseen data cohort key increase impact way large testing data size could reach share algorithm rather datum exploit datum share
Tom: Leveraging trend of the observed gradients for faster convergence,"  The success of deep learning can be attributed to various factors such as
increase in computational power, large datasets, deep convolutional neural
networks, optimizers etc. Particularly, the choice of optimizer affects the
generalization, convergence rate, and training stability. Stochastic Gradient
Descent (SGD) is a first order iterative optimizer that updates the gradient
uniformly for all parameters. This uniform update may not be suitable across
the entire training phase. A rudimentary solution for this is to employ a
fine-tuned learning rate scheduler which decreases learning rate as a function
of iteration. To eliminate the dependency of learning rate schedulers, adaptive
gradient optimizers such as AdaGrad, AdaDelta, RMSProp, Adam employ a
parameter-wise scaling term for learning rate which is a function of the
gradient itself. We propose Tom (Trend over Momentum) optimizer, which is a
novel variant of Adam that takes into account of the trend which is observed
for the gradients in the loss landscape traversed by the neural network. In the
proposed Tom optimizer, an additional smoothing equation is introduced to
address the trend observed during the process of optimization. The smoothing
parameter introduced for the trend requires no tuning and can be used with
default values. Experimental results for classification datasets such as
CIFAR-10, CIFAR-100 and CINIC-10 image datasets show that Tom outperforms
Adagrad, Adadelta, RMSProp and Adam in terms of both accuracy and has a faster
convergence. The source code is publicly made available at
https://github.com/AnirudhMaiya/Tom
",success deep learning attribute various factor increase computational power large dataset deep convolutional neural network optimizer etc particularly choice optimizer affect generalization convergence rate training stability stochastic gradient descent sgd first order iterative optimizer update gradient uniformly parameter uniform update may suitable across entire training phase rudimentary solution employ fine tune learning rate scheduler decrease learn rate function iteration eliminate dependency learning rate scheduler adaptive gradient optimizer adagrad adadelta rmsprop adam employ parameter wise scale term learn rate function gradient propose tom trend momentum optimizer novel variant adam take account trend observe gradient loss landscape traverse neural network propose tom optimizer additional smoothing equation introduce address trend observe process optimization smooth parameter introduce trend require tune use default value experimental result classification dataset cifar-10 cifar-100 cinic-10 image dataset show tom outperform adagrad adadelta rmsprop adam term accuracy fast convergence source code publicly make available https
The Architecture of Mr. DLib's Scientific Recommender-System API,"  Recommender systems in academia are not widely available. This may be in part
due to the difficulty and cost of developing and maintaining recommender
systems. Many operators of academic products such as digital libraries and
reference managers avoid this effort, although a recommender system could
provide significant benefits to their users. In this paper, we introduce Mr.
DLib's ""Recommendations as-a-Service"" (RaaS) API that allows operators of
academic products to easily integrate a scientific recommender system into
their products. Mr. DLib generates recommendations for research articles but in
the future, recommendations may include call for papers, grants, etc. Operators
of academic products can request recommendations from Mr. DLib and display
these recommendations to their users. Mr. DLib can be integrated in just a few
hours or days; creating an equivalent recommender system from scratch would
require several months for an academic operator. Mr. DLib has been used by
GESIS Sowiport and by the reference manager JabRef. Mr. DLib is open source and
its goal is to facilitate the application of, and research on, scientific
recommender systems. In this paper, we present the motivation for Mr. DLib, the
architecture and details about the effectiveness. Mr. DLib has delivered 94m
recommendations over a span of two years with an average click-through rate of
0.12%.
",recommender system academia widely available may part due difficulty cost develop maintain recommender system many operator academic product digital library reference manager avoid effort although recommender system could provide significant benefit user paper introduce dlib recommendation as a service raas api allow operator academic product easily integrate scientific recommender system product dlib generate recommendation research article future recommendation may include call paper grant etc operator academic product request recommendation dlib display recommendation user dlib integrate hour day create equivalent recommender system scratch would require several month academic operator dlib use gesis sowiport reference manager jabref dlib open source goal facilitate application research scientific recommender system paper present motivation dlib architecture detail effectiveness dlib deliver 94 m recommendation span two year average click through rate
"Toward Understanding the Impact of Staleness in Distributed Machine
  Learning","  Many distributed machine learning (ML) systems adopt the non-synchronous
execution in order to alleviate the network communication bottleneck, resulting
in stale parameters that do not reflect the latest updates. Despite much
development in large-scale ML, the effects of staleness on learning are
inconclusive as it is challenging to directly monitor or control staleness in
complex distributed environments. In this work, we study the convergence
behaviors of a wide array of ML models and algorithms under delayed updates.
Our extensive experiments reveal the rich diversity of the effects of staleness
on the convergence of ML algorithms and offer insights into seemingly
contradictory reports in the literature. The empirical findings also inspire a
new convergence analysis of stochastic gradient descent in non-convex
optimization under staleness, matching the best-known convergence rate of
O(1/\sqrt{T}).
",many distribute machine learning ml system adopt non synchronous execution order alleviate network communication bottleneck result stale parameter reflect late update despite much development large scale ml effect staleness learn inconclusive challenging directly monitor control staleness complex distribute environment work study convergence behavior wide array ml model algorithm delay update extensive experiment reveal rich diversity effect staleness convergence ml algorithm offer insight seemingly contradictory report literature empirical finding also inspire new convergence analysis stochastic gradient descent non convex optimization staleness match well know convergence rate
On the Error Resistance of Hinge Loss Minimization,"  Commonly used classification algorithms in machine learning, such as support
vector machines, minimize a convex surrogate loss on training examples. In
practice, these algorithms are surprisingly robust to errors in the training
data. In this work, we identify a set of conditions on the data under which
such surrogate loss minimization algorithms provably learn the correct
classifier. This allows us to establish, in a unified framework, the robustness
of these algorithms under various models on data as well as error. In
particular, we show that if the data is linearly classifiable with a slightly
non-trivial margin (i.e. a margin at least $C/\sqrt{d}$ for $d$-dimensional
unit vectors), and the class-conditional distributions are near isotropic and
logconcave, then surrogate loss minimization has negligible error on the
uncorrupted data even when a constant fraction of examples are adversarially
mislabeled.
",commonly use classification algorithm machine learn support vector machine minimize convex surrogate loss training example practice algorithm surprisingly robust error train datum work identify set condition datum surrogate loss minimization algorithm provably learn correct classifier allow we establish unified framework robustness algorithm various model datum well error particular show datum linearly classifiable slightly non trivial margin margin least -dimensional unit vector class conditional distribution near isotropic logconcave surrogate loss minimization negligible error uncorrupted datum even constant fraction example adversarially mislabele
Variational Heteroscedastic Volatility Model,"  We propose Variational Heteroscedastic Volatility Model (VHVM) -- an
end-to-end neural network architecture capable of modelling heteroscedastic
behaviour in multivariate financial time series. VHVM leverages recent advances
in several areas of deep learning, namely sequential modelling and
representation learning, to model complex temporal dynamics between different
asset returns. At its core, VHVM consists of a variational autoencoder to
capture relationships between assets, and a recurrent neural network to model
the time-evolution of these dependencies. The outputs of VHVM are time-varying
conditional volatilities in the form of covariance matrices. We demonstrate the
effectiveness of VHVM against existing methods such as Generalised
AutoRegressive Conditional Heteroscedasticity (GARCH) and Stochastic Volatility
(SV) models on a wide range of multivariate foreign currency (FX) datasets.
",propose variational heteroscedastic volatility model vhvm end to end neural network architecture capable modelling heteroscedastic behaviour multivariate financial time series vhvm leverage recent advance several area deep learn namely sequential modelling representation learn model complex temporal dynamic different asset return core vhvm consist variational autoencoder capture relationship asset recurrent neural network model time evolution dependencie output vhvm time vary conditional volatility form covariance matrix demonstrate effectiveness vhvm exist method generalise autoregressive conditional heteroscedasticity garch stochastic volatility sv model wide range multivariate foreign currency fx dataset
"Diffusion $K$-means clustering on manifolds: provable exact recovery via
  semidefinite relaxations","  We introduce the {\it diffusion $K$-means} clustering method on Riemannian
submanifolds, which maximizes the within-cluster connectedness based on the
diffusion distance. The diffusion $K$-means constructs a random walk on the
similarity graph with vertices as data points randomly sampled on the manifolds
and edges as similarities given by a kernel that captures the local geometry of
manifolds. The diffusion $K$-means is a multi-scale clustering tool that is
suitable for data with non-linear and non-Euclidean geometric features in mixed
dimensions. Given the number of clusters, we propose a polynomial-time convex
relaxation algorithm via the semidefinite programming (SDP) to solve the
diffusion $K$-means. In addition, we also propose a nuclear norm regularized
SDP that is adaptive to the number of clusters. In both cases, we show that
exact recovery of the SDPs for diffusion $K$-means can be achieved under
suitable between-cluster separability and within-cluster connectedness of the
submanifolds, which together quantify the hardness of the manifold clustering
problem. We further propose the {\it localized diffusion $K$-means} by using
the local adaptive bandwidth estimated from the nearest neighbors. We show that
exact recovery of the localized diffusion $K$-means is fully adaptive to the
local probability density and geometric structures of the underlying
submanifolds.
",introduce diffusion k -mean cluster method riemannian submanifold maximize within cluster connectedness base diffusion distance diffusion k -mean construct random walk similarity graph vertice data point randomly sample manifold edge similarity give kernel capture local geometry manifold diffusion k -mean multi scale clustering tool suitable datum non linear non euclidean geometric feature mixed dimension give number cluster propose polynomial time convex relaxation algorithm via semidefinite programming sdp solve diffusion k -mean addition also propose nuclear norm regularize sdp adaptive number cluster case show exact recovery sdps diffusion k -mean achieve suitable between cluster separability within cluster connectedness submanifold together quantify hardness manifold clustering problem propose localized diffusion k -mean use local adaptive bandwidth estimate near neighbor show exact recovery localize diffusion k -means fully adaptive local probability density geometric structure underlie submanifold
Multivariate Forecasting of Crude Oil Spot Prices using Neural Networks,"  Crude oil is a major component in most advanced economies of the world.
Accurately predicting and understanding the behavior of crude oil prices is
important for economists, analysts, forecasters, and traders, to name a few.
The price of crude oil has declined in the past decade and is seeing a phase of
stability; but will this stability last? This work is an empirical study on how
multivariate analysis may be employed to predict crude oil spot prices using
neural networks. The concept of using neural networks showed promising
potential. A very simple neural network model was able to perform on par with
ARIMA models - the state-of-the-art model in time-series forecasting. Advanced
neural network models using larger datasets may be used in the future to extend
this proof-of-concept to a full scale framework.
",crude oil major component advanced economy world accurately predict understand behavior crude oil price important economist analyst forecaster trader name price crude oil decline past decade see phase stability stability last work empirical study multivariate analysis may employ predict crude oil spot price use neural network concept use neural network show promise potential simple neural network model able perform par arima model state of the art model time series forecasting advanced neural network model use large dataset may use future extend proof of concept full scale framework
DOGE-Train: Discrete Optimization on GPU with End-to-end Training,"  We present a fast, scalable, data-driven approach for solving linear
relaxations of 0-1 integer linear programs using a graph neural network. Our
solver is based on the Lagrange decomposition based algorithm FastDOG (Abbas et
al. (2022)). We make the algorithm differentiable and perform backpropagation
through the dual update scheme for end-to-end training of its algorithmic
parameters. This allows to preserve the algorithm's theoretical properties
including feasibility and guaranteed non-decrease in the lower bound. Since
FastDOG can get stuck in suboptimal fixed points, we provide additional freedom
to our graph neural network to predict non-parametric update steps for escaping
such points while maintaining dual feasibility. For training of the graph
neural network we use an unsupervised loss and perform experiments on
large-scale real world datasets. We train on smaller problems and test on
larger ones showing strong generalization performance with a graph neural
network comprising only around 10k parameters. Our solver achieves
significantly faster performance and better dual objectives than its
non-learned version. In comparison to commercial solvers our learned solver
achieves close to optimal objective values of LP relaxations and is faster by
up to an order of magnitude on very large problems from structured prediction
and on selected combinatorial optimization problems.
",present fast scalable data drive approach solve linear relaxation 0 1 integer linear program use graph neural network solver base lagrange decomposition base algorithm fastdog abbas et al 2022 make algorithm differentiable perform backpropagation dual update scheme end to end training algorithmic parameter allow preserve algorithm theoretical property include feasibility guarantee non decrease lower bind since fastdog get stick suboptimal fix point provide additional freedom graph neural network predict non parametric update step escape point maintain dual feasibility training graph neural network use unsupervised loss perform experiment large scale real world dataset train small problem test large one show strong generalization performance graph neural network comprise around 10k parameter solver achieve significantly fast performance well dual objective non learned version comparison commercial solver learn solver achieve close optimal objective value lp relaxation fast order magnitude large problem structure prediction select combinatorial optimization problem
Never Give Up: Learning Directed Exploration Strategies,"  We propose a reinforcement learning agent to solve hard exploration games by
learning a range of directed exploratory policies. We construct an episodic
memory-based intrinsic reward using k-nearest neighbors over the agent's recent
experience to train the directed exploratory policies, thereby encouraging the
agent to repeatedly revisit all states in its environment. A self-supervised
inverse dynamics model is used to train the embeddings of the nearest neighbour
lookup, biasing the novelty signal towards what the agent can control. We
employ the framework of Universal Value Function Approximators (UVFA) to
simultaneously learn many directed exploration policies with the same neural
network, with different trade-offs between exploration and exploitation. By
using the same neural network for different degrees of
exploration/exploitation, transfer is demonstrated from predominantly
exploratory policies yielding effective exploitative policies. The proposed
method can be incorporated to run with modern distributed RL agents that
collect large amounts of experience from many actors running in parallel on
separate environment instances. Our method doubles the performance of the base
agent in all hard exploration in the Atari-57 suite while maintaining a very
high score across the remaining games, obtaining a median human normalised
score of 1344.0%. Notably, the proposed method is the first algorithm to
achieve non-zero rewards (with a mean score of 8,400) in the game of Pitfall!
without using demonstrations or hand-crafted features.
",propose reinforcement learn agent solve hard exploration game learn range direct exploratory policy construct episodic memory base intrinsic reward use k near neighbor agent recent experience train direct exploratory policy thereby encourage agent repeatedly revisit states environment self supervise inverse dynamic model use train embedding near neighbour lookup bias novelty signal towards agent control employ framework universal value function approximator uvfa simultaneously learn many direct exploration policy neural network different trade offs exploration exploitation use neural network different degree transfer demonstrate predominantly exploratory policy yield effective exploitative policy propose method incorporate run modern distribute rl agent collect large amount experience many actor run parallel separate environment instance method double performance base agent hard exploration atari-57 suite maintain high score across remain game obtain median human normalise score notably propose method first algorithm achieve non zero reward mean score game pitfall without use demonstration hand craft feature
Asymptotics of Network Embeddings Learned via Subsampling,"  Network data are ubiquitous in modern machine learning, with tasks of
interest including node classification, node clustering and link prediction. A
frequent approach begins by learning an Euclidean embedding of the network, to
which algorithms developed for vector-valued data are applied. For large
networks, embeddings are learned using stochastic gradient methods where the
sub-sampling scheme can be freely chosen. Despite the strong empirical
performance of such methods, they are not well understood theoretically. Our
work encapsulates representation methods using a subsampling approach, such as
node2vec, into a single unifying framework. We prove, under the assumption that
the graph is exchangeable, that the distribution of the learned embedding
vectors asymptotically decouples. Moreover, we characterize the asymptotic
distribution and provided rates of convergence, in terms of the latent
parameters, which includes the choice of loss function and the embedding
dimension. This provides a theoretical foundation to understand what the
embedding vectors represent and how well these methods perform on downstream
tasks. Notably, we observe that typically used loss functions may lead to
shortcomings, such as a lack of Fisher consistency.
",network datum ubiquitous modern machine learn task interest include node classification node clustering link prediction frequent approach begins learn euclidean embed network algorithm develop vector value datum apply large network embedding learn use stochastic gradient method sub sampling scheme freely choose despite strong empirical performance method well understand theoretically work encapsulate representation method use subsample approach node2vec single unifying framework prove assumption graph exchangeable distribution learn embed vector asymptotically decouple moreover characterize asymptotic distribution provide rate convergence term latent parameter include choice loss function embed dimension provide theoretical foundation understand embed vector represent well method perform downstream task notably observe typically use loss function may lead shortcoming lack fisher consistency
A Variational Perspective on Accelerated Methods in Optimization,"  Accelerated gradient methods play a central role in optimization, achieving
optimal rates in many settings. While many generalizations and extensions of
Nesterov's original acceleration method have been proposed, it is not yet clear
what is the natural scope of the acceleration concept. In this paper, we study
accelerated methods from a continuous-time perspective. We show that there is a
Lagrangian functional that we call the \emph{Bregman Lagrangian} which
generates a large class of accelerated methods in continuous time, including
(but not limited to) accelerated gradient descent, its non-Euclidean extension,
and accelerated higher-order gradient methods. We show that the continuous-time
limit of all of these methods correspond to traveling the same curve in
spacetime at different speeds. From this perspective, Nesterov's technique and
many of its generalizations can be viewed as a systematic way to go from the
continuous-time curves generated by the Bregman Lagrangian to a family of
discrete-time accelerated algorithms.
",accelerate gradient method play central role optimization achieve optimal rate many setting many generalization extension nesterov original acceleration method propose yet clear natural scope acceleration concept paper study accelerate method continuous time perspective show lagrangian functional call bregman lagrangian generate large class accelerate method continuous time include limited accelerate gradient descent non euclidean extension accelerate high order gradient method show continuous time limit method correspond travel curve spacetime different speed perspective nesterov technique many generalization view systematic way go continuous time curve generate bregman lagrangian family discrete time accelerate algorithm
Binary Space Partitioning Forests,"  The Binary Space Partitioning~(BSP)-Tree process is proposed to produce
flexible 2-D partition structures which are originally used as a Bayesian
nonparametric prior for relational modelling. It can hardly be applied to other
learning tasks such as regression trees because extending the BSP-Tree process
to a higher dimensional space is nontrivial. This paper is the first attempt to
extend the BSP-Tree process to a d-dimensional (d>2) space. We propose to
generate a cutting hyperplane, which is assumed to be parallel to d-2
dimensions, to cut each node in the d-dimensional BSP-tree. By designing a
subtle strategy to sample two free dimensions from d dimensions, the extended
BSP-Tree process can inherit the essential self-consistency property from the
original version. Based on the extended BSP-Tree process, an ensemble model,
which is named the BSP-Forest, is further developed for regression tasks.
Thanks to the retained self-consistency property, we can thus significantly
reduce the geometric calculations in the inference stage. Compared to its
counterpart, the Mondrian Forest, the BSP-Forest can achieve similar
performance with fewer cuts due to its flexibility. The BSP-Forest also
outperforms other (Bayesian) regression forests on a number of real-world data
sets.
",binary space bsp -tree process propose produce flexible 2 d partition structure originally use bayesian nonparametric prior relational modelling hardly apply learn task regression tree extend bsp tree process high dimensional space nontrivial paper first attempt extend bsp tree process d dimensional 2 space propose generate cut hyperplane assume parallel d-2 dimension cut node d dimensional bsp tree design subtle strategy sample two free dimension dimension extend bsp tree process inherit essential self consistency property original version base extended bsp tree process ensemble model name bsp forest develop regression task thank retain self consistency property thus significantly reduce geometric calculation inference stage compare counterpart mondrian forest bsp forest achieve similar performance few cut due flexibility bsp forest also outperform bayesian regression forest number real world datum set
DQLEL: Deep Q-Learning for Energy-Optimized LoS/NLoS UWB Node Selection,"  Recent advancements in Internet of Things (IoTs) have brought about a surge
of interest in indoor positioning for the purpose of providing reliable,
accurate, and energy-efficient indoor navigation/localization systems. Ultra
Wide Band (UWB) technology has been emerged as a potential candidate to satisfy
the aforementioned requirements. Although UWB technology can enhance the
accuracy of indoor positioning due to the use of a wide-frequency spectrum,
there are key challenges ahead for its efficient implementation. On the one
hand, achieving high precision in positioning relies on the
identification/mitigation Non Line of Sight (NLoS) links, leading to a
significant increase in the complexity of the localization framework. On the
other hand, UWB beacons have a limited battery life, which is especially
problematic in practical circumstances with certain beacons located in
strategic positions. To address these challenges, we introduce an efficient
node selection framework to enhance the location accuracy without using complex
NLoS mitigation methods, while maintaining a balance between the remaining
battery life of UWB beacons. Referred to as the Deep Q-Learning
Energy-optimized LoS/NLoS (DQLEL) UWB node selection framework, the mobile user
is autonomously trained to determine the optimal set of UWB beacons to be
localized based on the 2-D Time Difference of Arrival (TDoA) framework. The
effectiveness of the proposed DQLEL framework is evaluated in terms of the link
condition, the deviation of the remaining battery life of UWB beacons, location
error, and cumulative rewards. Based on the simulation results, the proposed
DQLEL framework significantly outperformed its counterparts across the
aforementioned aspects.
",recent advancement internet thing iot bring surge interest indoor positioning purpose provide reliable accurate energy efficient indoor system ultra wide band uwb technology emerge potential candidate satisfy aforementione requirement although uwb technology enhance accuracy indoor positioning due use wide frequency spectrum key challenge ahead efficient implementation one hand achieve high precision positioning rely non line sight nlo link lead significant increase complexity localization framework hand uwb beacon limit battery life especially problematic practical circumstance certain beacon locate strategic position address challenge introduce efficient node selection framework enhance location accuracy without use complex nlo mitigation method maintain balance remain battery life uwb beacon refer deep q learn energy optimize dqlel uwb node selection framework mobile user autonomously train determine optimal set uwb beacon localize base 2 d time difference arrival tdoa framework effectiveness propose dqlel framework evaluate term link condition deviation remain battery life uwb beacon location error cumulative reward base simulation result propose dqlel framework significantly outperform counterpart across aforementioned aspect
Unified and Effective Ensemble Knowledge Distillation,"  Ensemble knowledge distillation can extract knowledge from multiple teacher
models and encode it into a single student model. Many existing methods learn
and distill the student model on labeled data only. However, the teacher models
are usually learned on the same labeled data, and their predictions have high
correlations with groudtruth labels. Thus, they cannot provide sufficient
knowledge complementary to task labels for student teaching. Distilling on
unseen unlabeled data has the potential to enhance the knowledge transfer from
the teachers to the student. In this paper, we propose a unified and effective
ensemble knowledge distillation method that distills a single student model
from an ensemble of teacher models on both labeled and unlabeled data. Since
different teachers may have diverse prediction correctness on the same sample,
on labeled data we weight the predictions of different teachers according to
their correctness. In addition, we weight the distillation loss based on the
overall prediction correctness of the teacher ensemble to distill high-quality
knowledge. On unlabeled data, there is no groundtruth to evaluate prediction
correctness. Fortunately, the disagreement among teachers is an indication of
sample hardness, and thereby we weight the distillation loss based on teachers'
disagreement to emphasize knowledge distillation on important samples.
Extensive experiments on four datasets show the effectiveness of our proposed
ensemble distillation method.
",ensemble knowledge distillation extract knowledge multiple teacher model encode single student model many exist method learn distill student model label datum however teacher model usually learn label data prediction high correlation groudtruth label thus provide sufficient knowledge complementary task label student teach distil unseen unlabeled datum potential enhance knowledge transfer teacher student paper propose unified effective ensemble knowledge distillation method distill single student model ensemble teacher model label unlabeled datum since different teacher may diverse prediction correctness sample label datum weight prediction different teacher accord correctness addition weight distillation loss base overall prediction correctness teacher ensemble distill high quality knowledge unlabeled data groundtruth evaluate prediction correctness fortunately disagreement among teacher indication sample hardness thereby weight distillation loss base teacher disagreement emphasize knowledge distillation important sample extensive experiment four dataset show effectiveness propose ensemble distillation method
Infinite Variational Autoencoder for Semi-Supervised Learning,"  This paper presents an infinite variational autoencoder (VAE) whose capacity
adapts to suit the input data. This is achieved using a mixture model where the
mixing coefficients are modeled by a Dirichlet process, allowing us to
integrate over the coefficients when performing inference. Critically, this
then allows us to automatically vary the number of autoencoders in the mixture
based on the data. Experiments show the flexibility of our method, particularly
for semi-supervised learning, where only a small number of training samples are
available.
",paper present infinite variational autoencoder vae whose capacity adapt suit input datum achieve use mixture model mix coefficient model dirichlet process allow we integrate coefficient perform inference critically allow we automatically vary number autoencoder mixture base datum experiment show flexibility method particularly semi supervised learn small number training sample available
"Focusing on Shadows for Predicting Heightmaps from Single Remotely
  Sensed RGB Images with Deep Learning","  Estimating the heightmaps of buildings and vegetation in single remotely
sensed images is a challenging problem. Effective solutions to this problem can
comprise the stepping stone for solving complex and demanding problems that
require 3D information of aerial imagery in the remote sensing discipline,
which might be expensive or not feasible to require. We propose a task-focused
Deep Learning (DL) model that takes advantage of the shadow map of a remotely
sensed image to calculate its heightmap. The shadow is computed efficiently and
does not add significant computation complexity. The model is trained with
aerial images and their Lidar measurements, achieving superior performance on
the task. We validate the model with a dataset covering a large area of
Manchester, UK, as well as the 2018 IEEE GRSS Data Fusion Contest Lidar
dataset. Our work suggests that the proposed DL architecture and the technique
of injecting shadows information into the model are valuable for improving the
heightmap estimation task for single remotely sensed imagery.
",estimate heightmap building vegetation single remotely sense image challenge problem effective solution problem comprise step stone solve complex demand problem require 3d information aerial imagery remote sense discipline might expensive feasible require propose task focus deep learning dl model take advantage shadow map remotely sense image calculate heightmap shadow compute efficiently add significant computation complexity model train aerial image lidar measurement achieve superior performance task validate model dataset cover large area manchester uk well 2018 ieee grss datum fusion contest lidar dataset work suggest propose dl architecture technique inject shadow information model valuable improve heightmap estimation task single remotely sense imagery
"Machine learning based iterative learning control for non-repetitive
  time-varying systems","  The repetitive tracking task for time-varying systems (TVSs) with
non-repetitive time-varying parameters, which is also called non-repetitive
TVSs, is realized in this paper using iterative learning control (ILC). A
machine learning (ML) based nominal model update mechanism, which utilizes the
linear regression technique to update the nominal model at each ILC trial only
using the current trial information, is proposed for non-repetitive TVSs in
order to enhance the ILC performance. Given that the ML mechanism forces the
model uncertainties to remain within the ILC robust tolerance, an ILC update
law is proposed to deal with non-repetitive TVSs. How to tune parameters inside
ML and ILC algorithms to achieve the desired aggregate performance is also
provided. The robustness and reliability of the proposed method are verified by
simulations. Comparison with current state-of-the-art demonstrates its superior
control performance in terms of controlling precision. This paper broadens ILC
applications from time-invariant systems to non-repetitive TVSs, adopts ML
regression technique to estimate non-repetitive time-varying parameters between
two ILC trials and proposes a detailed parameter tuning mechanism to achieve
desired performance, which are the main contributions.
",repetitive tracking task time vary system tvss non repetitive time vary parameter also call non repetitive tvss realize paper use iterative learning control ilc machine learning ml base nominal model update mechanism utilize linear regression technique update nominal model ilc trial use current trial information propose non repetitive tvss order enhance ilc performance give ml mechanism force model uncertainty remain within ilc robust tolerance ilc update law propose deal non repetitive tvss tune parameter inside ml ilc algorithms achieve desire aggregate performance also provide robustness reliability propose method verify simulation comparison current state of the art demonstrate superior control performance term control precision paper broaden ilc application time invariant system non repetitive tvss adopt ml regression technique estimate non repetitive time vary parameter two ilc trial propose detailed parameter tuning mechanism achieve desire performance main contribution
Anti-Bandit Neural Architecture Search for Model Defense,"  Deep convolutional neural networks (DCNNs) have dominated as the best
performers in machine learning, but can be challenged by adversarial attacks.
In this paper, we defend against adversarial attacks using neural architecture
search (NAS) which is based on a comprehensive search of denoising blocks,
weight-free operations, Gabor filters and convolutions. The resulting
anti-bandit NAS (ABanditNAS) incorporates a new operation evaluation measure
and search process based on the lower and upper confidence bounds (LCB and
UCB). Unlike the conventional bandit algorithm using UCB for evaluation only,
we use UCB to abandon arms for search efficiency and LCB for a fair competition
between arms. Extensive experiments demonstrate that ABanditNAS is faster than
other NAS methods, while achieving an $8.73\%$ improvement over prior arts on
CIFAR-10 under PGD-$7$.
",deep convolutional neural network dcnns dominate good performer machine learning challenge adversarial attack paper defend adversarial attack use neural architecture search nas base comprehensive search denoising block weight free operation gabor filter convolution result anti bandit nas abanditnas incorporate new operation evaluation measure search process base low upper confidence bound lcb ucb unlike conventional bandit algorithm use ucb evaluation use ucb abandon arm search efficiency lcb fair competition arm extensive experiment demonstrate abanditnas fast nas method achieve improvement prior art cifar-10 pgd- 7
Domain Adaptive Hand Keypoint and Pixel Localization in the Wild,"  We aim to improve the performance of regressing hand keypoints and segmenting
pixel-level hand masks under new imaging conditions (e.g., outdoors) when we
only have labeled images taken under very different conditions (e.g., indoors).
In the real world, it is important that the model trained for both tasks works
under various imaging conditions. However, their variation covered by existing
labeled hand datasets is limited. Thus, it is necessary to adapt the model
trained on the labeled images (source) to unlabeled images (target) with unseen
imaging conditions. While self-training domain adaptation methods (i.e.,
learning from the unlabeled target images in a self-supervised manner) have
been developed for both tasks, their training may degrade performance when the
predictions on the target images are noisy. To avoid this, it is crucial to
assign a low importance (confidence) weight to the noisy predictions during
self-training. In this paper, we propose to utilize the divergence of two
predictions to estimate the confidence of the target image for both tasks.
These predictions are given from two separate networks, and their divergence
helps identify the noisy predictions. To integrate our proposed confidence
estimation into self-training, we propose a teacher-student framework where the
two networks (teachers) provide supervision to a network (student) for
self-training, and the teachers are learned from the student by knowledge
distillation. Our experiments show its superiority over state-of-the-art
methods in adaptation settings with different lighting, grasping objects,
backgrounds, and camera viewpoints. Our method improves by 4% the multi-task
score on HO3D compared to the latest adversarial adaptation method. We also
validate our method on Ego4D, egocentric videos with rapid changes in imaging
conditions outdoors.
",aim improve performance regress hand keypoint segment pixel level hand mask new imaging condition outdoors label image take different condition indoor real world important model train task work various imaging condition however variation cover exist label hand dataset limit thus necessary adapt model train label image source unlabeled image target unseen imaging condition self train domain adaptation method learn unlabeled target image self supervise manner develop task training may degrade performance prediction target image noisy avoid crucial assign low importance confidence weight noisy prediction self train paper propose utilize divergence two prediction estimate confidence target image task prediction give two separate network divergence helps identify noisy prediction integrate propose confidence estimation self training propose teacher student framework two network teacher provide supervision network student self train teacher learn student knowledge distillation experiment show superiority state of the art method adaptation setting different lighting grasp object background camera viewpoint method improve 4 multi task score ho3d compare late adversarial adaptation method also validate method ego4d egocentric video rapid change image condition outdoors
Neural Pruning via Growing Regularization,"  Regularization has long been utilized to learn sparsity in deep neural
network pruning. However, its role is mainly explored in the small penalty
strength regime. In this work, we extend its application to a new scenario
where the regularization grows large gradually to tackle two central problems
of pruning: pruning schedule and weight importance scoring. (1) The former
topic is newly brought up in this work, which we find critical to the pruning
performance while receives little research attention. Specifically, we propose
an L2 regularization variant with rising penalty factors and show it can bring
significant accuracy gains compared with its one-shot counterpart, even when
the same weights are removed. (2) The growing penalty scheme also brings us an
approach to exploit the Hessian information for more accurate pruning without
knowing their specific values, thus not bothered by the common Hessian
approximation problems. Empirically, the proposed algorithms are easy to
implement and scalable to large datasets and networks in both structured and
unstructured pruning. Their effectiveness is demonstrated with modern deep
neural networks on the CIFAR and ImageNet datasets, achieving competitive
results compared to many state-of-the-art algorithms. Our code and trained
models are publicly available at
https://github.com/mingsuntse/regularization-pruning.
",regularization long utilize learn sparsity deep neural network prune however role mainly explore small penalty strength regime work extend application new scenario regularization grow large gradually tackle two central problem prune pruning schedule weight importance score 1 former topic newly bring work find critical pruning performance receive little research attention specifically propose l2 regularization variant rise penalty factor show bring significant accuracy gain compare one shot counterpart even weight remove 2 grow penalty scheme also bring we approach exploit hessian information accurate pruning without know specific value thus bother common hessian approximation problem empirically propose algorithm easy implement scalable large dataset network structure unstructured pruning effectiveness demonstrate modern deep neural network cifar imagenet dataset achieve competitive result compare many state of the art algorithm code train model publicly available https
"Multi-domain Clinical Natural Language Processing with MedCAT: the
  Medical Concept Annotation Toolkit","  Electronic health records (EHR) contain large volumes of unstructured text,
requiring the application of Information Extraction (IE) technologies to enable
clinical analysis. We present the open-source Medical Concept Annotation
Toolkit (MedCAT) that provides: a) a novel self-supervised machine learning
algorithm for extracting concepts using any concept vocabulary including
UMLS/SNOMED-CT; b) a feature-rich annotation interface for customising and
training IE models; and c) integrations to the broader CogStack ecosystem for
vendor-agnostic health system deployment. We show improved performance in
extracting UMLS concepts from open datasets (F1:0.448-0.738 vs 0.429-0.650).
Further real-world validation demonstrates SNOMED-CT extraction at 3 large
London hospitals with self-supervised training over ~8.8B words from ~17M
clinical records and further fine-tuning with ~6K clinician annotated examples.
We show strong transferability (F1 > 0.94) between hospitals, datasets, and
concept types indicating cross-domain EHR-agnostic utility for accelerated
clinical and research use cases.
",electronic health record ehr contain large volume unstructured text require application information extraction ie technology enable clinical analysis present open source medical concept annotation toolkit medcat provide novel self supervise machine learn algorithm extract concept use concept vocabulary include b feature rich annotation interface customise train ie model c integration broad cogstack ecosystem vendor agnostic health system deployment show improve performance extract umls concept open dataset vs real world validation demonstrate snome ct extraction 3 large london hospital self supervise training word clinical record fine tune clinician annotated example show strong transferability f1 hospital dataset concept type indicate cross domain ehr agnostic utility accelerate clinical research use case
"Gram-Gauss-Newton Method: Learning Overparameterized Neural Networks for
  Regression Problems","  First-order methods such as stochastic gradient descent (SGD) are currently
the standard algorithm for training deep neural networks. Second-order methods,
despite their better convergence rate, are rarely used in practice due to the
prohibitive computational cost in calculating the second-order information. In
this paper, we propose a novel Gram-Gauss-Newton (GGN) algorithm to train deep
neural networks for regression problems with square loss. Our method draws
inspiration from the connection between neural network optimization and kernel
regression of neural tangent kernel (NTK). Different from typical second-order
methods that have heavy computational cost in each iteration, GGN only has
minor overhead compared to first-order methods such as SGD. We also give
theoretical results to show that for sufficiently wide neural networks, the
convergence rate of GGN is \emph{quadratic}. Furthermore, we provide
convergence guarantee for mini-batch GGN algorithm, which is, to our knowledge,
the first convergence result for the mini-batch version of a second-order
method on overparameterized neural networks. Preliminary experiments on
regression tasks demonstrate that for training standard networks, our GGN
algorithm converges much faster and achieves better performance than SGD.
",first order method stochastic gradient descent sgd currently standard algorithm training deep neural network second order method despite well convergence rate rarely use practice due prohibitive computational cost calculate second order information paper propose novel gram gauss newton ggn algorithm train deep neural network regression problem square loss method draw inspiration connection neural network optimization kernel regression neural tangent kernel ntk different typical second order method heavy computational cost iteration ggn minor overhead compare first order method sgd also give theoretical result show sufficiently wide neural network convergence rate ggn quadratic furthermore provide convergence guarantee mini batch ggn algorithm knowledge first convergence result mini batch version second order method overparameterize neural network preliminary experiment regression task demonstrate train standard network ggn algorithm converge much fast achieve well performance sgd
On Fair Selection in the Presence of Implicit Variance,"  Quota-based fairness mechanisms like the so-called Rooney rule or four-fifths
rule are used in selection problems such as hiring or college admission to
reduce inequalities based on sensitive demographic attributes. These mechanisms
are often viewed as introducing a trade-off between selection fairness and
utility. In recent work, however, Kleinberg and Raghavan showed that, in the
presence of implicit bias in estimating candidates' quality, the Rooney rule
can increase the utility of the selection process.
  We argue that even in the absence of implicit bias, the estimates of
candidates' quality from different groups may differ in another fundamental
way, namely, in their variance. We term this phenomenon implicit variance and
we ask: can fairness mechanisms be beneficial to the utility of a selection
process in the presence of implicit variance (even in the absence of implicit
bias)? To answer this question, we propose a simple model in which candidates
have a true latent quality that is drawn from a group-independent normal
distribution. To make the selection, a decision maker receives an unbiased
estimate of the quality of each candidate, with normal noise, but whose
variance depends on the candidate's group. We then compare the utility obtained
by imposing a fairness mechanism that we term $\gamma$-rule (it includes
demographic parity and the four-fifths rule as special cases), to that of a
group-oblivious selection algorithm that picks the candidates with the highest
estimated quality independently of their group. Our main result shows that the
demographic parity mechanism always increases the selection utility, while any
$\gamma$-rule weakly increases it. We extend our model to a two-stage selection
process where the true quality is observed at the second stage. We discuss
multiple extensions of our results, in particular to different distributions of
the true latent quality.
",quota base fairness mechanism like so call rooney rule four fifth rule use selection problem hire college admission reduce inequality base sensitive demographic attribute mechanism often view introduce trade off selection fairness utility recent work however kleinberg raghavan show presence implicit bias estimate candidate quality rooney rule increase utility selection process argue even absence implicit bias estimate candidate quality different group may differ another fundamental way namely variance term phenomenon implicit variance ask fairness mechanism beneficial utility selection process presence implicit variance even absence implicit bias answer question propose simple model candidate true latent quality draw group independent normal distribution make selection decision maker receive unbiased estimate quality candidate normal noise whose variance depend candidate group compare utility obtain impose fairness mechanism term -rule include demographic parity four fifth rule special case group oblivious selection algorithm pick candidate highest estimate quality independently group main result show demographic parity mechanism always increase selection utility -rule weakly increase extend model two stage selection process true quality observe second stage discuss multiple extension result particular different distribution true latent quality
Bag of Tricks for Adversarial Training,"  Adversarial training (AT) is one of the most effective strategies for
promoting model robustness. However, recent benchmarks show that most of the
proposed improvements on AT are less effective than simply early stopping the
training procedure. This counter-intuitive fact motivates us to investigate the
implementation details of tens of AT methods. Surprisingly, we find that the
basic settings (e.g., weight decay, training schedule, etc.) used in these
methods are highly inconsistent. In this work, we provide comprehensive
evaluations on CIFAR-10, focusing on the effects of mostly overlooked training
tricks and hyperparameters for adversarially trained models. Our empirical
observations suggest that adversarial robustness is much more sensitive to some
basic training settings than we thought. For example, a slightly different
value of weight decay can reduce the model robust accuracy by more than 7%,
which is probable to override the potential promotion induced by the proposed
methods. We conclude a baseline training setting and re-implement previous
defenses to achieve new state-of-the-art results. These facts also appeal to
more concerns on the overlooked confounders when benchmarking defenses.
",adversarial training one effective strategy promote model robustness however recent benchmark show propose improvement less effective simply early stop training procedure counter intuitive fact motivate we investigate implementation detail tens method surprisingly find basic setting weight decay training schedule etc use method highly inconsistent work provide comprehensive evaluation cifar-10 focus effect mostly overlook training trick hyperparameter adversarially train model empirical observation suggest adversarial robustness much sensitive basic training setting think example slightly different value weight decay reduce model robust accuracy 7 probable override potential promotion induce propose method conclude baseline training set re implement previous defense achieve new state of the art result fact also appeal concern overlook confounder benchmarking defense
SEIHAI: A Sample-efficient Hierarchical AI for the MineRL Competition,"  The MineRL competition is designed for the development of reinforcement
learning and imitation learning algorithms that can efficiently leverage human
demonstrations to drastically reduce the number of environment interactions
needed to solve the complex \emph{ObtainDiamond} task with sparse rewards. To
address the challenge, in this paper, we present \textbf{SEIHAI}, a
\textbf{S}ample-\textbf{e}ff\textbf{i}cient \textbf{H}ierarchical \textbf{AI},
that fully takes advantage of the human demonstrations and the task structure.
Specifically, we split the task into several sequentially dependent subtasks,
and train a suitable agent for each subtask using reinforcement learning and
imitation learning. We further design a scheduler to select different agents
for different subtasks automatically. SEIHAI takes the first place in the
preliminary and final of the NeurIPS-2020 MineRL competition.
",minerl competition design development reinforcement learning imitation learning algorithm efficiently leverage human demonstration drastically reduce number environment interaction need solve complex obtaindiamond task sparse reward address challenge paper present seihai e cient h ierarchical ai fully take advantage human demonstration task structure specifically split task several sequentially dependent subtask train suitable agent subtask use reinforcement learning imitation learn design scheduler select different agent different subtask automatically seihai take first place preliminary final neurips-2020 minerl competition
On the Robustness of Cooperative Multi-Agent Reinforcement Learning,"  In cooperative multi-agent reinforcement learning (c-MARL), agents learn to
cooperatively take actions as a team to maximize a total team reward. We
analyze the robustness of c-MARL to adversaries capable of attacking one of the
agents on a team. Through the ability to manipulate this agent's observations,
the adversary seeks to decrease the total team reward.
  Attacking c-MARL is challenging for three reasons: first, it is difficult to
estimate team rewards or how they are impacted by an agent mispredicting;
second, models are non-differentiable; and third, the feature space is
low-dimensional. Thus, we introduce a novel attack. The attacker first trains a
policy network with reinforcement learning to find a wrong action it should
encourage the victim agent to take. Then, the adversary uses targeted
adversarial examples to force the victim to take this action.
  Our results on the StartCraft II multi-agent benchmark demonstrate that
c-MARL teams are highly vulnerable to perturbations applied to one of their
agent's observations. By attacking a single agent, our attack method has highly
negative impact on the overall team reward, reducing it from 20 to 9.4. This
results in the team's winning rate to go down from 98.9% to 0%.
",cooperative multi agent reinforcement learn c marl agent learn cooperatively take action team maximize total team reward analyze robustness c marl adversary capable attack one agent team ability manipulate agent observation adversary seeks decrease total team reward attack c marl challenge three reason first difficult estimate team reward impact agent mispredicte second model non differentiable third feature space low dimensional thus introduce novel attack attacker first train policy network reinforcement learning find wrong action encourage victim agent take adversary use target adversarial example force victim take action result startcraft ii multi agent benchmark demonstrate c marl team highly vulnerable perturbation apply one agent observation attack single agent attack method highly negative impact overall team reward reduce 20 result team win rate go 0
Beyond permutation equivariance in graph networks,"  In this draft paper, we introduce a novel architecture for graph networks
which is equivariant to the Euclidean group in $n$-dimensions. The model is
designed to work with graph networks in their general form and can be shown to
include particular variants as special cases. Thanks to its equivariance
properties, we expect the proposed model to be more data efficient with respect
to classical graph architectures and also intrinsically equipped with a better
inductive bias. We defer investigating this matter to future work.
",draft paper introduce novel architecture graph network equivariant euclidean group n -dimension model design work graph network general form show include particular variant special case thank equivariance property expect propose model datum efficient respect classical graph architecture also intrinsically equip well inductive bias defer investigate matter future work
"Micro Batch Streaming: Allowing the Training of DNN models Using a large
  batch size on Small Memory Systems","  The size of the deep learning models has greatly increased over the past
decade. Such models are difficult to train using a large batch size, because
commodity machines do not have enough memory to accommodate both the model and
a large data size. The batch size is one of the hyper-parameters used in the
training model, and it is dependent on and is limited by the target machine
memory capacity and it is dependent on the remaining memory after the model is
uploaded. A smaller batch size usually results in performance degradation. This
paper proposes a framework called Micro-Batch Streaming (MBS) to address this
problem. This method helps deep learning models to train by providing a batch
streaming algorithm that splits a batch into the appropriate size for the
remaining memory size and streams them sequentially to the target machine. A
loss normalization algorithm based on the gradient accumulation is used to
maintain the performance. The purpose of our method is to allow deep learning
models to train using mathematically determined optimal batch sizes that cannot
fit into the memory of a target system.
",size deep learning model greatly increase past decade model difficult train use large batch size commodity machine enough memory accommodate model large datum size batch size one hyper parameter use training model dependent limited target machine memory capacity dependent remain memory model upload small batch size usually result performance degradation paper propose framework call micro batch stream mbs address problem method help deep learning model train provide batch streaming algorithm split batch appropriate size remain memory size stream sequentially target machine loss normalization algorithm base gradient accumulation use maintain performance purpose method allow deep learning model train use mathematically determine optimal batch size fit memory target system
A Robust Optimization Approach to Deep Learning,"  Many state-of-the-art adversarial training methods leverage upper bounds of
the adversarial loss to provide security guarantees. Yet, these methods require
computations at each training step that can not be incorporated in the gradient
for backpropagation. We introduce a new, more principled approach to
adversarial training based on a closed form solution of an upper bound of the
adversarial loss, which can be effectively trained with backpropagation. This
bound is facilitated by state-of-the-art tools from robust optimization. We
derive two new methods with our approach. The first method (Approximated Robust
Upper Bound or aRUB) uses the first order approximation of the network as well
as basic tools from linear robust optimization to obtain an approximate upper
bound of the adversarial loss that can be easily implemented. The second method
(Robust Upper Bound or RUB), computes an exact upper bound of the adversarial
loss. Across a variety of tabular and vision data sets we demonstrate the
effectiveness of our more principled approach -- RUB is substantially more
robust than state-of-the-art methods for larger perturbations, while aRUB
matches the performance of state-of-the-art methods for small perturbations.
Also, both RUB and aRUB run faster than standard adversarial training (at the
expense of an increase in memory). All the code to reproduce the results can be
found at https://github.com/kimvc7/Robustness.
",many state of the art adversarial training method leverage upper bound adversarial loss provide security guarantee yet method require computation training step incorporate gradient backpropagation introduce new principle approach adversarial training base closed form solution upper bind adversarial loss effectively train backpropagation bind facilitate state of the art tool robust optimization derive two new method approach first method approximate robust upper bind arub use first order approximation network well basic tool linear robust optimization obtain approximate upper bind adversarial loss easily implement second method robust upper bind rub compute exact upper bind adversarial loss across variety tabular vision datum set demonstrate effectiveness principle approach rub substantially robust state of the art method large perturbation arub match performance state of the art method small perturbation also rub arub run fast standard adversarial training expense increase memory code reproduce result find https
"Sentiment Analysis with Deep Learning Models: A Comparative Study on a
  Decade of Sinhala Language Facebook Data","  The relationship between Facebook posts and the corresponding reaction
feature is an interesting subject to explore and understand. To achieve this
end, we test state-of-the-art Sinhala sentiment analysis models against a data
set containing a decade worth of Sinhala posts with millions of reactions. For
the purpose of establishing benchmarks and with the goal of identifying the
best model for Sinhala sentiment analysis, we also test, on the same data set
configuration, other deep learning models catered for sentiment analysis. In
this study we report that the 3 layer Bidirectional LSTM model achieves an F1
score of 84.58% for Sinhala sentiment analysis, surpassing the current
state-of-the-art model; Capsule B, which only manages to get an F1 score of
82.04%. Further, since all the deep learning models show F1 scores above 75% we
conclude that it is safe to claim that Facebook reactions are suitable to
predict the sentiment of a text.
",relationship facebook post correspond reaction feature interesting subject explore understand achieve end test state of the art sinhala sentiment analysis model datum set contain decade worth sinhala post million reaction purpose establish benchmark goal identify good model sinhala sentiment analysis also test datum set configuration deep learning model cater sentiment analysis study report 3 layer bidirectional lstm model achieve f1 score sinhala sentiment analysis surpass current state of the art model capsule b manage get f1 score since deep learning model show f1 score 75 conclude safe claim facebook reaction suitable predict sentiment text
Deep Visual Foresight for Planning Robot Motion,"  A key challenge in scaling up robot learning to many skills and environments
is removing the need for human supervision, so that robots can collect their
own data and improve their own performance without being limited by the cost of
requesting human feedback. Model-based reinforcement learning holds the promise
of enabling an agent to learn to predict the effects of its actions, which
could provide flexible predictive models for a wide range of tasks and
environments, without detailed human supervision. We develop a method for
combining deep action-conditioned video prediction models with model-predictive
control that uses entirely unlabeled training data. Our approach does not
require a calibrated camera, an instrumented training set-up, nor precise
sensing and actuation. Our results show that our method enables a real robot to
perform nonprehensile manipulation -- pushing objects -- and can handle novel
objects not seen during training.
",key challenge scale robot learn many skill environment remove need human supervision robot collect datum improve performance without limited cost request human feedback model base reinforcement learning hold promise enable agent learn predict effect action could provide flexible predictive model wide range task environment without detailed human supervision develop method combine deep action condition video prediction model model predictive control use entirely unlabeled training datum approach require calibrated camera instrument training set up precise sense actuation result show method enable real robot perform nonprehensile manipulation push object handle novel object see training
Learning to Invert: Signal Recovery via Deep Convolutional Networks,"  The promise of compressive sensing (CS) has been offset by two significant
challenges. First, real-world data is not exactly sparse in a fixed basis.
Second, current high-performance recovery algorithms are slow to converge,
which limits CS to either non-real-time applications or scenarios where massive
back-end computing is available. In this paper, we attack both of these
challenges head-on by developing a new signal recovery framework we call {\em
DeepInverse} that learns the inverse transformation from measurement vectors to
signals using a {\em deep convolutional network}. When trained on a set of
representative images, the network learns both a representation for the signals
(addressing challenge one) and an inverse map approximating a greedy or convex
recovery algorithm (addressing challenge two). Our experiments indicate that
the DeepInverse network closely approximates the solution produced by
state-of-the-art CS recovery algorithms yet is hundreds of times faster in run
time. The tradeoff for the ultrafast run time is a computationally intensive,
off-line training procedure typical to deep networks. However, the training
needs to be completed only once, which makes the approach attractive for a host
of sparse recovery problems.
",promise compressive sense cs offset two significant challenge first real world datum exactly sparse fix basis second current high performance recovery algorithm slow converge limit cs either non real time application scenario massive back end computing available paper attack challenge head on develop new signal recovery framework call deepinverse learn inverse transformation measurement vector signal use deep convolutional network train set representative image network learn representation signal address challenge one inverse map approximate greedy convex recovery algorithm address challenge two experiment indicate deepinverse network closely approximate solution produce state of the art cs recovery algorithm yet hundred time fast run time tradeoff ultrafast run time computationally intensive off line training procedure typical deep network however training need complete make approach attractive host sparse recovery problem
Neural Network Encapsulation,"  A capsule is a collection of neurons which represents different variants of a
pattern in the network. The routing scheme ensures only certain capsules which
resemble lower counterparts in the higher layer should be activated. However,
the computational complexity becomes a bottleneck for scaling up to larger
networks, as lower capsules need to correspond to each and every higher
capsule. To resolve this limitation, we approximate the routing process with
two branches: a master branch which collects primary information from its
direct contact in the lower layer and an aide branch that replenishes master
based on pattern variants encoded in other lower capsules. Compared with
previous iterative and unsupervised routing scheme, these two branches are
communicated in a fast, supervised and one-time pass fashion. The complexity
and runtime of the model are therefore decreased by a large margin. Motivated
by the routing to make higher capsule have agreement with lower capsule, we
extend the mechanism as a compensation for the rapid loss of information in
nearby layers. We devise a feedback agreement unit to send back higher capsules
as feedback. It could be regarded as an additional regularization to the
network. The feedback agreement is achieved by comparing the optimal transport
divergence between two distributions (lower and higher capsules). Such an
add-on witnesses a unanimous gain in both capsule and vanilla networks. Our
proposed EncapNet performs favorably better against previous state-of-the-arts
on CIFAR10/100, SVHN and a subset of ImageNet.
",capsule collection neuron represent different variant pattern network route scheme ensure certain capsule resemble low counterpart high layer activate however computational complexity become bottleneck scale large network low capsule need correspond every high capsule resolve limitation approximate routing process two branch master branch collect primary information direct contact low layer aide branch replenishes master base pattern variant encode low capsule compare previous iterative unsupervised route scheme two branch communicate fast supervise one time pass fashion complexity runtime model therefore decrease large margin motivate route make high capsule agreement low capsule extend mechanism compensation rapid loss information nearby layer devise feedback agreement unit send back high capsule feedback could regard additional regularization network feedback agreement achieve compare optimal transport divergence two distribution low high capsule add on witness unanimous gain capsule vanilla network propose encapnet perform favorably well previous state of the art svhn subset imagenet
Moment Multicalibration for Uncertainty Estimation,"  We show how to achieve the notion of ""multicalibration"" from H\'ebert-Johnson
et al. [2018] not just for means, but also for variances and other higher
moments. Informally, it means that we can find regression functions which,
given a data point, can make point predictions not just for the expectation of
its label, but for higher moments of its label distribution as well-and those
predictions match the true distribution quantities when averaged not just over
the population as a whole, but also when averaged over an enormous number of
finely defined subgroups. It yields a principled way to estimate the
uncertainty of predictions on many different subgroups-and to diagnose
potential sources of unfairness in the predictive power of features across
subgroups. As an application, we show that our moment estimates can be used to
derive marginal prediction intervals that are simultaneously valid as averaged
over all of the (sufficiently large) subgroups for which moment
multicalibration has been obtained.
",show achieve notion multicalibration et al 2018 mean also variance high moment informally mean find regression function give datum point make point prediction expectation label high moment label distribution well and prediction match true distribution quantity average population whole also average enormous number finely define subgroup yield principle way estimate uncertainty prediction many different subgroup and diagnose potential source unfairness predictive power feature across subgroup application show moment estimate use derive marginal prediction interval simultaneously valid average sufficiently large subgroup moment multicalibration obtain
"Learning to segment clustered amoeboid cells from brightfield microscopy
  via multi-task learning with adaptive weight selection","  Detecting and segmenting individual cells from microscopy images is critical
to various life science applications. Traditional cell segmentation tools are
often ill-suited for applications in brightfield microscopy due to poor
contrast and intensity heterogeneity, and only a small subset are applicable to
segment cells in a cluster. In this regard, we introduce a novel supervised
technique for cell segmentation in a multi-task learning paradigm. A
combination of a multi-task loss, based on the region and cell boundary
detection, is employed for an improved prediction efficiency of the network.
The learning problem is posed in a novel min-max framework which enables
adaptive estimation of the hyper-parameters in an automatic fashion. The region
and cell boundary predictions are combined via morphological operations and
active contour model to segment individual cells.
  The proposed methodology is particularly suited to segment touching cells
from brightfield microscopy images without manual interventions.
Quantitatively, we observe an overall Dice score of 0.93 on the validation set,
which is an improvement of over 15.9% on a recent unsupervised method, and
outperforms the popular supervised U-net algorithm by at least $5.8\%$ on
average.
",detect segment individual cell microscopy image critical various life science application traditional cell segmentation tool often ill suit application brightfield microscopy due poor contrast intensity heterogeneity small subset applicable segment cell cluster regard introduce novel supervise technique cell segmentation multi task learn paradigm combination multi task loss base region cell boundary detection employ improve prediction efficiency network learning problem pose novel min max framework enable adaptive estimation hyper parameter automatic fashion region cell boundary prediction combine via morphological operation active contour model segment individual cell propose methodology particularly suit segment touch cell brightfield microscopy image without manual intervention quantitatively observe overall dice score validation set improvement recent unsupervised method outperform popular supervise u net algorithm least average
A Modern Self-Referential Weight Matrix That Learns to Modify Itself,"  The weight matrix (WM) of a neural network (NN) is its program. The programs
of many traditional NNs are learned through gradient descent in some error
function, then remain fixed. The WM of a self-referential NN, however, can keep
rapidly modifying all of itself during runtime. In principle, such NNs can
meta-learn to learn, and meta-meta-learn to meta-learn to learn, and so on, in
the sense of recursive self-improvement. While NN architectures potentially
capable of implementing such behavior have been proposed since the '90s, there
have been few if any practical studies. Here we revisit such NNs, building upon
recent successes of fast weight programmers and closely related linear
Transformers. We propose a scalable self-referential WM (SRWM) that uses outer
products and the delta update rule to modify itself. We evaluate our SRWM in
supervised few-shot learning and in multi-task reinforcement learning with
procedurally generated game environments. Our experiments demonstrate both
practical applicability and competitive performance of the proposed SRWM. Our
code is public.
",weight matrix wm neural network nn program program many traditional nn learn gradient descent error function remain fixed wm self referential nn however keep rapidly modify runtime principle nn meta learn learn meta meta learn meta learn learn sense recursive self improvement nn architecture potentially capable implement behavior propose since practical study revisit nn building upon recent success fast weight programmer closely relate linear transformer propose scalable self referential wm srwm use outer product delta update rule modify evaluate srwm supervise few shot learn multi task reinforcement learning procedurally generate game environment experiment demonstrate practical applicability competitive performance propose srwm code public
"A Trained Regularization Approach Based on Born Iterative Method for
  Electromagnetic Imaging","  A trained-based Born iterative method (TBIM) is developed for electromagnetic
imaging (EMI) applications. The proposed TBIM consists of a nested loop; the
outer loop executes TBIM iteration steps, while the inner loop executes a
trained iterative shrinkage thresholding algorithm (TISTA). The applied TISTA
runs linear Landweber iterations implemented with a trained regularization
network designed based on U-net architecture. A normalization process was
imposed in TISTA that made TISTA training applicable within the proposed TBIM.
The iterative utilization of the regularization network in TISTA is a
bottleneck that demands high memory allocation through the training process.
Therefore TISTA within each TBIM step was trained separately. The TISTA
regularization network in each TBIM step was initialized using the weights from
the previous TBIM step. The above approach achieved high-quality image
restoration after running few TBIM steps while maintained low memory allocation
through the training process. The proposed framework can be extended to Newton
or quasi-Newton schemes, where within each Newton iteration, a linear ill-posed
problem is optimized that differs from one example to another. The numerical
results illustrated in this work show the superiority of the proposed TBIM
compared to the conventional sparse-based Born iterative method (SBIM).
",train base bear iterative method tbim develop electromagnetic imaging emi application propose tbim consist nest loop outer loop execute tbim iteration step inner loop execute train iterative shrinkage thresholde algorithm tista applied tista run linear landweber iteration implement train regularization network design base u net architecture normalization process impose tista make tista training applicable within propose tbim iterative utilization regularization network tista bottleneck demand high memory allocation training process therefore tista within tbim step train separately tista regularization network tbim step initialize use weight previous tbim step approach achieve high quality image restoration run tbim step maintain low memory allocation training process propose framework extend newton quasi newton scheme within newton iteration linear ill pose problem optimize differ one example another numerical result illustrate work show superiority propose tbim compare conventional sparse base bear iterative method sbim
"DegreEmbed: incorporating entity embedding into logic rule learning for
  knowledge graph reasoning","  Knowledge graphs (KGs), as structured representations of real world facts,
are intelligent databases incorporating human knowledge that can help machine
imitate the way of human problem solving. However, due to the nature of rapid
iteration as well as incompleteness of data, KGs are usually huge and there are
inevitably missing facts in KGs. Link prediction for knowledge graphs is the
task aiming to complete missing facts by reasoning based on the existing
knowledge. Two main streams of research are widely studied: one learns
low-dimensional embeddings for entities and relations that can capture latent
patterns, and the other gains good interpretability by mining logical rules.
Unfortunately, previous studies rarely pay attention to heterogeneous KGs. In
this paper, we propose DegreEmbed, a model that combines embedding-based
learning and logic rule mining for inferring on KGs. Specifically, we study the
problem of predicting missing links in heterogeneous KGs that involve entities
and relations of various types from the perspective of the degrees of nodes.
Experimentally, we demonstrate that our DegreEmbed model outperforms the
state-of-the-art methods on real world datasets. Meanwhile, the rules mined by
our model are of high quality and interpretability.
",knowledge graph kgs structure representation real world fact intelligent database incorporate human knowledge help machine imitate way human problem solve however due nature rapid iteration well incompleteness datum kg usually huge inevitably miss fact kgs link prediction knowledge graph task aim complete miss fact reasoning base exist knowledge two main stream research widely study one learn low dimensional embedding entity relation capture latent pattern gain good interpretability mining logical rule unfortunately previous study rarely pay attention heterogeneous kg paper propose degreembe model combine embed base learn logic rule mining infer kg specifically study problem predict miss link heterogeneous kg involve entity relation various type perspective degree node experimentally demonstrate degreembe model outperform state of the art method real world dataset meanwhile rule mine model high quality interpretability
On Anomaly Interpretation via Shapley Values,"  Anomaly localization is an essential problem as anomaly detection is. Because
a rigorous localization requires a causal model of a target system, practically
we often resort to a relaxed problem of anomaly interpretation, for which we
are to obtain meaningful attribution of anomaly scores to input features. In
this paper, we investigate the use of the Shapley value for anomaly
interpretation. We focus on the semi-supervised anomaly detection and newly
propose a characteristic function, on which the Shapley value is computed,
specifically for anomaly scores. The idea of the proposed method is
approximating the absence of some features by minimizing an anomaly score with
regard to them. We examine the performance of the proposed method as well as
other general approaches to computing the Shapley value in interpreting anomaly
scores. We show the results of experiments on multiple datasets and anomaly
detection methods, which indicate the usefulness of the Shapley-based anomaly
interpretation toward anomaly localization.
",anomaly localization essential problem anomaly detection rigorous localization require causal model target system practically often resort relaxed problem anomaly interpretation obtain meaningful attribution anomaly score input feature paper investigate use shapley value anomaly interpretation focus semi supervised anomaly detection newly propose characteristic function shapley value compute specifically anomaly score idea propose method approximating absence feature minimize anomaly score regard examine performance propose method well general approach compute shapley value interpret anomaly score show result experiment multiple dataset anomaly detection method indicate usefulness shapley base anomaly interpretation toward anomaly localization
Distributed k-Means with Outliers in General Metrics,"  Center-based clustering is a pivotal primitive for unsupervised learning and
data analysis. A popular variant is undoubtedly the k-means problem, which,
given a set $P$ of points from a metric space and a parameter $k<|P|$, requires
to determine a subset $S$ of $k$ centers minimizing the sum of all squared
distances of points in $P$ from their closest center. A more general
formulation, known as k-means with $z$ outliers, introduced to deal with noisy
datasets, features a further parameter $z$ and allows up to $z$ points of $P$
(outliers) to be disregarded when computing the aforementioned sum. We present
a distributed coreset-based 3-round approximation algorithm for k-means with
$z$ outliers for general metric spaces, using MapReduce as a computational
model. Our distributed algorithm requires sublinear local memory per reducer,
and yields a solution whose approximation ratio is an additive term $O(\gamma)$
away from the one achievable by the best known sequential (possibly bicriteria)
algorithm, where $\gamma$ can be made arbitrarily small. An important feature
of our algorithm is that it obliviously adapts to the intrinsic complexity of
the dataset, captured by the doubling dimension $D$ of the metric space. To the
best of our knowledge, no previous distributed approaches were able to attain
similar quality-performance tradeoffs for general metrics.
",center base clustering pivotal primitive unsupervised learn datum analysis popular variant undoubtedly k means problem give set p point metric space parameter k require determine subset k center minimize sum square distance point p close center general formulation know k means z outlier introduce deal noisy dataset feature parameter z allow z point p outlier disregard compute aforementione sum present distribute coreset base 3 round approximation algorithm k mean z outlier general metric space use mapreduce computational model distribute algorithm require sublinear local memory per reducer yield solution whose approximation ratio additive term away one achievable well know sequential possibly bicriteria algorithm make arbitrarily small important feature algorithm obliviously adapt intrinsic complexity dataset capture double dimension metric space good knowledge previous distribute approach able attain similar quality performance tradeoff general metric
Dance Hit Song Prediction,"  Record companies invest billions of dollars in new talent around the globe
each year. Gaining insight into what actually makes a hit song would provide
tremendous benefits for the music industry. In this research we tackle this
question by focussing on the dance hit song classification problem. A database
of dance hit songs from 1985 until 2013 is built, including basic musical
features, as well as more advanced features that capture a temporal aspect. A
number of different classifiers are used to build and test dance hit prediction
models. The resulting best model has a good performance when predicting whether
a song is a ""top 10"" dance hit versus a lower listed position.
",record company invest billion dollar new talent around globe year gain insight actually make hit song would provide tremendous benefit music industry research tackle question focusse dance hit song classification problem database dance hit song 1985 2013 build include basic musical feature well advanced feature capture temporal aspect number different classifier use build test dance hit prediction model result good model good performance predict whether song top 10 dance hit versus low list position
Bayesian Federated Learning via Predictive Distribution Distillation,"  For most existing federated learning algorithms, each round consists of
minimizing a loss function at each client to learn an optimal model at the
client, followed by aggregating these client models at the server. Point
estimation of the model parameters at the clients does not take into account
the uncertainty in the models estimated at each client. In many situations,
however, especially in limited data settings, it is beneficial to take into
account the uncertainty in the client models for more accurate and robust
predictions. Uncertainty also provides useful information for other important
tasks, such as active learning and out-of-distribution (OOD) detection. We
present a framework for Bayesian federated learning where each client infers
the posterior predictive distribution using its training data and present
various ways to aggregate these client-specific predictive distributions at the
server. Since communicating and aggregating predictive distributions can be
challenging and expensive, our approach is based on distilling each client's
predictive distribution into a single deep neural network. This enables us to
leverage advances in standard federated learning to Bayesian federated learning
as well. Unlike some recent works that have tried to estimate model uncertainty
of each client, our work also does not make any restrictive assumptions, such
as the form of the client's posterior distribution. We evaluate our approach on
classification in federated setting, as well as active learning and OOD
detection in federated settings, on which our approach outperforms various
existing federated learning baselines.
",exist federate learning algorithm round consist minimize loss function client learn optimal model client follow aggregate client model server point estimation model parameter client take account uncertainty model estimate client many situation however especially limited data setting beneficial take account uncertainty client model accurate robust prediction uncertainty also provide useful information important task active learn out of distribution ood detection present framework bayesian federate learn client infer posterior predictive distribution use training datum present various way aggregate client specific predictive distribution server since communicate aggregate predictive distribution challenge expensive approach base distil client predictive distribution single deep neural network enable we leverage advance standard federate learn bayesian federate learning well unlike recent work try estimate model uncertainty client work also make restrictive assumption form client posterior distribution evaluate approach classification federate setting well active learn ood detection federate setting approach outperform various exist federate learn baseline
A Retrieve-and-Edit Framework for Predicting Structured Outputs,"  For the task of generating complex outputs such as source code, editing
existing outputs can be easier than generating complex outputs from scratch.
With this motivation, we propose an approach that first retrieves a training
example based on the input (e.g., natural language description) and then edits
it to the desired output (e.g., code). Our contribution is a computationally
efficient method for learning a retrieval model that embeds the input in a
task-dependent way without relying on a hand-crafted metric or incurring the
expense of jointly training the retriever with the editor. Our
retrieve-and-edit framework can be applied on top of any base model. We show
that on a new autocomplete task for GitHub Python code and the Hearthstone
cards benchmark, retrieve-and-edit significantly boosts the performance of a
vanilla sequence-to-sequence model on both tasks.
",task generating complex output source code editing exist output easy generate complex output scratch motivation propose approach first retrieve train example base input natural language description edit desire output code contribution computationally efficient method learn retrieval model embed input task dependent way without rely hand craft metric incur expense jointly train retriever editor retrieve and edit framework apply top base model show new autocomplete task github python code hearthstone card benchmark retrieve and edit significantly boost performance vanilla sequence to sequence model task
Machine Learning for Mediation in Armed Conflicts,"  Today's conflicts are becoming increasingly complex, fluid and fragmented,
often involving a host of national and international actors with multiple and
often divergent interests. This development poses significant challenges for
conflict mediation, as mediators struggle to make sense of conflict dynamics,
such as the range of conflict parties and the evolution of their political
positions, the distinction between relevant and less relevant actors in peace
making, or the identification of key conflict issues and their interdependence.
International peace efforts appear increasingly ill-equipped to successfully
address these challenges. While technology is being increasingly used in a
range of conflict related fields, such as conflict predicting or information
gathering, less attention has been given to how technology can contribute to
conflict mediation. This case study is the first to apply state-of-the-art
machine learning technologies to data from an ongoing mediation process. Using
dialogue transcripts from peace negotiations in Yemen, this study shows how
machine-learning tools can effectively support international mediators by
managing knowledge and offering additional conflict analysis tools to assess
complex information. Apart from illustrating the potential of machine learning
tools in conflict mediation, the paper also emphasises the importance of
interdisciplinary and participatory research design for the development of
context-sensitive and targeted tools and to ensure meaningful and responsible
implementation.
",today conflict become increasingly complex fluid fragmented often involve host national international actor multiple often divergent interest development pose significant challenge conflict mediation mediator struggle make sense conflict dynamic range conflict party evolution political position distinction relevant less relevant actor peace make identification key conflict issue interdependence international peace effort appear increasingly ill equip successfully address challenge technology increasingly use range conflict relate field conflict predict information gathering less attention give technology contribute conflict mediation case study first apply state of the art machine learn technology datum ongoing mediation process use dialogue transcript peace negotiation yemen study show machine learn tool effectively support international mediator manage knowledge offer additional conflict analysis tool assess complex information apart illustrate potential machine learning tool conflict mediation paper also emphasise importance interdisciplinary participatory research design development context sensitive target tool ensure meaningful responsible implementation
"Learning to Collide: An Adaptive Safety-Critical Scenarios Generating
  Method","  Long-tail and rare event problems become crucial when autonomous driving
algorithms are applied in the real world. For the purpose of evaluating systems
in challenging settings, we propose a generative framework to create
safety-critical scenarios for evaluating specific task algorithms. We first
represent the traffic scenarios with a series of autoregressive building blocks
and generate diverse scenarios by sampling from the joint distribution of these
blocks. We then train the generative model as an agent (or a generator) to
investigate the risky distribution parameters for a given driving algorithm
being evaluated. We regard the task algorithm as an environment (or a
discriminator) that returns a reward to the agent when a risky scenario is
generated. Through the experiments conducted on several scenarios in the
simulation, we demonstrate that the proposed framework generates
safety-critical scenarios more efficiently than grid search or human design
methods. Another advantage of this method is its adaptiveness to the routes and
parameters.
",long tail rare event problem become crucial autonomous driving algorithm apply real world purpose evaluate system challenge setting propose generative framework create safety critical scenario evaluate specific task algorithm first represent traffic scenarios series autoregressive building block generate diverse scenario sample joint distribution block train generative model agent generator investigate risky distribution parameter give drive algorithm evaluate regard task algorithm environment discriminator return reward agent risky scenario generate experiment conduct several scenario simulation demonstrate propose framework generate safety critical scenario efficiently grid search human design method another advantage method adaptiveness route parameter
A bag-to-class divergence approach to multiple-instance learning,"  In multi-instance (MI) learning, each object (bag) consists of multiple
feature vectors (instances), and is most commonly regarded as a set of points
in a multidimensional space. A different viewpoint is that the instances are
realisations of random vectors with corresponding probability distribution, and
that a bag is the distribution, not the realisations. In MI classification,
each bag in the training set has a class label, but the instances are
unlabelled. By introducing the probability distribution space to bag-level
classification problems, dissimilarities between probability distributions
(divergences) can be applied. The bag-to-bag Kullback-Leibler information is
asymptotically the best classifier, but the typical sparseness of MI training
sets is an obstacle. We introduce bag-to-class divergence to MI learning,
emphasising the hierarchical nature of the random vectors that makes bags from
the same class different. We propose two properties for bag-to-class
divergences, and an additional property for sparse training sets.
",multi instance mi learn object bag consist multiple feature vector instance commonly regard set point multidimensional space different viewpoint instance realisation random vector correspond probability distribution bag distribution realisation mi classification bag training set class label instance unlabelle introduce probability distribution space bag level classification problem dissimilaritie probability distribution divergence apply bag to bag kullback leibler information asymptotically well classifier typical sparseness mi training set obstacle introduce bag to class divergence mi learning emphasise hierarchical nature random vector make bag class different propose two property bag to class divergence additional property sparse training set
"Efficient Estimation and Evaluation of Prediction Rules in
  Semi-Supervised Settings under Stratified Sampling","  In many contemporary applications, large amounts of unlabeled data are
readily available while labeled examples are limited. There has been
substantial interest in semi-supervised learning (SSL) which aims to leverage
unlabeled data to improve estimation or prediction. However, current SSL
literature focuses primarily on settings where labeled data is selected
randomly from the population of interest. Non-random sampling, while posing
additional analytical challenges, is highly applicable to many real world
problems. Moreover, no SSL methods currently exist for estimating the
prediction performance of a fitted model under non-random sampling. In this
paper, we propose a two-step SSL procedure for evaluating a prediction rule
derived from a working binary regression model based on the Brier score and
overall misclassification rate under stratified sampling. In step I, we impute
the missing labels via weighted regression with nonlinear basis functions to
account for nonrandom sampling and to improve efficiency. In step II, we
augment the initial imputations to ensure the consistency of the resulting
estimators regardless of the specification of the prediction model or the
imputation model. The final estimator is then obtained with the augmented
imputations. We provide asymptotic theory and numerical studies illustrating
that our proposals outperform their supervised counterparts in terms of
efficiency gain. Our methods are motivated by electronic health records (EHR)
research and validated with a real data analysis of an EHR-based study of
diabetic neuropathy.
",many contemporary application large amount unlabeled datum readily available label example limited substantial interest semi supervised learning ssl aim leverage unlabeled datum improve estimation prediction however current ssl literature focus primarily setting label datum select randomly population interest non random sampling pose additional analytical challenge highly applicable many real world problem moreover ssl method currently exist estimating prediction performance fit model non random sampling paper propose two step ssl procedure evaluate prediction rule derive work binary regression model base brier score overall misclassification rate stratify sampling step impute miss label via weighted regression nonlinear basis function account nonrandom sampling improve efficiency step ii augment initial imputation ensure consistency result estimator regardless specification prediction model imputation model final estimator obtain augment imputation provide asymptotic theory numerical study illustrate proposal outperform supervise counterpart terms efficiency gain method motivate electronic health record ehr research validate real datum analysis ehr base study diabetic neuropathy
"A Unified Benchmark for the Unknown Detection Capability of Deep Neural
  Networks","  Deep neural networks have achieved outstanding performance over various
tasks, but they have a critical issue: over-confident predictions even for
completely unknown samples. Many studies have been proposed to successfully
filter out these unknown samples, but they only considered narrow and specific
tasks, referred to as misclassification detection, open-set recognition, or
out-of-distribution detection. In this work, we argue that these tasks should
be treated as fundamentally an identical problem because an ideal model should
possess detection capability for all those tasks. Therefore, we introduce the
unknown detection task, an integration of previous individual tasks, for a
rigorous examination of the detection capability of deep neural networks on a
wide spectrum of unknown samples. To this end, unified benchmark datasets on
different scales were constructed and the unknown detection capabilities of
existing popular methods were subject to comparison. We found that Deep
Ensemble consistently outperforms the other approaches in detecting unknowns;
however, all methods are only successful for a specific type of unknown. The
reproducible code and benchmark datasets are available at
https://github.com/daintlab/unknown-detection-benchmarks .
",deep neural network achieve outstanding performance various task critical issue over confident prediction even completely unknown sample many study propose successfully filter unknown sample consider narrow specific task refer misclassification detection open set recognition out of distribution detection work argue task treat fundamentally identical problem ideal model possess detection capability task therefore introduce unknown detection task integration previous individual task rigorous examination detection capability deep neural network wide spectrum unknown sample end unified benchmark dataset different scale construct unknown detection capability exist popular method subject comparison find deep ensemble consistently outperform approach detect unknown however method successful specific type unknown reproducible code benchmark dataset available https
"Convolutional Feature Extraction and Neural Arithmetic Logic Units for
  Stock Prediction","  Stock prediction is a topic undergoing intense study for many years. Finance
experts and mathematicians have been working on a way to predict the future
stock price so as to decide to buy the stock or sell it to make profit. Stock
experts or economists, usually analyze on the previous stock values using
technical indicators, sentiment analysis etc to predict the future stock price.
In recent years, many researches have extensively used machine learning for
predicting the stock behaviour. In this paper we propose data driven deep
learning approach to predict the future stock value with the previous price
with the feature extraction property of convolutional neural network and to use
Neural Arithmetic Logic Units with it.
",stock prediction topic undergo intense study many year finance expert mathematician work way predict future stock price decide buy stock sell make profit stock expert economist usually analyze previous stock value use technical indicator sentiment analysis etc predict future stock price recent year many research extensively use machine learning predict stock behaviour paper propose datum drive deep learning approach predict future stock value previous price feature extraction property convolutional neural network use neural arithmetic logic unit
Convex Calibrated Surrogates for the Multi-Label F-Measure,"  The F-measure is a widely used performance measure for multi-label
classification, where multiple labels can be active in an instance
simultaneously (e.g. in image tagging, multiple tags can be active in any
image). In particular, the F-measure explicitly balances recall (fraction of
active labels predicted to be active) and precision (fraction of labels
predicted to be active that are actually so), both of which are important in
evaluating the overall performance of a multi-label classifier. As with most
discrete prediction problems, however, directly optimizing the F-measure is
computationally hard. In this paper, we explore the question of designing
convex surrogate losses that are calibrated for the F-measure -- specifically,
that have the property that minimizing the surrogate loss yields (in the limit
of sufficient data) a Bayes optimal multi-label classifier for the F-measure.
We show that the F-measure for an $s$-label problem, when viewed as a $2^s
\times 2^s$ loss matrix, has rank at most $s^2+1$, and apply a result of
Ramaswamy et al. (2014) to design a family of convex calibrated surrogates for
the F-measure. The resulting surrogate risk minimization algorithms can be
viewed as decomposing the multi-label F-measure learning problem into $s^2+1$
binary class probability estimation problems. We also provide a quantitative
regret transfer bound for our surrogates, which allows any regret guarantees
for the binary problems to be transferred to regret guarantees for the overall
F-measure problem, and discuss a connection with the algorithm of Dembczynski
et al. (2013). Our experiments confirm our theoretical findings.
",f measure widely use performance measure multi label classification multiple label active instance simultaneously image tag multiple tag active image particular f measure explicitly balance recall fraction active label predict active precision fraction label predict active actually important evaluate overall performance multi label classifier discrete prediction problem however directly optimize f measure computationally hard paper explore question design convex surrogate loss calibrate f measure specifically property minimize surrogate loss yield limit sufficient datum bayes optimal multi label classifier f measure show f measure -label problem view loss matrix rank apply result ramaswamy et al 2014 design family convex calibrate surrogate f measure result surrogate risk minimization algorithm view decompose multi label f measure learn problem binary class probability estimation problem also provide quantitative regret transfer bind surrogate allow regret guarantee binary problem transfer regret guarantee overall f measure problem discuss connection algorithm dembczynski et al 2013 experiment confirm theoretical finding
SRDTI: Deep learning-based super-resolution for diffusion tensor MRI,"  High-resolution diffusion tensor imaging (DTI) is beneficial for probing
tissue microstructure in fine neuroanatomical structures, but long scan times
and limited signal-to-noise ratio pose significant barriers to acquiring DTI at
sub-millimeter resolution. To address this challenge, we propose a deep
learning-based super-resolution method entitled ""SRDTI"" to synthesize
high-resolution diffusion-weighted images (DWIs) from low-resolution DWIs.
SRDTI employs a deep convolutional neural network (CNN), residual learning and
multi-contrast imaging, and generates high-quality results with rich textural
details and microstructural information, which are more similar to
high-resolution ground truth than those from trilinear and cubic spline
interpolation.
",high resolution diffusion tensor imaging dti beneficial probe tissue microstructure fine neuroanatomical structure long scan times limited signal to noise ratio pose significant barrier acquire dti sub millimeter resolution address challenge propose deep learning base super resolution method entitle srdti synthesize high resolution diffusion weight image dwis low resolution dwis srdti employ deep convolutional neural network cnn residual learning multi contrast imaging generate high quality result rich textural details microstructural information similar high resolution ground truth trilinear cubic spline interpolation
"Bayesian Hierarchical Clustering with Exponential Family: Small-Variance
  Asymptotics and Reducibility","  Bayesian hierarchical clustering (BHC) is an agglomerative clustering method,
where a probabilistic model is defined and its marginal likelihoods are
evaluated to decide which clusters to merge. While BHC provides a few
advantages over traditional distance-based agglomerative clustering algorithms,
successive evaluation of marginal likelihoods and careful hyperparameter tuning
are cumbersome and limit the scalability. In this paper we relax BHC into a
non-probabilistic formulation, exploring small-variance asymptotics in
conjugate-exponential models. We develop a novel clustering algorithm, referred
to as relaxed BHC (RBHC), from the asymptotic limit of the BHC model that
exhibits the scalability of distance-based agglomerative clustering algorithms
as well as the flexibility of Bayesian nonparametric models. We also
investigate the reducibility of the dissimilarity measure emerged from the
asymptotic limit of the BHC model, allowing us to use scalable algorithms such
as the nearest neighbor chain algorithm. Numerical experiments on both
synthetic and real-world datasets demonstrate the validity and high performance
of our method.
",bayesian hierarchical clustering bhc agglomerative cluster method probabilistic model define marginal likelihood evaluate decide cluster merge bhc provide advantage traditional distance base agglomerative cluster algorithm successive evaluation marginal likelihood careful hyperparameter tune cumbersome limit scalability paper relax bhc non probabilistic formulation explore small variance asymptotic conjugate exponential model develop novel cluster algorithm refer relaxed bhc rbhc asymptotic limit bhc model exhibit scalability distance base agglomerative cluster algorithm well flexibility bayesian nonparametric model also investigate reducibility dissimilarity measure emerge asymptotic limit bhc model allow we use scalable algorithm near neighbor chain algorithm numerical experiment synthetic real world dataset demonstrate validity high performance method
Distributionally Robust Bayesian Quadrature Optimization,"  Bayesian quadrature optimization (BQO) maximizes the expectation of an
expensive black-box integrand taken over a known probability distribution. In
this work, we study BQO under distributional uncertainty in which the
underlying probability distribution is unknown except for a limited set of its
i.i.d. samples. A standard BQO approach maximizes the Monte Carlo estimate of
the true expected objective given the fixed sample set. Though Monte Carlo
estimate is unbiased, it has high variance given a small set of samples; thus
can result in a spurious objective function. We adopt the distributionally
robust optimization perspective to this problem by maximizing the expected
objective under the most adversarial distribution. In particular, we propose a
novel posterior sampling based algorithm, namely distributionally robust BQO
(DRBQO) for this purpose. We demonstrate the empirical effectiveness of our
proposed framework in synthetic and real-world problems, and characterize its
theoretical convergence via Bayesian regret.
",bayesian quadrature optimization bqo maximizes expectation expensive black box integrand take know probability distribution work study bqo distributional uncertainty underlie probability distribution unknown except limited set sample standard bqo approach maximize monte carlo estimate true expect objective give fix sample set though monte carlo estimate unbiased high variance give small set sample thus result spurious objective function adopt distributionally robust optimization perspective problem maximize expect objective adversarial distribution particular propose novel posterior sampling base algorithm namely distributionally robust bqo drbqo purpose demonstrate empirical effectiveness propose framework synthetic real world problem characterize theoretical convergence via bayesian regret
A Survey on Knowledge Graph-Based Recommender Systems,"  To solve the information explosion problem and enhance user experience in
various online applications, recommender systems have been developed to model
users preferences. Although numerous efforts have been made toward more
personalized recommendations, recommender systems still suffer from several
challenges, such as data sparsity and cold start. In recent years, generating
recommendations with the knowledge graph as side information has attracted
considerable interest. Such an approach can not only alleviate the
abovementioned issues for a more accurate recommendation, but also provide
explanations for recommended items. In this paper, we conduct a systematical
survey of knowledge graph-based recommender systems. We collect recently
published papers in this field and summarize them from two perspectives. On the
one hand, we investigate the proposed algorithms by focusing on how the papers
utilize the knowledge graph for accurate and explainable recommendation. On the
other hand, we introduce datasets used in these works. Finally, we propose
several potential research directions in this field.
",solve information explosion problem enhance user experience various online application recommender system develop model user preference although numerous effort make toward personalize recommendation recommender system still suffer several challenge datum sparsity cold start recent year generate recommendation knowledge graph side information attract considerable interest approach alleviate abovementione issue accurate recommendation also provide explanation recommend item paper conduct systematical survey knowledge graph base recommender system collect recently publish paper field summarize two perspective one hand investigate propose algorithm focus paper utilize knowledge graph accurate explainable recommendation hand introduce dataset use work finally propose several potential research direction field
Scalable Neural Architecture Search for 3D Medical Image Segmentation,"  In this paper, a neural architecture search (NAS) framework is proposed for
3D medical image segmentation, to automatically optimize a neural architecture
from a large design space. Our NAS framework searches the structure of each
layer including neural connectivities and operation types in both of the
encoder and decoder. Since optimizing over a large discrete architecture space
is difficult due to high-resolution 3D medical images, a novel stochastic
sampling algorithm based on a continuous relaxation is also proposed for
scalable gradient based optimization. On the 3D medical image segmentation
tasks with a benchmark dataset, an automatically designed architecture by the
proposed NAS framework outperforms the human-designed 3D U-Net, and moreover
this optimized architecture is well suited to be transferred for different
tasks.
",paper neural architecture search nas framework propose 3d medical image segmentation automatically optimize neural architecture large design space nas framework search structure layer include neural connectivity operation type encoder decoder since optimize large discrete architecture space difficult due high resolution 3d medical image novel stochastic sampling algorithm base continuous relaxation also propose scalable gradient base optimization 3d medical image segmentation task benchmark dataset automatically design architecture propose nas framework outperform human design 3d u net moreover optimize architecture well suit transfer different task
"RCC-Dual-GAN: An Efficient Approach for Outlier Detection with Few
  Identified Anomalies","  Outlier detection is an important task in data mining and many technologies
have been explored in various applications. However, due to the default
assumption that outliers are non-concentrated, unsupervised outlier detection
may not correctly detect group anomalies with higher density levels. As for the
supervised outlier detection, although high detection rates and optimal
parameters can usually be achieved, obtaining sufficient and correct labels is
a time-consuming task. To address these issues, we focus on semi-supervised
outlier detection with few identified anomalies, in the hope of using limited
labels to achieve high detection accuracy. First, we propose a novel detection
model Dual-GAN, which can directly utilize the potential information in
identified anomalies to detect discrete outliers and partially identified group
anomalies simultaneously. And then, considering the instances with similar
output values may not all be similar in a complex data structure, we replace
the two MO-GAN components in Dual-GAN with the combination of RCC and M-GAN
(RCC-Dual-GAN). In addition, to deal with the evaluation of Nash equilibrium
and the selection of optimal model, two evaluation indicators are created and
introduced into the two models to make the detection process more intelligent.
Extensive experiments on both benchmark datasets and two practical tasks
demonstrate that our proposed approaches (i.e., Dual-GAN and RCC-Dual-GAN) can
significantly improve the accuracy of outlier detection even with only a few
identified anomalies. Moreover, compared with the two MO-GAN components in
Dual-GAN, the network structure combining RCC and M-GAN has greater stability
in various situations.
",outlier detection important task datum mining many technology explore various application however due default assumption outlier non concentrated unsupervised outlier detection may correctly detect group anomaly high density level supervise outli detection although high detection rate optimal parameter usually achieve obtain sufficient correct label time consume task address issue focus semi supervised outlier detection identify anomaly hope use limited label achieve high detection accuracy first propose novel detection model dual gan directly utilize potential information identify anomaly detect discrete outlier partially identify group anomaly simultaneously consider instance similar output value may similar complex datum structure replace two mo gan component dual gan combination rcc m gan rcc dual gan addition deal evaluation nash equilibrium selection optimal model two evaluation indicator create introduce two model make detection process intelligent extensive experiment benchmark dataset two practical task demonstrate propose approach dual gin rcc dual gan significantly improve accuracy outlier detection even identify anomaly moreover compare two mo gan component dual gan network structure combine rcc m gan great stability various situation
Deep Learning Macroeconomics,"  Limited datasets and complex nonlinear relationships are among the challenges
that may emerge when applying econometrics to macroeconomic problems. This
research proposes deep learning as an approach to transfer learning in the
former case and to map relationships between variables in the latter case.
Although macroeconomists already apply transfer learning when assuming a given
a priori distribution in a Bayesian context, estimating a structural VAR with
signal restriction and calibrating parameters based on results observed in
other models, to name a few examples, advance in a more systematic transfer
learning strategy in applied macroeconomics is the innovation we are
introducing. We explore the proposed strategy empirically, showing that data
from different but related domains, a type of transfer learning, helps identify
the business cycle phases when there is no business cycle dating committee and
to quick estimate a economic-based output gap. Next, since deep learning
methods are a way of learning representations, those that are formed by the
composition of multiple non-linear transformations, to yield more abstract
representations, we apply deep learning for mapping low-frequency from
high-frequency variables. The results obtained show the suitability of deep
learning models applied to macroeconomic problems. First, models learned to
classify United States business cycles correctly. Then, applying transfer
learning, they were able to identify the business cycles of out-of-sample
Brazilian and European data. Along the same lines, the models learned to
estimate the output gap based on the U.S. data and obtained good performance
when faced with Brazilian data. Additionally, deep learning proved adequate for
mapping low-frequency variables from high-frequency data to interpolate,
distribute, and extrapolate time series by related series.
",limited dataset complex nonlinear relationship among challenge may emerge apply econometrics macroeconomic problem research propose deep learning approach transfer learn former case map relationship variable latter case although macroeconomist already apply transfer learning assume give priori distribution bayesian context estimate structural var signal restriction calibrate parameter base result observe model name example advance systematic transfer learning strategy apply macroeconomic innovation introduce explore propose strategy empirically show datum different relate domain type transfer learning help identify business cycle phase business cycle date committee quick estimate economic base output gap next since deep learning method way learn representation form composition multiple non linear transformation yield abstract representation apply deep learning mapping low frequency high frequency variable result obtain show suitability deep learning model apply macroeconomic problem first model learn classify united states business cycle correctly apply transfer learn able identify business cycle out of sample brazilian european datum along line model learn estimate output gap base datum obtain good performance face brazilian datum additionally deep learning prove adequate mapping low frequency variable high frequency datum interpolate distribute extrapolate time series relate series
"MMA Training: Direct Input Space Margin Maximization through Adversarial
  Training","  We study adversarial robustness of neural networks from a margin maximization
perspective, where margins are defined as the distances from inputs to a
classifier's decision boundary. Our study shows that maximizing margins can be
achieved by minimizing the adversarial loss on the decision boundary at the
""shortest successful perturbation"", demonstrating a close connection between
adversarial losses and the margins. We propose Max-Margin Adversarial (MMA)
training to directly maximize the margins to achieve adversarial robustness.
Instead of adversarial training with a fixed $\epsilon$, MMA offers an
improvement by enabling adaptive selection of the ""correct"" $\epsilon$ as the
margin individually for each datapoint. In addition, we rigorously analyze
adversarial training with the perspective of margin maximization, and provide
an alternative interpretation for adversarial training, maximizing either a
lower or an upper bound of the margins. Our experiments empirically confirm our
theory and demonstrate MMA training's efficacy on the MNIST and CIFAR10
datasets w.r.t. $\ell_\infty$ and $\ell_2$ robustness. Code and models are
available at https://github.com/BorealisAI/mma_training.
",study adversarial robustness neural network margin maximization perspective margin define distance inputs classifi decision boundary study show maximize margin achieve minimize adversarial loss decision boundary shortest successful perturbation demonstrate close connection adversarial loss margin propose max margin adversarial mma training directly maximize margin achieve adversarial robustness instead adversarial training fix mma offer improvement enable adaptive selection correct margin individually datapoint addition rigorously analyze adversarial training perspective margin maximization provide alternative interpretation adversarial training maximize either lower upper bind margin experiment empirically confirm theory demonstrate mma training efficacy mnist cifar10 dataset robustness code model available https
"Enhanced Audit Techniques Empowered by the Reinforcement Learning
  Pertaining to IFRS 16 Lease","  The purpose of accounting audit is to have clear understanding on the
financial activities of a company, which can be enhanced by machine learning or
reinforcement learning as numeric analysis better than manual analysis can be
made. For the purpose of assessment on the relevance, completeness and accuracy
of the information produced by entity pertaining to the newly implemented
International Financial Reporting Standard 16 Lease (IFRS 16) is one of such
candidates as its characteristic of requiring the understanding on the nature
of contracts and its complete analysis from listing up without omission, which
can be enhanced by the digitalization of contracts for the purpose of creating
the lists, still leaving the need of auditing cash flows of companies for the
possible omission due to the potential error at the stage of data collection,
especially for entities with various short or middle term business sites and
related leases, such as construction entities.
  The implementation of the reinforcement learning and its well-known code is
to be made for the purpose of drawing the possibility and utilizability of
interpreters from domain knowledge to numerical system, also can be called
'gamification interpreter' or 'numericalization interpreter' which can be
referred or compared to the extrapolation with nondimensional numbers, such as
Froude Number, in physics, which was a source of inspiration at this study.
Studies on the interpreters can be able to empower the utilizability of
artificial general intelligence in domain and commercial area.
",purpose accounting audit clear understand financial activity company enhance machine learn reinforcement learning numeric analysis well manual analysis make purpose assessment relevance completeness accuracy information produce entity pertain newly implement international financial reporting standard 16 lease ifrs 16 one candidate characteristic require understand nature contract complete analysis list without omission enhance digitalization contract purpose creating list still leave need auditing cash flow company possible omission due potential error stage datum collection especially entity various short middle term business site relate lease construction entity implementation reinforcement learn well know code make purpose drawing possibility utilizability interpreter domain knowledge numerical system also call interpreter interpreter refer compare extrapolation nondimensional number froude number physics source inspiration study study interpreter able empower utilizability artificial general intelligence domain commercial area
Anomaly Detection With Partitioning Overfitting Autoencoder Ensembles,"  In this paper, we propose POTATOES (Partitioning OverfiTting AuTOencoder
EnSemble), a new method for unsupervised outlier detection (UOD). More
precisely, given any autoencoder for UOD, this technique can be used to improve
its accuracy while at the same time removing the burden of tuning its
regularization. The idea is to not regularize at all, but to rather randomly
partition the data into sufficiently many equally sized parts, overfit each
part with its own autoencoder, and to use the maximum over all autoencoder
reconstruction errors as the anomaly score. We apply our model to various
realistic datasets and show that if the set of inliers is dense enough, our
method indeed improves the UOD performance of a given autoencoder
significantly. For reproducibility, the code is made available on github so the
reader can recreate the results in this paper as well as apply the method to
other autoencoders and datasets.
",paper propose potato partition overfitte autoencoder ensemble new method unsupervised outlier detection uod precisely give autoencoder uod technique use improve accuracy time remove burden tune regularization idea regularize rather randomly partition datum sufficiently many equally sized part overfit part autoencoder use maximum autoencoder reconstruction error anomaly score apply model various realistic dataset show set inlier dense enough method indeed improve uod performance give autoencoder significantly reproducibility code make available github reader recreate result paper well apply method autoencoder dataset
Outlier Robust Online Learning,"  We consider the problem of learning from noisy data in practical settings
where the size of data is too large to store on a single machine. More
challenging, the data coming from the wild may contain malicious outliers. To
address the scalability and robustness issues, we present an online robust
learning (ORL) approach. ORL is simple to implement and has provable robustness
guarantee -- in stark contrast to existing online learning approaches that are
generally fragile to outliers. We specialize the ORL approach for two concrete
cases: online robust principal component analysis and online linear regression.
We demonstrate the efficiency and robustness advantages of ORL through
comprehensive simulations and predicting image tags on a large-scale data set.
We also discuss extension of the ORL to distributed learning and provide
experimental evaluations.
",consider problem learn noisy datum practical setting size datum large store single machine challenge datum come wild may contain malicious outlier address scalability robustness issue present online robust learning orl approach orl simple implement provable robustness guarantee stark contrast exist online learning approach generally fragile outlier specialize orl approach two concrete case online robust principal component analysis online linear regression demonstrate efficiency robustness advantage orl comprehensive simulation predict image tag large scale datum set also discuss extension orl distribute learning provide experimental evaluation
Learning Colour Representations of Search Queries,"  Image search engines rely on appropriately designed ranking features that
capture various aspects of the content semantics as well as the historic
popularity. In this work, we consider the role of colour in this relevance
matching process. Our work is motivated by the observation that a significant
fraction of user queries have an inherent colour associated with them. While
some queries contain explicit colour mentions (such as 'black car' and 'yellow
daisies'), other queries have implicit notions of colour (such as 'sky' and
'grass'). Furthermore, grounding queries in colour is not a mapping to a single
colour, but a distribution in colour space. For instance, a search for 'trees'
tends to have a bimodal distribution around the colours green and brown. We
leverage historical clickthrough data to produce a colour representation for
search queries and propose a recurrent neural network architecture to encode
unseen queries into colour space. We also show how this embedding can be learnt
alongside a cross-modal relevance ranker from impression logs where a subset of
the result images were clicked. We demonstrate that the use of a query-image
colour distance feature leads to an improvement in the ranker performance as
measured by users' preferences of clicked versus skipped images.
",image search engine rely appropriately design rank feature capture various aspect content semantic well historic popularity work consider role colour relevance matching process work motivate observation significant fraction user query inherent colour associate query contain explicit colour mention car daisy query implicit notion colour furthermore ground query colour mapping single colour distribution colour space instance search tend bimodal distribution around colour green brown leverage historical clickthrough datum produce colour representation search query propose recurrent neural network architecture encode unseen query colour space also show embed learn alongside cross modal relevance ranker impression log subset result image click demonstrate use query image colour distance feature lead improvement ranker performance measure user preference click versus skip image
Regularization Shortcomings for Continual Learning,"  In most machine learning algorithms, training data is assumed to be
independent and identically distributed (iid). When it is not the case, the
algorithm's performances are challenged, leading to the famous phenomenon of
catastrophic forgetting. Algorithms dealing with it are gathered in the
Continual Learning research field. In this paper, we study the regularization
based approaches to continual learning and show that those approaches can not
learn to discriminate classes from different tasks in an elemental continual
benchmark: the class-incremental scenario. We make theoretical reasoning to
prove this shortcoming and illustrate it with examples and experiments.
Moreover, we show that it can have some important consequences on continual
multi-tasks reinforcement learning or in pre-trained models used for continual
learning. We believe that highlighting and understanding the shortcomings of
regularization strategies will help us to use them more efficiently.
",machine learning algorithm training datum assume independent identically distribute iid case algorithm performance challenge lead famous phenomenon catastrophic forgetting algorithm deal gather continual learning research field paper study regularization base approach continual learning show approach learn discriminate class different task elemental continual benchmark class incremental scenario make theoretical reasoning prove shortcome illustrate example experiment moreover show important consequence continual multi tasks reinforcement learn pre train model use continual learning believe highlight understand shortcoming regularization strategy help we use efficiently
"One-class Autoencoder Approach for Optimal Electrode Set-up
  Identification in Wearable EEG Event Monitoring","  A limiting factor towards the wide routine use of wearables devices for
continuous healthcare monitoring is their cumbersome and obtrusive nature. This
is particularly true for electroencephalography (EEG) recordings, which require
the placement of multiple electrodes in contact with the scalp. In this work,
we propose to identify the optimal wearable EEG electrode set-up, in terms of
minimal number of electrodes, comfortable location and performance, for
EEG-based event detection and monitoring. By relying on the demonstrated power
of autoencoder (AE) networks to learn latent representations from
high-dimensional data, our proposed strategy trains an AE architecture in a
one-class classification setup with different electrode set-ups as input data.
The resulting models are assessed using the F-score and the best set-up is
chosen according to the established optimal criteria. Using alpha wave
detection as use case, we demonstrate that the proposed method allows to detect
an alpha state from an optimal set-up consisting of electrodes in the forehead
and behind the ear, with an average F-score of 0.78. Our results suggest that a
learning-based approach can be used to enable the design and implementation of
optimized wearable devices for real-life healthcare monitoring.
",limit factor towards wide routine use wearable device continuous healthcare monitor cumbersome obtrusive nature particularly true electroencephalography eeg recordings require placement multiple electrode contact scalp work propose identify optimal wearable eeg electrode set up term minimal number electrode comfortable location performance eeg base event detection monitoring rely demonstrate power autoencoder ae network learn latent representation high dimensional datum propose strategy train ae architecture one class classification setup different electrode set up input datum result model assess use f score good set up choose accord establish optimal criterion use alpha wave detection use case demonstrate propose method allow detect alpha state optimal set up consist electrode forehead behind ear average f score result suggest learning base approach use enable design implementation optimize wearable device real life healthcare monitoring
"BERTgrid: Contextualized Embedding for 2D Document Representation and
  Understanding","  For understanding generic documents, information like font sizes, column
layout, and generally the positioning of words may carry semantic information
that is crucial for solving a downstream document intelligence task. Our novel
BERTgrid, which is based on Chargrid by Katti et al. (2018), represents a
document as a grid of contextualized word piece embedding vectors, thereby
making its spatial structure and semantics accessible to the processing neural
network. The contextualized embedding vectors are retrieved from a BERT
language model. We use BERTgrid in combination with a fully convolutional
network on a semantic instance segmentation task for extracting fields from
invoices. We demonstrate its performance on tabulated line item and document
header field extraction.
",understand generic document information like font size column layout generally position word may carry semantic information crucial solve downstream document intelligence task novel bertgrid base chargrid katti et al 2018 represent document grid contextualize word piece embed vector thereby make spatial structure semantic accessible processing neural network contextualize embed vector retrieve bert language model use bertgrid combination fully convolutional network semantic instance segmentation task extract field invoice demonstrate performance tabulate line item document header field extraction
A Geometric View of Conjugate Priors,"  In Bayesian machine learning, conjugate priors are popular, mostly due to
mathematical convenience. In this paper, we show that there are deeper reasons
for choosing a conjugate prior. Specifically, we formulate the conjugate prior
in the form of Bregman divergence and show that it is the inherent geometry of
conjugate priors that makes them appropriate and intuitive. This geometric
interpretation allows one to view the hyperparameters of conjugate priors as
the {\it effective} sample points, thus providing additional intuition. We use
this geometric understanding of conjugate priors to derive the hyperparameters
and expression of the prior used to couple the generative and discriminative
components of a hybrid model for semi-supervised learning.
",bayesian machine learn conjugate prior popular mostly due mathematical convenience paper show deep reason choose conjugate prior specifically formulate conjugate prior form bregman divergence show inherent geometry conjugate prior make appropriate intuitive geometric interpretation allow one view hyperparameter conjugate prior effective sample point thus provide additional intuition use geometric understanding conjugate prior derive hyperparameter expression prior use couple generative discriminative component hybrid model semi supervised learning
"FinRL: A Deep Reinforcement Learning Library for Automated Stock Trading
  in Quantitative Finance","  As deep reinforcement learning (DRL) has been recognized as an effective
approach in quantitative finance, getting hands-on experiences is attractive to
beginners. However, to train a practical DRL trading agent that decides where
to trade, at what price, and what quantity involves error-prone and arduous
development and debugging. In this paper, we introduce a DRL library FinRL that
facilitates beginners to expose themselves to quantitative finance and to
develop their own stock trading strategies. Along with easily-reproducible
tutorials, FinRL library allows users to streamline their own developments and
to compare with existing schemes easily. Within FinRL, virtual environments are
configured with stock market datasets, trading agents are trained with neural
networks, and extensive backtesting is analyzed via trading performance.
Moreover, it incorporates important trading constraints such as transaction
cost, market liquidity and the investor's degree of risk-aversion. FinRL is
featured with completeness, hands-on tutorial and reproducibility that favors
beginners: (i) at multiple levels of time granularity, FinRL simulates trading
environments across various stock markets, including NASDAQ-100, DJIA, S&P 500,
HSI, SSE 50, and CSI 300; (ii) organized in a layered architecture with modular
structure, FinRL provides fine-tuned state-of-the-art DRL algorithms (DQN,
DDPG, PPO, SAC, A2C, TD3, etc.), commonly-used reward functions and standard
evaluation baselines to alleviate the debugging workloads and promote the
reproducibility, and (iii) being highly extendable, FinRL reserves a complete
set of user-import interfaces. Furthermore, we incorporated three application
demonstrations, namely single stock trading, multiple stock trading, and
portfolio allocation. The FinRL library will be available on Github at link
https://github.com/AI4Finance-LLC/FinRL-Library.
",deep reinforcement learning drl recognize effective approach quantitative finance get hand on experience attractive beginner however train practical drl trading agent decide trade price quantity involve error prone arduous development debug paper introduce drl library finrl facilitate beginner expose quantitative finance develop stock trading strategy along easily reproducible tutorial finrl library allow user streamline development compare exist scheme easily within finrl virtual environment configure stock market dataset trading agent train neural network extensive backtesting analyze via trading performance moreover incorporate important trading constraint transaction cost market liquidity investor degree risk aversion finrl feature completeness hand on tutorial reproducibility favor beginner multiple level time granularity finrl simulate trading environment across various stock market include nasdaq-100 djia p 500 hsi sse 50 csi 300 ii organize layered architecture modular structure finrl provide fine tuned state of the art drl algorithm dqn ddpg ppo sac a2c td3 etc commonly use reward function standard evaluation baseline alleviate debug workload promote reproducibility iii highly extendable finrl reserve complete set user import interface furthermore incorporate three application demonstration namely single stock trading multiple stock trading portfolio allocation finrl library available github link https
On a class of geodesically convex optimization problems solved via Euclidean MM methods,"We study geodesically convex (g-convex) problems that can be written as a
difference of Euclidean convex functions. This structure arises in several
optimization problems in statistics and machine learning, e.g., for matrix
scaling, M-estimators for covariances, and Brascamp-Lieb inequalities. Our work
offers efficient algorithms that on the one hand exploit g-convexity to ensure
global optimality along with guarantees on iteration complexity. On the other
hand, the split structure permits us to develop Euclidean
Majorization-Minorization algorithms that help us bypass the need to compute
expensive Riemannian operations such as exponential maps and parallel
transport. We illustrate our results by specializing them to a few concrete
optimization problems that have been previously studied in the machine learning
literature. Ultimately, we hope our work helps motivate the broader search for
mixed Euclidean-Riemannian optimization algorithms.",study geodesically convex g convex problem write difference euclidean convex function structure arise several optimization problem statistic machine learn matrix scale m estimator covariance brascamp lieb inequality work offer efficient algorithm one hand exploit g convexity ensure global optimality along guarantee iteration complexity hand split structure permit we develop euclidean majorization minorization algorithm help we bypass need compute expensive riemannian operation exponential map parallel transport illustrate result specialize concrete optimization problem previously study machine learn literature ultimately hope work help motivate broad search mixed euclidean riemannian optimization algorithm
Towards Large-Scale Exploratory Search over Heterogeneous Sources,"  Since time immemorial, people have been looking for ways to organize
scientific knowledge into some systems to facilitate search and discovery of
new ideas. The problem was partially solved in the pre-Internet era using
library classifications, but nowadays it is nearly impossible to classify all
scientific and popular scientific knowledge manually. There is a clear gap
between the diversity and the amount of data available on the Internet and the
algorithms for automatic structuring of such data. In our preliminary study, we
approach the problem of knowledge discovery on web-scale data with diverse text
sources and propose an algorithm to aggregate multiple collections into a
single hierarchical topic model. We implement a web service named Rysearch to
demonstrate the concept of topical exploratory search and make it available
online.
",since time immemorial people look way organize scientific knowledge system facilitate search discovery new idea problem partially solved pre internet era use library classification nowadays nearly impossible classify scientific popular scientific knowledge manually clear gap diversity amount datum available internet algorithm automatic structure datum preliminary study approach problem knowledge discovery web scale datum diverse text source propose algorithm aggregate multiple collection single hierarchical topic model implement web service name rysearch demonstrate concept topical exploratory search make available online
"Counterfactual State Explanations for Reinforcement Learning Agents via
  Generative Deep Learning","  Counterfactual explanations, which deal with ""why not?"" scenarios, can
provide insightful explanations to an AI agent's behavior. In this work, we
focus on generating counterfactual explanations for deep reinforcement learning
(RL) agents which operate in visual input environments like Atari. We introduce
counterfactual state explanations, a novel example-based approach to
counterfactual explanations based on generative deep learning. Specifically, a
counterfactual state illustrates what minimal change is needed to an Atari game
image such that the agent chooses a different action. We also evaluate the
effectiveness of counterfactual states on human participants who are not
machine learning experts. Our first user study investigates if humans can
discern if the counterfactual state explanations are produced by the actual
game or produced by a generative deep learning approach. Our second user study
investigates if counterfactual state explanations can help non-expert
participants identify a flawed agent; we compare against a baseline approach
based on a nearest neighbor explanation which uses images from the actual game.
Our results indicate that counterfactual state explanations have sufficient
fidelity to the actual game images to enable non-experts to more effectively
identify a flawed RL agent compared to the nearest neighbor baseline and to
having no explanation at all.
",counterfactual explanation deal scenario provide insightful explanation ai agent behavior work focus generate counterfactual explanation deep reinforcement learn rl agent operate visual input environment like atari introduce counterfactual state explanation novel example base approach counterfactual explanation base generative deep learning specifically counterfactual state illustrate minimal change need atari game image agent choose different action also evaluate effectiveness counterfactual state human participant machine learn expert first user study investigate human discern counterfactual state explanation produce actual game produce generative deep learning approach second user study investigate counterfactual state explanation help non expert participant identify flawed agent compare baseline approach base near neighbor explanation use image actual game result indicate counterfactual state explanation sufficient fidelity actual game image enable non expert effectively identify flawed rl agent compare near neighbor baseline explanation
Visual Time Series Forecasting: An Image-driven Approach,"  In this work, we address time-series forecasting as a computer vision task.
We capture input data as an image and train a model to produce the subsequent
image. This approach results in predicting distributions as opposed to
pointwise values. To assess the robustness and quality of our approach, we
examine various datasets and multiple evaluation metrics. Our experiments show
that our forecasting tool is effective for cyclic data but somewhat less for
irregular data such as stock prices. Importantly, when using image-based
evaluation metrics, we find our method to outperform various baselines,
including ARIMA, and a numerical variation of our deep learning approach.
",work address time series forecasting computer vision task capture input datum image train model produce subsequent image approach result predict distribution oppose pointwise value assess robustness quality approach examine various dataset multiple evaluation metric experiment show forecasting tool effective cyclic datum somewhat less irregular datum stock price importantly use image base evaluation metric find method outperform various baseline include arima numerical variation deep learning approach
Intelligent Warehouse Allocator for Optimal Regional Utilization,"  In this paper, we describe a novel solution to compute optimal warehouse
allocations for fashion inventory. Procured inventory must be optimally
allocated to warehouses in proportion to the regional demand around the
warehouse. This will ensure that demand is fulfilled by the nearest warehouse
thereby minimizing the delivery logistics cost and delivery times. These are
key metrics to drive profitability and customer experience respectively.
Warehouses have capacity constraints and allocations must minimize inter
warehouse redistribution cost of the inventory. This leads to maximum Regional
Utilization (RU). We use machine learning and optimization methods to build an
efficient solution to this warehouse allocation problem. We use machine
learning models to estimate the geographical split of the demand for every
product. We use Integer Programming methods to compute the optimal feasible
warehouse allocations considering the capacity constraints. We conduct a
back-testing by using this solution and validate the efficiency of this model
by demonstrating a significant uptick in two key metrics Regional Utilization
(RU) and Percentage Two-day-delivery (2DD). We use this process to
intelligently create purchase orders with warehouse assignments for Myntra, a
leading online fashion retailer.
",paper describe novel solution compute optimal warehouse allocation fashion inventory procure inventory must optimally allocate warehouse proportion regional demand around warehouse ensure demand fulfil near warehouse thereby minimize delivery logistic cost delivery time key metric drive profitability customer experience respectively warehouse capacity constraint allocation must minimize inter warehouse redistribution cost inventory lead maximum regional utilization ru use machine learn optimization method build efficient solution warehouse allocation problem use machine learning model estimate geographical split demand every product use integer programming method compute optimal feasible warehouse allocation consider capacity constraint conduct back testing use solution validate efficiency model demonstrate significant uptick two key metric regional utilization ru percentage two day delivery 2dd use process intelligently create purchase order warehouse assignments myntra lead online fashion retailer
Enriching Consumer Health Vocabulary Using Enhanced GloVe Word Embedding,"  Open-Access and Collaborative Consumer Health Vocabulary (OAC CHV, or CHV for
short), is a collection of medical terms written in plain English. It provides
a list of simple, easy, and clear terms that laymen prefer to use rather than
an equivalent professional medical term. The National Library of Medicine (NLM)
has integrated and mapped the CHV terms to their Unified Medical Language
System (UMLS). These CHV terms mapped to 56000 professional concepts on the
UMLS. We found that about 48% of these laymen's terms are still jargon and
matched with the professional terms on the UMLS. In this paper, we present an
enhanced word embedding technique that generates new CHV terms from a
consumer-generated text. We downloaded our corpus from a healthcare social
media and evaluated our new method based on iterative feedback to word
embedding using ground truth built from the existing CHV terms. Our feedback
algorithm outperformed unmodified GLoVe and new CHV terms have been detected.
",open access collaborative consumer health vocabulary oac chv chv short collection medical term write plain english provide list simple easy clear term layman prefer use rather equivalent professional medical term national library medicine nlm integrate map chv term unify medical language system umls chv term map 56000 professional concept umls find 48 layman term still jargon match professional term umls paper present enhance word embed technique generate new chv term consumer generate text download corpus healthcare social medium evaluate new method base iterative feedback word embed use ground truth build exist chv term feedback algorithm outperform unmodified glove new chv term detect
Measuring the Reliability of Reinforcement Learning Algorithms,"  Lack of reliability is a well-known issue for reinforcement learning (RL)
algorithms. This problem has gained increasing attention in recent years, and
efforts to improve it have grown substantially. To aid RL researchers and
production users with the evaluation and improvement of reliability, we propose
a set of metrics that quantitatively measure different aspects of reliability.
In this work, we focus on variability and risk, both during training and after
learning (on a fixed policy). We designed these metrics to be general-purpose,
and we also designed complementary statistical tests to enable rigorous
comparisons on these metrics. In this paper, we first describe the desired
properties of the metrics and their design, the aspects of reliability that
they measure, and their applicability to different scenarios. We then describe
the statistical tests and make additional practical recommendations for
reporting results. The metrics and accompanying statistical tools have been
made available as an open-source library at
https://github.com/google-research/rl-reliability-metrics. We apply our metrics
to a set of common RL algorithms and environments, compare them, and analyze
the results.
",lack reliability well know issue reinforcement learning rl algorithm problem gain increase attention recent year effort improve grow substantially aid rl researcher production user evaluation improvement reliability propose set metric quantitatively measure different aspect reliability work focus variability risk training learn fix policy design metric general purpose also design complementary statistical test enable rigorous comparison metric paper first describe desire property metric design aspect reliability measure applicability different scenario describe statistical test make additional practical recommendation report result metric accompany statistical tool make available open source library https apply metric set common rl algorithms environment compare analyze result
"Investigating a Baseline Of Self Supervised Learning Towards Reducing
  Labeling Costs For Image Classification","  Data labeling in supervised learning is considered an expensive and
infeasible tool in some conditions. The self-supervised learning method is
proposed to tackle the learning effectiveness with fewer labeled data, however,
there is a lack of confidence in the size of labeled data needed to achieve
adequate results. This study aims to draw a baseline on the proportion of the
labeled data that models can appreciate to yield competent accuracy when
compared to training with additional labels. The study implements the
kaggle.com' cats-vs-dogs dataset, Mnist and Fashion-Mnist to investigate the
self-supervised learning task by implementing random rotations augmentation on
the original datasets. To reveal the true effectiveness of the pretext process
in self-supervised learning, the original dataset is divided into smaller
batches, and learning is repeated on each batch with and without the pretext
pre-training. Results show that the pretext process in the self-supervised
learning improves the accuracy around 15% in the downstream classification task
when compared to the plain supervised learning.
",datum label supervise learning consider expensive infeasible tool condition self supervise learning method propose tackle learn effectiveness few label datum however lack confidence size label datum need achieve adequate result study aim draw baseline proportion label data model appreciate yield competent accuracy compare train additional label study implement cat vs dog dataset mnist fashion mnist investigate self supervise learning task implement random rotation augmentation original dataset reveal true effectiveness pretext process self supervise learn original dataset divide small batch learn repeat batch without pretext pre training result show pretext process self supervise learning improve accuracy around 15 downstream classification task compare plain supervised learning
Improving the Speed and Quality of GAN by Adversarial Training,"  Generative adversarial networks (GAN) have shown remarkable results in image
generation tasks. High fidelity class-conditional GAN methods often rely on
stabilization techniques by constraining the global Lipschitz continuity. Such
regularization leads to less expressive models and slower convergence speed;
other techniques, such as the large batch training, require unconventional
computing power and are not widely accessible. In this paper, we develop an
efficient algorithm, namely FastGAN (Free AdverSarial Training), to improve the
speed and quality of GAN training based on the adversarial training technique.
We benchmark our method on CIFAR10, a subset of ImageNet, and the full ImageNet
datasets. We choose strong baselines such as SNGAN and SAGAN; the results
demonstrate that our training algorithm can achieve better generation quality
(in terms of the Inception score and Frechet Inception distance) with less
overall training time. Most notably, our training algorithm brings ImageNet
training to the broader public by requiring 2-4 GPUs.
",generative adversarial network gan show remarkable result image generation task high fidelity class conditional gan method often rely stabilization technique constrain global lipschitz continuity regularization lead less expressive model slow convergence speed technique large batch training require unconventional computing power widely accessible paper develop efficient algorithm namely fastgan free adversarial training improve speed quality gan training base adversarial training technique benchmark method cifar10 subset imagenet full imagenet dataset choose strong baseline sngan sagan result demonstrate train algorithm achieve well generation quality term inception score frechet inception distance less overall training time notably train algorithm bring imagenet training broad public require 2 4 gpu
An unsupervised deep learning framework for medical image denoising,"  Medical image acquisition is often intervented by unwanted noise that
corrupts the information content. This paper introduces an unsupervised medical
image denoising technique that learns noise characteristics from the available
images and constructs denoised images. It comprises of two blocks of data
processing, viz., patch-based dictionaries that indirectly learn the noise and
residual learning (RL) that directly learns the noise. The model is generalized
to account for both 2D and 3D images considering different medical imaging
instruments. The images are considered one-by-one from the stack of MRI/CT
images as well as the entire stack is considered, and decomposed into
overlapping image/volume patches. These patches are given to the patch-based
dictionary learning to learn noise characteristics via sparse representation
while given to the RL part to directly learn the noise properties. K-singular
value decomposition (K-SVD) algorithm for sparse representation is used for
training patch-based dictionaries. On the other hand, residue in the patches is
trained using the proposed deep residue network. Iterating on these two parts,
an optimum noise characterization for each image/volume patch is captured and
in turn it is subtracted from the available respective image/volume patch. The
obtained denoised image/volume patches are finally assembled to a denoised
image or 3D stack. We provide an analysis of the proposed approach with other
approaches. Experiments on MRI/CT datasets are run on a GPU-based supercomputer
and the comparative results show that the proposed algorithm preserves the
critical information in the images as well as improves the visual quality of
the images.
",medical image acquisition often intervente unwanted noise corrupt information content paper introduce unsupervised medical image denoising technique learn noise characteristic available image construct denoise image comprise two block datum processing patch base dictionary indirectly learn noise residual learning rl directly learn noise model generalize account 2d 3d image consider different medical imaging instrument image consider one by one stack image well entire stack consider decompose overlapping patch patch give patch base dictionary learning learn noise characteristic via sparse representation give rl part directly learn noise property k singular value decomposition k svd algorithm sparse representation use training patch base dictionary hand residue patch train use propose deep residue network iterate two part optimum noise characterization patch capture turn subtract available respective patch obtain denoise patch finally assemble denoise image 3d stack provide analysis propose approach approach experiment dataset run gpu base supercomputer comparative result show propose algorithm preserve critical information image well improve visual quality image
"Measuring the Algorithmic Convergence of Randomized Ensembles: The
  Regression Setting","  When randomized ensemble methods such as bagging and random forests are
implemented, a basic question arises: Is the ensemble large enough? In
particular, the practitioner desires a rigorous guarantee that a given ensemble
will perform nearly as well as an ideal infinite ensemble (trained on the same
data). The purpose of the current paper is to develop a bootstrap method for
solving this problem in the context of regression --- which complements our
companion paper in the context of classification (Lopes 2019). In contrast to
the classification setting, the current paper shows that theoretical guarantees
for the proposed bootstrap can be established under much weaker assumptions. In
addition, we illustrate the flexibility of the method by showing how it can be
adapted to measure algorithmic convergence for variable selection. Lastly, we
provide numerical results demonstrating that the method works well in a range
of situations.
",randomize ensemble method bag random forest implement basic question arise ensemble large enough particular practitioner desire rigorous guarantee give ensemble perform nearly well ideal infinite ensemble train datum purpose current paper develop bootstrap method solve problem context regression complement companion paper context classification lope 2019 contrast classification set current paper show theoretical guarantee propose bootstrap establish much weak assumption addition illustrate flexibility method show adapt measure algorithmic convergence variable selection lastly provide numerical result demonstrate method work well range situation
Patch Learning,"  There have been different strategies to improve the performance of a machine
learning model, e.g., increasing the depth, width, and/or nonlinearity of the
model, and using ensemble learning to aggregate multiple base/weak learners in
parallel or in series. This paper proposes a novel strategy called patch
learning (PL) for this problem. It consists of three steps: 1) train an initial
global model using all training data; 2) identify from the initial global model
the patches which contribute the most to the learning error, and train a
(local) patch model for each such patch; and, 3) update the global model using
training data that do not fall into any patch. To use a PL model, we first
determine if the input falls into any patch. If yes, then the corresponding
patch model is used to compute the output. Otherwise, the global model is used.
We explain in detail how PL can be implemented using fuzzy systems. Five
regression problems on 1D/2D/3D curve fitting, nonlinear system identification,
and chaotic time-series prediction, verified its effectiveness. To our
knowledge, the PL idea has not appeared in the literature before, and it opens
up a promising new line of research in machine learning.
",different strategy improve performance machine learning model increase depth width nonlinearity model use ensemble learn aggregate multiple learner parallel series paper propose novel strategy call patch learn pl problem consist three step 1 train initial global model use training datum 2 identify initial global model patch contribute learn error train local patch model patch 3 update global model use training datum fall patch use pl model first determine input fall patch yes correspond patch model use compute output otherwise global model use explain detail pl implement use fuzzy system five regression problem curve fitting nonlinear system identification chaotic time series prediction verify effectiveness knowledge pl idea appear literature open promise new line research machine learning
"Segmentation of the Carotid Lumen and Vessel Wall using Deep Learning
  and Location Priors","  In this report we want to present our method and results for the Carotid
Artery Vessel Wall Segmentation Challenge. We propose an image-based pipeline
utilizing the U-Net architecture and location priors to solve the segmentation
problem at hand.
",report want present method result carotid artery vessel wall segmentation challenge propose image base pipeline utilize u net architecture location prior solve segmentation problem hand
Universal MMSE Filtering With Logarithmic Adaptive Regret,"  We consider the problem of online estimation of a real-valued signal
corrupted by oblivious zero-mean noise using linear estimators. The estimator
is required to iteratively predict the underlying signal based on the current
and several last noisy observations, and its performance is measured by the
mean-square-error. We describe and analyze an algorithm for this task which: 1.
Achieves logarithmic adaptive regret against the best linear filter in
hindsight. This bound is assyptotically tight, and resolves the question of
Moon and Weissman [1]. 2. Runs in linear time in terms of the number of filter
coefficients. Previous constructions required at least quadratic time.
",consider problem online estimation real value signal corrupt oblivious zero mean noise use linear estimator estimator require iteratively predict underlying signal base current several last noisy observation performance measure mean square error describe analyze algorithm task achieve logarithmic adaptive regret good linear filter hindsight bind assyptotically tight resolve question moon weissman 1 run linear time term number filter coefficient previous construction require least quadratic time
Continual Horizontal Federated Learning for Heterogeneous Data,"  Federated learning is a promising machine learning technique that enables
multiple clients to collaboratively build a model without revealing the raw
data to each other. Among various types of federated learning methods,
horizontal federated learning (HFL) is the best-studied category and handles
homogeneous feature spaces. However, in the case of heterogeneous feature
spaces, HFL uses only common features and leaves client-specific features
unutilized. In this paper, we propose a HFL method using neural networks named
continual horizontal federated learning (CHFL), a continual learning approach
to improve the performance of HFL by taking advantage of unique features of
each client. CHFL splits the network into two columns corresponding to common
features and unique features, respectively. It jointly trains the first column
by using common features through vanilla HFL and locally trains the second
column by using unique features and leveraging the knowledge of the first one
via lateral connections without interfering with the federated training of it.
We conduct experiments on various real world datasets and show that CHFL
greatly outperforms vanilla HFL that only uses common features and local
learning that uses all features that each client has.
",federate learning promise machine learn technique enable multiple client collaboratively build model without reveal raw datum among various type federate learning method horizontal federate learning hfl well study category handle homogeneous feature space however case heterogeneous feature space hfl use common feature leave client specific feature unutilized paper propose hfl method use neural network name continual horizontal federated learning chfl continual learning approach improve performance hfl take advantage unique feature client chfl split network two column correspond common feature unique feature respectively jointly train first column use common feature vanilla hfl locally train second column use unique feature leverage knowledge first one via lateral connection without interfere federated training conduct experiment various real world dataset show chfl greatly outperform vanilla hfl use common feature local learning use feature client
"Physical Accuracy of Deep Neural Networks for 2D and 3D Multi-Mineral
  Segmentation of Rock micro-CT Images","  Segmentation of 3D micro-Computed Tomographic uCT) images of rock samples is
essential for further Digital Rock Physics (DRP) analysis, however,
conventional methods such as thresholding, watershed segmentation, and
converging active contours are susceptible to user-bias. Deep Convolutional
Neural Networks (CNNs) have produced accurate pixelwise semantic segmentation
results with natural images and $\mu$CT rock images, however, physical accuracy
is not well documented. The performance of 4 CNN architectures is tested for 2D
and 3D cases in 10 configurations. Manually segmented uCT images of Mt. Simon
Sandstone are treated as ground truth and used as training and validation data,
with a high voxelwise accuracy (over 99%) achieved. Downstream analysis is then
used to validate physical accuracy. The topology of each segmented phase is
calculated, and the absolute permeability and multiphase flow is modelled with
direct simulation in single and mixed wetting cases. These physical measures of
connectivity, and flow characteristics show high variance and uncertainty, with
models that achieve 95\%+ in voxelwise accuracy possessing permeabilities and
connectivities orders of magnitude off. A new network architecture is also
introduced as a hybrid fusion of U-net and ResNet, combining short and long
skip connections in a Network-in-Network configuration. The 3D implementation
outperforms all other tested models in voxelwise and physical accuracy
measures. The network architecture and the volume fraction in the dataset (and
associated weighting), are factors that not only influence the accuracy
trade-off in the voxelwise case, but is especially important in training a
physically accurate model for segmentation.
",segmentation 3d micro computed tomographic uct image rock sample essential digital rock physics drp analysis however conventional method thresholde watershed segmentation converge active contour susceptible user bias deep convolutional neural network cnn produce accurate pixelwise semantic segmentation result natural image ct rock image however physical accuracy well document performance 4 cnn architecture test 2d 3d case 10 configuration manually segment uct image mt simon sandstone treat ground truth use training validation datum high voxelwise accuracy 99 achieve downstream analysis use validate physical accuracy topology segment phase calculate absolute permeability multiphase flow model direct simulation single mixed wetting case physical measure connectivity flow characteristic show high variance uncertainty model achieve voxelwise accuracy possess permeability connectivity order magnitude new network architecture also introduce hybrid fusion u net resnet combine short long skip connection network in network configuration 3d implementation outperform test model voxelwise physical accuracy measure network architecture volume fraction dataset associate weighting factor influence accuracy trade off voxelwise case especially important training physically accurate model segmentation
"Semi Conditional Variational Auto-Encoder for Flow Reconstruction and
  Uncertainty Quantification from Limited Observations","  We present a new data-driven model to reconstruct nonlinear flow from
spatially sparse observations. The model is a version of a conditional
variational auto-encoder (CVAE), which allows for probabilistic reconstruction
and thus uncertainty quantification of the prediction. We show that in our
model, conditioning on the measurements from the complete flow data leads to a
CVAE where only the decoder depends on the measurements. For this reason we
call the model as Semi-Conditional Variational Autoencoder (SCVAE). The method,
reconstructions and associated uncertainty estimates are illustrated on the
velocity data from simulations of 2D flow around a cylinder and bottom currents
from the Bergen Ocean Model. The reconstruction errors are compared to those of
the Gappy Proper Orthogonal Decomposition (GPOD) method.
",present new data drive model reconstruct nonlinear flow spatially sparse observation model version conditional variational auto encoder cvae allow probabilistic reconstruction thus uncertainty quantification prediction show model conditioning measurement complete flow datum lead cvae decoder depend measurement reason call model semi conditional variational autoencoder scvae method reconstruction associate uncertainty estimate illustrate velocity datum simulation 2d flow around cylinder bottom current bergen ocean model reconstruction error compare gappy proper orthogonal decomposition gpod method
"Decomposition Principles and Online Learning in Cross-Layer Optimization
  for Delay-Sensitive Applications","  In this paper, we propose a general cross-layer optimization framework in
which we explicitly consider both the heterogeneous and dynamically changing
characteristics of delay-sensitive applications and the underlying time-varying
network conditions. We consider both the independently decodable data units
(DUs, e.g. packets) and the interdependent DUs whose dependencies are captured
by a directed acyclic graph (DAG). We first formulate the cross-layer design as
a non-linear constrained optimization problem by assuming complete knowledge of
the application characteristics and the underlying network conditions. The
constrained cross-layer optimization is decomposed into several cross-layer
optimization subproblems for each DU and two master problems. The proposed
decomposition method determines the necessary message exchanges between layers
for achieving the optimal cross-layer solution. However, the attributes (e.g.
distortion impact, delay deadline etc) of future DUs as well as the network
conditions are often unknown in the considered real-time applications. The
impact of current cross-layer actions on the future DUs can be characterized by
a state-value function in the Markov decision process (MDP) framework. Based on
the dynamic programming solution to the MDP, we develop a low-complexity
cross-layer optimization algorithm using online learning for each DU
transmission. This online algorithm can be implemented in real-time in order to
cope with unknown source characteristics, network dynamics and resource
constraints. Our numerical results demonstrate the efficiency of the proposed
online algorithm.
",paper propose general cross layer optimization framework explicitly consider heterogeneous dynamically change characteristic delay sensitive application underlie time vary network condition consider independently decodable datum unit dus packet interdependent dus whose dependency capture direct acyclic graph dag first formulate cross layer design non linear constrained optimization problem assume complete knowledge application characteristic underlie network condition constrain cross layer optimization decompose several cross layer optimization subproblem du two master problem propose decomposition method determine necessary message exchange layer achieve optimal cross layer solution however attribute distortion impact delay deadline etc future dus well network condition often unknown consider real time application impact current cross layer action future dus characterize state value function markov decision process mdp framework base dynamic programming solution mdp develop low complexity cross layer optimization algorithm use online learn du transmission online algorithm implement real time order cope unknown source characteristic network dynamic resource constraint numerical result demonstrate efficiency propose online algorithm
"A Deep Learning-based Multimodal Depth-Aware Dynamic Hand Gesture
  Recognition System","  The dynamic hand gesture recognition task has seen studies on various
unimodal and multimodal methods. Previously, researchers have explored depth
and 2D-skeleton-based multimodal fusion CRNNs (Convolutional Recurrent Neural
Networks) but have had limitations in getting expected recognition results. In
this paper, we revisit this approach to hand gesture recognition and suggest
several improvements. We observe that raw depth images possess low contrast in
the hand regions of interest (ROI). They do not highlight important fine
details, such as finger orientation, overlap between the finger and palm, or
overlap between multiple fingers. We thus propose quantizing the depth values
into several discrete regions, to create a higher contrast between several key
parts of the hand. In addition, we suggest several ways to tackle the high
variance problem in existing multimodal fusion CRNN architectures. We evaluate
our method on two benchmarks: the DHG-14/28 dataset and the SHREC'17 track
dataset. Our approach shows a significant improvement in accuracy and parameter
efficiency over previous similar multimodal methods, with a comparable result
to the state-of-the-art.
",dynamic hand gesture recognition task see study various unimodal multimodal method previously researcher explore depth 2d skeleton base multimodal fusion crnn convolutional recurrent neural network limitation getting expect recognition result paper revisit approach hand gesture recognition suggest several improvement observe raw depth image possess low contrast hand region interest roi highlight important fine detail finger orientation overlap finger palm overlap multiple finger thus propose quantize depth value several discrete region create high contrast several key part hand addition suggest several way tackle high variance problem exist multimodal fusion crnn architecture evaluate method two benchmark dataset track dataset approach show significant improvement accuracy parameter efficiency previous similar multimodal method comparable result state of the art
"Improving Non-autoregressive Neural Machine Translation with Monolingual
  Data","  Non-autoregressive (NAR) neural machine translation is usually done via
knowledge distillation from an autoregressive (AR) model. Under this framework,
we leverage large monolingual corpora to improve the NAR model's performance,
with the goal of transferring the AR model's generalization ability while
preventing overfitting. On top of a strong NAR baseline, our experimental
results on the WMT14 En-De and WMT16 En-Ro news translation tasks confirm that
monolingual data augmentation consistently improves the performance of the NAR
model to approach the teacher AR model's performance, yields comparable or
better results than the best non-iterative NAR methods in the literature and
helps reduce overfitting in the training process.
",non autoregressive nar neural machine translation usually do via knowledge distillation autoregressive ar model framework leverage large monolingual corpora improve nar model performance goal transfer ar model generalization ability prevent overfitte top strong nar baseline experimental result wmt14 en de wmt16 en ro news translation task confirm monolingual datum augmentation consistently improve performance nar model approach teacher ar model performance yield comparable well result good non iterative nar method literature help reduce overfitte training process
Mini-Batch Spectral Clustering,"  The cost of computing the spectrum of Laplacian matrices hinders the
application of spectral clustering to large data sets. While approximations
recover computational tractability, they can potentially affect clustering
performance. This paper proposes a practical approach to learn spectral
clustering based on adaptive stochastic gradient optimization. Crucially, the
proposed approach recovers the exact spectrum of Laplacian matrices in the
limit of the iterations, and the cost of each iteration is linear in the number
of samples. Extensive experimental validation on data sets with up to half a
million samples demonstrate its scalability and its ability to outperform
state-of-the-art approximate methods to learn spectral clustering for a given
computational budget.
",cost computing spectrum laplacian matrix hinder application spectral cluster large datum set approximation recover computational tractability potentially affect clustering performance paper propose practical approach learn spectral clustering base adaptive stochastic gradient optimization crucially propose approach recover exact spectrum laplacian matrix limit iteration cost iteration linear number sample extensive experimental validation datum set half million sample demonstrate scalability ability outperform state of the art approximate method learn spectral clustering give computational budget
Quantum Deep Learning for Mutant COVID-19 Strain Prediction,"  New COVID-19 epidemic strains like Delta and Omicron with increased
transmissibility and pathogenicity emerge and spread across the whole world
rapidly while causing high mortality during the pandemic period. Early
prediction of possible variants (especially spike protein) of COVID-19 epidemic
strains based on available mutated SARS-CoV-2 RNA sequences may lead to early
prevention and treatment. Here, combining the advantage of quantum and
quantum-inspired algorithms with the wide application of deep learning, we
propose a development tool named DeepQuantum, and use this software to realize
the goal of predicting spike protein variation structure of COVID-19 epidemic
strains. In addition, this hybrid quantum-classical model for the first time
achieves quantum-inspired blur convolution similar to classical depthwise
convolution and also successfully applies quantum progressive training with
quantum circuits, both of which guarantee that our model is the quantum
counterpart of the famous style-based GAN. The results state that the
fidelities of random generating spike protein variation structure are always
beyond 96% for Delta, 94% for Omicron. The training loss curve is more stable
and converges better with multiple loss functions compared with the
corresponding classical algorithm. At last, evidences that quantum-inspired
algorithms promote the classical deep learning and hybrid models effectively
predict the mutant strains are strong.
",new covid-19 epidemic strain like delta omicron increase transmissibility pathogenicity emerge spread across whole world rapidly cause high mortality pandemic period early prediction possible variant especially spike protein covid-19 epidemic strain base available mutate sar cov-2 rna sequence may lead early prevention treatment combine advantage quantum quantum inspire algorithm wide application deep learning propose development tool name deepquantum use software realize goal predict spike protein variation structure covid-19 epidemic strain addition hybrid quantum classical model first time achieve quantum inspire blur convolution similar classical depthwise convolution also successfully apply quantum progressive training quantum circuit guarantee model quantum counterpart famous style base gan result state fidelity random generating spike protein variation structure always beyond 96 delta 94 omicron training loss curve stable converge well multiple loss function compare correspond classical algorithm last evidence quantum inspire algorithm promote classical deep learn hybrid model effectively predict mutant strain strong
"Convergence of Generalized Belief Propagation Algorithm on Graphs with
  Motifs","  Belief propagation is a fundamental message-passing algorithm for numerous
applications in machine learning. It is known that belief propagation algorithm
is exact on tree graphs. However, belief propagation is run on loopy graphs in
most applications. So, understanding the behavior of belief propagation on
loopy graphs has been a major topic for researchers in different areas. In this
paper, we study the convergence behavior of generalized belief propagation
algorithm on graphs with motifs (triangles, loops, etc.) We show under a
certain initialization, generalized belief propagation converges to the global
optimum of the Bethe free energy for ferromagnetic Ising models on graphs with
motifs.
",belief propagation fundamental message pass algorithm numerous application machine learn know belief propagation algorithm exact tree graph however belief propagation run loopy graph application understand behavior belief propagation loopy graph major topic researcher different area paper study convergence behavior generalize belief propagation algorithm graph motif triangle loop etc show certain initialization generalize belief propagation converge global optimum bethe free energy ferromagnetic ising model graph motif
"Automatic Tuning of Stochastic Gradient Descent with Bayesian
  Optimisation","  Many machine learning models require a training procedure based on running
stochastic gradient descent. A key element for the efficiency of those
algorithms is the choice of the learning rate schedule. While finding good
learning rates schedules using Bayesian optimisation has been tackled by
several authors, adapting it dynamically in a data-driven way is an open
question. This is of high practical importance to users that need to train a
single, expensive model. To tackle this problem, we introduce an original
probabilistic model for traces of optimisers, based on latent Gaussian
processes and an auto-/regressive formulation, that flexibly adjusts to abrupt
changes of behaviours induced by new learning rate values. As illustrated, this
model is well-suited to tackle a set of problems: first, for the on-line
adaptation of the learning rate for a cold-started run; then, for tuning the
schedule for a set of similar tasks (in a classical BO setup), as well as
warm-starting it for a new task.
",many machine learning model require training procedure base run stochastic gradient descent key element efficiency algorithm choice learn rate schedule find good learning rate schedule use bayesian optimisation tackle several author adapt dynamically data drive way open question high practical importance user need train single expensive model tackle problem introduce original probabilistic model trace optimiser base latent gaussian process formulation flexibly adjust abrupt change behaviour induce new learning rate value illustrate model well suit tackle set problem first on line adaptation learn rate cold start run tuning schedule set similar task classical bo setup well warm start new task
Parzen Window Approximation on Riemannian Manifold,"  In graph motivated learning, label propagation largely depends on data
affinity represented as edges between connected data points. The affinity
assignment implicitly assumes even distribution of data on the manifold. This
assumption may not hold and may lead to inaccurate metric assignment due to
drift towards high-density regions. The drift affected heat kernel based
affinity with a globally fixed Parzen window either discards genuine neighbors
or forces distant data points to become a member of the neighborhood. This
yields a biased affinity matrix. In this paper, the bias due to uneven data
sampling on the Riemannian manifold is catered to by a variable Parzen window
determined as a function of neighborhood size, ambient dimension, flatness
range, etc. Additionally, affinity adjustment is used which offsets the effect
of uneven sampling responsible for the bias. An affinity metric which takes
into consideration the irregular sampling effect to yield accurate label
propagation is proposed. Extensive experiments on synthetic and real-world data
sets confirm that the proposed method increases the classification accuracy
significantly and outperforms existing Parzen window estimators in graph
Laplacian manifold regularization methods.
",graph motivate learn label propagation largely depend datum affinity represent edge connect datum point affinity assignment implicitly assume even distribution datum manifold assumption may hold may lead inaccurate metric assignment due drift towards high density region drift affect heat kernel base affinity globally fix parzen window either discard genuine neighbor force distant datum point become member neighborhood yield bias affinity matrix paper bias due uneven datum sample riemannian manifold catered variable parzen window determine function neighborhood size ambient dimension flatness range etc additionally affinity adjustment use offset effect uneven sample responsible bias affinity metric take consideration irregular sampling effect yield accurate label propagation propose extensive experiment synthetic real world datum set confirm propose method increase classification accuracy significantly outperform exist parzen window estimator graph laplacian manifold regularization method
Episodic Curiosity through Reachability,"  Rewards are sparse in the real world and most of today's reinforcement
learning algorithms struggle with such sparsity. One solution to this problem
is to allow the agent to create rewards for itself - thus making rewards dense
and more suitable for learning. In particular, inspired by curious behaviour in
animals, observing something novel could be rewarded with a bonus. Such bonus
is summed up with the real task reward - making it possible for RL algorithms
to learn from the combined reward. We propose a new curiosity method which uses
episodic memory to form the novelty bonus. To determine the bonus, the current
observation is compared with the observations in memory. Crucially, the
comparison is done based on how many environment steps it takes to reach the
current observation from those in memory - which incorporates rich information
about environment dynamics. This allows us to overcome the known ""couch-potato""
issues of prior work - when the agent finds a way to instantly gratify itself
by exploiting actions which lead to hardly predictable consequences. We test
our approach in visually rich 3D environments in ViZDoom, DMLab and MuJoCo. In
navigational tasks from ViZDoom and DMLab, our agent outperforms the
state-of-the-art curiosity method ICM. In MuJoCo, an ant equipped with our
curiosity module learns locomotion out of the first-person-view curiosity only.
",reward sparse real world today reinforcement learning algorithm struggle sparsity one solution problem allow agent create reward thus make reward dense suitable learn particular inspire curious behaviour animal observe something novel could reward bonus bonus sum real task reward make possible rl algorithm learn combined reward propose new curiosity method use episodic memory form novelty bonus determine bonus current observation compare observation memory crucially comparison do base many environment step takes reach current observation memory incorporate rich information environment dynamic allow we overcome know couch potato issue prior work agent find way instantly gratify exploit action lead hardly predictable consequence test approach visually rich 3d environment vizdoom dmlab mujoco navigational task vizdoom dmlab agent outperform state of the art curiosity method icm mujoco ant equipped curiosity module learn locomotion first person view curiosity
"Decoding visual stimuli in human brain by using Anatomical Pattern
  Analysis on fMRI images","  A universal unanswered question in neuroscience and machine learning is
whether computers can decode the patterns of the human brain. Multi-Voxels
Pattern Analysis (MVPA) is a critical tool for addressing this question.
However, there are two challenges in the previous MVPA methods, which include
decreasing sparsity and noises in the extracted features and increasing the
performance of prediction. In overcoming mentioned challenges, this paper
proposes Anatomical Pattern Analysis (APA) for decoding visual stimuli in the
human brain. This framework develops a novel anatomical feature extraction
method and a new imbalance AdaBoost algorithm for binary classification.
Further, it utilizes an Error-Correcting Output Codes (ECOC) method for
multi-class prediction. APA can automatically detect active regions for each
category of the visual stimuli. Moreover, it enables us to combine homogeneous
datasets for applying advanced classification. Experimental studies on 4 visual
categories (words, consonants, objects and scrambled photos) demonstrate that
the proposed approach achieves superior performance to state-of-the-art
methods.
",universal unanswered question neuroscience machine learn whether computer decode pattern human brain multi voxels pattern analysis mvpa critical tool addressing question however two challenge previous mvpa method include decrease sparsity noise extract feature increase performance prediction overcome mention challenge paper propose anatomical pattern analysis apa decode visual stimulus human brain framework develop novel anatomical feature extraction method new imbalance adaboost algorithm binary classification utilize error correct output code ecoc method multi class prediction apa automatically detect active region category visual stimulus moreover enable we combine homogeneous dataset apply advanced classification experimental study 4 visual category word consonant object scramble photo demonstrate propose approach achieve superior performance state of the art method
On Empirical Comparisons of Optimizers for Deep Learning,"  Selecting an optimizer is a central step in the contemporary deep learning
pipeline. In this paper, we demonstrate the sensitivity of optimizer
comparisons to the hyperparameter tuning protocol. Our findings suggest that
the hyperparameter search space may be the single most important factor
explaining the rankings obtained by recent empirical comparisons in the
literature. In fact, we show that these results can be contradicted when
hyperparameter search spaces are changed. As tuning effort grows without bound,
more general optimizers should never underperform the ones they can approximate
(i.e., Adam should never perform worse than momentum), but recent attempts to
compare optimizers either assume these inclusion relationships are not
practically relevant or restrict the hyperparameters in ways that break the
inclusions. In our experiments, we find that inclusion relationships between
optimizers matter in practice and always predict optimizer comparisons. In
particular, we find that the popular adaptive gradient methods never
underperform momentum or gradient descent. We also report practical tips around
tuning often ignored hyperparameters of adaptive gradient methods and raise
concerns about fairly benchmarking optimizers for neural network training.
",select optimizer central step contemporary deep learning pipeline paper demonstrate sensitivity optimizer comparison hyperparameter tune protocol finding suggest hyperparameter search space may single important factor explain ranking obtain recent empirical comparison literature fact show result contradict hyperparameter search space change tuning effort grow without bound general optimizer never underperform one approximate adam never perform bad momentum recent attempt compare optimizer either assume inclusion relationship practically relevant restrict hyperparameter way break inclusion experiment find inclusion relationship optimizer matter practice always predict optimizer comparison particular find popular adaptive gradient method never underperform momentum gradient descent also report practical tip around tuning often ignore hyperparameter adaptive gradient method raise concern fairly benchmarke optimizer neural network training
Encoders and Decoders for Quantum Expander Codes Using Machine Learning,"  Quantum key distribution (QKD) allows two distant parties to share encryption
keys with security based on laws of quantum mechanics. In order to share the
keys, the quantum bits have to be transmitted from the sender to the receiver
over a noisy quantum channel. In order to transmit this information, efficient
encoders and decoders need to be designed. However, large-scale design of
quantum encoders and decoders have to depend on the channel characteristics and
require look-up tables which require memory that is exponential in the number
of qubits. In order to alleviate that, this paper aims to design the quantum
encoders and decoders for expander codes by adapting techniques from machine
learning including reinforcement learning and neural networks to the quantum
domain. The proposed quantum decoder trains a neural network which is trained
using the maximum aposteriori error for the syndromes, eliminating the use of
large lookup tables. The quantum encoder uses deep Q-learning based techniques
to optimize the generator matrices in the quantum Calderbank-Shor-Steane (CSS)
codes. The evaluation results demonstrate improved performance of the proposed
quantum encoder and decoder designs as compared to the quantum expander codes.
",quantum key distribution qkd allow two distant party share encryption key security base law quantum mechanic order share key quantum bit transmit sender receiver noisy quantum channel order transmit information efficient encoder decoder need design however large scale design quantum encoder decoder depend channel characteristic require look up table require memory exponential number qubit order alleviate paper aim design quantum encoder decoder expander code adapt technique machine learning include reinforcement learn neural network quantum domain propose quantum decoder train neural network train use maximum aposteriori error syndrome eliminate use large lookup table quantum encoder use deep q learning base technique optimize generator matrice quantum calderbank shor steane css code evaluation result demonstrate improved performance propose quantum encoder decoder design compare quantum expander code
"Learning to Match Jobs with Resumes from Sparse Interaction Data using
  Multi-View Co-Teaching Network","  With the ever-increasing growth of online recruitment data, job-resume
matching has become an important task to automatically match jobs with suitable
resumes. This task is typically casted as a supervised text matching problem.
Supervised learning is powerful when the labeled data is sufficient. However,
on online recruitment platforms, job-resume interaction data is sparse and
noisy, which affects the performance of job-resume match algorithms. To
alleviate these problems, in this paper, we propose a novel multi-view
co-teaching network from sparse interaction data for job-resume matching. Our
network consists of two major components, namely text-based matching model and
relation-based matching model. The two parts capture semantic compatibility in
two different views, and complement each other. In order to address the
challenges from sparse and noisy data, we design two specific strategies to
combine the two components. First, two components share the learned parameters
or representations, so that the original representations of each component can
be enhanced. More importantly, we adopt a co-teaching mechanism to reduce the
influence of noise in training data. The core idea is to let the two components
help each other by selecting more reliable training instances. The two
strategies focus on representation enhancement and data enhancement,
respectively. Compared with pure text-based matching models, the proposed
approach is able to learn better data representations from limited or even
sparse interaction data, which is more resistible to noise in training data.
Experiment results have demonstrated that our model is able to outperform
state-of-the-art methods for job-resume matching.
",ever increase growth online recruitment datum job resume matching become important task automatically match job suitable resume task typically cast supervised text matching problem supervise learning powerful label datum sufficient however online recruitment platform job resume interaction datum sparse noisy affect performance job resume match algorithm alleviate problem paper propose novel multi view co teaching network sparse interaction datum job resume matching network consist two major component namely text base matching model relation base matching model two part capture semantic compatibility two different view complement order address challenge sparse noisy datum design two specific strategy combine two component first two component share learn parameter representation original representation component enhance importantly adopt co teaching mechanism reduce influence noise training datum core idea let two component help select reliable training instance two strategy focus representation enhancement datum enhancement respectively compare pure text base matching model propose approach able learn well datum representation limit even sparse interaction datum resistible noise training datum experiment result demonstrate model able outperform state of the art method job resume matching
Routing and Placement of Macros using Deep Reinforcement Learning,"Chip placement has been one of the most time consuming task in any semi
conductor area, Due to this negligence, many projects are pushed and chips
availability in real markets get delayed. An engineer placing macros on a chip
also needs to place it optimally to reduce the three important factors like
power, performance and time. Looking at these prior problems we wanted to
introduce a new method using Reinforcement Learning where we train the model to
place the nodes of a chip netlist onto a chip canvas. We want to build a neural
architecture that will accurately reward the agent across a wide variety of
input netlist correctly.",chip placement one time consume task semi conductor area due negligence many project push chip availability real market get delay engineer place macros chip also need place optimally reduce three important factor like power performance time look prior problem wanted introduce new method use reinforcement learn train model place nodes chip netlist onto chip canvas want build neural architecture accurately reward agent across wide variety input netlist correctly
Learning causal representations for robust domain adaptation,"  Domain adaptation solves the learning problem in a target domain by
leveraging the knowledge in a relevant source domain. While remarkable advances
have been made, almost all existing domain adaptation methods heavily require
large amounts of unlabeled target domain data for learning domain invariant
representations to achieve good generalizability on the target domain. In fact,
in many real-world applications, target domain data may not always be
available. In this paper, we study the cases where at the training phase the
target domain data is unavailable and only well-labeled source domain data is
available, called robust domain adaptation. To tackle this problem, under the
assumption that causal relationships between features and the class variable
are robust across domains, we propose a novel Causal AutoEncoder (CAE), which
integrates deep autoencoder and causal structure learning into a unified model
to learn causal representations only using data from a single source domain.
Specifically, a deep autoencoder model is adopted to learn low-dimensional
representations, and a causal structure learning model is designed to separate
the low-dimensional representations into two groups: causal representations and
task-irrelevant representations. Using three real-world datasets the extensive
experiments have validated the effectiveness of CAE compared to eleven
state-of-the-art methods.
",domain adaptation solve learn problem target domain leverage knowledge relevant source domain remarkable advance make almost exist domain adaptation method heavily require large amount unlabeled target domain datum learn domain invariant representation achieve good generalizability target domain fact many real world application target domain datum may always available paper study case training phase target domain datum unavailable well label source domain datum available call robust domain adaptation tackle problem assumption causal relationship feature class variable robust across domain propose novel causal autoencoder cae integrate deep autoencoder causal structure learn unified model learn causal representation use datum single source domain specifically deep autoencoder model adopt learn low dimensional representation causal structure learning model design separate low dimensional representation two group causal representation task irrelevant representation use three real world dataset extensive experiment validate effectiveness cae compare eleven state of the art method
"Hierarchical reinforcement learning for efficient exploration and
  transfer","  Sparse-reward domains are challenging for reinforcement learning algorithms
since significant exploration is needed before encountering reward for the
first time. Hierarchical reinforcement learning can facilitate exploration by
reducing the number of decisions necessary before obtaining a reward. In this
paper, we present a novel hierarchical reinforcement learning framework based
on the compression of an invariant state space that is common to a range of
tasks. The algorithm introduces subtasks which consist of moving between the
state partitions induced by the compression. Results indicate that the
algorithm can successfully solve complex sparse-reward domains, and transfer
knowledge to solve new, previously unseen tasks more quickly.
",sparse reward domain challenge reinforcement learning algorithm since significant exploration need encounter reward first time hierarchical reinforcement learning facilitate exploration reduce number decision necessary obtain reward paper present novel hierarchical reinforcement learning framework base compression invariant state space common range task algorithm introduce subtask consist move state partition induce compression result indicate algorithm successfully solve complex sparse reward domain transfer knowledge solve new previously unseen task quickly
Algorithmic stability and hypothesis complexity,"  We introduce a notion of algorithmic stability of learning algorithms---that
we term \emph{argument stability}---that captures stability of the hypothesis
output by the learning algorithm in the normed space of functions from which
hypotheses are selected. The main result of the paper bounds the generalization
error of any learning algorithm in terms of its argument stability. The bounds
are based on martingale inequalities in the Banach space to which the
hypotheses belong. We apply the general bounds to bound the performance of some
learning algorithms based on empirical risk minimization and stochastic
gradient descent.
",introduce notion algorithmic stability learn algorithm -that term argument stability -that capture stability hypothesis output learn algorithm norme space function hypothesis select main result paper bound generalization error learn algorithm term argument stability bound base martingale inequality banach space hypothesis belong apply general bound bind performance learn algorithm base empirical risk minimization stochastic gradient descent
"Context-Aware Safe Reinforcement Learning for Non-Stationary
  Environments","  Safety is a critical concern when deploying reinforcement learning agents for
realistic tasks. Recently, safe reinforcement learning algorithms have been
developed to optimize the agent's performance while avoiding violations of
safety constraints. However, few studies have addressed the non-stationary
disturbances in the environments, which may cause catastrophic outcomes. In
this paper, we propose the context-aware safe reinforcement learning (CASRL)
method, a meta-learning framework to realize safe adaptation in non-stationary
environments. We use a probabilistic latent variable model to achieve fast
inference of the posterior environment transition distribution given the
context data. Safety constraints are then evaluated with uncertainty-aware
trajectory sampling. The high cost of safety violations leads to the rareness
of unsafe records in the dataset. We address this issue by enabling prioritized
sampling during model training and formulating prior safety constraints with
domain knowledge during constrained planning. The algorithm is evaluated in
realistic safety-critical environments with non-stationary disturbances.
Results show that the proposed algorithm significantly outperforms existing
baselines in terms of safety and robustness.
",safety critical concern deploy reinforcement learn agent realistic task recently safe reinforcement learning algorithm develop optimize agent performance avoid violation safety constraint however study address non stationary disturbance environment may cause catastrophic outcomes paper propose context aware safe reinforcement learn casrl method meta learn framework realize safe adaptation non stationary environment use probabilistic latent variable model achieve fast inference posterior environment transition distribution give context datum safety constraint evaluate uncertainty aware trajectory sample high cost safety violation lead rareness unsafe record dataset address issue enable prioritize sampling model training formulate prior safety constraint domain knowledge constrain planning algorithm evaluate realistic safety critical environment non stationary disturbance result show propose algorithm significantly outperform exist baseline term safety robustness
"Online NEAT for Credit Evaluation -- a Dynamic Problem with Sequential
  Data","  In this paper, we describe application of Neuroevolution to a P2P lending
problem in which a credit evaluation model is updated based on streaming data.
We apply the algorithm Neuroevolution of Augmenting Topologies (NEAT) which has
not been widely applied generally in the credit evaluation domain. In addition
to comparing the methodology with other widely applied machine learning
techniques, we develop and evaluate several enhancements to the algorithm which
make it suitable for the particular aspects of online learning that are
relevant in the problem. These include handling unbalanced streaming data, high
computation costs, and maintaining model similarity over time, that is training
the stochastic learning algorithm with new data but minimizing model change
except where there is a clear benefit for model performance
",paper describe application neuroevolution p2p lending problem credit evaluation model update base stream datum apply algorithm neuroevolution augment topology neat widely apply generally credit evaluation domain addition compare methodology widely apply machine learn technique develop evaluate several enhancement algorithm make suitable particular aspect online learn relevant problem include handle unbalanced streaming datum high computation cost maintain model similarity time training stochastic learn algorithm new datum minimize model change except clear benefit model performance
"Hybrid Model for Anomaly Detection on Call Detail Records by Time Series
  Forecasting","  Mobile network operators store an enormous amount of information like log
files that describe various events and users' activities. Analysis of these
logs might be used in many critical applications such as detecting
cyber-attacks, finding behavioral patterns of users, security incident
response, network forensics, etc. In a cellular network Call Detail Records
(CDR) is one type of such logs containing metadata of calls and usually
includes valuable information about contact such as the phone numbers of
originating and receiving subscribers, call duration, the area of activity,
type of call (SMS or voice call) and a timestamp. With anomaly detection, it is
possible to determine abnormal reduction or increment of network traffic in an
area or for a particular person. This paper's primary goal is to study
subscribers' behavior in a cellular network, mainly predicting the number of
calls in a region and detecting anomalies in the network traffic. In this
paper, a new hybrid method is proposed based on various anomaly detection
methods such as GARCH, K-means, and Neural Network to determine the anomalous
data. Moreover, we have discussed the possible causes of such anomalies.
",mobile network operator store enormous amount information like log file describe various event user activitie analysis log might use many critical application detect cyber attack find behavioral pattern user security incident response network forensic etc cellular network call detail record cdr one type log contain metadata call usually include valuable information contact phone number originate receive subscriber call duration area activity type call sms voice call timestamp anomaly detection possible determine abnormal reduction increment network traffic area particular person paper primary goal study subscriber behavior cellular network mainly predict number call region detecting anomaly network traffic paper new hybrid method propose base various anomaly detection method garch k means neural network determine anomalous datum moreover discuss possible cause anomaly
Fast Sparse Classification for Generalized Linear and Additive Models,"  We present fast classification techniques for sparse generalized linear and
additive models. These techniques can handle thousands of features and
thousands of observations in minutes, even in the presence of many highly
correlated features. For fast sparse logistic regression, our computational
speed-up over other best-subset search techniques owes to linear and quadratic
surrogate cuts for the logistic loss that allow us to efficiently screen
features for elimination, as well as use of a priority queue that favors a more
uniform exploration of features. As an alternative to the logistic loss, we
propose the exponential loss, which permits an analytical solution to the line
search at each iteration. Our algorithms are generally 2 to 5 times faster than
previous approaches. They produce interpretable models that have accuracy
comparable to black box models on challenging datasets.
",present fast classification technique sparse generalize linear additive model technique handle thousand feature thousand observation minute even presence many highly correlate feature fast sparse logistic regression computational speed up well subset search technique owe linear quadratic surrogate cut logistic loss allow we efficiently screen feature elimination well use priority queue favor uniform exploration feature alternative logistic loss propose exponential loss permit analytical solution line search iteration algorithm generally 2 5 time fast previous approach produce interpretable model accuracy comparable black box model challenge dataset
"Translation and Rotation Equivariant Normalizing Flow (TRENF) for
  Optimal Cosmological Analysis","  Our universe is homogeneous and isotropic, and its perturbations obey
translation and rotation symmetry. In this work we develop Translation and
Rotation Equivariant Normalizing Flow (TRENF), a generative Normalizing Flow
(NF) model which explicitly incorporates these symmetries, defining the data
likelihood via a sequence of Fourier space-based convolutions and pixel-wise
nonlinear transforms. TRENF gives direct access to the high dimensional data
likelihood p(x|y) as a function of the labels y, such as cosmological
parameters. In contrast to traditional analyses based on summary statistics,
the NF approach has no loss of information since it preserves the full
dimensionality of the data. On Gaussian random fields, the TRENF likelihood
agrees well with the analytical expression and saturates the Fisher information
content in the labels y. On nonlinear cosmological overdensity fields from
N-body simulations, TRENF leads to significant improvements in constraining
power over the standard power spectrum summary statistic. TRENF is also a
generative model of the data, and we show that TRENF samples agree well with
the N-body simulations it trained on, and that the inverse mapping of the data
agrees well with a Gaussian white noise both visually and on various summary
statistics: when this is perfectly achieved the resulting p(x|y) likelihood
analysis becomes optimal. Finally, we develop a generalization of this model
that can handle effects that break the symmetry of the data, such as the survey
mask, which enables likelihood analysis on data without periodic boundaries.
",universe homogeneous isotropic perturbation obey translation rotation symmetry work develop translation rotation equivariant normalizing flow trenf generative normalizing flow nf model explicitly incorporate symmetry define datum likelihood via sequence fouri space base convolution pixel wise nonlinear transform trenf give direct access high dimensional datum likelihood p function label cosmological parameter contrast traditional analysis base summary statistic nf approach loss information since preserve full dimensionality datum gaussian random field trenf likelihood agree well analytical expression saturate fisher information content label nonlinear cosmological overdensity field n body simulation trenf lead significant improvement constrain power standard power spectrum summary statistic trenf also generative model datum show trenf sample agree well n body simulation train inverse mapping datum agree well gaussian white noise visually various summary statistic perfectly achieve result p likelihood analysis become optimal finally develop generalization model handle effect break symmetry datum survey mask enable likelihood analysis datum without periodic boundary
"Feedback GAN (FBGAN) for DNA: a Novel Feedback-Loop Architecture for
  Optimizing Protein Functions","  Generative Adversarial Networks (GANs) represent an attractive and novel
approach to generate realistic data, such as genes, proteins, or drugs, in
synthetic biology. Here, we apply GANs to generate synthetic DNA sequences
encoding for proteins of variable length. We propose a novel feedback-loop
architecture, called Feedback GAN (FBGAN), to optimize the synthetic gene
sequences for desired properties using an external function analyzer. The
proposed architecture also has the advantage that the analyzer need not be
differentiable. We apply the feedback-loop mechanism to two examples: 1)
generating synthetic genes coding for antimicrobial peptides, and 2) optimizing
synthetic genes for the secondary structure of their resulting peptides. A
suite of metrics demonstrate that the GAN generated proteins have desirable
biophysical properties. The FBGAN architecture can also be used to optimize
GAN-generated datapoints for useful properties in domains beyond genomics.
",generative adversarial network gan represent attractive novel approach generate realistic datum gene protein drug synthetic biology apply gan generate synthetic dna sequence encode protein variable length propose novel feedback loop architecture call feedback gan fbgan optimize synthetic gene sequence desire property use external function analyzer propose architecture also advantage analyzer need differentiable apply feedback loop mechanism two example 1 generate synthetic gene code antimicrobial peptide 2 optimize synthetic gene secondary structure result peptide suite metric demonstrate gan generate protein desirable biophysical property fbgan architecture also use optimize gan generate datapoint useful property domain beyond genomic
Differentially Private Clustering: Tight Approximation Ratios,"  We study the task of differentially private clustering. For several basic
clustering problems, including Euclidean DensestBall, 1-Cluster, k-means, and
k-median, we give efficient differentially private algorithms that achieve
essentially the same approximation ratios as those that can be obtained by any
non-private algorithm, while incurring only small additive errors. This
improves upon existing efficient algorithms that only achieve some large
constant approximation factors.
  Our results also imply an improved algorithm for the Sample and Aggregate
privacy framework. Furthermore, we show that one of the tools used in our
1-Cluster algorithm can be employed to get a faster quantum algorithm for
ClosestPair in a moderate number of dimensions.
",study task differentially private cluster several basic cluster problem include euclidean densestball 1 cluster k mean k median give efficient differentially private algorithm achieve essentially approximation ratio obtain non private algorithm incur small additive error improve upon exist efficient algorithm achieve large constant approximation factor result also imply improve algorithm sample aggregate privacy framework furthermore show one tool use 1 cluster algorithm employ get fast quantum algorithm closestpair moderate number dimension
"ECO-AMLP: A Decision Support System using an Enhanced Class Outlier with
  Automatic Multilayer Perceptron for Diabetes Prediction","  With advanced data analytical techniques, efforts for more accurate decision
support systems for disease prediction are on rise. Surveys by World Health
Organization (WHO) indicate a great increase in number of diabetic patients and
related deaths each year. Early diagnosis of diabetes is a major concern among
researchers and practitioners. The paper presents an application of
\textit{Automatic Multilayer Perceptron }which\textit{ }is combined with an
outlier detection method \textit{Enhanced Class Outlier Detection using
distance based algorithm }to create a prediction framework named as Enhanced
Class Outlier with Automatic Multi layer Perceptron (ECO-AMLP). A series of
experiments are performed on publicly available Pima Indian Diabetes Dataset to
compare ECO-AMLP with other individual classifiers as well as ensemble based
methods. The outlier technique used in our framework gave better results as
compared to other pre-processing and classification techniques. Finally, the
results are compared with other state-of-the-art methods reported in literature
for diabetes prediction on PIDD and achieved accuracy of 88.7\% bests all other
reported studies.
",advanced datum analytical technique effort accurate decision support system disease prediction rise survey world health organization indicate great increase number diabetic patient relate death year early diagnosis diabete major concern among researcher practitioner paper present application automatic multilayer perceptron combine outlier detection method enhance class outlier detection use distance base algorithm create prediction framework name enhanced class outlier automatic multi layer perceptron eco amlp series experiment perform publicly available pima indian diabetes dataset compare eco amlp individual classifier well ensemble base method outlier technique use framework give well result compare pre processing classification technique finally result compare state of the art method report literature diabetes prediction pidd achieve accuracy best report study
Sharing pattern submodels for prediction with missing values,"Missing values are unavoidable in many applications of machine learning and
present a challenge both during training and at test time. When variables are
missing in recurring patterns, fitting separate pattern submodels have been
proposed as a solution. However, independent models do not make efficient use
of all available data. Conversely, fitting a shared model to the full data set
typically relies on imputation which may be suboptimal when missingness depends
on unobserved factors. We propose an alternative approach, called sharing
pattern submodels, which make predictions that are a) robust to missing values
at test time, b) maintains or improves the predictive power of pattern
submodels, and c) has a short description enabling improved interpretability.
We identify cases where sharing is provably optimal, even when missingness
itself is predictive and when the prediction target depends on unobserved
variables. Classification and regression experiments on synthetic data and two
healthcare data sets demonstrate that our models achieve a favorable trade-off
between pattern specialization and information sharing.",miss value unavoidable many application machine learn present challenge training test time variable miss recur pattern fitting separate pattern submodel propose solution however independent model make efficient use available datum conversely fitting share model full datum set typically rely imputation may suboptimal missingness depend unobserved factor propose alternative approach call sharing pattern submodel make prediction robust missing value test time b maintain improve predictive power pattern submodel c short description enable improved interpretability identify case share provably optimal even missingness predictive prediction target depend unobserved variable classification regression experiment synthetic datum two healthcare datum set demonstrate model achieve favorable trade off pattern specialization information sharing
"Subject Selection on a Riemannian Manifold for Unsupervised
  Cross-subject Seizure Detection","  Inter-subject variability between individuals poses a challenge in
inter-subject brain signal analysis problems. A new algorithm for
subject-selection based on clustering covariance matrices on a Riemannian
manifold is proposed. After unsupervised selection of the subsets of relevant
subjects, data in a cluster is mapped to a tangent space at the mean point of
covariance matrices in that cluster and an SVM classifier on labeled data from
relevant subjects is trained. Experiment on an EEG seizure database shows that
the proposed method increases the accuracy over state-of-the-art from 86.83% to
89.84% and specificity from 87.38% to 89.64% while reducing the false positive
rate/hour from 0.8/hour to 0.77/hour.
",inter subject variability individual pose challenge inter subject brain signal analysis problem new algorithm subject selection base clustering covariance matrix riemannian manifold propose unsupervised selection subset relevant subject datum cluster map tangent space mean point covariance matrix cluster svm classifier label datum relevant subject train experiment eeg seizure database show propose method increase accuracy state of the art specificity reduce false positive
Crit\`eres de qualit\'e d'un classifieur g\'en\'eraliste,"  This paper considers the problem of choosing a good classifier. For each
problem there exist an optimal classifier, but none are optimal, regarding the
error rate, in all cases. Because there exists a large number of classifiers, a
user would rather prefer an all-purpose classifier that is easy to adjust, in
the hope that it will do almost as good as the optimal. In this paper we
establish a list of criteria that a good generalist classifier should satisfy .
We first discuss data analytic, these criteria are presented. Six among the
most popular classifiers are selected and scored according to these criteria.
Tables allow to easily appreciate the relative values of each. In the end,
random forests turn out to be the best classifiers.
",paper consider problem choose good classifier problem exist optimal classifier none optimal regard error rate case exist large number classifier user would rather prefer all purpose classifier easy adjust hope almost good optimal paper establish list criterion good generalist classifier satisfy first discuss datum analytic criterion present six among popular classifier select score accord criterion table allow easily appreciate relative value end random forest turn good classifier
Revisiting Concentration of Missing Mass,"  We revisit the problem of \emph{missing mass concentration}, developing a new
method of estimating concentration of heterogenic sums, in spirit of celebrated
Rosenthal's inequality. As a result we slightly improve the state-of-art bounds
due to Ben-Hamou at al., and simplify the proofs.
",revisit problem miss mass concentration develop new method estimate concentration heterogenic sums spirit celebrate rosenthal inequality result slightly improve state of art bound due ben hamou simplify proof
"Relaxed Majorization-Minimization for Non-smooth and Non-convex
  Optimization","  We propose a new majorization-minimization (MM) method for non-smooth and
non-convex programs, which is general enough to include the existing MM
methods. Besides the local majorization condition, we only require that the
difference between the directional derivatives of the objective function and
its surrogate function vanishes when the number of iterations approaches
infinity, which is a very weak condition. So our method can use a surrogate
function that directly approximates the non-smooth objective function. In
comparison, all the existing MM methods construct the surrogate function by
approximating the smooth component of the objective function. We apply our
relaxed MM methods to the robust matrix factorization (RMF) problem with
different regularizations, where our locally majorant algorithm shows
advantages over the state-of-the-art approaches for RMF. This is the first
algorithm for RMF ensuring, without extra assumptions, that any limit point of
the iterates is a stationary point.
",propose new majorization minimization mm method non smooth non convex program general enough include exist mm method besides local majorization condition require difference directional derivative objective function surrogate function vanish number iteration approach infinity weak condition method use surrogate function directly approximate non smooth objective function comparison exist mm method construct surrogate function approximate smooth component objective function apply relaxed mm method robust matrix factorization rmf problem different regularization locally majorant algorithm show advantage state of the art approach rmf first algorithm rmf ensure without extra assumption limit point iterate stationary point
Vprop: Variational Inference using RMSprop,"  Many computationally-efficient methods for Bayesian deep learning rely on
continuous optimization algorithms, but the implementation of these methods
requires significant changes to existing code-bases. In this paper, we propose
Vprop, a method for Gaussian variational inference that can be implemented with
two minor changes to the off-the-shelf RMSprop optimizer. Vprop also reduces
the memory requirements of Black-Box Variational Inference by half. We derive
Vprop using the conjugate-computation variational inference method, and
establish its connections to Newton's method, natural-gradient methods, and
extended Kalman filters. Overall, this paper presents Vprop as a principled,
computationally-efficient, and easy-to-implement method for Bayesian deep
learning.
",many computationally efficient method bayesian deep learning rely continuous optimization algorithm implementation method require significant change exist code basis paper propose vprop method gaussian variational inference implement two minor change off the shelf rmsprop optimizer vprop also reduce memory requirement black box variational inference half derive vprop use conjugate computation variational inference method establish connection newton method natural gradient method extend kalman filter overall paper present vprop principle computationally efficient easy to implement method bayesian deep learning
"A jamming transition from under- to over-parametrization affects loss
  landscape and generalization","  We argue that in fully-connected networks a phase transition delimits the
over- and under-parametrized regimes where fitting can or cannot be achieved.
Under some general conditions, we show that this transition is sharp for the
hinge loss. In the whole over-parametrized regime, poor minima of the loss are
not encountered during training since the number of constraints to satisfy is
too small to hamper minimization. Our findings support a link between this
transition and the generalization properties of the network: as we increase the
number of parameters of a given model, starting from an under-parametrized
network, we observe that the generalization error displays three phases: (i)
initial decay, (ii) increase until the transition point --- where it displays a
cusp --- and (iii) slow decay toward a constant for the rest of the
over-parametrized regime. Thereby we identify the region where the classical
phenomenon of over-fitting takes place, and the region where the model keeps
improving, in line with previous empirical observations for modern neural
networks.
",argue fully connect network phase transition delimit over- under parametrize regime fitting achieve general condition show transition sharp hinge loss whole over parametrize regime poor minima loss encounter training since number constraint satisfy small hamper minimization finding support link transition generalization property network increase number parameter give model start under parametrize network observe generalization error display three phase initial decay ii increase transition point display cusp iii slow decay toward constant rest over parametrize regime thereby identify region classical phenomenon over fitting take place region model keep improve line previous empirical observation modern neural network
Meta Reinforcement Learning with Latent Variable Gaussian Processes,"  Learning from small data sets is critical in many practical applications
where data collection is time consuming or expensive, e.g., robotics, animal
experiments or drug design. Meta learning is one way to increase the data
efficiency of learning algorithms by generalizing learned concepts from a set
of training tasks to unseen, but related, tasks. Often, this relationship
between tasks is hard coded or relies in some other way on human expertise. In
this paper, we frame meta learning as a hierarchical latent variable model and
infer the relationship between tasks automatically from data. We apply our
framework in a model-based reinforcement learning setting and show that our
meta-learning model effectively generalizes to novel tasks by identifying how
new tasks relate to prior ones from minimal data. This results in up to a 60%
reduction in the average interaction time needed to solve tasks compared to
strong baselines.
",learn small datum set critical many practical application datum collection time consume expensive robotic animal experiment drug design meta learn one way increase datum efficiency learn algorithm generalizing learn concept set training task unseen relate task often relationship task hard code relie way human expertise paper frame meta learn hierarchical latent variable model infer relationship task automatically datum apply framework model base reinforcement learning set show meta learn model effectively generalize novel task identify new task relate prior one minimal datum result 60 reduction average interaction time need solve task compare strong baseline
Scalable Semi-Supervised Aggregation of Classifiers,"  We present and empirically evaluate an efficient algorithm that learns to
aggregate the predictions of an ensemble of binary classifiers. The algorithm
uses the structure of the ensemble predictions on unlabeled data to yield
significant performance improvements. It does this without making assumptions
on the structure or origin of the ensemble, without parameters, and as scalably
as linear learning. We empirically demonstrate these performance gains with
random forests.
",present empirically evaluate efficient algorithm learn aggregate prediction ensemble binary classifier algorithm use structure ensemble prediction unlabeled datum yield significant performance improvement without make assumption structure origin ensemble without parameter scalably linear learning empirically demonstrate performance gain random forest
"Community structure: A comparative evaluation of community detection
  methods","  Discovering community structure in complex networks is a mature field since a
tremendous number of community detection methods have been introduced in the
literature. Nevertheless, it is still very challenging for practioners to
determine which method would be suitable to get insights into the structural
information of the networks they study. Many recent efforts have been devoted
to investigating various quality scores of the community structure, but the
problem of distinguishing between different types of communities is still open.
In this paper, we propose a comparative, extensive and empirical study to
investigate what types of communities many state-of-the-art and well-known
community detection methods are producing. Specifically, we provide
comprehensive analyses on computation time, community size distribution, a
comparative evaluation of methods according to their optimisation schemes as
well as a comparison of their partioning strategy through validation metrics.
We process our analyses on a very large corpus of hundreds of networks from
five different network categories and propose ways to classify community
detection methods, helping a potential user to navigate the complex landscape
of community detection.
",discover community structure complex network mature field since tremendous number community detection method introduce literature nevertheless still challenge practioner determine method would suitable get insight structural information network study many recent effort devote investigate various quality score community structure problem distinguish different type community still open paper propose comparative extensive empirical study investigate type communitie many state of the art well know community detection method produce specifically provide comprehensive analysis computation time community size distribution comparative evaluation method accord optimisation scheme well comparison partione strategy validation metric process analyse large corpus hundred network five different network category propose way classify community detection method help potential user navigate complex landscape community detection
Optimizing Photonic Nanostructures via Multi-fidelity Gaussian Processes,"  We apply numerical methods in combination with finite-difference-time-domain
(FDTD) simulations to optimize transmission properties of plasmonic mirror
color filters using a multi-objective figure of merit over a five-dimensional
parameter space by utilizing novel multi-fidelity Gaussian processes approach.
We compare these results with conventional derivative-free global search
algorithms, such as (single-fidelity) Gaussian Processes optimization scheme,
and Particle Swarm Optimization---a commonly used method in nanophotonics
community, which is implemented in Lumerical commercial photonics software. We
demonstrate the performance of various numerical optimization approaches on
several pre-collected real-world datasets and show that by properly trading off
expensive information sources with cheap simulations, one can more effectively
optimize the transmission properties with a fixed budget.
",apply numerical method combination finite difference time domain fdtd simulation optimize transmission property plasmonic mirror color filter use multi objective figure merit five dimensional parameter space utilize novel multi fidelity gaussian process approach compare result conventional derivative free global search algorithm single fidelity gaussian process optimization scheme particle swarm optimization -a commonly use method nanophotonic community implement lumerical commercial photonic software demonstrate performance various numerical optimization approach several pre collected real world dataset show properly trade expensive information source cheap simulation one effectively optimize transmission property fix budget
Budgeted Reinforcement Learning in Continuous State Space,"  A Budgeted Markov Decision Process (BMDP) is an extension of a Markov
Decision Process to critical applications requiring safety constraints. It
relies on a notion of risk implemented in the shape of a cost signal
constrained to lie below an - adjustable - threshold. So far, BMDPs could only
be solved in the case of finite state spaces with known dynamics. This work
extends the state-of-the-art to continuous spaces environments and unknown
dynamics. We show that the solution to a BMDP is a fixed point of a novel
Budgeted Bellman Optimality operator. This observation allows us to introduce
natural extensions of Deep Reinforcement Learning algorithms to address
large-scale BMDPs. We validate our approach on two simulated applications:
spoken dialogue and autonomous driving.
",budget markov decision process bmdp extension markov decision process critical application require safety constraint rely notion risk implement shape cost signal constrain lie adjustable threshold far bmdps could solved case finite state space know dynamic work extend state of the art continuous space environment unknown dynamic show solution bmdp fix point novel budget bellman optimality operator observation allow we introduce natural extension deep reinforcement learning algorithm address large scale bmdps validate approach two simulate application speak dialogue autonomous driving
Robust Depth Completion with Uncertainty-Driven Loss Functions,"  Recovering a dense depth image from sparse LiDAR scans is a challenging task.
Despite the popularity of color-guided methods for sparse-to-dense depth
completion, they treated pixels equally during optimization, ignoring the
uneven distribution characteristics in the sparse depth map and the accumulated
outliers in the synthesized ground truth. In this work, we introduce
uncertainty-driven loss functions to improve the robustness of depth completion
and handle the uncertainty in depth completion. Specifically, we propose an
explicit uncertainty formulation for robust depth completion with Jeffrey's
prior. A parametric uncertain-driven loss is introduced and translated to new
loss functions that are robust to noisy or missing data. Meanwhile, we propose
a multiscale joint prediction model that can simultaneously predict depth and
uncertainty maps. The estimated uncertainty map is also used to perform
adaptive prediction on the pixels with high uncertainty, leading to a residual
map for refining the completion results. Our method has been tested on KITTI
Depth Completion Benchmark and achieved the state-of-the-art robustness
performance in terms of MAE, IMAE, and IRMSE metrics.
",recover dense depth image sparse lidar scan challenge task despite popularity color guide method sparse to dense depth completion treat pixel equally optimization ignore uneven distribution characteristic sparse depth map accumulate outlier synthesize ground truth work introduce uncertainty drive loss function improve robustness depth completion handle uncertainty depth completion specifically propose explicit uncertainty formulation robust depth completion jeffrey prior parametric uncertain drive loss introduce translate new loss function robust noisy miss datum meanwhile propose multiscale joint prediction model simultaneously predict depth uncertainty map estimate uncertainty map also use perform adaptive prediction pixel high uncertainty lead residual map refine completion result method test kitti depth completion benchmark achieve state of the art robustness performance term mae imae irmse metric
"Artificial Intelligence in the Battle against Coronavirus (COVID-19): A
  Survey and Future Research Directions","  Artificial intelligence (AI) has been applied widely in our daily lives in a
variety of ways with numerous success stories. AI has also contributed to
dealing with the coronavirus disease (COVID-19) pandemic, which has been
happening around the globe. This paper presents a survey of AI methods being
used in various applications in the fight against the COVID-19 outbreak and
outlines the crucial role of AI research in this unprecedented battle. We touch
on areas where AI plays as an essential component, from medical image
processing, data analytics, text mining and natural language processing, the
Internet of Things, to computational biology and medicine. A summary of
COVID-19 related data sources that are available for research purposes is also
presented. Research directions on exploring the potential of AI and enhancing
its capability and power in the pandemic battle are thoroughly discussed. We
identify 13 groups of problems related to the COVID-19 pandemic and highlight
promising AI methods and tools that can be used to address these problems. It
is envisaged that this study will provide AI researchers and the wider
community with an overview of the current status of AI applications, and
motivate researchers to harness AI's potential in the fight against COVID-19.
",artificial intelligence ai apply widely daily life variety way numerous success story ai also contribute deal coronavirus disease covid-19 pandemic happen around globe paper present survey ai method use various application fight covid-19 outbreak outline crucial role ai research unprecedented battle touch area ai play essential component medical image processing datum analytic text mining natural language processing internet thing computational biology medicine summary covid-19 relate data source available research purpose also present research direction explore potential ai enhance capability power pandemic battle thoroughly discuss identify 13 group problem relate covid-19 pandemic highlight promising ai method tool use address problem envisage study provide ai researcher wide community overview current status ai application motivate researcher harness ai potential fight covid-19
Sparse Methods for Automatic Relevance Determination,"  This work considers methods for imposing sparsity in Bayesian regression with
applications in nonlinear system identification. We first review automatic
relevance determination (ARD) and analytically demonstrate the need to
additional regularization or thresholding to achieve sparse models. We then
discuss two classes of methods, regularization based and thresholding based,
which build on ARD to learn parsimonious solutions to linear problems. In the
case of orthogonal covariates, we analytically demonstrate favorable
performance with regards to learning a small set of active terms in a linear
system with a sparse solution. Several example problems are presented to
compare the set of proposed methods in terms of advantages and limitations to
ARD in bases with hundreds of elements. The aim of this paper is to analyze and
understand the assumptions that lead to several algorithms and to provide
theoretical and empirical results so that the reader may gain insight and make
more informed choices regarding sparse Bayesian regression.
",work consider method impose sparsity bayesian regression application nonlinear system identification first review automatic relevance determination ard analytically demonstrate need additional regularization thresholding achieve sparse model discuss two class method regularization base thresholding base build ard learn parsimonious solution linear problem case orthogonal covariate analytically demonstrate favorable performance regard learn small set active term linear system sparse solution several example problem present compare set propose method term advantage limitation ard basis hundred element aim paper analyze understand assumption lead several algorithm provide theoretical empirical result reader may gain insight make informed choice regard sparse bayesian regression
Characterization of Overlap in Observational Studies,"  Overlap between treatment groups is required for non-parametric estimation of
causal effects. If a subgroup of subjects always receives the same
intervention, we cannot estimate the effect of intervention changes on that
subgroup without further assumptions. When overlap does not hold globally,
characterizing local regions of overlap can inform the relevance of causal
conclusions for new subjects, and can help guide additional data collection. To
have impact, these descriptions must be interpretable for downstream users who
are not machine learning experts, such as policy makers. We formalize overlap
estimation as a problem of finding minimum volume sets subject to coverage
constraints and reduce this problem to binary classification with Boolean rule
classifiers. We then generalize this method to estimate overlap in off-policy
policy evaluation. In several real-world applications, we demonstrate that
these rules have comparable accuracy to black-box estimators and provide
intuitive and informative explanations that can inform policy making.
",overlap treatment group require non parametric estimation causal effect subgroup subject always receive intervention estimate effect intervention change subgroup without assumption overlap hold globally characterize local region overlap inform relevance causal conclusion new subject help guide additional data collection impact description must interpretable downstream user machine learn expert policy maker formalize overlap estimation problem find minimum volume set subject coverage constraint reduce problem binary classification boolean rule classifier generalize method estimate overlap off policy policy evaluation several real world application demonstrate rule comparable accuracy black box estimator provide intuitive informative explanation inform policy making
Plug-in Approach to Active Learning,"  We present a new active learning algorithm based on nonparametric estimators
of the regression function. Our investigation provides probabilistic bounds for
the rates of convergence of the generalization error achievable by proposed
method over a broad class of underlying distributions. We also prove minimax
lower bounds which show that the obtained rates are almost tight.
",present new active learning algorithm base nonparametric estimator regression function investigation provide probabilistic bound rate convergence generalization error achievable propose method broad class underlie distribution also prove minimax low bound show obtain rate almost tight
"Neural network architectures using min plus algebra for solving certain
  high dimensional optimal control problems and Hamilton-Jacobi PDEs","  Solving high dimensional optimal control problems and corresponding
Hamilton-Jacobi PDEs are important but challenging problems in control
engineering. In this paper, we propose two abstract neural network
architectures which respectively represent the value function and the state
feedback characterisation of the optimal control for certain class of high
dimensional optimal control problems. We provide the mathematical analysis for
the two abstract architectures. We also show several numerical results computed
using the deep neural network implementations of these abstract architectures.
This work paves the way to leverage efficient dedicated hardware designed for
neural networks to solve high dimensional optimal control problems and
Hamilton-Jacobi PDEs.
",solve high dimensional optimal control problem correspond hamilton jacobi pde important challenging problem control engineering paper propose two abstract neural network architecture respectively represent value function state feedback characterisation optimal control certain class high dimensional optimal control problem provide mathematical analysis two abstract architecture also show several numerical result compute use deep neural network implementation abstract architecture work pave way leverage efficient dedicated hardware design neural network solve high dimensional optimal control problem hamilton jacobi pde
"SemanticCAP: Chromatin Accessibility Prediction Enhanced by Features
  Learning from a Language Model","  A large number of inorganic and organic compounds are able to bind DNA and
form complexes, among which drug-related molecules are important. Chromatin
accessibility changes not only directly affects drug-DNA interactions, but also
promote or inhibit the expression of critical genes associated with drug
resistance by affecting the DNA binding capacity of TFs and transcriptional
regulators. However, Biological experimental techniques for measuring it are
expensive and time consuming. In recent years, several kinds of computational
methods have been proposed to identify accessible regions of the genome.
Existing computational models mostly ignore the contextual information of bases
in gene sequences. To address these issues, we proposed a new solution named
SemanticCAP. It introduces a gene language model which models the context of
gene sequences, thus being able to provide an effective representation of a
certain site in gene sequences. Basically, we merge the features provided by
the gene language model into our chromatin accessibility model. During the
process, we designed some methods to make feature fusion smoother. Compared
with other systems under public benchmarks, our model proved to have better
performance.
",large number inorganic organic compound able bind dna form complex among drug relate molecule important chromatin accessibility change directly affect drug dna interaction also promote inhibit expression critical gene associate drug resistance affect dna bind capacity tfs transcriptional regulator however biological experimental technique measure expensive time consume recent year several kind computational method propose identify accessible region genome exist computational model mostly ignore contextual information basis gene sequence address issue propose new solution name semanticcap introduce gene language model model context gene sequence thus able provide effective representation certain site gene sequence basically merge feature provide gene language model chromatin accessibility model process design method make feature fusion smoother compare system public benchmark model prove well performance
Streaming Kernel PCA with $\tilde{O}(\sqrt{n})$ Random Features,"  We study the statistical and computational aspects of kernel principal
component analysis using random Fourier features and show that under mild
assumptions, $O(\sqrt{n} \log n)$ features suffices to achieve
$O(1/\epsilon^2)$ sample complexity. Furthermore, we give a memory efficient
streaming algorithm based on classical Oja's algorithm that achieves this rate.
",study statistical computational aspect kernel principal component analysis use random fouri feature show mild assumption n n feature suffice achieve sample complexity furthermore give memory efficient streaming algorithm base classical oja algorithm achieve rate
"Towards Understanding Adversarial Examples Systematically: Exploring
  Data Size, Task and Model Factors","  Most previous works usually explained adversarial examples from several
specific perspectives, lacking relatively integral comprehension about this
problem. In this paper, we present a systematic study on adversarial examples
from three aspects: the amount of training data, task-dependent and
model-specific factors. Particularly, we show that adversarial generalization
(i.e. test accuracy on adversarial examples) for standard training requires
more data than standard generalization (i.e. test accuracy on clean examples);
and uncover the global relationship between generalization and robustness with
respect to the data size especially when data is augmented by generative
models. This reveals the trade-off correlation between standard generalization
and robustness in limited training data regime and their consistency when data
size is large enough. Furthermore, we explore how different task-dependent and
model-specific factors influence the vulnerability of deep neural networks by
extensive empirical analysis. Relevant recommendations on defense against
adversarial attacks are provided as well. Our results outline a potential path
towards the luminous and systematic understanding of adversarial examples.
",previous work usually explain adversarial example several specific perspective lack relatively integral comprehension problem paper present systematic study adversarial example three aspect amount training datum task dependent model specific factor particularly show adversarial generalization test accuracy adversarial example standard training require datum standard generalization test accuracy clean example uncover global relationship generalization robustness respect datum size especially datum augment generative model reveal trade off correlation standard generalization robustness limited training datum regime consistency datum size large enough furthermore explore different task dependent model specific factor influence vulnerability deep neural network extensive empirical analysis relevant recommendation defense adversarial attack provide well result outline potential path towards luminous systematic understanding adversarial example
CoCon: A Self-Supervised Approach for Controlled Text Generation,"  Pretrained Transformer-based language models (LMs) display remarkable natural
language generation capabilities. With their immense potential, controlling
text generation of such LMs is getting attention. While there are studies that
seek to control high-level attributes (such as sentiment and topic) of
generated text, there is still a lack of more precise control over its content
at the word- and phrase-level. Here, we propose Content-Conditioner (CoCon) to
control an LM's output text with a content input, at a fine-grained level. In
our self-supervised approach, the CoCon block learns to help the LM complete a
partially-observed text sequence by conditioning with content inputs that are
withheld from the LM. Through experiments, we show that CoCon can naturally
incorporate target content into generated texts and control high-level text
attributes in a zero-shot manner.
",pretraine transformer base language model lm display remarkable natural language generation capabilitie immense potential control text generation lm get attention study seek control high level attribute sentiment topic generate text still lack precise control content word- phrase level propose content conditioner cocon control lm output text content input fine grain level self supervise approach cocon block learn help lm complete partially observe text sequence conditioning content input withhold lm experiment show cocon naturally incorporate target content generate text control high level text attribute zero shot manner
"Spatio-Temporal CNN baseline method for the Sports Video Task of
  MediaEval 2021 benchmark","  This paper presents the baseline method proposed for the Sports Video task
part of the MediaEval 2021 benchmark. This task proposes a stroke detection and
a stroke classification subtasks. This baseline addresses both subtasks. The
spatio-temporal CNN architecture and the training process of the model are
tailored according to the addressed subtask. The method has the purpose of
helping the participants to solve the task and is not meant to reach
stateof-the-art performance. Still, for the detection task, the baseline is
performing better than the other participants, which stresses the difficulty of
such a task.
",paper present baseline method propose sport video task part mediaeval 2021 benchmark task propose stroke detection stroke classification subtask baseline address subtask spatio temporal cnn architecture training process model tailor accord address subtask method purpose help participant solve task mean reach stateof the art performance still detection task baseline perform well participant stress difficulty task
Back to Basics: Deep Reinforcement Learning in Traffic Signal Control,"  In this paper we revisit some of the fundamental premises for a reinforcement
learning (RL) approach to self-learning traffic lights. We propose RLight, a
combination of choices that offers robust performance and good generalization
to unseen traffic flows. In particular, our main contributions are threefold:
our lightweight and cluster-aware state representation leads to improved
performance; we reformulate the Markov Decision Process (MDP) such that it
skips redundant timesteps of yellow light, speeding up learning by 30%; and we
investigate the action space and provide insight into the difference in
performance between acyclic and cyclic phase transitions. Additionally, we
provide insights into the generalisation of the methods to unseen traffic.
Evaluations using the real-world Hangzhou traffic dataset show that RLight
outperforms state-of-the-art rule-based and deep reinforcement learning
algorithms, demonstrating the potential of RL-based methods to improve urban
traffic flows.
",paper revisit fundamental premise reinforcement learning rl approach self learn traffic light propose rlight combination choice offer robust performance good generalization unseen traffic flow particular main contribution threefold lightweight cluster aware state representation lead improved performance reformulate markov decision process mdp skips redundant timestep yellow light speeding learn 30 investigate action space provide insight difference performance acyclic cyclic phase transition additionally provide insight generalisation method unseen traffic evaluation use real world hangzhou traffic dataset show rlight outperform state of the art rule base deep reinforcement learning algorithm demonstrate potential rl base method improve urban traffic flow
Plan Arithmetic: Compositional Plan Vectors for Multi-Task Control,"  Autonomous agents situated in real-world environments must be able to master
large repertoires of skills. While a single short skill can be learned quickly,
it would be impractical to learn every task independently. Instead, the agent
should share knowledge across behaviors such that each task can be learned
efficiently, and such that the resulting model can generalize to new tasks,
especially ones that are compositions or subsets of tasks seen previously. A
policy conditioned on a goal or demonstration has the potential to share
knowledge between tasks if it sees enough diversity of inputs. However, these
methods may not generalize to a more complex task at test time. We introduce
compositional plan vectors (CPVs) to enable a policy to perform compositions of
tasks without additional supervision. CPVs represent trajectories as the sum of
the subtasks within them. We show that CPVs can be learned within a one-shot
imitation learning framework without any additional supervision or information
about task hierarchy, and enable a demonstration-conditioned policy to
generalize to tasks that sequence twice as many skills as the tasks seen during
training.
  Analogously to embeddings such as word2vec in NLP, CPVs can also support
simple arithmetic operations -- for example, we can add the CPVs for two
different tasks to command an agent to compose both tasks, without any
additional training.
",autonomous agent situate real world environment must able master large repertoire skill single short skill learn quickly would impractical learn every task independently instead agent share knowledge across behavior task learn efficiently result model generalize new task especially one composition subset task see previously policy condition goal demonstration potential share knowledge task see enough diversity input however method may generalize complex task test time introduce compositional plan vector cpvs enable policy perform composition task without additional supervision cpvs represent trajectory sum subtask within show cpvs learn within one shot imitation learn framework without additional supervision information task hierarchy enable demonstration condition policy generalize task sequence twice many skill task see training analogously embedding word2vec nlp cpvs also support simple arithmetic operation example add cpvs two different task command agent compose task without additional training
"Model-based actor-critic: GAN (model generator) + DRL (actor-critic) =>
  AGI","  Our effort is toward unifying GAN and DRL algorithms into a unifying AI model
(AGI or general-purpose AI or artificial general intelligence which has
general-purpose applications to: (A) offline learning (of stored data) like GAN
in (un/semi-/fully-)SL setting such as big data analytics (mining) and
visualization; (B) online learning (of real or simulated devices) like DRL in
RL setting (with/out environment reward) such as (real or simulated) robotics
and control; Our core proposal is adding an (generative/predictive) environment
model to the actor-critic (model-free) architecture which results in a
model-based actor-critic architecture with temporal-differencing (TD) error and
an episodic memory. The proposed AI model is similar to (model-free) DDPG and
therefore it's called model-based DDPG. To evaluate it, we compare it with
(model-free) DDPG by applying them both to a variety (wide range) of
independent simulated robotic and control task environments in OpenAI Gym and
Unity Agents. Our initial limited experiments show that DRL and GAN in
model-based actor-critic results in an incremental goal-driven intellignce
required to solve each task with similar performance to (model-free) DDPG. Our
future focus is to investigate the proposed AI model potential to: (A) unify
DRL field inside AI by producing competitive performance compared to the best
of model-based (PlaNet) and model-free (D4PG) approaches; (B) bridge the gap
between AI and robotics communities by solving the important problem of reward
engineering with learning the reward function by demonstration.
",effort toward unify gin drl algorithm unifying ai model agi general purpose ai artificial general intelligence general purpose application offline learning store datum like gin sl set big data analytic mining visualization b online learn real simulate device like drl rl set environment reward real simulate robotic control core proposal add environment model actor critic model free architecture result model base actor critic architecture temporal difference td error episodic memory propose ai model similar model free ddpg therefore call model base ddpg evaluate compare model free ddpg apply variety wide range independent simulate robotic control task environment openai gym unity agent initial limited experiment show drl gin model base actor critic result incremental goal drive intellignce require solve task similar performance model free ddpg future focus investigate propose ai model potential unify drl field inside ai produce competitive performance compare good model base planet model free d4pg approach b bridge gap ai robotic community solve important problem reward engineering learn reward function demonstration
"Background-Foreground Segmentation for Interior Sensing in Automotive
  Industry","  To ensure safety in automated driving, the correct perception of the
situation inside the car is as important as its environment. Thus, seat
occupancy detection and classification of detected instances play an important
role in interior sensing. By the knowledge of the seat occupancy status, it is
possible to, e.g., automate the airbag deployment control. Furthermore, the
presence of a driver, which is necessary for partially automated driving cars
at the automation levels two to four can be verified. In this work, we compare
different statistical methods from the field of image segmentation to approach
the problem of background-foreground segmentation in camera based interior
sensing. In the recent years, several methods based on different techniques
have been developed and applied to images or videos from different
applications. The peculiarity of the given scenarios of interior sensing is,
that the foreground instances and the background both contain static as well as
dynamic elements. In data considered in this work, even the camera position is
not completely fixed. We review and benchmark three different methods ranging,
i.e., Gaussian Mixture Models (GMM), Morphological Snakes and a deep neural
network, namely a Mask R-CNN. In particular, the limitations of the classical
methods, GMM and Morphological Snakes, for interior sensing are shown.
Furthermore, it turns, that it is possible to overcome these limitations by
deep learning, e.g.\ using a Mask R-CNN. Although only a small amount of ground
truth data was available for training, we enabled the Mask R-CNN to produce
high quality background-foreground masks via transfer learning. Moreover, we
demonstrate that certain augmentation as well as pre- and post-processing
methods further enhance the performance of the investigated methods.
",ensure safety automate drive correct perception situation inside car important environment thus seat occupancy detection classification detect instance play important role interior sense knowledge seat occupancy status possible automate airbag deployment control furthermore presence driver necessary partially automate driving car automation level two four verified work compare different statistical method field image segmentation approach problem background foreground segmentation camera base interior sense recent year several method base different technique develop apply image video different application peculiarity give scenario interior sense foreground instance background contain static well dynamic element datum consider work even camera position completely fix review benchmark three different method range gaussian mixture model gmm morphological snake deep neural network namely mask r cnn particular limitation classical method gmm morphological snake interior sensing show furthermore turn possible overcome limitation deep learning use mask r cnn although small amount ground truth datum available training enable mask r cnn produce high quality background foreground mask via transfer learning moreover demonstrate certain augmentation well pre- post processing method enhance performance investigate method
Detecting Underspecification with Local Ensembles,"  We present local ensembles, a method for detecting underspecification -- when
many possible predictors are consistent with the training data and model class
-- at test time in a pre-trained model. Our method uses local second-order
information to approximate the variance of predictions across an ensemble of
models from the same class. We compute this approximation by estimating the
norm of the component of a test point's gradient that aligns with the
low-curvature directions of the Hessian, and provide a tractable method for
estimating this quantity. Experimentally, we show that our method is capable of
detecting when a pre-trained model is underspecified on test data, with
applications to out-of-distribution detection, detecting spurious correlates,
and active learning.
",present local ensemble method detect underspecification many possible predictor consistent training datum model class test time pre trained model method use local second order information approximate variance prediction across ensemble model class compute approximation estimate norm component test point gradient align low curvature direction hessian provide tractable method estimate quantity experimentally show method capable detect pre train model underspecifie test datum application out of distribution detection detect spurious correlate active learning
Federated Unsupervised Representation Learning,"  To leverage enormous unlabeled data on distributed edge devices, we formulate
a new problem in federated learning called Federated Unsupervised
Representation Learning (FURL) to learn a common representation model without
supervision while preserving data privacy. FURL poses two new challenges: (1)
data distribution shift (Non-IID distribution) among clients would make local
models focus on different categories, leading to the inconsistency of
representation spaces. (2) without the unified information among clients in
FURL, the representations across clients would be misaligned. To address these
challenges, we propose Federated Constrastive Averaging with dictionary and
alignment (FedCA) algorithm. FedCA is composed of two key modules: (1)
dictionary module to aggregate the representations of samples from each client
and share with all clients for consistency of representation space and (2)
alignment module to align the representation of each client on a base model
trained on a public data. We adopt the contrastive loss for local model
training. Through extensive experiments with three evaluation protocols in IID
and Non-IID settings, we demonstrate that FedCA outperforms all baselines with
significant margins.
",leverage enormous unlabeled datum distribute edge device formulate new problem federate learning call federate unsupervised representation learn furl learn common representation model without supervision preserve datum privacy furl pose two new challenge 1 datum distribution shift non iid distribution among client would make local model focus different category lead inconsistency representation space 2 without unified information among client furl representation across client would misaligned address challenge propose federate constrastive averaging dictionary alignment fedca algorithm fedca compose two key module 1 dictionary module aggregate representation sample client share client consistency representation space 2 alignment module align representation client base model train public datum adopt contrastive loss local model train extensive experiment three evaluation protocol iid non iid setting demonstrate fedca outperform baseline significant margin
Revocable Federated Learning: A Benchmark of Federated Forest,"  A learning federation is composed of multiple participants who use the
federated learning technique to collaboratively train a machine learning model
without directly revealing the local data. Nevertheless, the existing federated
learning frameworks have a serious defect that even a participant is revoked,
its data are still remembered by the trained model. In a company-level
cooperation, allowing the remaining companies to use a trained model that
contains the memories from a revoked company is obviously unacceptable, because
it can lead to a big conflict of interest. Therefore, we emphatically discuss
the participant revocation problem of federated learning and design a revocable
federated random forest (RF) framework, RevFRF, to further illustrate the
concept of revocable federated learning. In RevFRF, we first define the
security problems to be resolved by a revocable federated RF. Then, a suite of
homomorphic encryption based secure protocols are designed for federated RF
construction, prediction and revocation. Through theoretical analysis and
experiments, we show that the protocols can securely and efficiently implement
collaborative training of an RF and ensure that the memories of a revoked
participant in the trained RF are securely removed.
",learn federation compose multiple participant use federated learning technique collaboratively train machine learning model without directly reveal local datum nevertheless exist federate learning framework serious defect even participant revoke datum still remember train model company level cooperation allow remaining company use train model contain memory revoke company obviously unacceptable lead big conflict interest therefore emphatically discuss participant revocation problem federate learn design revocable federate random forest rf framework revfrf illustrate concept revocable federate learn revfrf first define security problem resolve revocable federate rf suite homomorphic encryption base secure protocol design federate rf construction prediction revocation theoretical analysis experiment show protocol securely efficiently implement collaborative training rf ensure memory revoke participant train rf securely remove
Learning ODE Models with Qualitative Structure Using Gaussian Processes,"  Recent advances in learning techniques have enabled the modelling of
dynamical systems for scientific and engineering applications directly from
data. However, in many contexts explicit data collection is expensive and
learning algorithms must be data-efficient to be feasible. This suggests using
additional qualitative information about the system, which is often available
from prior experiments or domain knowledge. We propose an approach to learning
a vector field of differential equations using sparse Gaussian Processes that
allows us to combine data and additional structural information, like Lie Group
symmetries and fixed points. We show that this combination improves
extrapolation performance and long-term behaviour significantly, while also
reducing the computational cost.
",recent advance learn technique enable model dynamical system scientific engineering application directly datum however many context explicit data collection expensive learning algorithm must data efficient feasible suggest use additional qualitative information system often available prior experiment domain knowledge propose approach learn vector field differential equation use sparse gaussian process allow we combine datum additional structural information like lie group symmetrie fix point show combination improve extrapolation performance long term behaviour significantly also reduce computational cost
"Emergent Complexity and Zero-shot Transfer via Unsupervised Environment
  Design","  A wide range of reinforcement learning (RL) problems - including robustness,
transfer learning, unsupervised RL, and emergent complexity - require
specifying a distribution of tasks or environments in which a policy will be
trained. However, creating a useful distribution of environments is error
prone, and takes a significant amount of developer time and effort. We propose
Unsupervised Environment Design (UED) as an alternative paradigm, where
developers provide environments with unknown parameters, and these parameters
are used to automatically produce a distribution over valid, solvable
environments. Existing approaches to automatically generating environments
suffer from common failure modes: domain randomization cannot generate
structure or adapt the difficulty of the environment to the agent's learning
progress, and minimax adversarial training leads to worst-case environments
that are often unsolvable. To generate structured, solvable environments for
our protagonist agent, we introduce a second, antagonist agent that is allied
with the environment-generating adversary. The adversary is motivated to
generate environments which maximize regret, defined as the difference between
the protagonist and antagonist agent's return. We call our technique
Protagonist Antagonist Induced Regret Environment Design (PAIRED). Our
experiments demonstrate that PAIRED produces a natural curriculum of
increasingly complex environments, and PAIRED agents achieve higher zero-shot
transfer performance when tested in highly novel environments.
",wide range reinforcement learning rl problem include robustness transfer learn unsupervised rl emergent complexity require specify distribution task environment policy train however create useful distribution environment error prone take significant amount developer time effort propose unsupervised environment design ued alternative paradigm developer provide environment unknown parameter parameter use automatically produce distribution valid solvable environment exist approach automatically generate environment suffer common failure mode domain randomization generate structure adapt difficulty environment agent learn progress minimax adversarial training lead bad case environment often unsolvable generate structure solvable environment protagonist agent introduce second antagonist agent ally environment generate adversary adversary motivated generate environment maximize regret define difference protagonist antagonist agent return call technique protagonist antagonist induce regret environment design pair experiment demonstrate pair produce natural curriculum increasingly complex environment pair agent achieve high zero shot transfer performance test highly novel environment
A Human-Grounded Evaluation of SHAP for Alert Processing,"  In the past years, many new explanation methods have been proposed to achieve
interpretability of machine learning predictions. However, the utility of these
methods in practical applications has not been researched extensively. In this
paper we present the results of a human-grounded evaluation of SHAP, an
explanation method that has been well-received in the XAI and related
communities. In particular, we study whether this local model-agnostic
explanation method can be useful for real human domain experts to assess the
correctness of positive predictions, i.e. alerts generated by a classifier. We
performed experimentation with three different groups of participants (159 in
total), who had basic knowledge of explainable machine learning. We performed a
qualitative analysis of recorded reflections of experiment participants
performing alert processing with and without SHAP information. The results
suggest that the SHAP explanations do impact the decision-making process,
although the model's confidence score remains to be a leading source of
evidence. We statistically test whether there is a significant difference in
task utility metrics between tasks for which an explanation was available and
tasks in which it was not provided. As opposed to common intuitions, we did not
find a significant difference in alert processing performance when a SHAP
explanation is available compared to when it is not.
",past year many new explanation method propose achieve interpretability machine learn prediction however utility method practical application research extensively paper present result human ground evaluation shap explanation method well receive xai relate community particular study whether local model agnostic explanation method useful real human domain expert assess correctness positive prediction alert generate classifier perform experimentation three different group participant 159 total basic knowledge explainable machine learn perform qualitative analysis record reflection experiment participant perform alert processing without shap information result suggest shap explanation impact decision make process although model confidence score remain lead source evidence statistically test whether significant difference task utility metric task explanation available task provide opposed common intuition find significant difference alert processing performance shap explanation available compare
"Demystifying Deep Learning in Predictive Spatio-Temporal Analytics: An
  Information-Theoretic Framework","  Deep learning has achieved incredible success over the past years, especially
in various challenging predictive spatio-temporal analytics (PSTA) tasks, such
as disease prediction, climate forecast, and traffic prediction, where
intrinsic dependency relationships among data exist and generally manifest at
multiple spatio-temporal scales. However, given a specific PSTA task and the
corresponding dataset, how to appropriately determine the desired configuration
of a deep learning model, theoretically analyze the model's learning behavior,
and quantitatively characterize the model's learning capacity remains a
mystery. In order to demystify the power of deep learning for PSTA, in this
paper, we provide a comprehensive framework for deep learning model design and
information-theoretic analysis. First, we develop and demonstrate a novel
interactively- and integratively-connected deep recurrent neural network
(I$^2$DRNN) model. I$^2$DRNN consists of three modules: an Input module that
integrates data from heterogeneous sources; a Hidden module that captures the
information at different scales while allowing the information to flow
interactively between layers; and an Output module that models the integrative
effects of information from various hidden layers to generate the output
predictions. Second, to theoretically prove that our designed model can learn
multi-scale spatio-temporal dependency in PSTA tasks, we provide an
information-theoretic analysis to examine the information-based learning
capacity (i-CAP) of the proposed model. Third, to validate the I$^2$DRNN model
and confirm its i-CAP, we systematically conduct a series of experiments
involving both synthetic datasets and real-world PSTA tasks. The experimental
results show that the I$^2$DRNN model outperforms both classical and
state-of-the-art models, and is able to capture meaningful multi-scale
spatio-temporal dependency.
",deep learning achieve incredible success past year especially various challenging predictive spatio temporal analytic psta task disease prediction climate forecast traffic prediction intrinsic dependency relationship among datum exist generally manif multiple spatio temporal scale however give specific psta task correspond dataset appropriately determine desire configuration deep learning model theoretically analyze model learn behavior quantitatively characterize model learning capacity remain mystery order demystify power deep learn psta paper provide comprehensive framework deep learning model design information theoretic analysis first develop demonstrate novel interactively- integratively connect deep recurrent neural network drnn model drnn consist three module input module integrate datum heterogeneous source hide module capture information different scale allow information flow interactively layer output module model integrative effect information various hide layer generate output prediction second theoretically prove design model learn multi scale spatio temporal dependency psta task provide information theoretic analysis examine information base learning capacity i cap propose model third validate drnn model confirm i cap systematically conduct series experiment involve synthetic dataset real world psta task experimental result show drnn model outperform classical state of the art model able capture meaningful multi scale spatio temporal dependency
"Extended UCB Policy for Multi-Armed Bandit with Light-Tailed Reward
  Distributions","  We consider the multi-armed bandit problems in which a player aims to accrue
reward by sequentially playing a given set of arms with unknown reward
statistics. In the classic work, policies were proposed to achieve the optimal
logarithmic regret order for some special classes of light-tailed reward
distributions, e.g., Auer et al.'s UCB1 index policy for reward distributions
with finite support. In this paper, we extend Auer et al.'s UCB1 index policy
to achieve the optimal logarithmic regret order for all light-tailed (or
equivalently, locally sub-Gaussian) reward distributions defined by the (local)
existence of the moment-generating function.
",consider multi armed bandit problem player aim accrue reward sequentially play give set arm unknown reward statistic classic work policy propose achieve optimal logarithmic regret order special class light tail reward distribution auer et al ucb1 index policy reward distribution finite support paper extend auer et al ucb1 index policy achieve optimal logarithmic regret order light tail equivalently locally sub gaussian reward distribution define local existence moment generate function
"A Unified Approximation Framework for Compressing and Accelerating Deep
  Neural Networks","  Deep neural networks (DNNs) have achieved significant success in a variety of
real world applications, i.e., image classification. However, tons of
parameters in the networks restrict the efficiency of neural networks due to
the large model size and the intensive computation. To address this issue,
various approximation techniques have been investigated, which seek for a light
weighted network with little performance degradation in exchange of smaller
model size or faster inference. Both low-rankness and sparsity are appealing
properties for the network approximation. In this paper we propose a unified
framework to compress the convolutional neural networks (CNNs) by combining
these two properties, while taking the nonlinear activation into consideration.
Each layer in the network is approximated by the sum of a structured sparse
component and a low-rank component, which is formulated as an optimization
problem. Then, an extended version of alternating direction method of
multipliers (ADMM) with guaranteed convergence is presented to solve the
relaxed optimization problem. Experiments are carried out on VGG-16, AlexNet
and GoogLeNet with large image classification datasets. The results outperform
previous work in terms of accuracy degradation, compression rate and speedup
ratio. The proposed method is able to remarkably compress the model (with up to
4.9x reduction of parameters) at a cost of little loss or without loss on
accuracy.
",deep neural network dnn achieve significant success variety real world application image classification however ton parameter network restrict efficiency neural network due large model size intensive computation address issue various approximation technique investigate seek light weight network little performance degradation exchange small model size fast inference low rankness sparsity appeal property network approximation paper propose unified framework compress convolutional neural network cnn combine two property take nonlinear activation consideration layer network approximate sum structure sparse component low rank component formulate optimization problem extend version alternate direction method multiplier admm guarantee convergence present solve relaxed optimization problem experiment carry vgg-16 alexnet googlenet large image classification dataset result outperform previous work term accuracy degradation compression rate speedup ratio propose method able remarkably compress model reduction parameter cost little loss without loss accuracy
"A simple coding for cross-domain matching with dimension reduction via
  spectral graph embedding","  Data vectors are obtained from multiple domains. They are feature vectors of
images or vector representations of words. Domains may have different numbers
of data vectors with different dimensions. These data vectors from multiple
domains are projected to a common space by linear transformations in order to
search closely related vectors across domains. We would like to find projection
matrices to minimize distances between closely related data vectors. This
formulation of cross-domain matching is regarded as an extension of the
spectral graph embedding to multi-domain setting, and it includes several
multivariate analysis methods of statistics such as multiset canonical
correlation analysis, correspondence analysis, and principal component
analysis. Similar approaches are very popular recently in pattern recognition
and vision. In this paper, instead of proposing a novel method, we will
introduce an embarrassingly simple idea of coding the data vectors for
explaining all the above mentioned approaches. A data vector is concatenated
with zero vectors from all other domains to make an augmented vector. The
cross-domain matching is solved by applying the single-domain version of
spectral graph embedding to these augmented vectors of all the domains. An
interesting connection to the classical associative memory model of neural
networks is also discussed by noticing a coding for association. A
cross-validation method for choosing the dimension of the common space and a
regularization parameter will be discussed in an illustrative numerical
example.
",data vector obtain multiple domain feature vector image vector representation word domain may different number datum vector different dimension datum vector multiple domain project common space linear transformation order search closely relate vector across domain would like find projection matrix minimize distance closely relate data vector formulation cross domain matching regard extension spectral graph embed multi domain setting include several multivariate analysis method statistic multiset canonical correlation analysis correspondence analysis principal component analysis similar approach popular recently pattern recognition vision paper instead propose novel method introduce embarrassingly simple idea code datum vector explain mention approach data vector concatenate zero vector domain make augment vector cross domain matching solve apply single domain version spectral graph embed augment vector domain interesting connection classical associative memory model neural network also discuss notice code association cross validation method choose dimension common space regularization parameter discuss illustrative numerical example
Optimistic Temporal Difference Learning for 2048,"  Temporal difference (TD) learning and its variants, such as multistage TD
(MS-TD) learning and temporal coherence (TC) learning, have been successfully
applied to 2048. These methods rely on the stochasticity of the environment of
2048 for exploration. In this paper, we propose to employ optimistic
initialization (OI) to encourage exploration for 2048, and empirically show
that the learning quality is significantly improved. This approach
optimistically initializes the feature weights to very large values. Since
weights tend to be reduced once the states are visited, agents tend to explore
those states which are unvisited or visited few times. Our experiments show
that both TD and TC learning with OI significantly improve the performance. As
a result, the network size required to achieve the same performance is
significantly reduced. With additional tunings such as expectimax search,
multistage learning, and tile-downgrading technique, our design achieves the
state-of-the-art performance, namely an average score of 625 377 and a rate of
72% reaching 32768 tiles. In addition, for sufficiently large tests, 65536
tiles are reached at a rate of 0.02%.
",temporal difference td learn variant multistage td ms td learn temporal coherence tc learning successfully apply method rely stochasticity environment 2048 exploration paper propose employ optimistic initialization oi encourage exploration 2048 empirically show learn quality significantly improve approach optimistically initialize feature weight large value since weight tend reduce state visit agent tend explore state unvisite visit time experiment show td tc learning oi significantly improve performance result network size require achieve performance significantly reduce additional tuning expectimax search multistage learn tile downgrade technique design achieve state of the art performance namely average score 625 377 rate 72 reach 32768 tile addition sufficiently large test 65536 tile reach rate
"Model-based Deep Reinforcement Learning for Dynamic Portfolio
  Optimization","  Dynamic portfolio optimization is the process of sequentially allocating
wealth to a collection of assets in some consecutive trading periods, based on
investors' return-risk profile. Automating this process with machine learning
remains a challenging problem. Here, we design a deep reinforcement learning
(RL) architecture with an autonomous trading agent such that, investment
decisions and actions are made periodically, based on a global objective, with
autonomy. In particular, without relying on a purely model-free RL agent, we
train our trading agent using a novel RL architecture consisting of an infused
prediction module (IPM), a generative adversarial data augmentation module
(DAM) and a behavior cloning module (BCM). Our model-based approach works with
both on-policy or off-policy RL algorithms. We further design the back-testing
and execution engine which interact with the RL agent in real time. Using
historical {\em real} financial market data, we simulate trading with practical
constraints, and demonstrate that our proposed model is robust, profitable and
risk-sensitive, as compared to baseline trading strategies and model-free RL
agents from prior work.
",dynamic portfolio optimization process sequentially allocate wealth collection asset consecutive trading period base investor return risk profile automate process machine learning remain challenge problem design deep reinforcement learning rl architecture autonomous trading agent investment decision action make periodically base global objective autonomy particular without rely purely model free rl agent train trading agent use novel rl architecture consist infuse prediction module ipm generative adversarial datum augmentation module dam behavior clone module bcm model base approach work on policy off policy rl algorithm design back testing execution engine interact rl agent real time use historical real financial market datum simulate trading practical constraint demonstrate propose model robust profitable risk sensitive compare baseline trading strategy model free rl agent prior work
"Semi-Supervised Nonlinear Distance Metric Learning via Forests of
  Max-Margin Cluster Hierarchies","  Metric learning is a key problem for many data mining and machine learning
applications, and has long been dominated by Mahalanobis methods. Recent
advances in nonlinear metric learning have demonstrated the potential power of
non-Mahalanobis distance functions, particularly tree-based functions. We
propose a novel nonlinear metric learning method that uses an iterative,
hierarchical variant of semi-supervised max-margin clustering to construct a
forest of cluster hierarchies, where each individual hierarchy can be
interpreted as a weak metric over the data. By introducing randomness during
hierarchy training and combining the output of many of the resulting
semi-random weak hierarchy metrics, we can obtain a powerful and robust
nonlinear metric model. This method has two primary contributions: first, it is
semi-supervised, incorporating information from both constrained and
unconstrained points. Second, we take a relaxed approach to constraint
satisfaction, allowing the method to satisfy different subsets of the
constraints at different levels of the hierarchy rather than attempting to
simultaneously satisfy all of them. This leads to a more robust learning
algorithm. We compare our method to a number of state-of-the-art benchmarks on
$k$-nearest neighbor classification, large-scale image retrieval and
semi-supervised clustering problems, and find that our algorithm yields results
comparable or superior to the state-of-the-art, and is significantly more
robust to noise.
",metric learning key problem many datum mining machine learning application long dominate mahalanobis method recent advance nonlinear metric learning demonstrate potential power non mahalanobis distance function particularly tree base function propose novel nonlinear metric learning method use iterative hierarchical variant semi supervised max margin clustering construct forest cluster hierarchies individual hierarchy interpret weak metric datum introduce randomness hierarchy training combine output many result semi random weak hierarchy metric obtain powerful robust nonlinear metric model method two primary contribution first semi supervised incorporate information constrain unconstrained point second take relaxed approach constraint satisfaction allow method satisfy different subset constraint different level hierarchy rather attempt simultaneously satisfy lead robust learning algorithm compare method number state of the art benchmark k -near neighbor classification large scale image retrieval semi supervised cluster problem find algorithm yield result comparable superior state of the art significantly robust noise
Occam's Gates,"  We present a complimentary objective for training recurrent neural networks
(RNN) with gating units that helps with regularization and interpretability of
the trained model. Attention-based RNN models have shown success in many
difficult sequence to sequence classification problems with long and short term
dependencies, however these models are prone to overfitting. In this paper, we
describe how to regularize these models through an L1 penalty on the activation
of the gating units, and show that this technique reduces overfitting on a
variety of tasks while also providing to us a human-interpretable visualization
of the inputs used by the network. These tasks include sentiment analysis,
paraphrase recognition, and question answering.
",present complimentary objective training recurrent neural network rnn gate unit help regularization interpretability train model attention base rnn model show success many difficult sequence sequence classification problem long short term dependencie however model prone overfitte paper describe regularize model l1 penalty activation gate unit show technique reduce overfitte variety task also provide we human interpretable visualization input use network task include sentiment analysis paraphrase recognition question answer
"Zero-shot Learning and Knowledge Transfer in Music Classification and
  Tagging","  Music classification and tagging is conducted through categorical supervised
learning with a fixed set of labels. In principle, this cannot make predictions
on unseen labels. Zero-shot learning is an approach to solve the problem by
using side information about the semantic labels. We recently investigated this
concept of zero-shot learning in music classification and tagging task by
projecting both audio and label space on a single semantic space. In this work,
we extend the work to verify the generalization ability of zero-shot learning
model by conducting knowledge transfer to different music corpora.
",music classification tagging conduct categorical supervise learning fix set label principle make prediction unseen label zero shot learn approach solve problem use side information semantic label recently investigate concept zero shot learn music classification tag task project audio label space single semantic space work extend work verify generalization ability zero shot learning model conduct knowledge transfer different music corpora
"Understanding Human Context in 3D Scenes by Learning Spatial Affordances
  with Virtual Skeleton Models","  Robots are often required to operate in environments where humans are not
present, but yet require the human context information for better human-robot
interaction. Even when humans are present in the environment, detecting their
presence in cluttered environments could be challenging. As a solution to this
problem, this paper presents the concept of spatial affordance map which learns
human context by looking at geometric features of the environment. Instead of
observing real humans to learn human context, it uses virtual human models and
their relationships with the environment to map hidden human affordances in 3D
scenes by placing virtual skeleton models in 3D scenes with their confidence
values. The spatial affordance map learning problem is formulated as a
multi-label classification problem that can be learned using Support Vector
Machine (SVM) based learners. Experiments carried out in a real 3D scene
dataset recorded promising results and proved the applicability of
affordance-map for mapping human context.
",robot often require operate environment human present yet require human context information well human robot interaction even human present environment detect presence cluttered environment could challenge solution problem paper present concept spatial affordance map learn human context look geometric feature environment instead observe real human learn human context use virtual human model relationship environment map hide human affordance 3d scene place virtual skeleton model 3d scene confidence value spatial affordance map learn problem formulate multi label classification problem learn use support vector machine svm base learner experiment carry real 3d scene dataset record promising result prove applicability affordance map mapping human context
"DarwinML: A Graph-based Evolutionary Algorithm for Automated Machine
  Learning","  As an emerging field, Automated Machine Learning (AutoML) aims to reduce or
eliminate manual operations that require expertise in machine learning. In this
paper, a graph-based architecture is employed to represent flexible
combinations of ML models, which provides a large searching space compared to
tree-based and stacking-based architectures. Based on this, an evolutionary
algorithm is proposed to search for the best architecture, where the mutation
and heredity operators are the key for architecture evolution. With Bayesian
hyper-parameter optimization, the proposed approach can automate the workflow
of machine learning. On the PMLB dataset, the proposed approach shows the
state-of-the-art performance compared with TPOT, Autostacker, and auto-sklearn.
Some of the optimized models are with complex structures which are difficult to
obtain in manual design.
",emerge field automate machine learn automl aim reduce eliminate manual operation require expertise machine learn paper graph base architecture employ represent flexible combination ml model provide large search space compare tree base stacking base architecture base evolutionary algorithm propose search good architecture mutation heredity operator key architecture evolution bayesian hyper parameter optimization propose approach automate workflow machine learning pmlb dataset propose approach show state of the art performance compare tpot autostacker auto sklearn optimize model complex structure difficult obtain manual design
"Online Learning with Cumulative Oversampling: Application to Budgeted
  Influence Maximization","  We propose a cumulative oversampling (CO) method for online learning. Our key
idea is to sample parameter estimations from the updated belief space once in
each round (similar to Thompson Sampling), and utilize the cumulative samples
up to the current round to construct optimistic parameter estimations that
asymptotically concentrate around the true parameters as tighter upper
confidence bounds compared to the ones constructed with standard UCB methods.
We apply CO to a novel budgeted variant of the Influence Maximization (IM)
semi-bandits with linear generalization of edge weights, whose offline problem
is NP-hard. Combining CO with the oracle we design for the offline problem, our
online learning algorithm simultaneously tackles budget allocation, parameter
learning, and reward maximization. We show that for IM semi-bandits, our
CO-based algorithm achieves a scaled regret comparable to that of the UCB-based
algorithms in theory, and performs on par with Thompson Sampling in numerical
experiments.
",propose cumulative oversampling co method online learn key idea sample parameter estimation update belief space round similar thompson sampling utilize cumulative sample current round construct optimistic parameter estimation asymptotically concentrate around true parameter tight upper confidence bound compare one construct standard ucb method apply co novel budget variant influence maximization I m semi bandit linear generalization edge weight whose offline problem np hard combining co oracle design offline problem online learning algorithm simultaneously tackle budget allocation parameter learn reward maximization show I m semi bandit co base algorithm achieve scale regret comparable ucb base algorithm theory perform par thompson sample numerical experiment
DNN Speaker Tracking with Embeddings,"  In multi-speaker applications is common to have pre-computed models from
enrolled speakers. Using these models to identify the instances in which these
speakers intervene in a recording is the task of speaker tracking. In this
paper, we propose a novel embedding-based speaker tracking method.
Specifically, our design is based on a convolutional neural network that mimics
a typical speaker verification PLDA (probabilistic linear discriminant
analysis) classifier and finds the regions uttered by the target speakers in an
online fashion. The system was studied from two different perspectives:
diarization and tracking; results on both show a significant improvement over
the PLDA baseline under the same experimental conditions. Two standard public
datasets, CALLHOME and DIHARD II single channel, were modified to create
two-speaker subsets with overlapping and non-overlapping regions. We evaluate
the robustness of our supervised approach with models generated from different
segment lengths. A relative improvement of 17% in DER for DIHARD II single
channel shows promising performance. Furthermore, to make the baseline system
similar to speaker tracking, non-target speakers were added to the recordings.
Even in these adverse conditions, our approach is robust enough to outperform
the PLDA baseline.
",multi speaker application common pre computed model enrol speaker use model identify instance speaker intervene recording task speaker tracking paper propose novel embed base speaker tracking method specifically design base convolutional neural network mimic typical speaker verification plda probabilistic linear discriminant analysis classifier find region utter target speaker online fashion system study two different perspective diarization tracking result show significant improvement plda baseline experimental condition two standard public dataset callhome dihard ii single channel modify create two speaker subset overlap non overlapping region evaluate robustness supervised approach model generate different segment length relative improvement 17 der dihard ii single channel show promising performance furthermore make baseline system similar speaker track non target speaker add recording even adverse condition approach robust enough outperform plda baseline
On Margins and Generalisation for Voting Classifiers,"We study the generalisation properties of majority voting on finite ensembles
of classifiers, proving margin-based generalisation bounds via the PAC-Bayes
theory. These provide state-of-the-art guarantees on a number of classification
tasks. Our central results leverage the Dirichlet posteriors studied recently
by Zantedeschi et al. [2021] for training voting classifiers; in contrast to
that work our bounds apply to non-randomised votes via the use of margins. Our
contributions add perspective to the debate on the ""margins theory"" proposed by
Schapire et al. [1998] for the generalisation of ensemble classifiers.",study generalisation property majority vote finite ensemble classifier prove margin base generalisation bound via pac bayes theory provide state of the art guarantee number classification task central result leverage dirichlet posterior study recently zantedeschi et al 2021 training voting classifier contrast work bound apply non randomised vote via use margin contribution add perspective debate margin theory propose schapire et al 1998 generalisation ensemble classifier
On Disentangled and Locally Fair Representations,"  We study the problem of performing classification in a manner that is fair
for sensitive groups, such as race and gender. This problem is tackled through
the lens of disentangled and locally fair representations. We learn a locally
fair representation, such that, under the learned representation, the
neighborhood of each sample is balanced in terms of the sensitive attribute.
For instance, when a decision is made to hire an individual, we ensure that the
$K$ most similar hired individuals are racially balanced. Crucially, we ensure
that similar individuals are found based on attributes not correlated to their
race. To this end, we disentangle the embedding space into two representations.
The first of which is correlated with the sensitive attribute while the second
is not. We apply our local fairness objective only to the second, uncorrelated,
representation. Through a set of experiments, we demonstrate the necessity of
both disentangled and local fairness for obtaining fair and accurate
representations. We evaluate our method on real-world settings such as
predicting income and re-incarceration rate and demonstrate the advantage of
our method.
",study problem perform classification manner fair sensitive group race gender problem tackled lens disentangle locally fair representation learn locally fair representation learn representation neighborhood sample balanced term sensitive attribute instance decision make hire individual ensure k similar hire individual racially balance crucially ensure similar individual find base attribute correlate race end disentangle embed space two representation first correlate sensitive attribute second apply local fairness objective second uncorrelated representation set experiment demonstrate necessity disentangle local fairness obtain fair accurate representation evaluate method real world setting predict income re incarceration rate demonstrate advantage method
"Efficient, Noise-Tolerant, and Private Learning via Boosting","  We introduce a simple framework for designing private boosting algorithms. We
give natural conditions under which these algorithms are differentially
private, efficient, and noise-tolerant PAC learners. To demonstrate our
framework, we use it to construct noise-tolerant and private PAC learners for
large-margin halfspaces whose sample complexity does not depend on the
dimension.
  We give two sample complexity bounds for our large-margin halfspace learner.
One bound is based only on differential privacy, and uses this guarantee as an
asset for ensuring generalization. This first bound illustrates a general
methodology for obtaining PAC learners from privacy, which may be of
independent interest. The second bound uses standard techniques from the theory
of large-margin classification (the fat-shattering dimension) to match the best
known sample complexity for differentially private learning of large-margin
halfspaces, while additionally tolerating random label noise.
",introduce simple framework design private boosting algorithm give natural condition algorithm differentially private efficient noise tolerant pac learner demonstrate framework use construct noise tolerant private pac learner large margin halfspace whose sample complexity depend dimension give two sample complexity bound large margin halfspace learner one bind base differential privacy use guarantee asset ensure generalization first bind illustrate general methodology obtain pac learner privacy may independent interest second bind use standard technique theory large margin classification fat shatter dimension match well know sample complexity differentially private learn large margin halfspace additionally tolerate random label noise
Fast Wavenet Generation Algorithm,"  This paper presents an efficient implementation of the Wavenet generation
process called Fast Wavenet. Compared to a naive implementation that has
complexity O(2^L) (L denotes the number of layers in the network), our proposed
approach removes redundant convolution operations by caching previous
calculations, thereby reducing the complexity to O(L) time. Timing experiments
show significant advantages of our fast implementation over a naive one. While
this method is presented for Wavenet, the same scheme can be applied anytime
one wants to perform autoregressive generation or online prediction using a
model with dilated convolution layers. The code for our method is publicly
available.
",paper present efficient implementation wavenet generation process call fast wavenet compare naive implementation complexity l denote number layer network propose approach remove redundant convolution operation cache previous calculation thereby reduce complexity l time time experiment show significant advantage fast implementation naive one method present wavenet scheme apply anytime one want perform autoregressive generation online prediction use model dilate convolution layer code method publicly available
Kernel absolute summability is only sufficient for RKHS stability,"  Regularized approaches have been successfully applied to linear system
identification in recent years. Many of them model unknown impulse responses
exploiting the so called Reproducing Kernel Hilbert spaces (RKHSs) that enjoy
the notable property of being in one-to-one correspondence with the class of
positive semidefinite kernels. The necessary and sufficient condition for a
RKHS to be stable, i.e. to contain only BIBO stable linear dynamic systems, has
been known in the literature at least since 2006. However, an open question
still persists and concerns the equivalence of such condition with the absolute
summability of the kernel. This paper provides a definite answer to this matter
by proving that such correspondence does not hold. A counterexample is
introduced that illustrates the existence of stable RKHSs that are induced by
non-absolutely summable kernels.
",regularize approach successfully apply linear system identification recent year many model unknown impulse response exploit call reproduce kernel hilbert space rkhss enjoy notable property one to one correspondence class positive semidefinite kernel necessary sufficient condition rkhs stable contain bibo stable linear dynamic system know literature least since however open question still persist concern equivalence condition absolute summability kernel paper provide definite answer matter prove correspondence hold counterexample introduce illustrate existence stable rkhss induce non absolutely summable kernel
Explanation-Guided Training for Cross-Domain Few-Shot Classification,"  Cross-domain few-shot classification task (CD-FSC) combines few-shot
classification with the requirement to generalize across domains represented by
datasets. This setup faces challenges originating from the limited labeled data
in each class and, additionally, from the domain shift between training and
test sets. In this paper, we introduce a novel training approach for existing
FSC models. It leverages on the explanation scores, obtained from existing
explanation methods when applied to the predictions of FSC models, computed for
intermediate feature maps of the models. Firstly, we tailor the layer-wise
relevance propagation (LRP) method to explain the predictions of FSC models.
Secondly, we develop a model-agnostic explanation-guided training strategy that
dynamically finds and emphasizes the features which are important for the
predictions. Our contribution does not target a novel explanation method but
lies in a novel application of explanations for the training phase. We show
that explanation-guided training effectively improves the model generalization.
We observe improved accuracy for three different FSC models: RelationNet, cross
attention network, and a graph neural network-based formulation, on five
few-shot learning datasets: miniImagenet, CUB, Cars, Places, and Plantae. The
source code is available at https://github.com/SunJiamei/few-shot-lrp-guided
",cross domain few shot classification task cd fsc combine few shot classification requirement generalize across domain represent dataset setup face challenge originate limited label datum class additionally domain shift training test set paper introduce novel training approach exist fsc model leverage explanation score obtain exist explanation method apply prediction fsc model compute intermediate feature map model firstly tailor layer wise relevance propagation lrp method explain prediction fsc model secondly develop model agnostic explanation guide training strategy dynamically find emphasize feature important prediction contribution target novel explanation method lie novel application explanation training phase show explanation guide training effectively improve model generalization observe improve accuracy three different fsc model relationnet cross attention network graph neural network base formulation five few shot learning dataset miniimagenet cub car place plantae source code available https
OneAligner: Zero-shot Cross-lingual Transfer with One Rich-Resource Language Pair for Low-Resource Sentence Retrieval,"Aligning parallel sentences in multilingual corpora is essential to curating
data for downstream applications such as Machine Translation. In this work, we
present OneAligner, an alignment model specially designed for sentence
retrieval tasks. This model is able to train on only one language pair and
transfers, in a cross-lingual fashion, to low-resource language pairs with
negligible degradation in performance. When trained with all language pairs of
a large-scale parallel multilingual corpus (OPUS-100), this model achieves the
state-of-the-art result on the Tateoba dataset, outperforming an equally-sized
previous model by 8.0 points in accuracy while using less than 0.6% of their
parallel data. When finetuned on a single rich-resource language pair, be it
English-centered or not, our model is able to match the performance of the ones
finetuned on all language pairs under the same data budget with less than 2.0
points decrease in accuracy. Furthermore, with the same setup, scaling up the
number of rich-resource language pairs monotonically improves the performance,
reaching a minimum of 0.4 points discrepancy in accuracy, making it less
mandatory to collect any low-resource parallel data. Finally, we conclude
through empirical results and analyses that the performance of the sentence
alignment task depends mostly on the monolingual and parallel data size, up to
a certain size threshold, rather than on what language pairs are used for
training or evaluation.",align parallel sentence multilingual corpora essential curate datum downstream application machine translation work present onealigner alignment model specially design sentence retrieval task model able train one language pair transfer cross lingual fashion low resource language pair negligible degradation performance train language pair large scale parallel multilingual corpus opus-100 model achieve state of the art result tateoba dataset outperform equally sized previous model point accuracy use less parallel datum finetune single rich resource language pair english center model able match performance one finetune language pair datum budget less point decrease accuracy furthermore setup scale number rich resource language pair monotonically improve performance reach minimum point discrepancy accuracy make less mandatory collect low resource parallel datum finally conclude empirical result analyse performance sentence alignment task depend mostly monolingual parallel datum size certain size threshold rather language pair use training evaluation
Impression Space from Deep Template Network,"  It is an innate ability for humans to imagine something only according to
their impression, without having to memorize all the details of what they have
seen. In this work, we would like to demonstrate that a trained convolutional
neural network also has the capability to ""remember"" its input images. To
achieve this, we propose a simple but powerful framework to establish an
{\emph{Impression Space}} upon an off-the-shelf pretrained network. This
network is referred to as the {\emph{Template Network}} because its filters
will be used as templates to reconstruct images from the impression. In our
framework, the impression space and image space are bridged by a layer-wise
encoding and iterative decoding process. It turns out that the impression space
indeed captures the salient features from images, and it can be directly
applied to tasks such as unpaired image translation and image synthesis through
impression matching without further network training. Furthermore, the
impression naturally constructs a high-level common space for different data.
Based on this, we propose a mechanism to model the data relations inside the
impression space, which is able to reveal the feature similarity between
images. Our code will be released.
",innate ability human imagine something accord impression without memorize detail see work would like demonstrate train convolutional neural network also capability remember input image achieve propose simple powerful framework establish impression space upon off the shelf pretraine network network refer template network filter use template reconstruct image impression framework impression space image space bridge layer wise encode iterative decode process turn impression space indeed capture salient feature image directly apply task unpaire image translation image synthesis impression matching without network training furthermore impression naturally construct high level common space different datum base propose mechanism model data relation inside impression space able reveal feature similarity image code release
"Bayesian Sparse Tucker Models for Dimension Reduction and Tensor
  Completion","  Tucker decomposition is the cornerstone of modern machine learning on
tensorial data analysis, which have attracted considerable attention for
multiway feature extraction, compressive sensing, and tensor completion. The
most challenging problem is related to determination of model complexity (i.e.,
multilinear rank), especially when noise and missing data are present. In
addition, existing methods cannot take into account uncertainty information of
latent factors, resulting in low generalization performance. To address these
issues, we present a class of probabilistic generative Tucker models for tensor
decomposition and completion with structural sparsity over multilinear latent
space. To exploit structural sparse modeling, we introduce two group sparsity
inducing priors by hierarchial representation of Laplace and Student-t
distributions, which facilitates fully posterior inference. For model learning,
we derived variational Bayesian inferences over all model (hyper)parameters,
and developed efficient and scalable algorithms based on multilinear
operations. Our methods can automatically adapt model complexity and infer an
optimal multilinear rank by the principle of maximum lower bound of model
evidence. Experimental results and comparisons on synthetic, chemometrics and
neuroimaging data demonstrate remarkable performance of our models for
recovering ground-truth of multilinear rank and missing entries.
",tucker decomposition cornerstone modern machine learn tensorial datum analysis attract considerable attention multiway feature extraction compressive sense tensor completion challenge problem relate determination model complexity multilinear rank especially noise miss datum present addition exist method take account uncertainty information latent factor result low generalization performance address issue present class probabilistic generative tucker model tensor decomposition completion structural sparsity multilinear latent space exploit structural sparse modeling introduce two group sparsity induce prior hierarchial representation laplace student t distribution facilitate fully posterior inference model learning derive variational bayesian inference model hyper parameter develop efficient scalable algorithm base multilinear operation method automatically adapt model complexity infer optimal multilinear rank principle maximum lower bind model evidence experimental result comparison synthetic chemometric neuroimage datum demonstrate remarkable performance model recover ground truth multilinear rank miss entry
"A hybrid virtual sensing approach for approximating non-linear dynamic
  system behavior using LSTM networks","  Modern Internet of Things solutions are used in a variety of different areas,
ranging from connected vehicles and healthcare to industrial applications. They
rely on a large amount of interconnected sensors, which can lead to both
technical and economical challenges. Virtual sensing techniques aim to reduce
the number of physical sensors in a system by using data from available
measurements to estimate additional unknown quantities of interest. Successful
model-based solutions include Kalman filters or the combination of finite
element models and modal analysis, while many data-driven methods rely on
machine learning algorithms. The presented hybrid virtual sensing approach
combines Long Short-Term Memory networks with frequency response function
models in order to estimate the behavior of non-linear dynamic systems with
multiple input and output channels. Network training and prediction make use of
short signal subsequences, which are later recombined by applying a windowing
technique. The frequency response function model acts as a baseline estimate
which perfectly captures linear dynamic systems and is augmented by the
non-linear Long Short-Term Memory network following two different hybrid
modeling strategies. The approach is tested using a non-linear experimental
dataset, which results from measurements of a three-component servo-hydraulic
fatigue test bench. A variety of metrics in time and frequency domains, as well
as fatigue strength under variable amplitudes are used to evaluate the
approximation quality of the proposed method. In addition to virtual sensing,
the algorithm is also applied to a forward prediction task. Synthetic data are
used in a separate study to estimate the prediction quality on datasets of
different size.
",modern internet thing solution use variety different area range connected vehicle healthcare industrial application rely large amount interconnect sensor lead technical economical challenge virtual sense technique aim reduce number physical sensor system use datum available measurement estimate additional unknown quantity interest successful model base solution include kalman filter combination finite element model modal analysis many data drive method rely machine learning algorithm present hybrid virtual sense approach combine long short term memory network frequency response function model order estimate behavior non linear dynamic system multiple input output channel network training prediction make use short signal subsequence later recombine apply windowe technique frequency response function model act baseline estimate perfectly capture linear dynamic system augment non linear long short term memory network follow two different hybrid modeling strategy approach test use non linear experimental dataset result measurement three component servo hydraulic fatigue test bench variety metric time frequency domain well fatigue strength variable amplitude use evaluate approximation quality propose method addition virtual sensing algorithm also apply forward prediction task synthetic datum use separate study estimate prediction quality dataset different size
"Jury Learning: Integrating Dissenting Voices into Machine Learning
  Models","  Whose labels should a machine learning (ML) algorithm learn to emulate? For
ML tasks ranging from online comment toxicity to misinformation detection to
medical diagnosis, different groups in society may have irreconcilable
disagreements about ground truth labels. Supervised ML today resolves these
label disagreements implicitly using majority vote, which overrides minority
groups' labels. We introduce jury learning, a supervised ML approach that
resolves these disagreements explicitly through the metaphor of a jury:
defining which people or groups, in what proportion, determine the classifier's
prediction. For example, a jury learning model for online toxicity might
centrally feature women and Black jurors, who are commonly targets of online
harassment. To enable jury learning, we contribute a deep learning architecture
that models every annotator in a dataset, samples from annotators' models to
populate the jury, then runs inference to classify. Our architecture enables
juries that dynamically adapt their composition, explore counterfactuals, and
visualize dissent.
",whose label machine learn ml algorithm learn emulate ml task range online comment toxicity misinformation detection medical diagnosis different group society may irreconcilable disagreement ground truth label supervise ml today resolve label disagreement implicitly use majority vote override minority group label introduce jury learning supervise ml approach resolve disagreement explicitly metaphor jury define people group proportion determine classifi prediction example jury learn model online toxicity might centrally feature woman black juror commonly target online harassment enable jury learning contribute deep learn architecture model every annotator dataset sample annotator model populate jury run inference classify architecture enable jury dynamically adapt composition explore counterfactual visualize dissent
"CUP: A Conservative Update Policy Algorithm for Safe Reinforcement
  Learning","  Safe reinforcement learning (RL) is still very challenging since it requires
the agent to consider both return maximization and safe exploration. In this
paper, we propose CUP, a Conservative Update Policy algorithm with a
theoretical safety guarantee. We derive the CUP based on the new proposed
performance bounds and surrogate functions. Although using bounds as surrogate
functions to design safe RL algorithms have appeared in some existing works, we
develop them at least three aspects: (i) We provide a rigorous theoretical
analysis to extend the surrogate functions to generalized advantage estimator
(GAE). GAE significantly reduces variance empirically while maintaining a
tolerable level of bias, which is an efficient step for us to design CUP; (ii)
The proposed bounds are tighter than existing works, i.e., using the proposed
bounds as surrogate functions are better local approximations to the objective
and safety constraints. (iii) The proposed CUP provides a non-convex
implementation via first-order optimizers, which does not depend on any convex
approximation. Finally, extensive experiments show the effectiveness of CUP
where the agent satisfies safe constraints. We have opened the source code of
CUP at https://github.com/RL-boxes/Safe-RL.
",safe reinforcement learning rl still challenge since require agent consider return maximization safe exploration paper propose cup conservative update policy algorithm theoretical safety guarantee derive cup base new propose performance bound surrogate function although use bound surrogate function design safe rl algorithm appear exist work develop least three aspect provide rigorous theoretical analysis extend surrogate function generalize advantage estimator gae gae significantly reduce variance empirically maintain tolerable level bias efficient step us design cup ii propose bound tight exist work use propose bound surrogate function well local approximation objective safety constraint iii propose cup provide non convex implementation via first order optimizer depend convex approximation finally extensive experiment show effectiveness cup agent satisfie safe constraint open source code cup https
Robust priors for regularized regression,"  Induction benefits from useful priors. Penalized regression approaches, like
ridge regression, shrink weights toward zero but zero association is usually
not a sensible prior. Inspired by simple and robust decision heuristics humans
use, we constructed non-zero priors for penalized regression models that
provide robust and interpretable solutions across several tasks. Our approach
enables estimates from a constrained model to serve as a prior for a more
general model, yielding a principled way to interpolate between models of
differing complexity. We successfully applied this approach to a number of
decision and classification problems, as well as analyzing simulated brain
imaging data. Models with robust priors had excellent worst-case performance.
Solutions followed from the form of the heuristic that was used to derive the
prior. These new algorithms can serve applications in data analysis and machine
learning, as well as help in understanding how people transition from novice to
expert performance.
",induction benefit useful prior penalize regression approach like ridge regression shrink weight toward zero zero association usually sensible prior inspire simple robust decision heuristic human use construct non zero prior penalize regression model provide robust interpretable solution across several task approach enable estimate constrain model serve prior general model yield principle way interpolate model differ complexity successfully apply approach number decision classification problem well analyze simulated brain imaging datum model robust prior excellent bad case performance solution follow form heuristic use derive prior new algorithm serve application datum analysis machine learning well help understand people transition novice expert performance
Barely Biased Learning for Gaussian Process Regression,"  Recent work in scalable approximate Gaussian process regression has discussed
a bias-variance-computation trade-off when estimating the log marginal
likelihood. We suggest a method that adaptively selects the amount of
computation to use when estimating the log marginal likelihood so that the bias
of the objective function is guaranteed to be small. While simple in principle,
our current implementation of the method is not competitive computationally
with existing approximations.
",recent work scalable approximate gaussian process regression discuss bias variance computation trade off estimating log marginal likelihood suggest method adaptively select amount computation use estimate log marginal likelihood bias objective function guarantee small simple principle current implementation method competitive computationally exist approximation
Efficient entity-based reinforcement learning,"  Recent deep reinforcement learning (DRL) successes rely on end-to-end
learning from fixed-size observational inputs (e.g. image, state-variables).
However, many challenging and interesting problems in decision making involve
observations or intermediary representations which are best described as a set
of entities: either the image-based approach would miss small but important
details in the observations (e.g. ojects on a radar, vehicles on satellite
images, etc.), the number of sensed objects is not fixed (e.g. robotic
manipulation), or the problem simply cannot be represented in a meaningful way
as an image (e.g. power grid control, or logistics). This type of structured
representations is not directly compatible with current DRL architectures,
however, there has been an increase in machine learning techniques directly
targeting structured information, potentially addressing this issue. We propose
to combine recent advances in set representations with slot attention and graph
neural networks to process structured data, broadening the range of
applications of DRL algorithms. This approach allows to address entity-based
problems in an efficient and scalable way. We show that it can improve training
time and robustness significantly, and demonstrate their potential to handle
structured as well as purely visual domains, on multiple environments from the
Atari Learning Environment and Simple Playgrounds.
",recent deep reinforcement learn drl success rely end to end learn fix size observational input image state variable however many challenging interesting problem decision making involve observation intermediary representation well describe set entity either image base approach would miss small important detail observation oject radar vehicle satellite image etc number sense object fix robotic manipulation problem simply represent meaningful way image power grid control logistics type structure representation directly compatible current drl architecture however increase machine learn technique directly target structured information potentially address issue propose combine recent advance set representation slot attention graph neural network process structure datum broaden range application drl algorithm approach allow address entity base problem efficient scalable way show improve training time robustness significantly demonstrate potential handle structure well purely visual domain multiple environment atari learn environment simple playground
"An Efficient and Effective Second-Order Training Algorithm for
  LSTM-based Adaptive Learning","  We study adaptive (or online) nonlinear regression with
Long-Short-Term-Memory (LSTM) based networks, i.e., LSTM-based adaptive
learning. In this context, we introduce an efficient Extended Kalman filter
(EKF) based second-order training algorithm. Our algorithm is truly online,
i.e., it does not assume any underlying data generating process and future
information, except that the target sequence is bounded. Through an extensive
set of experiments, we demonstrate significant performance gains achieved by
our algorithm with respect to the state-of-the-art methods. Here, we mainly
show that our algorithm consistently provides 10 to 45\% improvement in the
accuracy compared to the widely-used adaptive methods Adam, RMSprop, and DEKF,
and comparable performance to EKF with a 10 to 15 times reduction in the
run-time.
",study adaptive online nonlinear regression long short term memory lstm base network lstm base adaptive learn context introduce efficient extend kalman filter ekf base second order training algorithm algorithm truly online assume underlie data generating process future information except target sequence bound extensive set experiment demonstrate significant performance gain achieve algorithm respect state of the art method mainly show algorithm consistently provide 10 improvement accuracy compare widely use adaptive method adam rmsprop dekf comparable performance ekf 10 15 time reduction run time
"Bi-directional LSTM Recurrent Neural Network for Chinese Word
  Segmentation","  Recurrent neural network(RNN) has been broadly applied to natural language
processing(NLP) problems. This kind of neural network is designed for modeling
sequential data and has been testified to be quite efficient in sequential
tagging tasks. In this paper, we propose to use bi-directional RNN with long
short-term memory(LSTM) units for Chinese word segmentation, which is a crucial
preprocess task for modeling Chinese sentences and articles. Classical methods
focus on designing and combining hand-craft features from context, whereas
bi-directional LSTM network(BLSTM) does not need any prior knowledge or
pre-designing, and it is expert in keeping the contextual information in both
directions. Experiment result shows that our approach gets state-of-the-art
performance in word segmentation on both traditional Chinese datasets and
simplified Chinese datasets.
",recurrent neural network rnn broadly apply natural language processing nlp problem kind neural network design model sequential datum testify quite efficient sequential tagging task paper propose use bi directional rnn long short term memory lstm unit chinese word segmentation crucial preprocess task model chinese sentence article classical method focus design combine hand craft feature context whereas bi directional lstm network blstm need prior knowledge pre designing expert keep contextual information direction experiment result show approach get state of the art performance word segmentation traditional chinese dataset simplify chinese dataset
"A Two-Stage Federated Transfer Learning Framework in Medical Images
  Classification on Limited Data: A COVID-19 Case Study","  COVID-19 pandemic has spread rapidly and caused a shortage of global medical
resources. The efficiency of COVID-19 diagnosis has become highly significant.
As deep learning and convolutional neural network (CNN) has been widely
utilized and been verified in analyzing medical images, it has become a
powerful tool for computer-assisted diagnosis. However, there are two most
significant challenges in medical image classification with the help of deep
learning and neural networks, one of them is the difficulty of acquiring enough
samples, which may lead to model overfitting. Privacy concerns mainly bring the
other challenge since medical-related records are often deemed patients'
private information and protected by laws such as GDPR and HIPPA. Federated
learning can ensure the model training is decentralized on different devices
and no data is shared among them, which guarantees privacy. However, with data
located on different devices, the accessible data of each device could be
limited. Since transfer learning has been verified in dealing with limited data
with good performance, therefore, in this paper, We made a trial to implement
federated learning and transfer learning techniques using CNNs to classify
COVID-19 using lung CT scans. We also explored the impact of dataset
distribution at the client-side in federated learning and the number of
training epochs a model is trained. Finally, we obtained very high performance
with federated learning, demonstrating our success in leveraging accuracy and
privacy.
",covid-19 pandemic spread rapidly cause shortage global medical resource efficiency covid-19 diagnosis become highly significant deep learning convolutional neural network cnn widely utilize verify analyze medical image become powerful tool computer assist diagnosis however two significant challenge medical image classification help deep learning neural network one difficulty acquire enough sample may lead model overfitte privacy concern mainly bring challenge since medical relate record often deem patient private information protect law gdpr hippa federate learning ensure model training decentralize different device datum share among guarantee privacy however datum locate different device accessible data device could limit since transfer learn verify deal limited datum good performance therefore paper make trial implement federate learn transfer learn technique use cnn classify covid-19 use lung ct scan also explore impact dataset distribution client side federate learning number training epoch model train finally obtain high performance federate learning demonstrate success leverage accuracy privacy
FedRel: An Adaptive Federated Relevance Framework for Spatial Temporal Graph Learning,"Spatial-temporal data contains rich information and has been widely studied
in recent years due to the rapid development of relevant applications in many
fields. For instance, medical institutions often use electrodes attached to
different parts of a patient to analyse the electorencephal data rich with
spatial and temporal features for health assessment and disease diagnosis.
Existing research has mainly used deep learning techniques such as
convolutional neural network (CNN) or recurrent neural network (RNN) to extract
hidden spatial-temporal features. Yet, it is challenging to incorporate both
inter-dependencies spatial information and dynamic temporal changes
simultaneously. In reality, for a model that leverages these spatial-temporal
features to fulfil complex prediction tasks, it often requires a colossal
amount of training data in order to obtain satisfactory model performance.
Considering the above-mentioned challenges, we propose an adaptive federated
relevance framework, namely FedRel, for spatial-temporal graph learning in this
paper. After transforming the raw spatial-temporal data into high quality
features, the core Dynamic Inter-Intra Graph (DIIG) module in the framework is
able to use these features to generate the spatial-temporal graphs capable of
capturing the hidden topological and long-term temporal correlation information
in these graphs. To improve the model generalization ability and performance
while preserving the local data privacy, we also design a relevance-driven
federated learning module in our framework to leverage diverse data
distributions from different participants with attentive aggregations of their
models.",spatial temporal datum contain rich information widely study recent year due rapid development relevant application many field instance medical institution often use electrode attach different part patient analyse electorencephal datum rich spatial temporal feature health assessment disease diagnosis exist research mainly use deep learning technique convolutional neural network cnn recurrent neural network rnn extract hide spatial temporal feature yet challenging incorporate inter dependencie spatial information dynamic temporal change simultaneously reality model leverage spatial temporal feature fulfil complex prediction task often require colossal amount training datum order obtain satisfactory model performance consider above mention challenge propose adaptive federate relevance framework namely fedrel spatial temporal graph learn paper transform raw spatial temporal datum high quality features core dynamic inter intra graph diig module framework able use feature generate spatial temporal graph capable capture hidden topological long term temporal correlation information graph improve model generalization ability performance preserve local datum privacy also design relevance drive federate learn module framework leverage diverse data distribution different participant attentive aggregation model
A Strongly Quasiconvex PAC-Bayesian Bound,"  We propose a new PAC-Bayesian bound and a way of constructing a hypothesis
space, so that the bound is convex in the posterior distribution and also
convex in a trade-off parameter between empirical performance of the posterior
distribution and its complexity. The complexity is measured by the
Kullback-Leibler divergence to a prior. We derive an alternating procedure for
minimizing the bound. We show that the bound can be rewritten as a
one-dimensional function of the trade-off parameter and provide sufficient
conditions under which the function has a single global minimum. When the
conditions are satisfied the alternating minimization is guaranteed to converge
to the global minimum of the bound. We provide experimental results
demonstrating that rigorous minimization of the bound is competitive with
cross-validation in tuning the trade-off between complexity and empirical
performance. In all our experiments the trade-off turned to be quasiconvex even
when the sufficient conditions were violated.
",propose new pac bayesian bind way construct hypothesis space bind convex posterior distribution also convex trade off parameter empirical performance posterior distribution complexity complexity measure kullback leibler divergence prior derive alternate procedure minimize bind show bind rewrite one dimensional function trade off parameter provide sufficient condition function single global minimum condition satisfy alternating minimization guarantee converge global minimum bind provide experimental result demonstrate rigorous minimization bind competitive cross validation tune trade off complexity empirical performance experiment trade off turn quasiconvex even sufficient condition violate
Neuro-Symbolic Causal Language Planning with Commonsense Prompting,"  Language planning aims to implement complex high-level goals by decomposition
into sequential simpler low-level steps. Such procedural reasoning ability is
essential for applications such as household robots and virtual assistants.
Although language planning is a basic skill set for humans in daily life, it
remains a challenge for large language models (LLMs) that lack deep-level
commonsense knowledge in the real world. Previous methods require either manual
exemplars or annotated programs to acquire such ability from LLMs. In contrast,
this paper proposes Neuro-Symbolic Causal Language Planner (CLAP) that elicits
procedural knowledge from the LLMs with commonsense-infused prompting.
Pre-trained knowledge in LLMs is essentially an unobserved confounder that
causes spurious correlations between tasks and action plans. Through the lens
of a Structural Causal Model (SCM), we propose an effective strategy in CLAP to
construct prompts as a causal intervention toward our SCM. Using graph sampling
techniques and symbolic program executors, our strategy formalizes the
structured causal prompts from commonsense knowledge bases. CLAP obtains
state-of-the-art performance on WikiHow and RobotHow, achieving a relative
improvement of 5.28% in human evaluations under the counterfactual setting.
This indicates the superiority of CLAP in causal language planning semantically
and sequentially.
",language planning aim implement complex high level goal decomposition sequential simple low level step procedural reasoning ability essential application household robot virtual assistant although language plan basic skill set human daily life remain challenge large language model llm lack deep level commonsense knowledge real world previous method require either manual exemplar annotate program acquire ability llm contrast paper propose neuro symbolic causal language planner clap elicit procedural knowledge llm commonsense infuse prompt pre train knowledge llm essentially unobserved confounder cause spurious correlation task action plan lens structural causal model scm propose effective strategy clap construct prompt causal intervention toward scm use graph sampling technique symbolic program executor strategy formalize structure causal prompt commonsense knowledge basis clap obtain state of the art performance wikihow robothow achieve relative improvement human evaluation counterfactual setting indicate superiority clap causal language planning semantically sequentially
"What are We Depressed about When We Talk about COVID19: Mental Health
  Analysis on Tweets Using Natural Language Processing","  The outbreak of coronavirus disease 2019 (COVID-19) recently has affected
human life to a great extent. Besides direct physical and economic threats, the
pandemic also indirectly impact people's mental health conditions, which can be
overwhelming but difficult to measure. The problem may come from various
reasons such as unemployment status, stay-at-home policy, fear for the virus,
and so forth. In this work, we focus on applying natural language processing
(NLP) techniques to analyze tweets in terms of mental health. We trained deep
models that classify each tweet into the following emotions: anger,
anticipation, disgust, fear, joy, sadness, surprise and trust. We build the
EmoCT (Emotion-Covid19-Tweet) dataset for the training purpose by manually
labeling 1,000 English tweets. Furthermore, we propose and compare two methods
to find out the reasons that are causing sadness and fear.
",outbreak coronavirus disease 2019 covid-19 recently affect human life great extent besides direct physical economic threat pandemic also indirectly impact people mental health condition overwhelm difficult measure problem may come various reason unemployment status stay at home policy fear virus forth work focus apply natural language processing nlp technique analyze tweet term mental health train deep model classify tweet follow emotion anger anticipation disgust fear joy sadness surprise trust build emoct emotion covid19 tweet dataset training purpose manually label english tweet furthermore propose compare two method find reason cause sadness fear
"On the ""steerability"" of generative adversarial networks","  An open secret in contemporary machine learning is that many models work
beautifully on standard benchmarks but fail to generalize outside the lab. This
has been attributed to biased training data, which provide poor coverage over
real world events. Generative models are no exception, but recent advances in
generative adversarial networks (GANs) suggest otherwise - these models can now
synthesize strikingly realistic and diverse images. Is generative modeling of
photos a solved problem? We show that although current GANs can fit standard
datasets very well, they still fall short of being comprehensive models of the
visual manifold. In particular, we study their ability to fit simple
transformations such as camera movements and color changes. We find that the
models reflect the biases of the datasets on which they are trained (e.g.,
centered objects), but that they also exhibit some capacity for generalization:
by ""steering"" in latent space, we can shift the distribution while still
creating realistic images. We hypothesize that the degree of distributional
shift is related to the breadth of the training data distribution. Thus, we
conduct experiments to quantify the limits of GAN transformations and introduce
techniques to mitigate the problem. Code is released on our project page:
https://ali-design.github.io/gan_steerability/
",open secret contemporary machine learn many model work beautifully standard benchmark fail generalize outside lab attribute biased training datum provide poor coverage real world event generative model exception recent advance generative adversarial network gan suggest otherwise model synthesize strikingly realistic diverse image generative modeling photo solve problem show although current gan fit standard dataset well still fall short comprehensive model visual manifold particular study ability fit simple transformation camera movement color change find model reflect bias dataset train center object also exhibit capacity generalization steering latent space shift distribution still create realistic image hypothesize degree distributional shift relate breadth training datum distribution thus conduct experiment quantify limit gan transformation introduce technique mitigate problem code release project page https
"Learning from multivariate discrete sequential data using a restricted
  Boltzmann machine model","  A restricted Boltzmann machine (RBM) is a generative neural-network model
with many novel applications such as collaborative filtering and acoustic
modeling. An RBM lacks the capacity to retain memory, making it inappropriate
for dynamic data modeling as in time-series analysis. In this paper we address
this issue by proposing the p-RBM model, a generalization of the regular RBM
model, capable of retaining memory of p past states. We further show how to
train the p-RBM model using contrastive divergence and test our model on the
problem of predicting the stock market direction considering 100 stocks of the
NASDAQ-100 index. Obtained results show that the p-RBM offer promising
prediction potential.
",restrict boltzmann machine rbm generative neural network model many novel application collaborative filter acoustic modeling rbm lack capacity retain memory make inappropriate dynamic datum model time series analysis paper address issue propose p rbm model generalization regular rbm model capable retain memory p past state show train p rbm model use contrastive divergence test model problem predict stock market direction consider 100 stock nasdaq-100 index obtain result show p rbm offer promise prediction potential
Active Distance-Based Clustering using K-medoids,"  k-medoids algorithm is a partitional, centroid-based clustering algorithm
which uses pairwise distances of data points and tries to directly decompose
the dataset with $n$ points into a set of $k$ disjoint clusters. However,
k-medoids itself requires all distances between data points that are not so
easy to get in many applications. In this paper, we introduce a new method
which requires only a small proportion of the whole set of distances and makes
an effort to estimate an upper-bound for unknown distances using the inquired
ones. This algorithm makes use of the triangle inequality to calculate an
upper-bound estimation of the unknown distances. Our method is built upon a
recursive approach to cluster objects and to choose some points actively from
each bunch of data and acquire the distances between these prominent points
from oracle. Experimental results show that the proposed method using only a
small subset of the distances can find proper clustering on many real-world and
synthetic datasets.
",k medoid algorithm partitional centroid base cluster algorithm use pairwise distance datum point try directly decompose dataset n point set k disjoint cluster however k medoid require distance datum point easy get many application paper introduce new method require small proportion whole set distance make effort estimate upper bind unknown distance use inquired one algorithm make use triangle inequality calculate upper bind estimation unknown distance method build upon recursive approach cluster object choose point actively bunch datum acquire distance prominent point oracle experimental result show propose method use small subset distance find proper cluster many real world synthetic dataset
Perception Prioritized Training of Diffusion Models,"  Diffusion models learn to restore noisy data, which is corrupted with
different levels of noise, by optimizing the weighted sum of the corresponding
loss terms, i.e., denoising score matching loss. In this paper, we show that
restoring data corrupted with certain noise levels offers a proper pretext task
for the model to learn rich visual concepts. We propose to prioritize such
noise levels over other levels during training, by redesigning the weighting
scheme of the objective function. We show that our simple redesign of the
weighting scheme significantly improves the performance of diffusion models
regardless of the datasets, architectures, and sampling strategies.
",diffusion model learn restore noisy datum corrupt different level noise optimize weight sum correspond loss term denoise score matching loss paper show restore datum corrupt certain noise level offer proper pretext task model learn rich visual concept propose prioritize noise level level train redesigning weighting scheme objective function show simple redesign weighting scheme significantly improve performance diffusion model regardless dataset architecture sampling strategy
"Shallow Encoder Deep Decoder (SEDD) Networks for Image Encryption and
  Decryption","  This paper explores a new framework for lossy image encryption and decryption
using a simple shallow encoder neural network E for encryption, and a complex
deep decoder neural network D for decryption. E is kept simple so that encoding
can be done on low power and portable devices and can in principle be any
nonlinear function which outputs an encoded vector. D is trained to decode the
encodings using the dataset of image - encoded vector pairs obtained from E and
happens independently of E. As the encodings come from E which while being a
simple neural network, still has thousands of random parameters and therefore
the encodings would be practically impossible to crack without D. This approach
differs from autoencoders as D is trained completely independently of E,
although the structure may seem similar. Therefore, this paper also explores
empirically if a deep neural network can learn to reconstruct the original data
in any useful form given the output of a neural network or any other nonlinear
function, which can have very useful applications in Cryptanalysis. Experiments
demonstrate the potential of the framework through qualitative and quantitative
evaluation of the decoded images from D along with some limitations.
",paper explore new framework lossy image encryption decryption use simple shallow encoder neural network e encryption complex deep decoder neural network decryption e keep simple encoding do low power portable device principle nonlinear function output encode vector train decode encoding use dataset image encode vector pair obtain e happen independently encoding come e simple neural network still thousand random parameter therefore encoding would practically impossible crack without approach differ autoencoder train completely independently e although structure may seem similar therefore paper also explore empirically deep neural network learn reconstruct original datum useful form give output neural network nonlinear function useful application cryptanalysis experiment demonstrate potential framework qualitative quantitative evaluation decode image along limitation
Finite-Sample Guarantees for High-Dimensional DML,"Debiased machine learning (DML) offers an attractive way to estimate
treatment effects in observational settings, where identification of causal
parameters requires a conditional independence or unconfoundedness assumption,
since it allows to control flexibly for a potentially very large number of
covariates. This paper gives novel finite-sample guarantees for joint inference
on high-dimensional DML, bounding how far the finite-sample distribution of the
estimator is from its asymptotic Gaussian approximation. These guarantees are
useful to applied researchers, as they are informative about how far off the
coverage of joint confidence bands can be from the nominal level. There are
many settings where high-dimensional causal parameters may be of interest, such
as the ATE of many treatment profiles, or the ATE of a treatment on many
outcomes. We also cover infinite-dimensional parameters, such as impacts on the
entire marginal distribution of potential outcomes. The finite-sample
guarantees in this paper complement the existing results on consistency and
asymptotic normality of DML estimators, which are either asymptotic or treat
only the one-dimensional case.",debiase machine learn dml offer attractive way estimate treatment effect observational setting identification causal parameter require conditional independence unconfoundedness assumption since allow control flexibly potentially large number covariate paper give novel finite sample guarantee joint inference high dimensional dml bound far finite sample distribution estimator asymptotic gaussian approximation guarantee useful apply researcher informative far coverage joint confidence band nominal level many setting high dimensional causal parameter may interest eat many treatment profile eat treatment many outcome also cover infinite dimensional parameter impact entire marginal distribution potential outcome finite sample guarantee paper complement exist result consistency asymptotic normality dml estimator either asymptotic treat one dimensional case
Isolation Mondrian Forest for Batch and Online Anomaly Detection,"  We propose a new method, named isolation Mondrian forest (iMondrian forest),
for batch and online anomaly detection. The proposed method is a novel hybrid
of isolation forest and Mondrian forest which are existing methods for batch
anomaly detection and online random forest, respectively. iMondrian forest
takes the idea of isolation, using the depth of a node in a tree, and
implements it in the Mondrian forest structure. The result is a new data
structure which can accept streaming data in an online manner while being used
for anomaly detection. Our experiments show that iMondrian forest mostly
performs better than isolation forest in batch settings and has better or
comparable performance against other batch and online anomaly detection
methods.
",propose new method name isolation mondrian forest imondrian forest batch online anomaly detection propose method novel hybrid isolation forest mondrian forest exist method batch anomaly detection online random forest respectively imondrian forest take idea isolation use depth node tree implement mondrian forest structure result new datum structure accept stream datum online manner use anomaly detection experiment show imondrian forest mostly perform well isolation forest batch setting well comparable performance batch online anomaly detection method
Intriguing properties of neural networks,"  Deep neural networks are highly expressive models that have recently achieved
state of the art performance on speech and visual recognition tasks. While
their expressiveness is the reason they succeed, it also causes them to learn
uninterpretable solutions that could have counter-intuitive properties. In this
paper we report two such properties.
  First, we find that there is no distinction between individual high level
units and random linear combinations of high level units, according to various
methods of unit analysis. It suggests that it is the space, rather than the
individual units, that contains of the semantic information in the high layers
of neural networks.
  Second, we find that deep neural networks learn input-output mappings that
are fairly discontinuous to a significant extend. We can cause the network to
misclassify an image by applying a certain imperceptible perturbation, which is
found by maximizing the network's prediction error. In addition, the specific
nature of these perturbations is not a random artifact of learning: the same
perturbation can cause a different network, that was trained on a different
subset of the dataset, to misclassify the same input.
",deep neural network highly expressive model recently achieve state art performance speech visual recognition task expressiveness reason succeed also cause learn uninterpretable solution could counter intuitive property paper report two property first find distinction individual high level unit random linear combination high level unit accord various method unit analysis suggest space rather individual unit contain semantic information high layer neural network second find deep neural network learn input output mapping fairly discontinuous significant extend cause network misclassify image apply certain imperceptible perturbation find maximize network prediction error addition specific nature perturbation random artifact learn perturbation cause different network train different subset dataset misclassify input
"Continuous Release of Data Streams under both Centralized and Local
  Differential Privacy","  In this paper, we study the problem of publishing a stream of real-valued
data satisfying differential privacy (DP). One major challenge is that the
maximal possible value can be quite large; thus it is necessary to estimate a
threshold so that numbers above it are truncated to reduce the amount of noise
that is required to all the data. The estimation must be done based on the data
in a private fashion. We develop such a method that uses the Exponential
Mechanism with a quality function that approximates well the utility goal while
maintaining a low sensitivity. Given the threshold, we then propose a novel
online hierarchical method and several post-processing techniques.
  Building on these ideas, we formalize the steps into a framework for private
publishing of stream data. Our framework consists of three components: a
threshold optimizer that privately estimates the threshold, a perturber that
adds calibrated noises to the stream, and a smoother that improves the result
using post-processing. Within our framework, we design an algorithm satisfying
the more stringent setting of DP called local DP (LDP). To our knowledge, this
is the first LDP algorithm for publishing streaming data. Using four real-world
datasets, we demonstrate that our mechanism outperforms the state-of-the-art by
a factor of 6-10 orders of magnitude in terms of utility (measured by the mean
squared error of answering a random range query).
",paper study problem publish stream real value datum satisfy differential privacy dp one major challenge maximal possible value quite large thus necessary estimate threshold number truncate reduce amount noise require datum estimation must do base datum private fashion develop method use exponential mechanism quality function approximate well utility goal maintain low sensitivity give threshold propose novel online hierarchical method several post processing technique build idea formalize step framework private publishing stream datum framework consist three component threshold optimizer privately estimate threshold perturber add calibrate noise stream smoother improve result use post processing within framework design algorithm satisfy stringent setting dp call local dp ldp knowledge first ldp algorithm publish streaming datum use four real world dataset demonstrate mechanism outperform state of the art factor 6 10 order magnitude term utility measure mean square error answer random range query
Max-margin Deep Generative Models,"  Deep generative models (DGMs) are effective on learning multilayered
representations of complex data and performing inference of input data by
exploring the generative ability. However, little work has been done on
examining or empowering the discriminative ability of DGMs on making accurate
predictions. This paper presents max-margin deep generative models (mmDGMs),
which explore the strongly discriminative principle of max-margin learning to
improve the discriminative power of DGMs, while retaining the generative
capability. We develop an efficient doubly stochastic subgradient algorithm for
the piecewise linear objective. Empirical results on MNIST and SVHN datasets
demonstrate that (1) max-margin learning can significantly improve the
prediction performance of DGMs and meanwhile retain the generative ability; and
(2) mmDGMs are competitive to the state-of-the-art fully discriminative
networks by employing deep convolutional neural networks (CNNs) as both
recognition and generative models.
",deep generative model dgms effective learn multilayere representation complex datum perform inference input datum explore generative ability however little work do examine empower discriminative ability dgms make accurate prediction paper present max margin deep generative model mmdgms explore strongly discriminative principle max margin learning improve discriminative power dgms retain generative capability develop efficient doubly stochastic subgradient algorithm piecewise linear objective empirical result mnist svhn dataset demonstrate 1 max margin learning significantly improve prediction performance dgms meanwhile retain generative ability 2 mmdgms competitive state of the art fully discriminative network employ deep convolutional neural network cnns recognition generative model
"Bag-of-Words Method Applied to Accelerometer Measurements for the
  Purpose of Classification and Energy Estimation","  Accelerometer measurements are the prime type of sensor information most
think of when seeking to measure physical activity. On the market, there are
many fitness measuring devices which aim to track calories burned and steps
counted through the use of accelerometers. These measurements, though good
enough for the average consumer, are noisy and unreliable in terms of the
precision of measurement needed in a scientific setting. The contribution of
this paper is an innovative and highly accurate regression method which uses an
intermediary two-stage classification step to better direct the regression of
energy expenditure values from accelerometer counts.
  We show that through an additional unsupervised layer of intermediate feature
construction, we can leverage latent patterns within accelerometer counts to
provide better grounds for activity classification than expert-constructed
timeseries features. For this, our approach utilizes a mathematical model
originating in natural language processing, the bag-of-words model, that has in
the past years been appearing in diverse disciplines outside of the natural
language processing field such as image processing. Further emphasizing the
natural language connection to stochastics, we use a gaussian mixture model to
learn the dictionary upon which the bag-of-words model is built. Moreover, we
show that with the addition of these features, we're able to improve regression
root mean-squared error of energy expenditure by approximately 1.4 units over
existing state-of-the-art methods.
",accelerometer measurement prime type sensor information think seek measure physical activity market many fitness measuring device aim track calorie burn step count use accelerometer measurement though good enough average consumer noisy unreliable term precision measurement need scientific setting contribution paper innovative highly accurate regression method use intermediary two stage classification step well direct regression energy expenditure value accelerometer count show additional unsupervised layer intermediate feature construction leverage latent pattern within accelerometer count provide well ground activity classification expert construct timeserie feature approach utilize mathematical model originate natural language processing bag of word model past year appear diverse discipline outside natural language processing field image processing emphasize natural language connection stochastic use gaussian mixture model learn dictionary upon bag of word model build moreover show addition feature able improve regression root mean square error energy expenditure approximately unit exist state of the art method
Recursive Reasoning Graph for Multi-Agent Reinforcement Learning,"  Multi-agent reinforcement learning (MARL) provides an efficient way for
simultaneously learning policies for multiple agents interacting with each
other. However, in scenarios requiring complex interactions, existing
algorithms can suffer from an inability to accurately anticipate the influence
of self-actions on other agents. Incorporating an ability to reason about other
agents' potential responses can allow an agent to formulate more effective
strategies. This paper adopts a recursive reasoning model in a
centralized-training-decentralized-execution framework to help learning agents
better cooperate with or compete against others. The proposed algorithm,
referred to as the Recursive Reasoning Graph (R2G), shows state-of-the-art
performance on multiple multi-agent particle and robotics games.
",multi agent reinforcement learn marl provide efficient way simultaneously learn policy multiple agent interact however scenario require complex interaction exist algorithm suffer inability accurately anticipate influence self action agent incorporate ability reason agent potential response allow agent formulate effective strategy paper adopt recursive reasoning model centralize training decentralize execution framework help learn agent well cooperate compete other propose algorithm refer recursive reasoning graph r2 g show state of the art performance multiple multi agent particle robotic game
Robust Semantic Interpretability: Revisiting Concept Activation Vectors,"  Interpretability methods for image classification assess model
trustworthiness by attempting to expose whether the model is systematically
biased or attending to the same cues as a human would. Saliency methods for
feature attribution dominate the interpretability literature, but these methods
do not address semantic concepts such as the textures, colors, or genders of
objects within an image. Our proposed Robust Concept Activation Vectors (RCAV)
quantifies the effects of semantic concepts on individual model predictions and
on model behavior as a whole. RCAV calculates a concept gradient and takes a
gradient ascent step to assess model sensitivity to the given concept. By
generalizing previous work on concept activation vectors to account for model
non-linearity, and by introducing stricter hypothesis testing, we show that
RCAV yields interpretations which are both more accurate at the image level and
robust at the dataset level. RCAV, like saliency methods, supports the
interpretation of individual predictions. To evaluate the practical use of
interpretability methods as debugging tools, and the scientific use of
interpretability methods for identifying inductive biases (e.g. texture over
shape), we construct two datasets and accompanying metrics for realistic
benchmarking of semantic interpretability methods. Our benchmarks expose the
importance of counterfactual augmentation and negative controls for quantifying
the practical usability of interpretability methods.
",interpretability method image classification assess model trustworthiness attempt expose whether model systematically biased attend cue human would saliency method feature attribution dominate interpretability literature method address semantic concept texture color gender object within image propose robust concept activation vector rcav quantifie effect semantic concept individual model prediction model behavior whole rcav calculate concept gradient take gradient ascent step assess model sensitivity give concept generalize previous work concept activation vector account model non linearity introduce strict hypothesis testing show rcav yield interpretation accurate image level robust dataset level rcav like saliency method support interpretation individual prediction evaluate practical use interpretability method debug tool scientific use interpretability method identify inductive bias texture shape construct two dataset accompany metric realistic benchmarke semantic interpretability method benchmark expose importance counterfactual augmentation negative control quantify practical usability interpretability method
Efficient random graph matching via degree profiles,"  Random graph matching refers to recovering the underlying vertex
correspondence between two random graphs with correlated edges; a prominent
example is when the two random graphs are given by Erd\H{o}s-R\'{e}nyi graphs
$G(n,\frac{d}{n})$. This can be viewed as an average-case and noisy version of
the graph isomorphism problem. Under this model, the maximum likelihood
estimator is equivalent to solving the intractable quadratic assignment
problem. This work develops an $\tilde{O}(n d^2+n^2)$-time algorithm which
perfectly recovers the true vertex correspondence with high probability,
provided that the average degree is at least $d = \Omega(\log^2 n)$ and the two
graphs differ by at most $\delta = O( \log^{-2}(n) )$ fraction of edges. For
dense graphs and sparse graphs, this can be improved to $\delta = O(
\log^{-2/3}(n) )$ and $\delta = O( \log^{-2}(d) )$ respectively, both in
polynomial time. The methodology is based on appropriately chosen distance
statistics of the degree profiles (empirical distribution of the degrees of
neighbors). Before this work, the best known result achieves $\delta=O(1)$ and
$n^{o(1)} \leq d \leq n^c$ for some constant $c$ with an $n^{O(\log n)}$-time
algorithm \cite{barak2018nearly} and $\delta=\tilde O((d/n)^4)$ and $d =
\tilde{\Omega}(n^{4/5})$ with a polynomial-time algorithm
\cite{dai2018performance}.
",random graph matching refer recover underlying vertex correspondence two random graph correlate edge prominent example two random graph give e nyi graph g n n view average case noisy version graph isomorphism problem model maximum likelihood estimator equivalent solve intractable quadratic assignment problem work develop n -time algorithm perfectly recover true vertex correspondence high probability provide average degree least n two graph differ -2 n fraction edge dense graph sparse graph improve n -2 respectively polynomial time methodology base appropriately choose distance statistic degree profile empirical distribution degree neighbor work well know result achieve 1 1 constant c n -time algorithm barak2018nearly polynomial time algorithm dai2018performance
"Feed-Forward Networks with Attention Can Solve Some Long-Term Memory
  Problems","  We propose a simplified model of attention which is applicable to
feed-forward neural networks and demonstrate that the resulting model can solve
the synthetic ""addition"" and ""multiplication"" long-term memory problems for
sequence lengths which are both longer and more widely varying than the best
published results for these tasks.
",propose simplified model attention applicable feed forward neural network demonstrate result model solve synthetic addition multiplication long term memory problem sequence length long widely vary well publish result task
Hierarchical Clustering of Asymmetric Networks,"  This paper considers networks where relationships between nodes are
represented by directed dissimilarities. The goal is to study methods that,
based on the dissimilarity structure, output hierarchical clusters, i.e., a
family of nested partitions indexed by a connectivity parameter. Our
construction of hierarchical clustering methods is built around the concept of
admissible methods, which are those that abide by the axioms of value - nodes
in a network with two nodes are clustered together at the maximum of the two
dissimilarities between them - and transformation - when dissimilarities are
reduced, the network may become more clustered but not less. Two particular
methods, termed reciprocal and nonreciprocal clustering, are shown to provide
upper and lower bounds in the space of admissible methods. Furthermore,
alternative clustering methodologies and axioms are considered. In particular,
modifying the axiom of value such that clustering in two-node networks occurs
at the minimum of the two dissimilarities entails the existence of a unique
admissible clustering method.
",paper consider network relationship node represent direct dissimilarity goal study method base dissimilarity structure output hierarchical cluster family nest partition index connectivity parameter construction hierarchical clustering method build around concept admissible method abide axiom value node network two node cluster together maximum two dissimilarity transformation dissimilarity reduce network may become cluster less two particular method term reciprocal nonreciprocal clustering show provide upper low bound space admissible method furthermore alternative cluster methodology axiom consider particular modifying axiom value cluster two node network occur minimum two dissimilarity entail existence unique admissible clustering method
AI for Earth: Rainforest Conservation by Acoustic Surveillance,"  Saving rainforests is a key to halting adverse climate changes. In this
paper, we introduce an innovative solution built on acoustic surveillance and
machine learning technologies to help rainforest conservation. In particular,
We propose new convolutional neural network (CNN) models for environmental
sound classification and achieved promising preliminary results on two
datasets, including a public audio dataset and our real rainforest sound
dataset. The proposed audio classification models can be easily extended in an
automated machine learning paradigm and integrated in cloud-based services for
real world deployment.
",save rainforest key halting adverse climate change paper introduce innovative solution build acoustic surveillance machine learn technology help rainforest conservation particular propose new convolutional neural network cnn model environmental sound classification achieve promise preliminary result two dataset include public audio dataset real rainforest sound dataset propose audio classification model easily extend automate machine learn paradigm integrate cloud base service real world deployment
Nonsmooth Implicit Differentiation for Machine Learning and Optimization,"  In view of training increasingly complex learning architectures, we establish
a nonsmooth implicit function theorem with an operational calculus. Our result
applies to most practical problems (i.e., definable problems) provided that a
nonsmooth form of the classical invertibility condition is fulfilled. This
approach allows for formal subdifferentiation: for instance, replacing
derivatives by Clarke Jacobians in the usual differentiation formulas is fully
justified for a wide class of nonsmooth problems. Moreover this calculus is
entirely compatible with algorithmic differentiation (e.g., backpropagation).
We provide several applications such as training deep equilibrium networks,
training neural nets with conic optimization layers, or hyperparameter-tuning
for nonsmooth Lasso-type models. To show the sharpness of our assumptions, we
present numerical experiments showcasing the extremely pathological gradient
dynamics one can encounter when applying implicit algorithmic differentiation
without any hypothesis.
",view train increasingly complex learning architecture establish nonsmooth implicit function theorem operational calculus result apply practical problem definable problem provide nonsmooth form classical invertibility condition fulfil approach allow formal subdifferentiation instance replace derivative clarke jacobian usual differentiation formula fully justify wide class nonsmooth problem moreover calculu entirely compatible algorithmic differentiation backpropagation provide several application train deep equilibrium network train neural net conic optimization layer hyperparameter tune nonsmooth lasso type model show sharpness assumption present numerical experiment showcase extremely pathological gradient dynamic one encounter apply implicit algorithmic differentiation without hypothesis
Deep kernel processes,"  We define deep kernel processes in which positive definite Gram matrices are
progressively transformed by nonlinear kernel functions and by sampling from
(inverse) Wishart distributions. Remarkably, we find that deep Gaussian
processes (DGPs), Bayesian neural networks (BNNs), infinite BNNs, and infinite
BNNs with bottlenecks can all be written as deep kernel processes. For DGPs the
equivalence arises because the Gram matrix formed by the inner product of
features is Wishart distributed, and as we show, standard isotropic kernels can
be written entirely in terms of this Gram matrix -- we do not need knowledge of
the underlying features. We define a tractable deep kernel process, the deep
inverse Wishart process, and give a doubly-stochastic inducing-point
variational inference scheme that operates on the Gram matrices, not on the
features, as in DGPs. We show that the deep inverse Wishart process gives
superior performance to DGPs and infinite BNNs on standard fully-connected
baselines.
",define deep kernel process positive definite gram matrix progressively transform nonlinear kernel function sample inverse wishart distribution remarkably find deep gaussian process dgps bayesian neural network bnn infinite bnns infinite bnn bottleneck write deep kernel process dgps equivalence arise gram matrix form inner product feature wishart distribute show standard isotropic kernel write entirely term gram matrix need knowledge underlie feature define tractable deep kernel process deep inverse wishart process give doubly stochastic induce point variational inference scheme operate gram matrix feature dgps show deep inverse wishart process give superior performance dgps infinite bnns standard fully connect baseline
Efficient Sub-structured Knowledge Distillation,"  Structured prediction models aim at solving a type of problem where the
output is a complex structure, rather than a single variable. Performing
knowledge distillation for such models is not trivial due to their
exponentially large output space. In this work, we propose an approach that is
much simpler in its formulation and far more efficient for training than
existing approaches. Specifically, we transfer the knowledge from a teacher
model to its student model by locally matching their predictions on all
sub-structures, instead of the whole output space. In this manner, we avoid
adopting some time-consuming techniques like dynamic programming (DP) for
decoding output structures, which permits parallel computation and makes the
training process even faster in practice. Besides, it encourages the student
model to better mimic the internal behavior of the teacher model. Experiments
on two structured prediction tasks demonstrate that our approach outperforms
previous methods and halves the time cost for one training epoch.
",structured prediction model aim solve type problem output complex structure rather single variable perform knowledge distillation model trivial due exponentially large output space work propose approach much simple formulation far efficient training exist approach specifically transfer knowledge teacher model student model locally match prediction sub structure instead whole output space manner avoid adopt time consume technique like dynamic programming dp decode output structure permit parallel computation make training process even fast practice besides encourage student model well mimic internal behavior teacher model experiment two structured prediction task demonstrate approach outperform previous method half time cost one training epoch
Spectral Algorithms for Computing Fair Support Vector Machines,"  Classifiers and rating scores are prone to implicitly codifying biases, which
may be present in the training data, against protected classes (i.e., age,
gender, or race). So it is important to understand how to design classifiers
and scores that prevent discrimination in predictions. This paper develops
computationally tractable algorithms for designing accurate but fair support
vector machines (SVM's). Our approach imposes a constraint on the covariance
matrices conditioned on each protected class, which leads to a nonconvex
quadratic constraint in the SVM formulation. We develop iterative algorithms to
compute fair linear and kernel SVM's, which solve a sequence of relaxations
constructed using a spectral decomposition of the nonconvex constraint. Its
effectiveness in achieving high prediction accuracy while ensuring fairness is
shown through numerical experiments on several data sets.
",classifier rating score prone implicitly codify bias may present training datum protect class age gender race important understand design classifier score prevent discrimination prediction paper develop computationally tractable algorithm design accurate fair support vector machine svm approach impose constraint covariance matrix condition protect class lead nonconvex quadratic constraint svm formulation develop iterative algorithm compute fair linear kernel svm solve sequence relaxation construct use spectral decomposition nonconvex constraint effectiveness achieve high prediction accuracy ensure fairness show numerical experiment several data set
Resource-aware Elastic Swap Random Forest for Evolving Data Streams,"  Continual learning based on data stream mining deals with ubiquitous sources
of Big Data arriving at high-velocity and in real-time. Adaptive Random Forest
({\em ARF}) is a popular ensemble method used for continual learning due to its
simplicity in combining adaptive leveraging bagging with fast random Hoeffding
trees. While the default ARF size provides competitive accuracy, it is usually
over-provisioned resulting in the use of additional classifiers that only
contribute to increasing CPU and memory consumption with marginal impact in the
overall accuracy. This paper presents Elastic Swap Random Forest ({\em ESRF}),
a method for reducing the number of trees in the ARF ensemble while providing
similar accuracy. {\em ESRF} extends {\em ARF} with two orthogonal components:
1) a swap component that splits learners into two sets based on their accuracy
(only classifiers with the highest accuracy are used to make predictions); and
2) an elastic component for dynamically increasing or decreasing the number of
classifiers in the ensemble. The experimental evaluation of {\em ESRF} and
comparison with the original {\em ARF} shows how the two new components
contribute to reducing the number of classifiers up to one third while
providing almost the same accuracy, resulting in speed-ups in terms of
per-sample execution time close to 3x.
",continual learning base datum stream mining deal ubiquitous source big datum arrive high velocity real time adaptive random forest arf popular ensemble method use continual learning due simplicity combine adaptive leverage bagging fast random hoeffding tree default arf size provide competitive accuracy usually over provision result use additional classifier contribute increase cpu memory consumption marginal impact overall accuracy paper present elastic swap random forest esrf method reduce number tree arf ensemble provide similar accuracy esrf extend arf two orthogonal component 1 swap component split learner two set base accuracy classifier high accuracy use make prediction 2 elastic component dynamically increase decrease number classifier ensemble experimental evaluation esrf comparison original arf show two new component contribute reduce number classifier one third provide almost accuracy result speed up term per sample execution time close 3x
"Overcoming Overfitting and Large Weight Update Problem in Linear
  Rectifiers: Thresholded Exponential Rectified Linear Units","  In past few years, linear rectified unit activation functions have shown its
significance in the neural networks, surpassing the performance of sigmoid
activations. RELU (Nair & Hinton, 2010), ELU (Clevert et al., 2015), PRELU (He
et al., 2015), LRELU (Maas et al., 2013), SRELU (Jin et al., 2016),
ThresholdedRELU, all these linear rectified activation functions have its own
significance over others in some aspect. Most of the time these activation
functions suffer from bias shift problem due to non-zero output mean, and high
weight update problem in deep complex networks due to unit gradient, which
results in slower training, and high variance in model prediction respectively.
In this paper, we propose, ""Thresholded exponential rectified linear unit""
(TERELU) activation function that works better in alleviating in overfitting:
large weight update problem. Along with alleviating overfitting problem, this
method also gives good amount of non-linearity as compared to other linear
rectifiers. We will show better performance on the various datasets using
neural networks, considering TERELU activation method compared to other
activations.
",past year linear rectify unit activation function show significance neural network surpass performance sigmoid activation relu nair hinton 2010 elu clevert et 2015 prelu et 2015 lrelu maas et 2013 srelu jin et 2016 thresholdedrelu linear rectify activation function significance other aspect time activation function suffer bias shift problem due non zero output mean high weight update problem deep complex network due unit gradient result slow train high variance model prediction respectively paper propose thresholded exponential rectify linear unit terelu activation function work well alleviate overfitte large weight update problem along alleviate overfitte problem method also give good amount non linearity compare linear rectifier show well performance various dataset use neural network consider terelu activation method compare activation
Symmetric Wasserstein Autoencoders,"  Leveraging the framework of Optimal Transport, we introduce a new family of
generative autoencoders with a learnable prior, called Symmetric Wasserstein
Autoencoders (SWAEs). We propose to symmetrically match the joint distributions
of the observed data and the latent representation induced by the encoder and
the decoder. The resulting algorithm jointly optimizes the modelling losses in
both the data and the latent spaces with the loss in the data space leading to
the denoising effect. With the symmetric treatment of the data and the latent
representation, the algorithm implicitly preserves the local structure of the
data in the latent space. To further improve the quality of the latent
representation, we incorporate a reconstruction loss into the objective, which
significantly benefits both the generation and reconstruction. We empirically
show the superior performance of SWAEs over the state-of-the-art generative
autoencoders in terms of classification, reconstruction, and generation.
",leverage framework optimal transport introduce new family generative autoencoder learnable prior call symmetric wasserstein autoencoder swaes propose symmetrically match joint distribution observe datum latent representation induced encoder decoder result algorithm jointly optimize modelling loss datum latent space loss datum space lead denoising effect symmetric treatment datum latent representation algorithm implicitly preserve local structure datum latent space improve quality latent representation incorporate reconstruction loss objective significantly benefit generation reconstruction empirically show superior performance swaes state of the art generative autoencoder term classification reconstruction generation
Prediction by Compression,"  It is well known that text compression can be achieved by predicting the next
symbol in the stream of text data based on the history seen up to the current
symbol. The better the prediction the more skewed the conditional probability
distribution of the next symbol and the shorter the codeword that needs to be
assigned to represent this next symbol. What about the opposite direction ?
suppose we have a black box that can compress text stream. Can it be used to
predict the next symbol in the stream ? We introduce a criterion based on the
length of the compressed data and use it to predict the next symbol. We examine
empirically the prediction error rate and its dependency on some compression
parameters.
",well know text compression achieve predict next symbol stream text datum base history see current symbol well prediction skew conditional probability distribution next symbol short codeword need assign represent next symbol opposite direction suppose black box compress text stream use predict next symbol stream introduce criterion base length compress datum use predict next symbol examine empirically prediction error rate dependency compression parameter
"Learning to Manipulate Object Collections Using Grounded State
  Representations","  We propose a method for sim-to-real robot learning which exploits simulator
state information in a way that scales to many objects. We first train a pair
of encoder networks to capture multi-object state information in a latent
space. One of these encoders is a CNN, which enables our system to operate on
RGB images in the real world; the other is a graph neural network (GNN) state
encoder, which directly consumes a set of raw object poses and enables more
accurate reward calculation and value estimation. Once trained, we use these
encoders in a reinforcement learning algorithm to train image-based policies
that can manipulate many objects. We evaluate our method on the task of pushing
a collection of objects to desired tabletop regions. Compared to methods which
rely only on images or use fixed-length state encodings, our method achieves
higher success rates, performs well in the real world without fine tuning, and
generalizes to different numbers and types of objects not seen during training.
",propose method sim to real robot learning exploit simulator state information way scale many object first train pair encoder network capture multi object state information latent space one encoder cnn enable system operate rgb image real world graph neural network gnn state encoder directly consume set raw object pose enable accurate reward calculation value estimation train use encoder reinforcement learn algorithm train image base policy manipulate many object evaluate method task push collection object desire tabletop region compare method rely image use fix length state encoding method achieve high success rate perform well real world without fine tuning generalize different number type objects see training
"Scalable Neural Network Compression and Pruning Using Hard Clustering
  and L1 Regularization","  We propose a simple and easy to implement neural network compression
algorithm that achieves results competitive with more complicated
state-of-the-art methods. The key idea is to modify the original optimization
problem by adding K independent Gaussian priors (corresponding to the k-means
objective) over the network parameters to achieve parameter quantization, as
well as an L1 penalty to achieve pruning. Unlike many existing
quantization-based methods, our method uses hard clustering assignments of
network parameters, which adds minimal change or overhead to standard network
training. We also demonstrate experimentally that tying neural network
parameters provides less gain in generalization performance than changing
network architecture and connectivity patterns entirely.
",propose simple easy implement neural network compression algorithm achieve result competitive complicated state of the art method key idea modify original optimization problem add k independent gaussian prior correspond k means objective network parameter achieve parameter quantization well l1 penalty achieve pruning unlike many exist quantization base method method use hard cluster assignment network parameter add minimal change overhead standard network training also demonstrate experimentally tie neural network parameter provide less gain generalization performance change network architecture connectivity pattern entirely
"How to Spend Your Robot Time: Bridging Kickstarting and Offline
  Reinforcement Learning for Vision-based Robotic Manipulation","  Reinforcement learning (RL) has been shown to be effective at learning
control from experience. However, RL typically requires a large amount of
online interaction with the environment. This limits its applicability to
real-world settings, such as in robotics, where such interaction is expensive.
In this work we investigate ways to minimize online interactions in a target
task, by reusing a suboptimal policy we might have access to, for example from
training on related prior tasks, or in simulation. To this end, we develop two
RL algorithms that can speed up training by using not only the action
distributions of teacher policies, but also data collected by such policies on
the task at hand. We conduct a thorough experimental study of how to use
suboptimal teachers on a challenging robotic manipulation benchmark on
vision-based stacking with diverse objects. We compare our methods to offline,
online, offline-to-online, and kickstarting RL algorithms. By doing so, we find
that training on data from both the teacher and student, enables the best
performance for limited data budgets. We examine how to best allocate a limited
data budget -- on the target task -- between the teacher and the student
policy, and report experiments using varying budgets, two teachers with
different degrees of suboptimality, and five stacking tasks that require a
diverse set of behaviors. Our analysis, both in simulation and in the real
world, shows that our approach is the best across data budgets, while standard
offline RL from teacher rollouts is surprisingly effective when enough data is
given.
",reinforcement learning rl show effective learn control experience however rl typically require large amount online interaction environment limit applicability real world setting robotic interaction expensive work investigate way minimize online interaction target task reuse suboptimal policy might access example training relate prior task simulation end develop two rl algorithm speed training use action distribution teacher policy also data collect policy task hand conduct thorough experimental study use suboptimal teacher challenge robotic manipulation benchmark vision base stack diverse object compare method offline online offline to online kickstarting rl algorithm find training datum teacher student enable good performance limited data budget examine well allocate limited data budget target task teacher student policy report experiment use varying budget two teacher different degree suboptimality five stacking task require diverse set behavior analysis simulation real world show approach well across datum budget standard offline rl teacher rollout surprisingly effective enough datum give
Deep Unfolded Recovery of Sub-Nyquist Sampled Ultrasound Image,"  The most common technique for generating B-mode ultrasound (US) images is
delay and sum (DAS) beamforming, where the signals received at the transducer
array are sampled before an appropriate delay is applied. This necessitates
sampling rates exceeding the Nyquist rate and the use of a large number of
antenna elements to ensure sufficient image quality. Recently we proposed
methods to reduce the sampling rate and the array size relying on image
recovery using iterative algorithms, based on compressed sensing (CS) and the
finite rate of innovation (FRI) frameworks. Iterative algorithms typically
require a large number of iterations, making them difficult to use in
real-time. Here, we propose a reconstruction method from sub-Nyquist samples in
the time and spatial domain, that is based on unfolding the ISTA algorithm,
resulting in an efficient and interpretable deep network. The inputs to our
network are the subsampled beamformed signals after summation and delay in the
frequency domain, requiring only a subset of the US signal to be stored for
recovery. Our method allows reducing the number of array elements, sampling
rate, and computational time while ensuring high quality imaging performance.
Using \emph{in vivo} data we demonstrate that the proposed method yields
high-quality images while reducing the data volume traditionally used up to 36
times. In terms of image resolution and contrast, our technique outperforms
previously suggested methods as well as DAS and minimum-variance (MV)
beamforming, paving the way to real-time applicable recovery methods.
",common technique generating b mode ultrasound we image delay sum das beamforming signal receive transducer array sample appropriate delay apply necessitate sampling rate exceed nyquist rate use large number antenna element ensure sufficient image quality recently propose method reduce sampling rate array size rely image recovery use iterative algorithm base compressed sense cs finite rate innovation fri framework iterative algorithm typically require large number iteration make difficult use real time propose reconstruction method sub nyquist sample time spatial domain base unfold ista algorithm result efficient interpretable deep network input network subsample beamforme signal summation delay frequency domain require subset we signal store recovery method allow reduce number array element sample rate computational time ensure high quality imaging performance use vivo datum demonstrate propose method yield high quality image reduce data volume traditionally use 36 time term image resolution contrast technique outperform previously suggest method well das minimum variance mv beamforming pave way real time applicable recovery method
"A Fast and Convergent Proximal Algorithm for Regularized Nonconvex and
  Nonsmooth Bi-level Optimization","  Many important machine learning applications involve regularized nonconvex
bi-level optimization. However, the existing gradient-based bi-level
optimization algorithms cannot handle nonconvex or nonsmooth regularizers, and
they suffer from a high computation complexity in nonconvex bi-level
optimization. In this work, we study a proximal gradient-type algorithm that
adopts the approximate implicit differentiation (AID) scheme for nonconvex
bi-level optimization with possibly nonconvex and nonsmooth regularizers. In
particular, the algorithm applies the Nesterov's momentum to accelerate the
computation of the implicit gradient involved in AID. We provide a
comprehensive analysis of the global convergence properties of this algorithm
through identifying its intrinsic potential function. In particular, we
formally establish the convergence of the model parameters to a critical point
of the bi-level problem, and obtain an improved computation complexity
$\mathcal{O}(\kappa^{3.5}\epsilon^{-2})$ over the state-of-the-art result.
Moreover, we analyze the asymptotic convergence rates of this algorithm under a
class of local nonconvex geometries characterized by a {\L}ojasiewicz-type
gradient inequality. Experiment on hyper-parameter optimization demonstrates
the effectiveness of our algorithm.
",many important machine learning application involve regularize nonconvex bi level optimization however exist gradient base bi level optimization algorithm handle nonconvex nonsmooth regularizer suffer high computation complexity nonconvex bi level optimization work study proximal gradient type algorithm adopt approximate implicit differentiation aid scheme nonconvex bi level optimization possibly nonconvex nonsmooth regularizer particular algorithm apply nesterov momentum accelerate computation implicit gradient involve aid provide comprehensive analysis global convergence property algorithm identify intrinsic potential function particular formally establish convergence model parameter critical point bi level problem obtain improve computation complexity -2 state of the art result moreover analyze asymptotic convergence rate algorithm class local nonconvex geometry characterize ojasiewicz type gradient inequality experiment hyper parameter optimization demonstrate effectiveness algorithm
"Surveillance of COVID-19 Pandemic using Social Media: A Reddit Study in
  North Carolina","  Coronavirus disease (COVID-19) pandemic has changed various aspects of
people's lives and behaviors. At this stage, there are no other ways to control
the natural progression of the disease than adopting mitigation strategies such
as wearing masks, watching distance, and washing hands. Moreover, at this time
of social distancing, social media plays a key role in connecting people and
providing a platform for expressing their feelings. In this study, we tap into
social media to surveil the uptake of mitigation and detection strategies, and
capture issues and concerns about the pandemic. In particular, we explore the
research question, ""how much can be learned regarding the public uptake of
mitigation strategies and concerns about COVID-19 pandemic by using natural
language processing on Reddit posts?"" After extracting COVID-related posts from
the four largest subreddit communities of North Carolina over six months, we
performed NLP-based preprocessing to clean the noisy data. We employed a custom
Named-entity Recognition (NER) system and a Latent Dirichlet Allocation (LDA)
method for topic modeling on a Reddit corpus. We observed that 'mask', 'flu',
and 'testing' are the most prevalent named-entities for ""Personal Protective
Equipment"", ""symptoms"", and ""testing"" categories, respectively. We also
observed that the most discussed topics are related to testing, masks, and
employment. The mitigation measures are the most prevalent theme of discussion
across all subreddits.
",coronavirus disease covid-19 pandemic change various aspect people live behavior stage way control natural progression disease adopt mitigation strategy wear mask watch distance washing hand moreover time social distancing social medium play key role connect people provide platform express feeling study tap social medium surveil uptake mitigation detection strategy capture issue concern pandemic particular explore research question much learn regard public uptake mitigation strategy concern covid-19 pandemic use natural language processing reddit post extract covid relate post four large subreddit community north carolina six month perform nlp base preprocessing clean noisy datum employ custom name entity recognition ner system latent dirichlet allocation lda method topic model reddit corpus observe prevalent name entity personal protective equipment symptom test category respectively also observe discuss topic relate testing mask employment mitigation measure prevalent theme discussion across subreddit
A survey on deep learning approaches for breast cancer diagnosis,"  Deep learning has introduced several learning-based methods to recognize
breast tumours and presents high applicability in breast cancer diagnostics. It
has presented itself as a practical installment in Computer-Aided Diagnostic
(CAD) systems to further assist radiologists in diagnostics for different
modalities. A deep learning network trained on images provided by hospitals or
public databases can perform classification, detection, and segmentation of
lesion types. Significant progress has been made in recognizing tumours on 2D
images but recognizing 3D images remains a frontier so far. The interconnection
of deep learning networks between different fields of study help propels
discoveries for more efficient, accurate, and robust networks. In this review
paper, the following topics will be explored: (i) theory and application of
deep learning, (ii) progress of 2D, 2.5D, and 3D CNN approaches in breast
tumour recognition from a performance metric perspective, and (iii) challenges
faced in CNN approaches.
",deep learning introduce several learning base method recognize breast tumour present high applicability breast cancer diagnostic present practical installment computer aid diagnostic cad system assist radiologist diagnostic different modality deep learning network train image provide hospital public database perform classification detection segmentation lesion type significant progress make recognize tumour 2d image recognize 3d image remain frontier far interconnection deep learning network different field study help propel discovery efficient accurate robust network review paper follow topic explore theory application deep learning ii progress 2d 3d cnn approach breast tumour recognition performance metric perspective iii challenge face cnn approach
"Spatial sensitivity analysis for urban land use prediction with
  physics-constrained conditional generative adversarial networks","  Accurately forecasting urban development and its environmental and climate
impacts critically depends on realistic models of the spatial structure of the
built environment, and of its dependence on key factors such as population and
economic development. Scenario simulation and sensitivity analysis, i.e.,
predicting how changes in underlying factors at a given location affect
urbanization outcomes at other locations, is currently not achievable at a
large scale with traditional urban growth models, which are either too
simplistic, or depend on detailed locally-collected socioeconomic data that is
not available in most places. Here we develop a framework to estimate, purely
from globally-available remote-sensing data and without parametric assumptions,
the spatial sensitivity of the (\textit{static}) rate of change of urban sprawl
to key macroeconomic development indicators. We formulate this spatial
regression problem as an image-to-image translation task using conditional
generative adversarial networks (GANs), where the gradients necessary for
comparative static analysis are provided by the backpropagation algorithm used
to train the model. This framework allows to naturally incorporate physical
constraints, e.g., the inability to build over water bodies. To validate the
spatial structure of model-generated built environment distributions, we use
spatial statistics commonly used in urban form analysis. We apply our method to
a novel dataset comprising of layers on the built environment, nightlighs
measurements (a proxy for economic development and energy use), and population
density for the world's most populous 15,000 cities.
",accurately forecast urban development environmental climate impact critically depend realistic model spatial structure build environment dependence key factor population economic development scenario simulation sensitivity analysis predict change underlie factor give location affect urbanization outcome location currently achievable large scale traditional urban growth model either simplistic depend detailed locally collect socioeconomic datum available place develop framework estimate purely globally available remote sense datum without parametric assumption spatial sensitivity static rate change urban sprawl key macroeconomic development indicator formulate spatial regression problem image to image translation task use conditional generative adversarial network gan gradient necessary comparative static analysis provide backpropagation algorithm use train model framework allow naturally incorporate physical constraint inability build water body validate spatial structure model generate build environment distribution use spatial statistic commonly use urban form analysis apply method novel dataset comprise layer build environment nightlighs measurement proxy economic development energy use population density world populous city
Efficient Saliency Maps for Explainable AI,"  We describe an explainable AI saliency map method for use with deep
convolutional neural networks (CNN) that is much more efficient than popular
fine-resolution gradient methods. It is also quantitatively similar or better
in accuracy. Our technique works by measuring information at the end of each
network scale which is then combined into a single saliency map. We describe
how saliency measures can be made more efficient by exploiting Saliency Map
Order Equivalence. We visualize individual scale/layer contributions by using a
Layer Ordered Visualization of Information. This provides an interesting
comparison of scale information contributions within the network not provided
by other saliency map methods. Using our method instead of Guided Backprop,
coarse-resolution class activation methods such as Grad-CAM and Grad-CAM++ seem
to yield demonstrably superior results without sacrificing speed. This will
make fine-resolution saliency methods feasible on resource limited platforms
such as robots, cell phones, low-cost industrial devices, astronomy and
satellite imagery.
",describe explainable ai saliency map method use deep convolutional neural network cnn much efficient popular fine resolution gradient method also quantitatively similar well accuracy technique work measure information end network scale combine single saliency map describe saliency measure make efficient exploit saliency map order equivalence visualize individual contribution use layer order visualization information provide interesting comparison scale information contribution within network provide saliency map method use method instead guide backprop coarse resolution class activation method grad cam seem yield demonstrably superior result without sacrifice speed make fine resolution saliency method feasible resource limited platform robot cell phone low cost industrial device astronomy satellite imagery
"Bayesian parameter estimation using conditional variational autoencoders
  for gravitational-wave astronomy","  Gravitational wave (GW) detection is now commonplace and as the sensitivity
of the global network of GW detectors improves, we will observe
$\mathcal{O}(100)$s of transient GW events per year. The current methods used
to estimate their source parameters employ optimally sensitive but
computationally costly Bayesian inference approaches where typical analyses
have taken between 6 hours and 5 days. For binary neutron star and neutron star
black hole systems prompt counterpart electromagnetic (EM) signatures are
expected on timescales of 1 second -- 1 minute and the current fastest method
for alerting EM follow-up observers, can provide estimates in $\mathcal{O}(1)$
minute, on a limited range of key source parameters. Here we show that a
conditional variational autoencoder pre-trained on binary black hole signals
can return Bayesian posterior probability estimates. The training procedure
need only be performed once for a given prior parameter space and the resulting
trained machine can then generate samples describing the posterior distribution
$\sim 6$ orders of magnitude faster than existing techniques.
",gravitational wave gw detection commonplace sensitivity global network gw detector improves observe 100 transient gw event per year current method use estimate source parameter employ optimally sensitive computationally costly bayesian inference approach typical analysis take 6 hour 5 day binary neutron star neutron star black hole system prompt counterpart electromagnetic em signature expect timescale 1 second 1 minute current fast method alert em follow up observer provide estimate 1 minute limited range key source parameter show conditional variational autoencoder pre train binary black hole signal return bayesian posterior probability estimate training procedure need perform give prior parameter space result train machine generate sample describe posterior distribution 6 order magnitude fast exist technique
"Learning Sparse Representations in Reinforcement Learning with Sparse
  Coding","  A variety of representation learning approaches have been investigated for
reinforcement learning; much less attention, however, has been given to
investigating the utility of sparse coding. Outside of reinforcement learning,
sparse coding representations have been widely used, with non-convex objectives
that result in discriminative representations. In this work, we develop a
supervised sparse coding objective for policy evaluation. Despite the
non-convexity of this objective, we prove that all local minima are global
minima, making the approach amenable to simple optimization strategies. We
empirically show that it is key to use a supervised objective, rather than the
more straightforward unsupervised sparse coding approach. We compare the
learned representations to a canonical fixed sparse representation, called
tile-coding, demonstrating that the sparse coding representation outperforms a
wide variety of tilecoding representations.
",variety representation learning approach investigate reinforcement learn much less attention however give investigate utility sparse code outside reinforcement learning sparse code representation widely use non convex objective result discriminative representation work develop supervise sparse code objective policy evaluation despite non convexity objective prove local minima global minima make approach amenable simple optimization strategy empirically show key use supervise objective rather straightforward unsupervised sparse code approach compare learn representation canonical fix sparse representation call tile code demonstrating sparse code representation outperform wide variety tilecoding representation
Infinite Arms Bandit: Optimality via Confidence Bounds,"  Berry et al. (1997) initiated the development of the infinite arms bandit
problem. They derived a regret lower bound of all allocation strategies for
Bernoulli rewards with uniform priors, and proposed strategies based on success
runs. Bonald and Prouti\`{e}re (2013) proposed a two-target algorithm that
achieves the regret lower bound, and extended optimality to Bernoulli rewards
with general priors. We present here a confidence bound target (CBT) algorithm
that achieves optimality for rewards that are bounded above. For each arm we
construct a confidence bound and compare it against each other and a target
value to determine if the arm should be sampled further. The target value
depends on the assumed priors of the arm means. In the absence of information
on the prior, the target value is determined empirically. Numerical studies
here show that CBT is versatile and outperforms its competitors.
",berry et al 1997 initiate development infinite arm bandit problem derive regret lower bind allocation strategy bernoulli reward uniform prior propose strategy base success run bonald e 2013 propose two target algorithm achieve regret lower bind extend optimality bernoulli reward general prior present confidence bind target cbt algorithm achieve optimality reward bound arm construct confidence bind compare target value determine arm sample target value depend assume prior arm mean absence information prior target value determine empirically numerical study show cbt versatile outperform competitor
Neural Networks with Small Weights and Depth-Separation Barriers,"  In studying the expressiveness of neural networks, an important question is
whether there are functions which can only be approximated by sufficiently deep
networks, assuming their size is bounded. However, for constant depths,
existing results are limited to depths $2$ and $3$, and achieving results for
higher depths has been an important open question. In this paper, we focus on
feedforward ReLU networks, and prove fundamental barriers to proving such
results beyond depth $4$, by reduction to open problems and natural-proof
barriers in circuit complexity. To show this, we study a seemingly unrelated
problem of independent interest: Namely, whether there are polynomially-bounded
functions which require super-polynomial weights in order to approximate with
constant-depth neural networks. We provide a negative and constructive answer
to that question, by showing that if a function can be approximated by a
polynomially-sized, constant depth $k$ network with arbitrarily large weights,
it can also be approximated by a polynomially-sized, depth $3k+3$ network,
whose weights are polynomially bounded.
",study expressiveness neural network important question whether function approximate sufficiently deep network assume size bound however constant depth exist result limit depth 2 3 achieve result high depth important open question paper focus feedforward relu network prove fundamental barrier prove result beyond depth 4 reduction open problem natural proof barrier circuit complexity show study seemingly unrelated problem independent interest namely whether polynomially bound function require super polynomial weight order approximate constant depth neural network provide negative constructive answer question showing function approximate polynomially sized constant depth k network arbitrarily large weight also approximate polynomially sized depth network whose weight polynomially bound
Adversarial Patterns: Building Robust Android Malware Classifiers,"  Deep learning-based classifiers have substantially improved recognition of
malware samples. However, these classifiers can be vulnerable to adversarial
input perturbations. Any vulnerability in malware classifiers poses significant
threats to the platforms they defend. Therefore, to create stronger defense
models against malware, we must understand the patterns in input perturbations
caused by an adversary. This survey paper presents a comprehensive study on
adversarial machine learning for android malware classifiers. We first present
an extensive background in building a machine learning classifier for android
malware, covering both image-based and text-based feature extraction
approaches. Then, we examine the pattern and advancements in the
state-of-the-art research in evasion attacks and defenses. Finally, we present
guidelines for designing robust malware classifiers and enlist research
directions for the future.
",deep learning base classifier substantially improve recognition malware sample however classifier vulnerable adversarial input perturbation vulnerability malware classifier pose significant threat platform defend therefore create strong defense model malware must understand pattern input perturbation cause adversary survey paper present comprehensive study adversarial machine learn android malware classifier first present extensive background building machine learn classifier android malware cover image base text base feature extraction approach examine pattern advancement state of the art research evasion attack defense finally present guideline design robust malware classifier enlist research direction future
"ALICE: Towards Understanding Adversarial Learning for Joint Distribution
  Matching","  We investigate the non-identifiability issues associated with bidirectional
adversarial training for joint distribution matching. Within a framework of
conditional entropy, we propose both adversarial and non-adversarial approaches
to learn desirable matched joint distributions for unsupervised and supervised
tasks. We unify a broad family of adversarial models as joint distribution
matching problems. Our approach stabilizes learning of unsupervised
bidirectional adversarial learning methods. Further, we introduce an extension
for semi-supervised learning tasks. Theoretical results are validated in
synthetic data and real-world applications.
",investigate non identifiability issue associate bidirectional adversarial training joint distribution matching within framework conditional entropy propose adversarial non adversarial approach learn desirable match joint distribution unsupervise supervise task unify broad family adversarial model joint distribution matching problem approach stabilize learn unsupervised bidirectional adversarial learning method introduce extension semi supervised learning task theoretical result validate synthetic datum real world application
Proceedings of the First Workshop on Weakly Supervised Learning (WeaSuL),"  Welcome to WeaSuL 2021, the First Workshop on Weakly Supervised Learning,
co-located with ICLR 2021. In this workshop, we want to advance theory, methods
and tools for allowing experts to express prior coded knowledge for automatic
data annotations that can be used to train arbitrary deep neural networks for
prediction. The ICLR 2021 Workshop on Weak Supervision aims at advancing
methods that help modern machine-learning methods to generalize from knowledge
provided by experts, in interaction with observable (unlabeled) data. In total,
15 papers were accepted. All the accepted contributions are listed in these
Proceedings.
",welcome weasul 2021 first workshop weakly supervise learning co located iclr workshop want advance theory method tool allow expert express prior code knowledge automatic datum annotation use train arbitrary deep neural network prediction iclr 2021 workshop weak supervision aim advance method help modern machine learn method generalize knowledge provide expert interaction observable unlabeled datum total 15 paper accept accept contribution list proceeding
"Empirical evaluation of shallow and deep learning classifiers for Arabic
  sentiment analysis","  This work presents a detailed comparison of the performance of deep learning
models such as convolutional neural networks (CNN), long short-term memory
(LSTM), gated recurrent units (GRU), their hybrids, and a selection of shallow
learning classifiers for sentiment analysis of Arabic reviews. Additionally,
the comparison includes state-of-the-art models such as the transformer
architecture and the araBERT pre-trained model. The datasets used in this study
are multi-dialect Arabic hotel and book review datasets, which are some of the
largest publicly available datasets for Arabic reviews. Results showed deep
learning outperforming shallow learning for binary and multi-label
classification, in contrast with the results of similar work reported in the
literature. This discrepancy in outcome was caused by dataset size as we found
it to be proportional to the performance of deep learning models. The
performance of deep and shallow learning techniques was analyzed in terms of
accuracy and F1 score. The best performing shallow learning technique was
Random Forest followed by Decision Tree, and AdaBoost. The deep learning models
performed similarly using a default embedding layer, while the transformer
model performed best when augmented with araBERT.
",work present detailed comparison performance deep learning model convolutional neural network cnn long short term memory lstm gate recurrent unit gru hybrid selection shallow learning classifier sentiment analysis arabic review additionally comparison include state of the art model transformer architecture arabert pre train model dataset use study multi dialect arabic hotel book review dataset large publicly available dataset arabic review result show deep learning outperform shallow learn binary multi label classification contrast result similar work report literature discrepancy outcome cause dataset size find proportional performance deep learning model performance deep shallow learning technique analyze term accuracy f1 score well perform shallow learning technique random forest follow decision tree adaboost deep learning model perform similarly use default embed layer transformer model perform well augment arabert
Safe and Efficient Model-free Adaptive Control via Bayesian Optimization,"  Adaptive control approaches yield high-performance controllers when a precise
system model or suitable parametrizations of the controller are available.
Existing data-driven approaches for adaptive control mostly augment standard
model-based methods with additional information about uncertainties in the
dynamics or about disturbances. In this work, we propose a purely data-driven,
model-free approach for adaptive control. Tuning low-level controllers based
solely on system data raises concerns on the underlying algorithm safety and
computational performance. Thus, our approach builds on GoOSE, an algorithm for
safe and sample-efficient Bayesian optimization. We introduce several
computational and algorithmic modifications in GoOSE that enable its practical
use on a rotational motion system. We numerically demonstrate for several types
of disturbances that our approach is sample efficient, outperforms constrained
Bayesian optimization in terms of safety, and achieves the performance optima
computed by grid evaluation. We further demonstrate the proposed adaptive
control approach experimentally on a rotational motion system.
",adaptive control approach yield high performance controller precise system model suitable parametrization controller available exist data drive approach adaptive control mostly augment standard model base method additional information uncertainty dynamic disturbance work propose purely data drive model free approach adaptive control tune low level controller base solely system datum raise concern underlie algorithm safety computational performance thus approach build goose algorithm safe sample efficient bayesian optimization introduce several computational algorithmic modification goose enable practical use rotational motion system numerically demonstrate several type disturbance approach sample efficient outperform constrain bayesian optimization term safety achieve performance optima compute grid evaluation demonstrate propose adaptive control approach experimentally rotational motion system
"Evaluating Community Detection Algorithms for Progressively Evolving
  Graphs","  Many algorithms have been proposed in the last ten years for the discovery of
dynamic communities. However, these methods are seldom compared between
themselves. In this article, we propose a generator of dynamic graphs with
planted evolving community structure, as a benchmark to compare and evaluate
such algorithms. Unlike previously proposed benchmarks, it is able to specify
any desired evolving community structure through a descriptive language, and
then to generate the corresponding progressively evolving network. We
empirically evaluate six existing algorithms for dynamic community detection in
terms of instantaneous and longitudinal similarity with the planted ground
truth, smoothness of dynamic partitions, and scalability. We notably observe
different types of weaknesses depending on their approach to ensure smoothness,
namely Glitches, Oversimplification and Identity loss. Although no method
arises as a clear winner, we observe clear differences between methods, and we
identified the fastest, those yielding the most smoothed or the most accurate
solutions at each step.
",many algorithm propose last ten year discovery dynamic community however method seldom compare article propose generator dynamic graph plant evolve community structure benchmark compare evaluate algorithm unlike previously propose benchmark able specify desire evolve community structure descriptive language generate correspond progressively evolve network empirically evaluate six exist algorithm dynamic community detection term instantaneous longitudinal similarity plant ground truth smoothness dynamic partition scalability notably observe different type weakness depend approach ensure smoothness namely glitche oversimplification identity loss although method arise clear winner observe clear difference method identify fast yield smoothed accurate solution step
On Almost Sure Convergence Rates of Stochastic Gradient Methods,"  The vast majority of convergence rates analysis for stochastic gradient
methods in the literature focus on convergence in expectation, whereas
trajectory-wise almost sure convergence is clearly important to ensure that any
instantiation of the stochastic algorithms would converge with probability one.
Here we provide a unified almost sure convergence rates analysis for stochastic
gradient descent (SGD), stochastic heavy-ball (SHB), and stochastic Nesterov's
accelerated gradient (SNAG) methods. We show, for the first time, that the
almost sure convergence rates obtained for these stochastic gradient methods on
strongly convex functions, are arbitrarily close to their optimal convergence
rates possible. For non-convex objective functions, we not only show that a
weighted average of the squared gradient norms converges to zero almost surely,
but also the last iterates of the algorithms. We further provide last-iterate
almost sure convergence rates analysis for stochastic gradient methods on
weakly convex smooth functions, in contrast with most existing results in the
literature that only provide convergence in expectation for a weighted average
of the iterates.
",vast majority convergence rate analysis stochastic gradient method literature focus convergence expectation whereas trajectory wise almost sure convergence clearly important ensure instantiation stochastic algorithm would converge probability one provide unified almost sure convergence rate analysis stochastic gradient descent sgd stochastic heavy ball shb stochastic nesterov accelerate gradient snag method show first time almost sure convergence rate obtain stochastic gradient method strongly convex function arbitrarily close optimal convergence rate possible non convex objective function show weight average square gradient norm converge zero almost surely also last iterate algorithm provide last iterate almost sure convergence rate analysis stochastic gradient method weakly convex smooth function contrast exist result literature provide convergence expectation weight average iterate
Truly shift-invariant convolutional neural networks,"  Thanks to the use of convolution and pooling layers, convolutional neural
networks were for a long time thought to be shift-invariant. However, recent
works have shown that the output of a CNN can change significantly with small
shifts in input: a problem caused by the presence of downsampling (stride)
layers. The existing solutions rely either on data augmentation or on
anti-aliasing, both of which have limitations and neither of which enables
perfect shift invariance. Additionally, the gains obtained from these methods
do not extend to image patterns not seen during training. To address these
challenges, we propose adaptive polyphase sampling (APS), a simple sub-sampling
scheme that allows convolutional neural networks to achieve 100% consistency in
classification performance under shifts, without any loss in accuracy. With
APS, the networks exhibit perfect consistency to shifts even before training,
making it the first approach that makes convolutional neural networks truly
shift-invariant.
",thank use convolution pool layer convolutional neural network long time think shift invariant however recent work show output cnn change significantly small shift input problem cause presence downsample stride layer exist solution rely either datum augmentation anti aliasing limitation neither enable perfect shift invariance additionally gain obtain method extend image pattern see training address challenge propose adaptive polyphase sample aps simple sub sampling scheme allow convolutional neural network achieve 100 consistency classification performance shift without loss accuracy aps network exhibit perfect consistency shift even training make first approach make convolutional neural network truly shift invariant
